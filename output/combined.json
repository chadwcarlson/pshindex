[
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "0b4685a7cf2ff912a6f6a120d9376dde",
        "title": "The big picture",
        "description": "",
        "text": "If you're new to Platform.sh, we recommend starting with Structure and Build \u0026 Deploy, which will get you started on the right track to best leverage Platform.sh.",
        "section": "The big picture",
        "subsections": "",
        "image": "",
        "url": "/overview.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "8142dabde8d4ef556ed9da8dbe0a37fe",
        "title": "Configuration",
        "description": "",
        "text": "Platform.sh projects are configured with three YAML files that define three types of containers in your virtual cluster: one *Router* container, one or more *Application* containers, and a number of optional *Service* containers. See how each of their configuration files are defined below.",
        "section": "Configuration",
        "subsections": "",
        "image": "",
        "url": "/configuration.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "0b1bf164ed9f4c760138f799c33a698e",
        "title": "Introduction",
        "description": "",
        "text": "The easiest way to get started working with Platform.sh is to try it out yourself. Open up a free trial account and explore the Getting Started guides below.",
        "section": "Introduction",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "5c7956314319fad802a71ee12cd46297",
        "title": "Best practices",
        "description": "",
        "text": "Here are some tips for getting the most out of Platform.sh's many features.",
        "section": "Best practices",
        "subsections": "",
        "image": "",
        "url": "/bestpractices.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "e5e8263f309f333d2956c6ac2d8e4ffd",
        "title": "Languages",
        "description": "",
        "text": "We sure do support a lot of runtimes.",
        "section": "Languages",
        "subsections": "",
        "image": "",
        "url": "/languages.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "9e07170edfe4469e380f9c5b8dba1a3e",
        "title": "Development",
        "description": "",
        "text": "This section contains resources related to tools and troubleshooting information related to developing with Platform.sh",
        "section": "Development",
        "subsections": "",
        "image": "",
        "url": "/development.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "049242b18eba78b1288a433742ae15b0",
        "title": "Integrations",
        "description": "",
        "text": "Integrate all the things!",
        "section": "Integrations",
        "subsections": "",
        "image": "",
        "url": "/integrations.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "c2a5a7a1505e690fbeddcea3b0342060",
        "title": "Going live",
        "description": "",
        "text": "The following resources will help you take your application live.",
        "section": "Going live",
        "subsections": "",
        "image": "",
        "url": "/golive.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "23f5e0e3b5aca70a7c3df162dfab6bc3",
        "title": "Administration",
        "description": "",
        "text": "Administration tasks for your Platform.sh projects is accessible from within the management console, as well as through the CLI.",
        "section": "Administration",
        "subsections": "",
        "image": "",
        "url": "/administration.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "63c867b8cb3d97c8f9ef5512f3ecc922",
        "title": "Security and compliance",
        "description": "",
        "text": "Platform.sh takes your privacy seriously. We’re compliant with the European GDPR (DPA available), German BDSG (DPA available), and Canadian PIPEDA.",
        "section": "Security and compliance",
        "subsections": "",
        "image": "",
        "url": "/security.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "8419fb6e269c2c7345a043f81b45a77a",
        "title": "Featured frameworks",
        "description": "",
        "text": "The following section covers specific web frameworks, as well as best practices and important considerations for deploying them on Platform.sh.",
        "section": "Featured frameworks",
        "subsections": "",
        "image": "",
        "url": "/frameworks.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "e39624dd3a8bc4b55caba216c2aa0231",
        "title": "Tutorials",
        "description": "",
        "text": "Here are a set of tutorials for migrating between regions, exporting data, and more.",
        "section": "Tutorials",
        "subsections": "",
        "image": "",
        "url": "/tutorials.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "2ddb9cd69efd6dc26336a1c22abd79ae",
        "title": "Dedicated",
        "description": "",
        "text": "Platform.sh Dedicated is a robust, redundant layer on top of Platform.sh Professional. This section contains all resources concerning the Dedicated product previously found at `ent.docs.platform.sh`.",
        "section": "Dedicated",
        "subsections": "",
        "image": "",
        "url": "/dedicated.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "f20e55772096606cc8f9fe290a1d7d90",
        "title": "Activity reference",
        "description": "",
        "text": " This hook allows you to capture any push events on platform and POST a JSON message describing the activity to the url of your choice. You can use this to further automate your Platform.sh workflow. Activity schema id A unique opaque value to identify the activity. project The Project ID for which the activity was triggered. Use this value if you want to have multiple projects POST to the same URL. type The type property specifies the event that happened. Its value is one of: project.modify.title: The human-friendly title of the project has been changed. project.create: A project has been created. Although it will appear in the activity feed exactly once, it will not be sent via a webhook as it will always happen before a webhook can be configured. project.domain.create: A new domain has been added to the project. project.domain.delete: A domain associated with the project has been removed. project.domain.update: A domain associated with the project has been updated, including modifying it’s SSL certificate. environment.access.add: A new user has been given access to the environment. environment.access.remove: A user has been removed from the environment. environment.backup: A user triggered a backup . environment.restore: A user restored a backup . environment.backup.delete: A user deleted a backup environment.push: A user has pushed code to a branch, either existing or new. environment.branch: A new branch has been created via the management console. (A branch created via a push will show up only as an environment.push.) environment.activate: A branch has been “activated”, and an environment created for it. environment.initialize: The master branch of the project has just been initialized with its first commit. environment.deactivate: A branch has been “deactivated”. The code is still there, but the environment was destroyed. environment.synchronize: An environment has had its data and/or code re-copied from its parent environment. environment.merge: A branch was merged through the management console or Platform.sh API. A basic Git merge will not trigger this event. environment.redeploy: An environment was redeployed. environment.delete: A branch was deleted. environment.route.create: A new route has been created through the management console. This will not fire for route edits made to the routes.yaml file directly. environment.route.delete: A route has been deleted through the management console. This will not fire for route edits made to the routes.yaml file directly. environment.route.update: A route has been modified through the management console. This will not fire for route edits made to the routes.yaml file directly. environment.variable.create: A new variable has been created. environment.variable.delete: A variable has been deleted. environment.variable.update: A variable has been modified. environment.update.http_access: HTTP access rules for an environment have been modified. environment.update.smtp: Sending of emails has been enabled/disabled for an environment. environment.update.restrict_robots: The block-all-robots feature has been enabled/disabled. environment.subscription.update: The master environment has been resized because the subscription has changed. There are no content changes. environment.cron: A cron task just completed. environment.source-operation: A source operation triggered and has completed. integration.bitbucket.fetch: Changes in BitBucket repository have been pulled. integration.bitbucket.register_hooks: Integration hook have been registered on BitBucket. integration.bitbucket_server.fetch: Changes in BitBucket repository have been pulled. integration.bitbucket_server.register_hooks: Integration hook have been registered on BitBucket. integration.github.fetch: Changes in GitHub repository have been pulled. integration.gitlab.fetch: Changes in GitLab repository have been pulled. integration.health.email: Health event sent by email. integration.health.pagerduty: Health event sent to pagerduty. integration.health.slack: Health event sent to slack. integration.webhook: Webhook triggered. integration.hipchat: Event sent to hipchat. integration.script: An activity script has run. environments An array listing the environments that were involved in the activity. This is usually single-value. result Whether the activity was completed successfully or not. It should be success if all went as planned. created_at, started_at, completed_at These values are all timestamps in UTC. If you need only a point in time when the action happened, use completed_at. You can also combine it with started_at to see how long the activity took. log A text description of the action that happened. This is a human-friendly string that may be displayed to a user but should not be parsed for data as its structure is not guaranteed. payload.environment This block contains information about the environment itself, after the action has taken place. The most notable properties of this key are name (the name of the branch) machine_name (the name of the environment) head_commit (the Git commit ID that triggered the event) payload.user The Platform.sh user that triggered the activity. deployment This large block details all information about all services in the environment. That includes the resulting configuration objects derived from routes.yaml , services.yaml , and .platform.app.yaml . Most notably, the deployment.routes object’s keys are all of the URLs made available by the environment. Note that some will be redirects. To find those that are live URLs filter to those objects whose type property is upstream. Example activity The following is an example of a webhook message. Specifically, this one was created by a “push” event. {  id :  774-this-is-an-example-valuexzs4no ,  _links : {  self : {  href :  https://eu.platform.sh/api/projects/sx-this-is-an-example-value-hu/activities/774-this-is-an-example-valuexzs4no ,  meta : {  get : {  responses : {  default : {  schema : {  properties : {  created_at : {  type :  string ,  format :  date-time  },  updated_at : {  type :  string ,  format :  date-time  },  type : {  type :  string  },  parameters : {  properties : { },  required : [ ] },  project : {  type :  string  },  environments : {  items : {  type :  string  },  type :  array  },  state : {  type :  string  },  result : {  type :  string  },  started_at : {  type :  string ,  format :  date-time  },  completed_at : {  type :  string ,  format :  date-time  },  completion_percent : {  type :  integer  },  log : {  type :  string  },  payload : {  properties : { },  required : [ ] } },  required : [  created_at ,  updated_at ,  type ,  parameters ,  project ,  environments ,  state ,  result ,  started_at ,  completed_at ,  completion_percent ,  log ,  payload  ] } } },  parameters : [ ] } } } },  created_at :  2017-12-07T10:45:30.870660\u0026#43;00:00 ,  updated_at :  2017-12-07T10:47:39.369761\u0026#43;00:00 ,  type :  environment.push ,  parameters : {  environment :  master ,  old_commit :  34be31cbabcc0f65d7fd8ec29769947396d0cabd ,  new_commit :  8d2e6003d50136c750fb8b65ec506ee3aa4a5b15 ,  user :  384491da-031e-4c23-b264-9f96040a6e36  },  project :  sx-this-is-an-example-value-hu ,  environments : [  master  ],  state :  complete ,  result :  success ,  started_at :  2017-12-07T10:45:30.996898\u0026#43;00:00 ,  completed_at :  2017-12-07T10:47:39.369741\u0026#43;00:00 ,  completion_percent : 100,  log :  Found 1 new application \u0026#39;myrubyapp\u0026#39; (runtime type: ruby:2.4-rc, tree: Generating runtime Executing build Fetching gem metadata from Resolving Installing concurrent-ruby Installing i18n Installing minitest Installing thread_safe ...more Installing unicorn Using bundler Your bundle is Use `bundle show [gemname]` to see where a bundled gem is Executing pre-flight Compressing Beaming package to its final Reusing existing environment Environment myrubyapp (type: ruby:2.4-rc, size: S, disk: postgresql (type: postgresql:9.3, size: S, disk: mongodb (type: mongodb:3.0, size: S, disk: redis (type: redis:3.2-rc, size: influxdb (type: influxdb:1.2, size: S, disk: elasticsearch (type: elasticsearch, size: S, disk: rabbitmq (type: rabbitmq:3.5, size: S, disk: mysql (type: mysql:10.0, size: S, disk: solr (type: solr:4.10, size: S, disk: Environment http://master-7rqtwti-sx-this-is-an-example-value-hu.eu.platform.sh/ redirects to https://master-7rqtwti-sx-this-is-an-example-value-hu.eu.platform.sh/ is served by application  payload : {  environment : {  status :  active ,  head_commit :  8d2e6003d50136c750fb8b65ec506ee3aa4a5b15 ,  machine_name :  master-7rqtwti ,  name :  master ,  parent : null,  title :  Master ,  created_at :  2017-11-22T14:43:17.154870\u0026#43;00:00 ,  updated_at :  2017-11-22T14:43:17.155103\u0026#43;00:00 ,  clone_parent_on_create : true,  project :  sx-this-is-an-example-value-hu ,  is_dirty : false,  restrict_robots : true,  has_code : true,  enable_smtp : true,  id :  master ,  deployment_target :  local ,  http_access : {  is_enabled : true,  addresses : [ ],  basic_auth : { } },  is_main : true },  commits : [ {  sha :  8d2e6003d50136c750fb8b65ec506ee3aa4a5b15 ,  message :  deploy with release candidates ,  parents : [  34be31cbabcc0f65d7fd8ec29769947396d0cabd  ],  author : {  email :  ori@example.com ,  name :  Ori Pekelman  } } ],  commits_count : 1,  user : {  created_at :  2017-12-07T10:45:13.491553\u0026#43;00:00 ,  display_name :  Ori Pekelman ,  id :  384491da-031e-4c23-b264-9f96040a6e36 ,  updated_at : null },  deployment : {  id :  current ,  services : {  mongodb : {  type :  mongodb:3.0 ,  size :  AUTO ,  disk : 500,  access : { },  configuration : { },  relationships : { } },  redis : {  type :  redis:3.2-rc ,  size :  AUTO ,  disk : null,  access : { },  configuration : { },  relationships : { } },  solr : {  type :  solr:4.10 ,  size :  AUTO ,  disk : 256,  access : { },  configuration : { },  relationships : { } } },  routes : {  https://master-7rqtwti-sx-this-is-an-example-value-hu.eu.platform.sh/ : {  type :  upstream ,  redirects : {  expires :  -1s ,  paths : { } },  original_url :  https://{default}/ ,  cache : {  enabled : true,  default_ttl : 0,  cookies : [  *  ],  headers : [  Accept ,  Accept-Language  ] },  ssi : {  enabled : false },  upstream :  myrubyapp  },  http://master-7rqtwti-sx-this-is-an-example-value-hu.eu.platform.sh/ : {  type :  redirect ,  redirects : {  expires :  -1s ,  paths : { } },  original_url :  http://{default}/ ,  to :  https://master-7rqtwti-sx-this-is-an-example-value-hu.eu.platform.sh/  } },  webapps : {  myrubyapp : {  size :  AUTO ,  disk : 2048,  access : {  ssh :  contributor  },  relationships : {  mongodb :  mongodb:mongodb ,  redis :  redis:redis ,  solr :  solr:solr  },  mounts : {  /public :  shared:files/files  },  timezone : null,  variables : { },  name :  myrubyapp ,  type :  ruby:2.4-rc ,  runtime : { },  preflight : {  enabled : true,  ignored_rules : [ ] },  web : {  locations : {  / : {  root :  public ,  expires :  1h ,  passthru : true,  scripts : true,  allow : true,  headers : { },  rules : { } } },  commands : {  start :  unicorn -l $SOCKET -E production config.ru ,  stop : null },  upstream : {  socket_family :  unix ,  protocol : null },  move_to_root : false },  hooks : {  build :  ruby -e \u0026#39;bundle  deploy : null } } },  workers : { } } } }",
        "section": "Activity scripts",
        "subsections": " Activity schema  id project type environments result created_at, started_at, completed_at log payload.environment payload.user deployment   Example activity  ",
        "image": "",
        "url": "/integrations/activity/reference.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "c838abdab87aa226228abffc45dd608b",
        "title": "External Integrations",
        "description": "",
        "text": " Platform.sh can be integrated with external services. Platform.sh supports native integrations with multiple services, first and foremost Git hosting services such as GitHub, GitLab, or Bitbucket. You can continue to use those tools for your development workflow, and have Platform.sh environments created automatically for your pull requests and branches. You can also add our native integrations with performance monitoring tools such as Blackfire, New Relic, or Tideways, as well as setting up health notifications. Or create your own integration using our webhooks. /integrations/overview.html#validating-integrations Be aware that only a project administrator (someone with admin level access to the project) can add or remove integrations. See User administration for more details. Listing active integrations With the CLI, you can list all your active integrations: platform integrations Info: If you have created your account using the Bitbucket or GitHub oAuth Login, then in order to use the Platform.sh CLI you will need to set up a password by visiting https://accounts.platform.sh/user/password. Validating integrations Once your integration has been configured, you can validate that it is functioning properly with the command: $ platform integration:validate Enter a number to choose an integration: [0] 5aut2djgt6kdd (health.slack) [1] a6535j9qp4sl8 (github) \u0026gt; 1 Validating the integration a6535j9qp4sl8 (type: github)... The integration is valid. Debugging integrations When integrations run, they trigger “activities.” Activities are actions that happen on Platform.sh, and they get logged. Those logs are available via the CLI. In most cases they are not necessary but may be useful for debugging an integration if it is misbehaving for some reason. There are a handful of commands available, all under the integrations section. List all activities The commands platform integration:activity:list or its alias platform integration:activities will list all activities on a given project and integration. For example, for the project for this site, the command platform integration:activity:list outputs: $ platform integration:activities Enter a number to choose an integration: [0] dxr45hfldrkoe (webhook) [1] n2ukd4p7qofg4 (health.email) [2] c4opi5tjv3yfd (github) \u0026gt; 2 Activities on the project Platform.sh | Docs (6b2eocegfkwwg), integration c4opi5tjv3yfd (github): \u0026#43;---------------\u0026#43;---------------------------\u0026#43;-------------------------------------------------------------\u0026#43;----------\u0026#43;---------\u0026#43; | ID | Created | Description | State | Result | \u0026#43;---------------\u0026#43;---------------------------\u0026#43;-------------------------------------------------------------\u0026#43;----------\u0026#43;---------\u0026#43; | 6456zmdtoykxa | 2020-04-14T16:38:09-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success | | wcwp34yjvydgk | 2020-04-14T16:35:22-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success | | w2bp3oa5xbfoe | 2020-04-14T16:33:13-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success | | uqqvdyxmcdmsa | 2020-04-14T16:31:45-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success | | 7x3wefhh4fwqc | 2020-04-14T16:30:36-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success | | a46aah3ga65gc | 2020-04-14T16:29:46-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success | | r7erid2jlixgi | 2020-04-14T16:24:50-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success | | ieufk3vvde5oc | 2020-04-14T16:24:49-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success | | bc7ghg36ty4ea | 2020-04-14T15:30:17-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success | | 4qojtv7a6rk2w | 2020-04-14T15:27:26-05:00 | Fetching from https://github.com/platformsh/platformsh-docs | complete | success | \u0026#43;---------------\u0026#43;---------------------------\u0026#43;-------------------------------------------------------------\u0026#43;----------\u0026#43;---------\u0026#43; You may also specify which integration to display in the command line directly: platform integration:activities c4opi5tjv3yfd. The ID is an internal identifier for the activity event. The Description field is an arbitrary string of text produced by the integration code. The State and Result fields indicate if the activity completed successfully, failed for some reason, or is currently in progress. See the --help output of the command for more options. Showing detailed information on an activity You can call up more detailed information on a specific activity by its ID, using the platform integration:activity:log command. It requires both the integration ID and an activity ID from the list above. It also works best with the -t option to include timestamps. $ platform integration:activity:log c4opi5tjv3yfd 6456zmdtoykxa -t Integration ID: ceopz5tgj3yfc Activity ID: 6456zmdtoykxa Type: integration.github.fetch Description: Fetching from https://github.com/platformsh/platformsh-docs Created: 2020-04-15T08:44:07-05:00 State: complete Log: [2020-04-15T13:44:17-05:00] Waiting for other activities to complete [2020-04-15T13:46:07-05:00] Fetching from GitHub repository platformsh/platformsh-docs [2020-04-15T13:46:09-05:00] No changes since last fetch [2020-04-15T13:46:09-05:00] [2020-04-15T13:46:09-05:00] Synchronizing branches [2020-04-15T13:46:09-05:00] [2020-04-15T13:46:09-05:00] Synchronizing pull requests [2020-04-15T13:46:59-05:00] [2020-04-15T13:46:59-05:00] W: No changes found, scheduling a retry.. That will show the full output of the activity, including timestamps. That can be especially helpful if trying to determine why an integration is not behaving as expected. See the --help output of the command for more options. If you omit the activity ID (the second random-seeming string), the command will default to the most recent activity recorded.",
        "section": "Integrations",
        "subsections": " Listing active integrations Validating integrations Debugging integrations  List all activities Showing detailed information on an activity    ",
        "image": "",
        "url": "/integrations/overview.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "f7974f8cb6da8fa46946d9a0ac2d552d",
        "title": "Activity scripts",
        "description": "",
        "text": " Platform.sh supports custom scripts that can fire in response to any activity. These scripts allow you to take arbitrary actions in response to actions in your project, such as when it deploys, when a new branch is created, etc. Check out examples from other users on our Community site. A legacy integration is also available for HipChat . Activity scripts Activity scripts are written in a scope-limited version of Javascript ES5 . That means they do not support newer ES6 and later features such as classes, nor do they support installing additional packages. A series of utility functions you can reuse are also available . Installing Activity scripts are configured as integrations. That means they are at the project level, not at the level of an individual environment. While you can store the scripts in your Git repository for easy access, they will have no effect there. To install a new activity script, use the Platform.sh CLI . platform integration:add --type script --file ./my_script.js That will install and enable the my_script.js file as an activity script on the current project. You can get its ID by listing the integrations on the current project: platform integrations +---------------+--------------+--------------+ | ID | Type | Summary | +---------------+--------------+--------------+ | nadbowmhd67do | script | ... | | rcqf6b69jdcx6 | health.email | From: | | | | To: #admins | +---------------+--------------+--------------+ The just-installed script’s ID in this example is nadbowmhd67do. Do not run the integration:add command a second time, or it will install a second integration that happens to have the same code. Updating To update an existing activity script, use the integration:update command. You will need the ID Of the integration to update (as above). platform integration:update --file ./my_script.js nadbowmhd67do That will update the integration in place, permanently overwriting the previous version. Removing To disable an activity script, use the integration:delete command: platform integration:delete nadbowmhd67do Debugging Activity logs are available through their own CLI command, platform integration:activities. Every time your activity script runs it will generate a new log entry, including the output from the script. Any output produced by console.log will be available in the activity log, and that is the recommended way to debug scripts. See the activity log documentation for further details. To get more readable output of a variable you’re trying to debug, you can make JSON.stringify use human-friendly formatting. console.log(JSON.stringify(project, null, 2)); Configuring scripts There are many types of activity to which a script could respond. By default, it will activate only after a successful git push operation. That trigger is configurable via command line switches when adding or updating a script. For example, to have a script trigger any time an environment is activated or deactivated, you would run: platform integration:update --events='environment.activate, environment.deactivate' nadbowmhd67do A complete list of possible events is available in the webhook documentation . Scripts can also trigger only when an action reaches a given state, such as “pending”, “in_progress”, or “complete”. The default is only when they reach “complete”. To have a script execute when a synchronize action first starts, for example, you would run: platform integration:update --events=environment.synchronize --states=in_progress nadbowmhd67do It is also possible to restrict scripts to certain environments by name. Most commonly that is used to have them execute only for the master environment, or for all environments except master. The following example executes only for backup actions on the master environment: platform integration:update --events=environment.backup --environments=master nadbowmhd67do There is also an --exclude-environments switch to excluded environments by name rather than allow. As a general rule, it is better to have an activity script only execute on the specific events and branches you’re interested in rather than firing on all activities and then filtering out undesired use cases in the script itself. Available APIs Activity scripts have a series of APIs available to them to facilitate building out custom functionality. underscore.js Underscore.js 1.9.2 is available out-of-the-box to make writing Activity scripts more pleasant. See Underscore’s documentation for available functions and utilities. activity Every activity script has a global variable activity that contains detailed information about the activity, including embedded, JSON-ified versions of the routes configuration and relevant .platform.app.yaml files. The activity variable is the same as the webhook payload . See the documentation there for details and a complete example. Several of the utility functions below work by pulling out common portions of the activity object. Most notably, scripts can be configured via Project-level variables that can be accessed from the activity object. project The project global variable includes information about the project subscription itself. That includes its ID and name, how many users are associated with the project, it’s SSH public key, and various other values. An example of this object is below: {  attributes : {},  created_at :  2020-04-15T19:50:09.514267+00:00 ,  default_domain : null,  description :   ,  id :  kpyhl5f8nuzef ,  owner :  ... ,  region :  eu-3.platform.sh ,  repository : {  client_ssh_key :  ssh-rsa ... ,  url :  kqyhl5f5nuzky@git.eu-3.platform.sh:kqyhl5f5nuzky.git  },  status : {  code :  provisioned ,  message :  ok  },  subscription : {  environments : 3,  included_users : 1,  license_uri :  ... ,  plan :  development ,  restricted : false,  storage : 5120,  subscription_management_uri :  ... ,  suspended : false,  user_licenses : 1 },  timezone :  Europe/Dublin ,  title :  Activity script examples ,  updated_at :  2020-04-21T17:15:35.526498+00:00  } Storage API Activity scripts have access to a limited key/value storage API to persist values from one execution to another. The API is similar to the Javscript LocalStorage API. // Access the storage API. It is not pre-required. var storage = require( storage ); // Retrieve a stored value. If the value is not set it will return null. var counter = storage.get('counter') || 0; if (counter) { // Generate debug output. console.log( Counter is:   + counter); } // Write a value into the storage. Only string-safe values are supported. // To save an object or array, run JSON.stringify() on it first. storage.set('counter', counter + 1); // Remove a value completely. storage.remove('counter'); // Remove all values in storage, unconditionally. storage.clear(); Fetch API Activity scripts support a modified version of the browser “Fetch API” for issuing HTTP requests. Unlike the typical browser version, however, they only support synchronous requests. That means the return value of fetch() is a response object, not a Promise for one. The API is otherwise essentially the same as that documented by Mozilla . For instance, this example sends a GET request every time it executes: var resp = fetch( http://example.com/site-deployed.php ); // resp.ok is true if the response was a 2xx, false otherwise. if (!resp.ok) { console.log( Well that didn't work. ); } While this example sends a POST request with a JSON string as the body: var body = JSON.stringify({  some :  value , }); var resp = fetch( http://example.com/ , { method:  POST , headers: { 'Content-Type': 'application/json', }, body: body, } ) if (!resp.ok) { console.log( Couldn't POST. ); } See the Mozilla Dev Network link above for more fetch() options. Cryptographic API A minimalist cryptographic API is also available to activity scripts. Its main use is for signing requests to 3rd party APIs. The crypto.createHmac() function allows you to create a secure HMAC hash and digest. var h = crypto.createHmac( sha256 ,  foo ); h.update( bar ); h.digest( hex ) The available hashing functions are 'sha256', 'sha1' and 'md5' as hashing functions. The available digest formats are 'base64', 'hex' or '' (empty). An empty digest will yield a byte string. For example, if you wanted to call an AWS API, you would calculate the signature like so: function HMAC(key, value) { var h = crypto.createHmac( sha256 , key); h.update(value); return h.digest(); } var kSecret =  wJalrXUtnFEMI/K7MDENG+bPxRfiCYEXAMPLEKEY ; HMAC(HMAC(HMAC(HMAC( AWS4  + kSecret, 20150830 ), us-east-1 ), iam ), aws4_request ); Example taken from the AWS documentation for signing API requests .",
        "section": "Integrations",
        "subsections": " Activity scripts  Installing Updating Removing   Debugging Configuring scripts Available APIs  underscore.js activity project Storage API Fetch API Cryptographic API    ",
        "image": "",
        "url": "/integrations/activity.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "43bef003df36edbaa72a852b58d0f5ba",
        "title": "Utility routines",
        "description": "",
        "text": " The following utility routines can help simplify common tasks in your activity scripts. They are free to copy, modify, bend, fold, spindle, and mutilate as needed for your own scripts. They also demonstrate some common patterns for working with the activity and project data structures in ES5 code. General utilities /** * Formats a string, injecting values in for placeholders. * * @param {string} format * A format string with placeholders in the form {0}, {1}, etc. * @param {string} args * A variable number of strings to replace by position. * @return {string} * The formatted string. */ function formatString (format) { var args = Array.prototype.slice.call(arguments, 1); return function(match, number) { return typeof args[number] != \u0026#39;undefined\u0026#39; ? args[number] : match ; }); } /** * Returns a key/value object containing all variables relevant for the activity. * * That includes project level variables, plus any variables visible for * the relevant environment for the activity, if any. * * Note that JSON-encoded values will show up as a string, and need to be * decoded with JSON.parse(). */ function variables() { var vars = {}; activity.payload.deployment.variables.forEach(function(variable) { vars[variable.name] = variable.value; }); return vars; } Route access /** * Returns just those routes that point to a valid upstream. * * This method is similar to routes(), but filters out redirect routes that are rarely * useful for app configuration. If desired it can also filter to just those routes * whose upstream is a given application name. To retrieve routes that point to the * current application where the code is being run, use: * * routes = getUpstreamRoutes(applicationName); * * @param {string|null} appName * The name of the upstream app on which to filter, if any. * @return {object} * An object map of route definitions. */ function getUpstreamRoutes(appName) { var upstreams = {}; Object.keys(activity.payload.deployment.routes).forEach(function (url) { var route = activity.payload.deployment.routes[url]; if (route.type ===  upstream ) { if (!appName || appName === route.upstream.split(\u0026#39;:\u0026#39;)[0]) { route.url = url; upstreams[url] = route; } } }); return upstreams; } /** * Returns the primary route. * * The primary route is the one marked primary in `routes.yaml`, or else * the first non-redirect route in that file if none are marked. * * @return {object} * The route definition. The generated URL of the route is added as a  url  key. */ function getPrimaryRoute() { var primary = {}; Object.keys(activity.payload.deployment.routes).forEach(function (url) { var route = activity.payload.deployment.routes[url]; if (route.primary) { route.url = url; primary = route; } }); return primary; } /** * Returns a single route definition. * * Note: If no route ID was specified in routes.yaml then it will not be possible * to look up a route by ID. * * @param {string} id * The ID of the route to load. * @return {object} * The route definition. The generated URL of the route is added as a  url  key. * @throws {Error} * If there is no route by that ID, an exception is thrown. */ function getRoute(id) { var found = null; Object.keys(activity.payload.deployment.routes).forEach(function (url) { var route = activity.payload.deployment.routes[url]; if (route.id === id) { route.url = url; found = route; } }); if (found) { return found; } throw new Error( No such route id found:   \u0026#43; id); }",
        "section": "Activity scripts",
        "subsections": " General utilities Route access  ",
        "image": "",
        "url": "/integrations/activity/utility.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "9dd6a1f59e9bdeed1313f1ab53b89be9",
        "title": "Source Integrations",
        "description": "",
        "text": "Platform.sh allows you to maintain your code base in a third party repository and link it to your Platform.sh project.",
        "section": "Integrations",
        "subsections": "",
        "image": "",
        "url": "/integrations/source.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "f28b0e09debf0d2e668ca9f7a78c3290",
        "title": "Profiling",
        "description": "",
        "text": "Platform.sh supports a number of profiling services for optimizing your code.",
        "section": "Integrations",
        "subsections": "",
        "image": "",
        "url": "/integrations/profiling.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "530ceed9de239bab0b518bd8a5fdd0bd",
        "title": "Health notifications",
        "description": "",
        "text": " Platform.sh can notify you when various events happen on your project, in any environment. At this time the only notification provided is a low disk space warning, but others may be added in the future. Note: Remember that you must have admin access to a project in order to add or modify an integration. See User administration roles for more details. Default low-disk email notifications When you create a new project, Platform.sh creates a default low-disk email notification for all Project Admins . Note: All projects created prior to 6 April 2020 that did not have any health notifications enabled had an email notification added for admin users. Available notifications Low-disk warning Platform.sh monitors disk space usage on all applications and services in your cluster. If and when available disk space drops below 20%, a warning notification is generated. If and when available disk space drops below 10%, a critical notification is generated. If and when available disk space goes back above 20% after previously having been lower, an all-clear notification is generated. Notifications are generated every 5 minutes, so there may be a brief delay between when the threshold is crossed and when the notification is triggered. Configuring notifications Health notifications can be set up via the Platform.sh CLI , through a number of different channels. Email notifications A notification can trigger an email to be sent, from an address of your choosing to one or more addresses of your choosing. You can view an email notification by running platform integration:get. platform integration:get \u0026#43;--------------\u0026#43;---------------\u0026#43; | Property | Value | \u0026#43;--------------\u0026#43;---------------\u0026#43; | id | abcdefghijklm | | type | health.email | | role | | | from_address | | | recipients | - \u0026#39;#admins\u0026#39; | \u0026#43;--------------\u0026#43;---------------\u0026#43; To edit the recipients that receive the default email notification, use the integration:update command. platform integration:update abcdefghijklm --recipients you@example.com The recipients field may be any valid email address, or one of the following special values. #admins maps to all project admins and up. #viewers maps to everyone with access to the project. To add a new email notification, register a health.email integration as follows: platform integration:add --type health.email --from-address you@example.com --recipients them@example.com --recipients others@example.com The from-address is whatever address you want the email to appear to be from. You must specify one or more recipients, each as its own switch. It is completely fine to use the same email address for both from-address and recipients. Slack notifications A notification can trigger a message to be sent to a Slack bot. First, create a new custom “ bot user ” for your Slack group and configure the channels you wish it to live in. Note the API token is the “Bot User OAuth Access Token” provided by Slack. Then register that Slack bot with Platform.sh using a health.slack integration: platform integration:add --type health.slack --token YOUR_API_TOKEN --channel \u0026#39;#channelname\u0026#39; That will trigger the corresponding bot to post a notification to the #channelname channel in your Slack group. PagerDuty notifications A notification can trigger a message to be sent via PagerDuty, if you are using that service. First, create a new PagerDuty “ integration ” that uses the Events API v2. Copy the “Integration Key” as known as the “routing key” for the integration. Now register a health.pagerduty integration as follows: platform integration:add --type health.pagerduty --routing-key YOUR_ROUTING_KEY Any notification will now trigger an alert in PagerDuty. Webhooks notifications A notification can trigger a message to be sent to a web endpoint. To do so, register a health.webhook integration as follows: platform integration:add --type health.webhook --url=A-URL-THAT-CAN-RECEIVE-THE-POSTED-JSON Any notification will now be posted to the health.webhook URL. In order to let you verify that requests are coming from the integration, you can use the optional shared-key parameter which will add a X-JWS-Signature request header containing the JSON Web Token Signature in JWS Compact Serialization with Unencoded Detached Payload ( RFC7797 ). platform integration:add --type health.webhook --url=A-URL-THAT-CAN-RECEIVE-THE-POSTED-JSON --shared-key JWS-SYMMETRIC-KEY The signature is calculated using the given shared-key and the fixed header: { alg : HS256 , b64 :false, crit :[ b64 ]} A simplified example payload with the corresponding signature might look like the following snippet: POST /health/notifications HTTP/1.0 Host: www.example.com Content-Length: 1495 Content-Type: application/json X-JWS-Signature: eyJhbGciOiJIUzI1NiIsImI2NCI6ZmFsc2UsImNyaXQiOlsiYjY0Il19..fYW9qrjShmEArV17Z1kH6yudoXzpBE3PzJXq_OqrIfM {...request body...} Signature verification is a simple 2 step process: # 1. Compute JWS Compact Serialization with Unencoded Detached Payload from jwcrypto import jws, jwk rfc7797_u_header = \u0026#39;{ alg : HS256 , b64 :false, crit :[ b64 ]}\u0026#39; json_web_key = jwk.JWK(kty= oct , k= JWS-SYMMETRIC-KEY ) sig = jws.JWS(request.body()) sig.add_signature(json_web_key, protected=rfc7797_u_header) sig.detach_payload() # 2. Verify the signature assert sig.serialize(compact=True) == request.headers[ X-JWS-Signature ] Please refer to the JOSE Cookbook for examples about protecting content using JavaScript Object Signing and Encryption (JOSE). Validate the integration You can then verify that your integration is functioning properly using the CLI command $ platform integration:validate",
        "section": "Integrations",
        "subsections": " Default low-disk email notifications Available notifications  Low-disk warning   Configuring notifications  Email notifications Slack notifications PagerDuty notifications Webhooks notifications   Validate the integration  ",
        "image": "",
        "url": "/integrations/notifications.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "1087c8c69a50fe7b603a99393e003068",
        "title": "Introduction",
        "description": "",
        "text": "Try out Platform.sh by either including few configuration files to your existing codebase, or by deploying one of over fifty maintained template projects.",
        "section": "Introduction",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "d98929356d78fe97ac1d17b712b87937",
        "title": "Alernative Node.js install",
        "description": "",
        "text": " How to use NVM to run different versions of Node.js Node Version Manager or NVM is a tool for managing multiple versions of Node.js in one installation. You can use NVM with any of our container types that have node installed to change or update the version. This may be useful, for example, where a container has a Long Term Release (LTS) version available, but you would like to use the latest. Installing NVM is done in the build hook of your .platform.app.yaml, which some additional calls to ensure that environment variables are set correctly. hooks:build:| unset NPM_CONFIG_PREFIXcurl-o- unset in a .environment file in the root of your project: # This is necessary for nvm to work. unset NPM_CONFIG_PREFIX # Disable npm update notifier; being a read only system it will probably annoy you. export NO_UPDATE_NOTIFIER=1 # This loads nvm for general usage. export NVM_DIR= $PLATFORM_APP_DIR/.nvm  [ -s  $NVM_DIR/nvm.sh  ] \u0026amp;\u0026amp;  $NVM_DIR/nvm.sh ",
        "section": "Node.js",
        "subsections": " How to use NVM to run different versions of Node.js  ",
        "image": "",
        "url": "/languages/nodejs/nvm.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "4c7dc60e710fc82c92d19784740f7519",
        "title": "API tokens",
        "description": "",
        "text": " Obtaining a token The Platform.sh CLI can also be used from CI services or other automation tools, and supports an API Token authentication option for that purpose. An API token can be created through the management console. Go to the “User” page from your account drop-down, then select the “Account Settings” tab, then “API Tokens”. Click the “Create an API Token” link. Enter a name to easily identify your token in the future, in case of multiple tokens (“CLI automated” is one example). Once done, the newly created token will be displayed at the top of the page, and can be copied to the clipboard using the Copy button. After this, you will not be able to view the API token again. Now set that token in an environment variable named PLATFORMSH_CLI_TOKEN on the system where the CLI will run. Consult the documentation for your CI system to see how to do that. Note: If running CLI commands from any automated system, including a Platform.sh cron task, we urge you to use the --no-wait flag on any commands that may take more than a second or two to avoid blocking the process. Machine users For security reasons we recommend creating a dedicated machine user to run automation tasks such as taking backups, renewing SSL certificates or triggering source operations. We also strongly recommend creating a unique machine user for each project to be automated. Like human users, every machine user account needs its own unique email address. The machine user can be given a very restrictive set of permissions limited to just its needed tasks. Backups, for instance, require Admin access but no SSH key, while checking out code from a CI server to run tests on it would require an SSH key but only Reader access. It will also show up in logs and activity streams as a separate entry from human users. Consult the Users documentation for more information about the differences between access levels. Install the CLI on a Platform.sh environment A common use case for an API token is to allow the Platform.sh CLI to be run on an app container, often via a cron hook. An API token is necessary for authentication, but the CLI will be able to auto-detect the current project and environment. First, create a machine user (see above) that you invite to your project. Then, log in as that machine user to obtain an API token. Set this token as the top-level environment variable env:PLATFORMSH_CLI_TOKEN either through the management console or via the CLI, like so: platform variable:create -e master --level environment --name env:PLATFORMSH_CLI_TOKEN --sensitive true --value \u0026#39;your API token\u0026#39; Note: It is important to include the env: so as to expose $PLATFORMSH_CLI_TOKEN on its own as a top level Unix environment variable, rather than as a part of $PLATFORM_VARIABLES like normal environment variables. Second, add a build hook to your .platform.app.yaml file to download the CLI as part of the build process. hooks:build:| curl -sS https://platform.sh/cli/installer | phpThis will download the CLI to a known directory, .platformsh/bin, which will be added to the PATH at runtime (via the .environment file). Because the API token is available, the CLI will now be able to run authenticated commands, acting as the user who created the token. You can now call the CLI from within the shell on the app container, or via a cron hook. Note that if you want a cron to run only on the production environment you will need to wrap it in an if-check on the $PLATFORM_BRANCH variable, like so: crons:backup:spec:\u0026#39;0 5 * * *\u0026#39;cmd:| if [  $PLATFORM_BRANCH  = master ]; thenplatformbackup:create--yes--no-waitfi Note: Seriously, please use --no-wait for all CLI commands placed in a cron hook. Failure to do so may result in long deploy times and site downtime.",
        "section": "CLI (Command line interface)",
        "subsections": " Obtaining a token Machine users Install the CLI on a Platform.sh environment  ",
        "image": "",
        "url": "/development/cli/api-tokens.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "6a306d1f4c635744235c86e403500ba6",
        "title": "Authenticated Composer repositories",
        "description": "",
        "text": " Some PHP projects may need to leverage a private, third party Composer repository in addition to the public Packagist.org repository. Often, such third party repositories require authentication in order to download packages, and not everyone is comfortable putting those credentials into their Git repository source code (for obvious reasons). To handle that situation, you can define a env:COMPOSER_AUTH project variable which allows you to set up authentication as an environment variable. The contents of the variable should be a JSON formatted object containing an http-basic object (see composer-auth specifications ). The advantage is that you can control who in your team has access to those variables. Specify a third party repository in composer.json For this example, consider that there are several packages we want to install from a private repository hosted at my-private-repos.example.com. List that repository in your composer.json file. {  repositories : [ {  type :  composer ,  url :  https://my-private-repos.example.com  } ] } Set the env:COMPOSER_AUTH project variable Set the Composer authentication by adding a project level variable called env:COMPOSER_AUTH as JSON and available only during build time. That can be done through the management console or via the command line, like so: platform variable:create --level project --name env:COMPOSER_AUTH --json true --visible-runtime false --sensitive true --visible-build true --value \u0026#39;{ http-basic : { my-private-repos.example.com : { username :  your-username ,  password :  your-password }}}\u0026#39; The env: prefix will make that variable appear as its own Unix environment variable available by Composer during the build process. The optional --no-visible-runtime flag means the variable will only be defined during the build hook, which offers slightly better security. Note: The authentication credentials may be cached in your project’s build container, so please make sure you clear the Composer cache upon changing any authentication credentials. You can use the platform project:clear-build-cache command. Build your application with Composer You simply need to enable the default Composer build mode in your .platform.app.yaml: build:flavor: composer In that case, Composer will be able to authenticate and download dependencies from your authenticated repository. Private repository hosting Typically, a private dependency will be hosted in a private Git repository. While Platform.sh supports private repositories for the site itself, that doesn’t help for pulling in third party dependencies from private repositories unless they have the same SSH keys associated with them. Fortunately, most private Composer tools (including Satis, Toran Proxy, and Private Packagist ) mirror tagged releases of dependencies and serve them directly rather than hitting the Git repository. Therefore as long as your dependencies specify tagged releases there should be no need to authenticate against a remote Git repository and there should be no authentication issue.",
        "section": "Tutorials",
        "subsections": " Specify a third party repository in composer.json Set the env:COMPOSER_AUTH project variable Build your application with Composer Private repository hosting  ",
        "image": "",
        "url": "/tutorials/composer-auth.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "05c31952e8b65cbf39e79d2ce374629f",
        "title": "Deploy on Platform.sh",
        "description": "",
        "text": " Platform.sh offers a number of project templates as part of the Project Creation Wizard to make bootstrapping a new project easy. However, you can also create arbitrary links to spawn projects on Platform.sh from an arbitrary Git repository or prepared template. There are two ways to create such a link, shown below. In each case, when a user clicks on the link they will be redirected to create a new Platform.sh project, with the template selection step skipped in favor of the template specified. If the user does not have a Platform.sh account yet they will be prompted to create one. You may include the link on your own project’s website, your company’s internal Wiki, or anywhere else a link can go to make launching your code base as simple as possible. Preparation To have a deployable template, you need to first prepare the repository. The Deploy on Platform.sh button will work with any Git repository that is deployable on Platform.sh; that is, it has the necessary .platform.app.yaml , .platform/services.yaml , and .platform/routes.yaml files in place. See the appropriate documentation for how to define those files. The repository must be available at a publicly-accessible Git URL. That may be hosted with GitHub, GitLab, Bitbucket, your own custom Git hosting, or any other publicly-accessible Git URL. (Optional) Make a template definition file You can create a Deploy on Platform.sh button for any compatible repository; however, you can also provide a YAML template definition file. A template definition file is a YAML file that references a Git repository but can also include additional information, such as limiting the resulting project to a certain minimum project size or only allowing it to be deployed in certain regions. Use this mechanism when you want more control over how the template gets deployed. The template definition file may be at any publicly-accessible URL. It can be in the template repository itself or separate. Note that if it is in the template repository then it will be included in every deployed user project from that template. (It won’t hurt anything as it has no effect at runtime, but users will have a copy of the file in their code base and may be confused by it.) A list of all Platform.sh-supported templates is available on GitHub. 3rd party templates are also available. You can also create your own template file and host it anywhere you wish. The template definition file’s format is documented in the 3rd party template repository. Making a button (The easy way) The easiest way to make a Deploy on Platform.sh button is to use our button builder widget . You provide it with either the Git URL of the repository or a URL to a corresponding template definition file. The button builder widget will give you an HTML fragment to copy and paste to wherever you want the button hosted. It will also include a tracking code so we can whose Deploy on Platform.sh button was clicked, but does not add any cookies to the site. Making a button manually Arbitrary Git repository Create a link in the following form: https://console.platform.sh/projects/create-project?template=GIT_URL Where GIT_URL is the URL of a publicly-visible Git repository. For example, to install Platform.sh’s Drupal 8 template on GitHub you would use: https://console.platform.sh/projects/create-project/?template=https://github.com/platformsh-templates/drupal8.git (Note that is the URL of the Git repository as if you were cloning it, NOT the URL of the repository’s home page on GitHub.) A new project will be created and then initialized with whatever code is at the tip of the master branch of that repository. This method will work for any publicly-visible Git repository, provided that it includes the necessary Platform.sh YAML configuration files. If those are missing the project will still initialize but fail to build. Defined Template Create a link in the following form: https://console.platform.sh/projects/create-project?template=TEMPLATE_URL Where TEMPLATE_URL is the URL of a publicly-visible template definition file. For example, to install Platform.sh’s Drupal 8 template you would use: https://console.platform.sh/projects/create-project/?template=https://github.com/platformsh/template-builder/blob/master/templates/drupal8/.platform.template.yaml A new project will be created, initialized with whatever code is at the tip of the master branch of the repository referenced by that file, provided that it includes the necessary Platform.sh YAML configuration files. If those are missing the project will still initialize but fail to build. Listing a repository Platform.sh welcomes project templates produced by the application vendor. If you have a Free Software application you want available in the Platform.sh setup wizard, create a template definition file and submit a pull request against the 3rd party templates repository. The Developer Relations team will review and evaluate the application and template, and may offer feedback before merging. Generally speaking, we welcome any Free Software application that is actively maintained and runs well on Platform.sh. Projects released under a non-Free license will not be accepted.",
        "section": "Featured frameworks",
        "subsections": " Preparation  (Optional) Make a template definition file   Making a button (The easy way) Making a button manually  Arbitrary Git repository Defined Template   Listing a repository  ",
        "image": "",
        "url": "/frameworks/deploy-button.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "e7568a0968d8e29058e876d7483a4127",
        "title": "Developing with Drupal",
        "description": "",
        "text": " Push changes to an environment Here, we’ll see how to make code changes to an environment. Note: You should never be working on the Master branch since it’s supposed to be your production environment. Make sure you’re on a working environment. In this example we’re on the sprint1 branch: $ git checkout sprint1 Now that you’re set up on your working branch, you can start developing on your website by making code changes and pushing those changes to Platform to test them live. There are three common ways you will be making code changes to Platform: Add contributed modules, themes, distributions, third-party libraries in the make file Create custom code (modules, themes, profiles, libraries) and commit them to your Platform codebase Modify the services grid configuration Add contributed projects Each time you push a commit, Platform.sh will rebuild your environment and run the Drush make command if a proper make file has been found. Add a Drupal module Each Drupal module you want to install on your project should be included in the make file. For example, if you want to add Drupal Commerce, you need to add the following lines to your project.make: ; Modules projects[addressfield][version] =  1.0-beta4  projects[addressfield][subdir] =  contrib  projects[ctools][version] =  1.3  projects[ctools][subdir] =  contrib  projects[commerce][version] =  1.8  projects[commerce][subdir] =  contrib  projects[entity][version] =  1.2  projects[entity][subdir] =  contrib  projects[rules][version] =  2.6  projects[rules][subdir] =  contrib  projects[views][version] =  3.7  projects[views][subdir] =  contrib  Add a Drupal theme You’d do the same if you want to add a theme. Add the following lines to your project.make: ; Zen Theme projects[] = zen Add a third-party library You’d do the same if you want to add a third-party library. For our example here, we’re adding the HTML5 Boilerplate library. Add the following lines to your project.make: ; Libraries libraries[html5bp][download][type] =  file  libraries[html5bp][download][url] =  http://github.com/h5bp/html5-boilerplate/zipball/v3.0.2stripped  Add custom code To commit your custom modules, themes or libraries, you need to commit them under a modules, themes or libraries folder at the root of your Git repository. $ ls libraries/ modules/ project.make themes/ When you push your code, Platform will build your environment and move your modules, themes, libraries to the correct location on your site (usually sites/default/). Change the services configuration You can change and define the topology of the services used in an environment, by modifying the configuration files. This means that you’re able to define and configure the services you want to use. Push your changes When you’re done, commit your changes to test them on your online environment. $ git add . $ git commit -m  Made changes to my make file.  $ git push You will see that Platform has found a make file and is starting to rebuild your environment. When it’s completed, you can see your changes on your site by clicking View this website under the name of Sprint1 environment on the Platform.sh management console. Note: The Drush Make processing doesn’t create any file in your Git repository. Your Git repository is the input of the process and not the output. You can see the directory structure that has been created by connecting via SSH to the environment. See the information in the Access information below the title of the environment. Merge code changes to Master Once you’ve got a branch with some changes, you’ll want to be able to push those changes up to your live environment. Platform.sh has a great button called Merge that you can click on and it will push the appropriate changes to master. A dialog box will appear that will provide commands to execute future merges from the command line using the Platform.sh CLI . Just click on the “Merge” button again and all of the commits you made on your branch will be merged into the master environment. Synchronizing data The easiest way to do that is to use Drush and the sql-sync command. You’ll need to have Drush aliases for both your Platform.sh site and your local site. If you are using the CLI and you’ve run platform get [platform_id] for a project, then your Drush aliases have already been set up. With the Drush aliases (depending on how yours are set up), you could use a command similar to this: $ drush sql-sync @platform.master @platform._local An alternate method that is appropriate for larger databases is to use the pipe | to stream the data, instead of making copies. $ drush @platform.master sql-dump | drush @platform._local sqlc",
        "section": "Getting Started",
        "subsections": " Push changes to an environment  Add contributed projects Add custom code Change the services configuration Push your changes   Merge code changes to Master Synchronizing data  ",
        "image": "",
        "url": "/frameworks/drupal7/developing-with-drupal.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "8a8e72c1fa9b6864f9fd403b7f9f439e",
        "title": "Developing with Drupal",
        "description": "",
        "text": " Push changes to an environment Here, we’ll see how to make code changes to an environment. Note: You should never be working on the Master branch since it’s supposed to be your production environment. Make sure you’re on a working environment. In this example we’re on the sprint1 branch: $ git checkout sprint1 Now that you’re set up on your working branch, you can start developing on your website by making code changes and pushing those changes to Platform.sh to test them live. There are three common ways you will be making code changes to Platform: Add contributed modules, themes, distributions, third-party libraries in the make file Create custom code (modules, themes, profiles, libraries) and commit them to your Platform.sh codebase Modify the services grid configuration Add contributed projects Each time you push a commit, Platform.sh will rebuild your environment and run the Composer command if a proper composer.json file has been found. Add a Drupal module or theme Each Drupal module or theme you want to install on your project should be included in your composer.json file. For example: $ composer require drupal/token That will update your composer.json and composer.lock files, which you can then commit. If you’re using Composer, 3rd party PHP libraries can be added in the exact same way as Drupal modules. Add custom code To commit your custom modules, themes, or libraries, add those to the web/modules/custom or web/themes/custom directory and commit them to Git as normal. Change the services configuration You can change and define the topology of the services used in an environment, by modifying the configuration files. This means that you’re able to define and configure the services you want to use. Push your changes When you’re done, commit your changes to test them on your online environment. $ git add . $ git commit -m  Made changes to my files.  $ git push You will see that Platform has found a make file and is starting to rebuild your environment. When it’s completed, you can see your changes on your site by clicking View this website under the name of Sprint1 environment on the Platform.sh management console. Note: The build process makes no changes to your Git repository. Your Git repository is the input of the process. A PHP container containing your code and dependencies is the output. You can see the directory structure that has been created by connecting via SSH to the environment. See the information in the Access information below the title of the environment. Merge code changes to Master Once you’ve got a branch with some changes, you’ll want to be able to push those changes up to your live environment. Platform.sh has a great button called Merge that you can click on and it will push the appropriate changes to master. A dialog box will appear that will provide commands to execute future merges from the command line using the Platform.sh CLI . Just click on the “Merge” button again and all of the commits you made on your branch will be merged into the master environment. Synchronizing data The easiest way to do that is to use Drush and the sql-sync command. You’ll need to have Drush aliases for both your Platform.sh site and your local site. If you are using the CLI and you’ve run platform get [platform_id] for a project, then your Drush aliases have already been set up. With the Drush aliases (depending on how yours are set up), you could use a command similar to this: $ drush sql-sync @platform.master @platform._local An alternate method that is appropriate for larger databases is to use the pipe | to stream the data, instead of making a copy of the dump file. $ drush @platform.master sql-dump | drush @platform._local sqlc",
        "section": "Getting Started",
        "subsections": " Push changes to an environment  Add contributed projects Add custom code Change the services configuration Push your changes   Merge code changes to Master Synchronizing data  ",
        "image": "",
        "url": "/frameworks/drupal8/developing-with-drupal.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "510d6192ab63668f989a8055467930aa",
        "title": "DNS management and Apex domains",
        "description": "",
        "text": " Platform.sh expects you to use a CNAME for all DNS records. However, that is problematic with some DNS registrars. Why CNAMEs? Platform.sh is a cloud hosting provider. That means each individual “site” is not its own computer but a set of containers running on one or more virtual machines, which are themselves running on any number of physical computers, all of which are shared with other customers running the same configuration. An entire region of projects runs behind our dedicated, high-performance edge routers, which are responsible for mapping incoming requests to the particular container on a particular host that is appropriate. All of that logic is quite robust and fast, but it does require that incoming requests all get sent first to the edge routers. While the IP addresses of the edge routers are fairly stable, they are not guaranteed to never change. We also may add or remove routers to help scale the region, or take them offline one at a time for upgrades and maintenance. It is therefore critical that inbound requests always know what the IPs are of the edge routers at the time of the request. All of Platform.sh’s “edge hostnames” (the auto-generated URLs in the form \u0026lt;branch\u0026gt;-\u0026lt;hash\u0026gt;-\u0026lt;project_id\u0026gt;.\u0026lt;region\u0026gt;.platformsh.site) are DNS records we control that resolve to the IP addresses of the edge routers for that region. If an edge router is updated, taken out of rotation, etc. then those domains will update quickly and automatically with no further action required. An A record pointed at the same IP addresses would need to be updated manually every time an edge router changes or is temporarily offline. That means every time Platform.sh is doing routine maintenance or upgrades on the edge routers there’s a significant potential for a site to experience a partial outage if a request comes in for an offline edge router. We don’t want that. You don’t want that. Using a CNAME DNS record pointing at the “edge hostname” will avoid that problem, as it will be updated almost immediately should our edge router configuration change. Why are CNAME records problematic? The DNS specification was originally published in 1987 in RFC 1034 and RFC 1035 , long before name-based HTTP hosting became prevalent. Those RFCs plus the many follow-ups to clarify and expand on it are somewhat vague on the behavior of CNAME, but it’s generally understood that an apex domain (example.com) may not be used as an alias in a CNAME record. That creates a problem if you want to use an apex domain with any container-based managed hosting service like Platform.sh, because of the point above. There’s a detailed thread on the subject that provides more technical detail. Handling Apex domains There are a number of ways of handling the CNAME-on-Apex limitation of DNS. Using a DNS provider with custom records Many DNS providers have found a way around the CNAME-on-Apex limitation. Some DNS registrars now offer custom, non-standard records (sometimes called ANAME or ALIAS) that you can manage like a CNAME but will do their own internal lookup behind the scenes and then respond to DNS lookups as if they were an A record. As these are non-standard their behavior (and quality) can vary, and not all DNS registrars offer such a feature. If you want your site to be accessible with https://example.com and not only https://www.example.com this is the best way to do so. Examples of such workaround records include: CNAME Flattening at CloudFlare ANAME at easyDNS , DNS Made Easy , or Name.com ALIAS at DNSimple or Cloudns Platform.sh recommends ensuring that your DNS Provider supports dynamic apex domains before registering your domain name with them. If you are using a DNS Provider that does not support dynamic apex domains then you will be unable to use example.com with Platform.sh, and will need to use only www.example.com (or similar) instead. (Alternate) Using a DNS provider with apex domain forwarding If you are willing to make the www. version of your site the canonical version (which is recommended), some registrars or DNS providers may provide a domain redirect feature—also known as domain forwarding—from the apex domain example.com to www.example.com. Before looking to change registrars, check whether your current provider supports both domain forwarding for the Apex and the DNS CNAME record to Platform.sh for the www. at the same time. The following DNS providers are known to support both apex forwarding and advanced DNS configurations simultaneously: Namecheap (Alternate) Using a www redirection service If your preferred registrar/DNS provider doesn’t support either custom records or the apex domain forwarding options above, the following free services both allow blind redirects and allow you to use a CNAME record to Platform.sh for www.example.com and an A record to their service at example.com, which will in turn send a redirect. WWWizer redirectssl (Alternate) Using A records If you absolutely cannot use a DNS provider that supports aliases or a redirection service, it is possible to use A records with Platform.sh. They will result in a sub-optimal experience, however. This process has a few limitations: Should we ever need to change one of those IPs your configuration will need to be manually updated. Until it is some requests will be lost. Directly pointing at the edge routers bypasses their load-balancing functionality. Should one of them go offline for maintenance (as happens periodically for upgrades) approximately 1/3 of requests to your site will go to the offline router and be lost, making the site appear offline. For that reason using A records is strongly discouraged and should only be used as a last resort. See the Public IP list for the 3 Inbound addresses for your region. In your DNS provider, configure 3 separate A records for your domain, one for each of those IP addresses. Incoming requests will then pick one of those IPs at random to use for that request.",
        "section": "Going Live - Steps",
        "subsections": " Why CNAMEs? Why are CNAME records problematic? Handling Apex domains  Using a DNS provider with custom records (Alternate) Using a DNS provider with apex domain forwarding (Alternate) Using a www redirection service (Alternate) Using A records    ",
        "image": "",
        "url": "/golive/steps/dns.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "54356fc25fb1d69e29ae7a2087a256e3",
        "title": "Download the code",
        "description": "",
        "text": " If you have already pushed your code to Platform.sh, then you should already have a local repository that you can build from. Otherwise, it will be necessary to download a local copy of your project first. Get project ID You will need the your project ID. You can retrieve this ID at any time using the CLI commands platform or platform project:list. Get a copy of the repository locally Next, use the CLI to download the code in your Platform.sh project using the command platform get \u0026lt;project id\u0026gt; Now that you have a local copy of your application that is configured to the Platform.sh remote repository, you can create a new . Back I have a local copy of my code",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/developing/dev-environments/download-code.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "f71b9b0dd624699141f8ced05415fcf2",
        "title": "Download the code",
        "description": "",
        "text": " If you have already pushed your code to Platform.sh, then you should already have a local repository that you can build from. Otherwise, it will be necessary to download a local copy of your project first. Get project ID You will need the your project ID. You can retrieve this ID at any time using the CLI command platform. Get a copy of the repository locally Next, use the CLI to download the code in your Platform.sh project using the command platform get \u0026lt;project id\u0026gt; Next you can now connect to its services and build it on your machine. Back I have a local copy of my code",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/developing/local-development/download-code.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "d84b4a45a729c5c6a88a44a778191f7f",
        "title": "Elasticsearch (Search service)",
        "description": "",
        "text": " Elasticsearch is a distributed RESTful search engine built for the cloud. See the Elasticsearch documentation for more information. Supported versions Grid Dedicated 6.5 7.2 5.2 6.5 Deprecated versions The following versions are available but are not receiving security updates from upstream, so their use is not recommended. They will be removed at some point in the future. Grid Dedicated 0.9 1.4 1.7 2.4 5.2 5.4 1.7 2.4 Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : {  cluster :  rjify4yjcwxaa-master-7rqtwti ,  fragment : null,  host :  elasticsearch.internal ,  host_mapped : false,  hostname :  gnenv2b23ltik7mrvu3fyrlybq.elasticsearch.service._.eu-3.platformsh.site ,  ip :  169.254.252.137 ,  password : null,  path : null,  port : 9200,  public : false,  query : [],  rel :  elasticsearch ,  scheme :  http ,  service :  elasticsearch ,  type :  elasticsearch:7.2 ,  username : null } Usage example In your .platform/services.yaml: searchelastic:type:elasticsearch:7.2disk:256 In your .platform.app.yaml: relationships:essearch: searchelastic:elasticsearch  Note: You will need to use the elasticsearch type when defining the service # .platform/services.yamlservice_name:type:elasticsearch:versiondisk:256 and the endpoint elasticsearch when defining the relationship # .platform.app.yamlrelationships:relationship_name:“service_name:elasticsearch” Your service_name and relationship_name are defined by you, but we recommend making them distinct from each other. You can then use the service in a configuration file of your application with something like: Java Nodejs PHP Python package sh.platform.languages.sample; import org.elasticsearch.action.admin.indices.refresh.RefreshRequest; import org.elasticsearch.action.admin.indices.refresh.RefreshResponse; import org.elasticsearch.action.delete.DeleteRequest; import org.elasticsearch.action.index.IndexRequest; import org.elasticsearch.action.search.SearchRequest; import org.elasticsearch.action.search.SearchResponse; import org.elasticsearch.client.RequestOptions; import org.elasticsearch.client.RestHighLevelClient; import org.elasticsearch.index.query.QueryBuilders; import org.elasticsearch.search.SearchHit; import org.elasticsearch.search.builder.SearchSourceBuilder; import sh.platform.config.Config; import sh.platform.config.Elasticsearch; import java.io.IOException; import java.util.Arrays; import java.util.HashMap; import java.util.List; import java.util.Map; import java.util.function.Supplier; import static java.util.concurrent.ThreadLocalRandom.current; public class ElasticsearchSample implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); Elasticsearch elasticsearch = config.getCredential( elasticsearch , Elasticsearch::new); // Create an Elasticsearch client object. RestHighLevelClient client = elasticsearch.get(); try { String index =  animals ; String type =  mammals ; // Index a few document. final List\u0026lt;String\u0026gt; animals = Arrays.asList( dog ,  cat ,  monkey ,  horse ); for (String animal : animals) { Map\u0026lt;String, Object\u0026gt; jsonMap = new HashMap\u0026lt;\u0026gt;(); jsonMap.put( name , animal); jsonMap.put( age , current().nextInt(1, 10)); jsonMap.put( is_cute , current().nextBoolean()); IndexRequest indexRequest = new IndexRequest(index, type) .id(animal).source(jsonMap); client.index(indexRequest, RequestOptions.DEFAULT); } RefreshRequest refresh = new RefreshRequest(index); // Force just-added items to be indexed RefreshResponse refreshResponse = client.indices().refresh(refresh, RequestOptions.DEFAULT); // Search for documents. SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); sourceBuilder.query(QueryBuilders.termQuery( name ,  dog )); SearchRequest searchRequest = new SearchRequest(); searchRequest.indices(index); searchRequest.source(sourceBuilder); SearchResponse search = client.search(searchRequest, RequestOptions.DEFAULT); for (SearchHit hit : search.getHits()) { String id = hit.getId(); final Map\u0026lt;String, Object\u0026gt; source = hit.getSourceAsMap(); logger.append(String.format( result id %s source: %s , id, } // Delete documents. for (String animal : animals) { client.delete(new DeleteRequest(index, type, animal), RequestOptions.DEFAULT); } } catch (IOException exp) { throw new RuntimeException( An error when execute Elasticsearch:   \u0026#43; exp.getMessage()); } return logger.toString(); } } const elasticsearch = require(\u0026#39;elasticsearch\u0026#39;); const config = require( platformsh-config ).config(); exports.usageExample = async function() { const credentials = config.credentials(\u0026#39;elasticsearch\u0026#39;); var client = new elasticsearch.Client({ host: `${credentials.host}:${credentials.port}`, }); let index = \u0026#39;my_index\u0026#39;; let type = \u0026#39;People\u0026#39;; // Index a few document. let names = [\u0026#39;Ada Lovelace\u0026#39;, \u0026#39;Alonzo Church\u0026#39;, \u0026#39;Barbara Liskov\u0026#39;]; let message = { refresh:  wait_for , body: [] }; names.forEach((name) =\u0026gt; { message.body.push({index: {_index: index, _type: type}}); message.body.push({name: name}); }); await client.bulk(message); // Search for documents. const response = await client.search({ index: index, q: \u0026#39;name:Barbara Liskov\u0026#39; }); let output = \u0026#39;\u0026#39;; if(response.hits.total.value \u0026gt; 0) { output \u0026#43;= `\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;ID\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt;`; response.hits.hits.forEach((record) =\u0026gt; { output \u0026#43;= }); output \u0026#43;= } else { output =  No records found. ; } // Clean up after ourselves. response.hits.hits.forEach((record) =\u0026gt; { client.delete({ index: index, type: type, id: record._id, }); }); return output; }; \u0026lt;?php declare(strict_types=1); use use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // Get the credentials to connect to the Elasticsearch service. $credentials = $config-\u0026gt;credentials(\u0026#39;elasticsearch\u0026#39;); try { // The Elasticsearch library lets you connect to multiple hosts. // On Platform.sh Standard there is only a single host so just // register that. $hosts = [ [ \u0026#39;scheme\u0026#39; =\u0026gt; $credentials[\u0026#39;scheme\u0026#39;], \u0026#39;host\u0026#39; =\u0026gt; $credentials[\u0026#39;host\u0026#39;], \u0026#39;port\u0026#39; =\u0026gt; $credentials[\u0026#39;port\u0026#39;], ] ]; // Create an Elasticsearch client object. $builder = ClientBuilder::create(); $builder-\u0026gt;setHosts($hosts); $client = $builder-\u0026gt;build(); $index = \u0026#39;my_index\u0026#39;; $type = \u0026#39;People\u0026#39;; // Index a few document. $params = [ \u0026#39;index\u0026#39; =\u0026gt; $index, \u0026#39;type\u0026#39; =\u0026gt; $type, ]; $names = [\u0026#39;Ada Lovelace\u0026#39;, \u0026#39;Alonzo Church\u0026#39;, \u0026#39;Barbara Liskov\u0026#39;]; foreach ($names as $name) { $params[\u0026#39;body\u0026#39;][\u0026#39;name\u0026#39;] = $name; $client-\u0026gt;index($params); } // Force just-added items to be indexed. $client-\u0026gt;indices()-\u0026gt;refresh(array(\u0026#39;index\u0026#39; =\u0026gt; $index)); // Search for documents. $result = $client-\u0026gt;search([ \u0026#39;index\u0026#39; =\u0026gt; $index, \u0026#39;type\u0026#39; =\u0026gt; $type, \u0026#39;body\u0026#39; =\u0026gt; [ \u0026#39;query\u0026#39; =\u0026gt; [ \u0026#39;match\u0026#39; =\u0026gt; [ \u0026#39;name\u0026#39; =\u0026gt; \u0026#39;Barbara Liskov\u0026#39;, ], ], ], ]); if (isset($result[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;])) { print \u0026lt;\u0026lt;\u0026lt;TABLE\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;ID\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt; TABLE; foreach ($result[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;] as $record) { $record[\u0026#39;_id\u0026#39;], $record[\u0026#39;_source\u0026#39;][\u0026#39;name\u0026#39;]); } print } // Delete documents. $params = [ \u0026#39;index\u0026#39; =\u0026gt; $index, \u0026#39;type\u0026#39; =\u0026gt; $type, ]; $ids = array_map(function($row) { return $row[\u0026#39;_id\u0026#39;]; }, $result[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]); foreach ($ids as $id) { $params[\u0026#39;id\u0026#39;] = $id; $client-\u0026gt;delete($params); } } catch (Exception $e) { print $e-\u0026gt;getMessage(); } import elasticsearch from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the Elasticsearch service. credentials = config.credentials(\u0026#39;elasticsearch\u0026#39;) try: # The Elasticsearch library lets you connect to multiple hosts. # On Platform.sh Standard there is only a single host so just register that. hosts = {  scheme : credentials[\u0026#39;scheme\u0026#39;],  host : credentials[\u0026#39;host\u0026#39;],  port : credentials[\u0026#39;port\u0026#39;] } # Create an Elasticsearch client object. client = elasticsearch.Elasticsearch([hosts]) # Index a few documents es_index = \u0026#39;my_index\u0026#39; es_type = \u0026#39;People\u0026#39; params = {  index : es_index,  type : es_type,  body : { name : \u0026#39;\u0026#39;} } names = [\u0026#39;Ada Lovelace\u0026#39;, \u0026#39;Alonzo Church\u0026#39;, \u0026#39;Barbara Liskov\u0026#39;] ids = {} for name in names: params[\u0026#39;body\u0026#39;][\u0026#39;name\u0026#39;] = name ids[name] = client.index(index=params[ index ], doc_type=params[ type ], body=params[\u0026#39;body\u0026#39;]) # Force just-added items to be indexed. client.indices.refresh(index=es_index) # Search for documents. result = client.search(index=es_index, body={ \u0026#39;query\u0026#39;: { \u0026#39;match\u0026#39;: { \u0026#39;name\u0026#39;: \u0026#39;Barbara Liskov\u0026#39; } } }) table = \u0026#39;\u0026#39;\u0026#39;\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;ID\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt;\u0026#39;\u0026#39;\u0026#39; if result[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]: for record in result[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]: table \u0026#43;= record[\u0026#39;_source\u0026#39;][\u0026#39;name\u0026#39;]) table \u0026#43;= # Delete documents. params = {  index : es_index,  type : es_type, } for name in names: client.delete(index=params[\u0026#39;index\u0026#39;], doc_type=params[\u0026#39;type\u0026#39;], id=ids[name][\u0026#39;_id\u0026#39;]) return table except Exception as e: return e Note: When you create an index on Elasticsearch, you should not specify number_of_shards and number_of_replicas settings in your Elasticsearch API call. These values will be set automatically based on available resources. Authentication By default, Elasticsearch has no authentication. No username or password is required to connect to it. Starting with Elasticsearch 7.2 you may optionally enable HTTP Basic authentication. To do so, include the following in your services.yaml configuration: search:type:elasticsearch:7.2disk:2048configuration:authentication:enabled:trueThat will enable mandatory HTTP Basic auth on all requests. The credentials will be available in any relationships that point at that service, in the username and password properties, respectively. This functionality is generally not required if Elasticsearch is not exposed on it own public HTTP route. However, certain applications may require it, or it allows you to safely expose Elasticsearch directly to the web. To do so, add a route to routes.yaml that has search:http as its upstream (where search is whatever you named the service in services.yaml). Plugins The Elasticsearch 2.4 and later services offer a number of plugins. To enable them, list them under the configuration.plugins key in your services.yaml file, like so: search:type: elasticsearch:7.2 disk:1024configuration:plugins:- analysis-icu- lang-pythonIn this example you’d have the ICU analysis plugin and Python script support plugin. If there is a publicly available plugin you need that is not listed here, please contact our support team. Available plugins This is the complete list of official Elasticsearch plugins that can be enabled: Plugin Description 2.4 5.2 5.4 6.5 7.2 analysis-icu Support ICU Unicode text analysis * * * * * analysis-nori Integrates Lucene nori analysis module into Elasticsearch * * analysis-kuromoji Japanese language support * * * * * analysis-smartcn Smart Chinese Analysis Plugins * * * * * analysis-stempel Stempel Polish Analysis Plugin * * * * * analysis-phonetic Phonetic analysis * * * * * analysis-ukrainian Ukrainian language support * * * * cloud-aws AWS Cloud plugin, allows storing indices on AWS S3 * delete-by-query Support for deleting documents matching a given query * discovery-multicast Ability to form a cluster using TCP/IP multicast messages * ingest-attachment Extract file attachments in common formats (such as PPT, XLS, and PDF) * * * * ingest-user-agent Extracts details from the user agent string a browser sends with its web requests * * * lang-javascript Javascript language plugin, allows the use of Javascript in Elasticsearch scripts * * lang-python Python language plugin, allows the use of Python in Elasticsearch scripts * * * mapper-annotated-text Adds support for text fields with markup used to inject annotation tokens into the index * * mapper-attachments Mapper attachments plugin for indexing common file types * * * mapper-murmur3 Murmur3 mapper plugin for computing hashes at index-time * * * * * mapper-size Size mapper plugin, enables the _size meta field * * * * * repository-s3 Support for using S3 as a repository for Snapshot/Restore * * * * Upgrading The Elasticsearch data format sometimes changes between versions in incompatible ways. Elasticsearch does not include a data upgrade mechanism as it is expected that all indexes can be regenerated from stable data if needed. To upgrade (or downgrade) Elasticsearch you will need to use a new service from scratch. There are two ways of doing that. Destructive In your services.yaml file, change the version of your Elasticsearch service and its name. Then update the name in the .platform.app.yaml relationships block. When you push that to Platform.sh, the old service will be deleted and a new one with the name name created, with no data. You can then have your application reindex data as appropriate. This approach is simple but has the downside of temporarily having an empty Elasticsearch instance, which your application may or may not handle gracefully, and needing to rebuild your index afterward. Depending on the size of your data that could take a while. Transitional For a transitional approach you will temporarily have two Elasticsearch services. Add a second Elasticsearch service with the new version a new name and give it a new relationship in .platform.app.yaml. You can optionally run in that configuration for a while to allow your application to populate indexes in the new service as well. Once you’re ready to cut over, remove the old Elasticsearch service and relationship. You may optionally have the new Elasticsearch service use the old relationship name if that’s easier for your application to handle. Your application is now using the new Elasticsearch service. This approach has the benefit of never being without a working Elasticsearch instance. On the downside, it requires two running Elasticsearch servers temporarily, each of which will consume resources and need adequate disk space. Depending on the size of your data that may be a lot of disk space.",
        "section": "Configure services",
        "subsections": " Supported versions  Deprecated versions   Relationship Usage example Authentication Plugins  Available plugins   Upgrading  Destructive Transitional    ",
        "image": "",
        "url": "/configuration/services/elasticsearch.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "01a8fa795000ebab32a6f3ec0bf49c7d",
        "title": "Extensions",
        "description": "",
        "text": " You can define the PHP extensions you want to enable or disable: # .platform.app.yamlruntime:extensions:- http- redis- ssh2disabled_extensions:- sqlite3The following extensions are enabled by default: bcmath bz2 (7.1 and later) common (7.1 and later) curl dba (7.1 and later) enchant (7.1 and later) gd interbase (7.1 and later) intl json (5.6 and later) ldap (7.1 and later) mbstring (7.1 and later) mcrypt (5.6 and earlier) mysql mysqli (not in 7.1) mysqlnd (not in 7.1) odbc (7.1 and later) openssl pdo (not in 7.1) pdo_mysql (not in 7.1) pdo_sqlite (not in 7.1) pgsql (7.1 and later) pspell (7.1 and later) readline (7.1 and later) recode (7.1 and later) snmp (7.1 and later) soap (7.1 and later) sodium (7.2) sqlite3 sockets (7.0 and later) sybase (7.1 and later) tidy (7.1 and later) xml (7.1 and later) xmlrpc (7.1 and later) zendopcache (5.4 only) / opcache (5.5 and later) zip (7.1 and later) You can disable those by adding them to the disabled_extensions list. This is the complete list of extensions that can be enabled: Extension 5.4 5.5 5.6 7.0 7.1 7.2 7.3 7.4 amqp * * * * * apc * * apcu * * * * * * * apcu_bc * * * * * applepay * * * bcmath * * * * * blackfire * * * * * * * * bz2 * * * * * calendar * * * * * ctype * * * * * curl * * * * * * * * dba * * * * * dom * * * * * enchant * * * * * * * * event * * * * exif * * * * * ffi * fileinfo * * * * * ftp * * * * * gd * * * * * * * * gearman * * * geoip * * * * * * * * gettext * * * * * gmp * * * * * * * * http * * * iconv * * * * * igbinary * * * * * imagick * * * * * * * imap * * * * * * * * interbase * * * * * * * * intl * * * * * * * * ioncube * * * json * * * * * * ldap * * * * * * * * mailparse * * * * mbstring * * * * * mcrypt * * * * * memcache * * * memcached * * * * * * * mongo * * * mongodb * * * * msgpack * * * * * mssql * * * mysql * * * mysqli * * * * * * * * mysqlnd * * * * * * * * newrelic * * * * * * oauth * * * * * odbc * * * * * * * * opcache * * * * * * * openssl pdo * * * * * * * * pdo_dblib * * * * * * * * pdo_firebird * * * * * * * * pdo_mysql * * * * * * * * pdo_odbc * * * * * * * * pdo_pgsql * * * * * * * * pdo_sqlite * * * * * * * * pdo_sqlsrv * * * * http * * pgsql * * * * * * * * phar * * * * * pinba * * * posix * * * * * propro * pspell * * * * * * * * pthreads * raphf * * readline * * * * * * * * recode * * * * * * * * redis * * * * * * * shmop * * * * * simplexml * * * * * snmp * * * * * * * * soap * * * * * sockets * * * * * sodium * * * sourceguardian * * spplus * * sqlite3 * * * * * * * * sqlsrv * * * * ssh2 * * * * * * * * sysvmsg * * * * * sysvsem * * * * * sysvshm * * * * * tideways * * * * * tideways-xhprof * * * * * tidy * * * * * * * * tokenizer * * * * * uuid * * * * wddx * * * * * xcache * * xhprof * * * xml * * * * * xmlreader * * * * * xmlrpc * * * * * * * * xmlwriter * * * * * xsl * * * * * * * * yaml * * * * zbarcode * * * * zendopcache * zip * * * * * Note: You can check out the output of ls /etc/php5/mods-available to see the up-to-date complete list of extensions after you SSH into your environment. For PHP 7, use ls /etc/php/*/mods-available. Custom PHP extensions It is possible to use an extension not listed here but it takes slightly more work: Download the .so file for the extension as part of your build hook using curl or similar. It can also be added to your Git repository if the file is not publicly downloadable, although committing large binary blobs to Git is generally not recommended. Provide a custom php.ini file in the application root (as a sibling of your .platform.app.yaml file) that loads the extension using an absolute path. For example, if the extension is named spiffy.so and is in the root of your application, you would have a php.ini file that reads: extension=/app/spiffy.so",
        "section": "PHP",
        "subsections": " Custom PHP extensions  ",
        "image": "",
        "url": "/languages/php/extensions.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "825951fcb54f90e0357f9249fad3567e",
        "title": "eZ Platform Enterprise with Fastly",
        "description": "",
        "text": " eZ Platform Enterprise is a “commercial extended” version of ez Platform that includes, among other things, support for push-based purging on the Fastly CDN. Remove Varnish configuration As of eZ Platform 1.13.5, 2.4.3 and 2.5.0, Varnish is enabled by default when deploying on Platform.sh. In order to use Fastly, Varnish must be disabled: Remove environment variable SYMFONY_TRUSTED_PROXIES:  TRUST_REMOTE  in .platform.app.yaml Remove the Varnish service in .platform/services.yaml In .platform/routes.yaml , change routes to use app instead of the varnish service you removed in previous step:  https://{default}/ : type: upstream - upstream:  varnish:http  \u0026#43; upstream:  app:http  Setting up eZ Platform to use Fastly eZ Platform’s documentation includes instructions on how to configure eZ Platform for Fastly . Follow the steps there to prepare eZ Platform for Fastly. Set credentials on Platform.sh The best way to provide the Fastly credentials and configuration to eZ Platform on Platform.sh is via environment variables. That way private credentials are never stored in Git. Using the CLI, run the following commands to set the configuration on your master environment. (Note that they will inherit to all other environments by default unless overridden.) platform variable:create -e master --level environment env:HTTPCACHE_PURGE_TYPE --value \u0026#39;fastly\u0026#39; platform variable:create -e master --level environment env:FASTLY_SERVICE_ID --value \u0026#39;YOUR_ID_HERE\u0026#39; platform variable:create -e master --level environment env:FASTLY_KEY --value \u0026#39;YOUR_ID_HERE\u0026#39; Replacing YOUR_ID_HERE with the Fastly Service ID and Key obtained from Fastly. Note: On a Platform.sh Dedicated Cluster, set those values on the production branch instead: platform variable:set -e production env:HTTPCACHE_PURGE_TYPE fastly platform variable:set -e production env:FASTLY_SERVICE_ID YOUR_ID_HERE platform variable:set -e production env:FASTLY_KEY YOUR_ID_HERE Configure Fastly and Platform.sh See the alternate Go-live process for Fastly on Platform.sh. This process is the same for any application.",
        "section": "eZ Platform",
        "subsections": " Remove Varnish configuration Setting up eZ Platform to use Fastly Set credentials on Platform.sh Configure Fastly and Platform.sh  ",
        "image": "",
        "url": "/frameworks/ez/fastly.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "8367814ee69067a64df014978d01ed92",
        "title": "Fastly",
        "description": "",
        "text": " In some cases you may want to opt to use a CDN such as Fastly rather than the Platform.sh router’s cache. Using a CDN can offer a better time-to-first-byte for cached content across a wider geographic region at the cost of the CDN service. A Fastly CDN is included for Platform.sh Dedicated instances. Platform.sh does not offer an integrated CDN on self-service Grid projects at this time, but it is a common choice for customers to self-configure. Launching a Platform.sh site with Fastly in front of it is nearly the same as launching normally. There are only two notable differences. Note that individual applications may have their own Fastly setup instructions or additional modules. Consult the documentation for your application for specific details. Set the Platform.sh domain on Fastly Rather than create a DNS CNAME for your Platform.sh master branch (for instance master-7rqtwti-qwertyqwerty.eu.platform.sh), configure Fastly to respond to requests for your domain name and to treat the Platform.sh master branch as its backend server. Be sure to enable TLS for the backend connection to Platform.sh. Then configure your DNS to point your domain at Fastly instead. See the Fastly documentation for further details. DNS TXT records If using the Fastly CDN that is included with a Platform.sh Enterprise subscription, You will need to obtain a DNS TXT record from your Customer Support Engineer prior to going live. You will need to enter that as a DNS TXT record with your domain registrar. This step should be done well in advance of the actual go-live. Anycast You have the option of using either a CNAME or a set of Anycast IP addresses . Fastly prefers that you use the CNAME but either work. If using the Anycast IP addresses on a Dedicated production environment, open a support ticket with the new A records to provide to our support team.",
        "section": "Content Delivery Networks",
        "subsections": " Set the Platform.sh domain on Fastly DNS TXT records Anycast  ",
        "image": "",
        "url": "/golive/cdn/fastly.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "2c9ba456146a67059fc31187ffd5ac78",
        "title": "First steps",
        "description": "",
        "text": " Before you take your site live, there are a few steps that will help you prepare the project. Register your domain and choose a suitable DNS provider If you plan on serving exclusively from a subdomain such as the historically common www. subdomain, you will be able use any DNS provider that supports CNAME records. If you wish to use the apex domain, eg. https://site.com, with no www. subdomain, choose one of the specialized DNS providers that allow you to use ALIAS or ANAME records . Make sure to do this before moving on to the next steps, as the CLI will reject attempts to add domains that do not allow CNAMEs. Test your routes Test your application and make sure that all of your routes are functioning as you intended. Consult the routes documentation as well to verify that your routes.yaml has been properly written. If any access restrictions have been enabled during development, be sure to remove them as well. (Optional) Obtain 3rd party SSL if needed Let’s Encrypt SSL certificates are automatically issued for Platform.sh projects at no charge to you. If you instead would like to use a 3rd party SSL certificate , make sure that you have purchased it and that it is active prior to going live. Additionally, if your application uses wildcard routes , it will require custom certificates for them, as Let’s Encrypt does not support wildcard certificates. After you have gone through the following checklist your application is ready to be taken live! Back I\u0026#39;m ready to go live",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/next-steps/going-live/first-steps.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "e3e63c152cb6e8996712109db5345431",
        "title": "GDPR Overview Page",
        "description": "",
        "text": " Platform.sh has taken numerous steps to ensure GDPR compliance. As part of our measures we have implemented the following: Data Protection Officer: Appointment of a Security Officer who also holds the Data Protection Officer (DPO) role. Data Breach Policy: We have updated our data breach policy and procedures and have reviewed that all our suppliers are compliant with breach notifications. Consent: We’ve confirmed that all of our customer communication, both business-related and marketing-related, is opt-in and no information is shared with us without a customer’s consent. Data Governance: We have internally audited all of our suppliers on their internal security and their GDPR compliance status and can confirm that our in-scope suppliers are GDPR compliant. Data Protection by design: We’ve implemented policies in the company to ensure all of our employees follow the necessary training and protocols around security. In addition, privacy protection is part of every project during instantiation. Enhanced Rights: The GDPR provides rights to individuals such as the right to portability, right of rectification, and the right to be forgotten. We’ve made sure we comply with these rights. Nearly all information can be edited through a user’s account, and we can delete accounts upon request. Personally identifiable information (PII): We’ve audited our systems to confirm that we encrypt and protect your personal data. Data Flows: We have identified data, mapped the high level data flow, and mapped data shared with vendors - including cross-border transfers. PIA: We have performed an internal Privacy Impact Analysis (PIA) using the CNIL’s PIA Software to ensure we comply with the GDPR. Security: We have created https://platform.sh/security to document our security features. Data Collection: We’ve documented information about what data we collect . Data Retention: We documented information about our data retention . DPA: We have revised our Terms of Service and Privacy Policy to align with the GDPR and we offer a pre-signed DPA agreement that can be downloaded at the top of the Privacy Policy",
        "section": "Security and compliance",
        "subsections": "",
        "image": "",
        "url": "/security/gdpr.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "d7b7e6b79418de441c413fb39a820835",
        "title": "Getting Started",
        "description": "",
        "text": " Structure your files Platform.sh is very flexible and allows you to structure your files as you wish within your Git repository, and will build your project based on how your files are organized. Here are the three build modes that can be used: Profile: Platform.sh builds your project like Drupal.org does for distributions. Project: Platform.sh builds your make file using drush make. You don’t need any Drupal core files nor any contributed modules, themes or libraries within your Git repository. Vanilla: Platform.sh builds your project as it is in your Git repository. You can push all Drupal files and contributed modules, themes or libraries. Profile mode If your repository contains a .profile file, Platform.sh builds your project in profile mode. This is similar to what Drupal.org does to build distributions. Everything you have in your repository will be copied to your profiles/[name] folder. This build mode supports having a project.make file for your contributed modules, themes or libraries. Note: When building as a profile, you need a make file for Drupal core called: project-core.make. See drush make files. .git/ project.make project-core.make my_profile.info my_profile.install my_profile.profile modules/ features/ my_feature_01/ ... custom/ my_custom_module/ ... themes/ custom/ my_custom_theme/ ... libraries/ custom/ my_custom_libraries/ ... translations/ ... Project mode If your repository doesn’t contain a .profile file, but contains a make file called: project.make (or even drupal-org.make), Platform.sh builds your project using Drush make. Everything you have in your repository will be copied to your sites/default folder. .git/ project.make modules/ features/ my_feature_01/ ... custom/ my_custom_module/ ... themes/ custom/ my_custom_theme/ ... libraries/ custom/ my_custom_libraries/ ... translations/ ... Vanilla mode In Vanilla mode, Platform.sh just takes your repository as-is without any additional reorganization. This is the behavior when there is no .make or .profile file, or when the build mode is set to none or composer rather than to drupal. It’s best to keep your docroot separate from your repository root, as that allows you to store private files outside of the docroot when needed. For example, your repository layout will likely resemble the following: .git/ private/ web/ index.php ... (other Drupal core files) sites/ all/ modules/ themes/ default/ If you already have a Drupal 7 site built from a tar.gz download from Drupal.org, this is likely the best path forward. Configuring Platform.sh for Drupal The ideal .platform.app.yaml file will vary from project project, and you are free to customize yours as needed. A recommended baseline Drupal 7 configuration is listed below, and can also be found in our Drupal 7 template project or Drupal 7 vanilla template project . Note: Your database for Drupal must be named “database” in the relationships. # This file describes an application. You can have multiple applications# in the same project.## See https://docs.platform.sh/user_guide/reference/platform-app-yaml.html# The name of this app. Must be unique within a project.name:'app'# The runtime the application uses.type:'php:7.2'# Configuration of the build of this application.build:flavor:drupal# The build-time dependencies of the app.dependencies:php: drush/drush :  ^8.0 # The relationships of the application with services or other applications.## The left-hand side is the name of the relationship as it will be exposed# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand# side is in the form `\u003cservice name\u003e:\u003cendpoint name\u003e`.relationships:database:'db:mysql'# The configuration of app when it is exposed to the web.web:# Specific parameters for different URL prefixes.locations:'/':# The folder from which to serve static assets, for this location.## This is a filesystem path, relative to the application root.root:'public'# How long to allow static assets from this location to be cached.## Can be a time in seconds, or -1 for no caching. Times can be# suffixed with  s  (seconds),  m  (minutes),  h  (hours),  d # (days),  w  (weeks),  M  (months, as 30 days) or  y  (years, as# 365 days).expires:5m# Whether to forward disallowed and missing resources from this# location to the application.## Can be true, false or a URI path string.passthru:'/index.php'# Deny access to static files in this location.allow:false# Rules for specific URI patterns.rules:# Allow access to common static Allow access to all files in the public files directory.allow:trueexpires:5mpassthru:'/index.php'root:'public/sites/default/files'# Do not execute PHP scripts.scripts:falserules:# Provide a longer TTL (2 weeks) for aggregated CSS and JS files.'^/sites/default/files/(css|js)':expires:2w# The size of the persistent disk of the application (in MB).disk:2048# The mounts that will be performed when the package is deployed.mounts:'/public/sites/default/files':source:localsource_path:'files''/tmp':source:localsource_path:'tmp''/private':source:localsource_path:'private''/drush-backups':source:localsource_path:'drush-backups'# The hooks executed at various points in the lifecycle of the application.hooks:# We run deploy hook after your application has been deployed and started.deploy:| set -ecdpublicdrush-yupdatedb# The configuration of scheduled execution.crons:drupal:spec:'*/20 * * * *'cmd:'cd public ; drush core-cron'",
        "section": "Featured frameworks",
        "subsections": " Structure your files  Profile mode Project mode Vanilla mode   Configuring Platform.sh for Drupal  ",
        "image": "",
        "url": "/frameworks/drupal7.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "ec8f3a3b9a988a75eb947e6bc77adcfb",
        "title": "Glossary",
        "description": "",
        "text": " Active Environment An environment which is deployed. You can deactivate an active environment from the environment configuration page on the Platform.sh management console. Cluster Every active environment is deployed as a cluster, that is, a collection of independent containers representing different services that make up your web application. That may include a database container, an Elasticsearch container, a container for your application, etc. They are always deployed together as a single unit. Drush Drush is a command-line shell and scripting interface for Drupal. Drush aliases Drush site aliases allow you to define short names that let you run Drush commands on specific local or remote Drupal installations. The Platform.sh CLI configures Drush aliases for you on your local environment (via platform get or platform drush-aliases). You can also configure them manually. Inactive environment An environment which is not deployed. You can activate an inactive environment from the environment configuration page on the Platform.sh management console. Live Environment An environment which is deployed from the master branch under a production plan. PaaS A Platform as a Service is an end-to-end hosting solution that includes workflow tools, APIs, and other functionality above and beyond basic hosting. The best example is Platform.sh (although we are a little biased). Production plan A subscription level which allows you to host your production website by adding a domain and a custom SSL certificate. TLS Transport Layer Security is the successor of SSL (Secure Socket Layer). It provides the cryptographic “S” in HTTPS. It’s often still referred to as SSL even though it has largely replaced SSL for online encrypted connections.",
        "section": "Platform.sh",
        "subsections": " Active Environment Cluster Drush Drush aliases Inactive environment Live Environment PaaS Production plan TLS  ",
        "image": "",
        "url": "/other/glossary.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "bdab87fc4b2a98e0864b07f5703a61c9",
        "title": "Going Live - Pre-Launch Checklist",
        "description": "",
        "text": " Before you can take your site live there are a few preparation steps to take. 1. Register a domain name with a supported provider You have a domain name registered for your site with a Registrar of your choice. The registrar must allow you to use CNAMEs for your domain. (Some registrars may call these Aliases or similar.). If your domain is currently active elsewhere, the Time-To-Live (TTL) on your domain is set to the lowest possible value in order to minimize transition time. Note: You will not be able to use a A record. Verify your DNS provider supports CNAMES. (If it does not you will want to run away from it anyway). Also you will be much happier if it supports Apex domains (more in the next chapter). 2. Test your site! Make sure your site is running and configured as you want it to be, on your master branch. In particular, see the Routes documentation . You will need your routes configured appropriately before you begin. Make sure you have turned off basic-authentication if it was turned on during development. If your production environment is on a Dedicated instance, ensure that the code is up to date in both your staging and production branches, as those are what will be mirrored to the Dedicated instances. Also ensure that the data on the production instance is up to date and ready to launch. 3. Optionally obtain a 3rd party TLS certificate Platform.sh automatically provides TLS certificates for all sites issued by Let’s Encrypt at no charge. In most cases this is sufficient and no further action is necessary. However, if you want to use a 3rd party TLS certificate to encrypt your production site you can obtain one from any number of 3rd party TLS issuers. Platform.sh does not charge for using a 3rd party TLS certificate, although the issuer may. Platform.sh supports all kinds of certificates including domain-validated certificates, extended validation (EV) certificates, high-assurance certificates and wildcard certificates. The use of HA or EV certificates is the main reason why you may wish to use a third party issuer rather than the default certificate. You will also need a custom certificate if you use wildcard routes, as Let’s Encrypt does not support wildcard certificates. If you do wish to use a 3rd party certificate, ensure it is purchased and active prior to going live. 4. Optionally configure your CDN If you are using a CDN, either one included with an Enterprise plan or one you provide for a self-service Grid project, ensure that your CDN account is registered and configured in advance. That includes setting the upstream on your CDN to point to the Platform.sh production instance. For a Grid-based project, that will be the master-xxxx domain. Run platform environment:info edge_hostname to get the domain name to use. For a Dedicated project, the upstream to use will be provided by your Platform.sh onboarding representative. Consult your CDN’s documentation for how to set the CDN’s upstream address. For Enterprise plans you may need to obtain a DNS TXT record from your Platform.sh support representative by opening a ticket. Consult the documentation for your CDN provider and our own CDN guide . Domain name is registered? Your DNS TTL is set as low as possible? Your code and data is tested and ready to launch on the master (Grid) or production (Dedicated) branch? Your custom TLS certificate is purchased, if you’re using one? Your CDN is configured to serve from Platform.sh, if you’re using one? Time to Go Live .",
        "section": "Going live",
        "subsections": " 1. Register a domain name with a supported provider 2. Test your site! 3. Optionally obtain a 3rd party TLS certificate 4. Optionally configure your CDN  ",
        "image": "",
        "url": "/golive/checklist.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "0dfa0071f415db0ea243e26092caab66",
        "title": "HTTP caching",
        "description": "",
        "text": " There are several different “levels” at which you could configure HTTP caching for your site on Platform.sh. Which one you want to use depends on your specific use case. You should use only one of these at a time and disable any others. Mixing them together will most likely result in stale and unclearable caches. The Platform.sh Router cache . Every project includes a router instance, which includes optional HTTP caching. It is reasonably configurable and obeys HTTP cache directives, but does not support push-based clearing. If you are uncertain what caching tool to use, start with this one. It is more than sufficient for the majority of use cases. A Content Delivery Network (CDN). Platform.sh is compatible with most commercial CDNs. If your Platform.sh Enterprise project has a Dedicated production environment it will typically come with the Fastly CDN . A CDN will generally offer the best performance as it is the only option that includes multiple geographic locations, but it also tends to be the most expensive. Functionality will vary widely depending on the CDN. Setup instructions for Fastly and Cloudflare are available, and will be similar for most other CDNs. Varnish . Platform.sh offers a Varnish service that you can declare as part of your application and insert between the router and your application. Performance will be roughly comparable to the Router cache. Varnish is more configurable than the Router cache as you are able to customize your VCL file, but make sure you are comfortable with Varnish configuration. Platform.sh does not provide assistance with VCL configuration and a misconfiguration may cause difficult to debug behavior. Generally speaking, you should use Varnish only if your application requires push-based clearing or relies on Varnish-specific business logic. Application-specific caching. Many web applications and frameworks include a built-in web cache layer that mimics what Varnish or the Router cache would do. Most of the time they will be slower than a dedicated caching service as they still require invoking the application server, and only serve as a fallback for users that do not have a dedicated caching service available. Generally speaking the only reason to use an application-specific web cache is if it includes some application-specific business logic that you depend on, such as application-sensitive selective cache clearing or partial page caching. Note that this refers only to HTTP level caching. Many applications have an internal application cache for data objects or similar. That should remain active regardless of the HTTP cache in use. Cookies and caching HTTP-based caching systems generally default to including cookie values in cache keys so as to avoid serving authenticated content to the wrong user. While a safe default, it also has the side effect that any cookie will effectively disable the cache, including mundane cookies like analytics. The solution is to specifically allow the cookies that should impact the cache and include only the application session cookie(s). For the Router cache see our documentation . For other cache systems consult their documentation.",
        "section": "Best practices",
        "subsections": " Cookies and caching  ",
        "image": "",
        "url": "/bestpractices/http-caching.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "e45bed09f02f1fe3cf547fdc980076dd",
        "title": "HTTPS",
        "description": "",
        "text": " Let’s Encrypt All environments on Platform.sh support both HTTP and HTTPS automatically. Production SSL certificates are provided by Let’s Encrypt . You may alternatively provide your own SSL certificate from a 3rd party issuer of your choice at no charge from us. Note: Let’s Encrypt certificate renewals are attempted each time your environment is deployed. If your project does not receive regular code commits, you will need to manually issue a re-deployment to ensure the certificate remains valid. We suggest that you do so when your project doesn’t receive any updates for over 1 month. This can be done by pushing a code change via git or issuing the following command from your local environment: platform redeploy Alternatively, see the section below on automatically redeploying the site in order to renew the certificate. Platform.sh recommends using HTTPS requests for all sites exclusively. Doing so provides better security, access to certain features that web browsers only permit over HTTPS, and access to HTTP/2 connections on all sites which can greatly improve performance. How HTTPS redirection is handled depends on the routes you have defined. Platform.sh recommends specifying all HTTPS routes in your routes.yaml file. That will result in all pages being served over SSL, and any requests for an HTTP URL will automatically be redirected to HTTPS.  https://{default}/ :type:upstreamupstream: app:http  https://www.{default}/ :type:redirectto: https://{default}/ Specifying only HTTP routes will result in duplicate HTTPS routes being created automatically, allowing the site to be served from both HTTP and HTTPS without redirects. Although Platform.sh does not recommend it, you can also redirect HTTPS requests to HTTP explicitly to serve the site over HTTP only. The use cases for this configuration are few.  http://{default}/ :type:upstreamupstream: app:http  http://www.{default}/ :type:redirectto: http://{default}/  https://{default}/ :type:redirectto: http://{default}/  https://www.{default}/ :type:redirectto: http://{default}/ Of course, more complex routing logic is possible if the situation calls for it. However, we recommend defining HTTPS routes exclusively. TLS configuration Optionally, it’s possible to further refine how secure TLS connections are handled on your cluster via the tls route property. https://{default}/:type:upstreamupstream:app:httptls:# ...min_version Note: This directive was put into place when Platform.sh supported older versions of TLS for customers. Currently only TLS v1.2 is supported. Support for TLS v1.3 will be added in the near future. Setting a minimum version of TLS will cause the server to automatically reject any connections using an older version of TLS. Rejecting older versions with known security vulnerabilities is necessary for some security compliance processes. tls:min_version:TLSv1.2The above configuration will result in requests using older TLS versions to be rejected. Legal values are TLSv1.2. Note that if multiple routes for the same domain have different min_versions specified, the highest specified will be used for the whole domain. strict_transport_security HTTP Strict Transport Security (HSTS) is a mechanism for telling browsers to use HTTPS exclusively with a particular website. You can toggle it on for your site at the router level without having to touch your application, and configure it’s behavior from routes.yaml. tls:strict_transport_security:enabled:trueinclude_subdomains:truepreload:trueThere are three sub-properties for the strict_transport_security property: enabled: Can be true, false, or null. Defaults to null. If false, the other properties wil be ignored. include_subdomains: Can be true or false. Defaults to false. If true, browsers will be instructed to apply HSTS restrictions to all subdomains as well. preload: Can be true or false. Defaults to false. If true, Google and others may add your site to a lookup reference of sites that should only ever be connected to over HTTPS. Many although not all browsers will consult this list before connecting to a site over HTTP and switch to HTTPS if instructed. Although not part of the HSTS specification it is supported by most browsers. If enabled, the Strict-Transport-Security header will always be sent with a lifetime of 1 year. The Mozilla Developer Network has more detailed information on HSTS. Note: If multiple routes for the same domain specify different HSTS settings, the entire domain will still use a shared configuration. Specifically, if any route on the domain has strict_transport_security.enabled set to false, HSTS will be disabled for the whole domain. Otherwise, it will be enabled for the whole domain if at least one such route has enabled set to true. As this logic may be tricky to configure correctly we strongly recommend picking a single configuration for the whole domain and adding it on only a single route. Client authenticated TLS In some non-browser applications (such as mobile applications, IoT devices, or other restricted-client-list use cases), it is beneficial to restrict access to selected devices using TLS. This process is known as client-authenticated TLS, and functions effectively as a more secure alternative to HTTP Basic Auth. By default, any valid SSL cert issued by one of the common certificate issuing authorities will be accepted. Alternatively, you can restrict access to SSL certs issued by just those certificate authorities you specify, including a custom authority. (The latter is generally only applicable if you are building a mass-market IoT device or similar.) To do so, set client_authentication required and then provide a list of the certificates of the certificate authorities you wish to allow. tls:client_authentication: require client_certificate_authorities:- !includetype:stringpath:root-ca1.crt- !includetype:stringpath:root-ca2.crtIn this case, the certificate files are resolved relative to the .platform directory. Alternatively, the certificates can be specified inline in the file: tls:client_authentication: require client_certificate_authorities:- | -----BEGIN CERTIFICATE-----### Several lines of random characters here ###-----ENDCERTIFICATE----- - |-----BEGINCERTIFICATE----- ### Several lines of different random characters here ###-----ENDCERTIFICATE-----Automated SSL certificate renewal using Cron If the Let’s Encrypt certificate is due to expire in less than one month then it will be renewed automatically during a deployment. That makes it feasible to set up regular auto-renewal of the Let’s Encrypt certificate. The caveat is that, like any deploy, there is a very brief downtime (a few seconds, usually) so it’s best to do during off-hours. You will first need to install the CLI in your application container. See the section on API tokens for instructions on how to do so. Note: Automated SSL certificate renewal using cron requires you to get an API token and install the CLI in your application container. Once the CLI is installed in your application container and an API token configured you can add a cron task to run twice a month to trigger a redeploy. For example: crons:renewcert:# Force a redeploy at 10 am (UTC) on the 1st and 15th of every month.spec:\u0026#39;0 10 1,15 * *\u0026#39;cmd:| if [  $PLATFORM_BRANCH  = master ]; thenplatformredeploy--yes--no-waitfiThe above cron task will run on the 1st and 15th of the month at 10 am (UTC), and, if the current environment is the master branch, it will run platform redeploy on the current project and environment. The --yes flag will skip any user-interaction. The --no-wait flag will cause the command to complete immediately rather than waiting for the redeploy to complete. We recommend adjusting the cron schedule to whenever is off-peak time for your site, and to random days within the month. Warning: It is very important to include the --no-wait flag. If you do not, the cron process will block waiting on the deployment to finish, but the deployment will be blocked by the running cron task. That will take your site offline until you log in and manually terminate the running cron task. You want the --no-wait flag. We’re not joking. The certificate will not renew unless it has less than one month remaining; trying twice a month is sufficient to ensure a certificate is never less than 2 weeks from expiring. As the redeploy does cause a momentary pause in service we recommend running during non-peak hours for your site. Let’s Encrypt limits and branch names You may encounter Let’s Encrypt certificates failing to provision after the build hook has completed: Provisioning certificates Validating 2 new domains E: Error provisioning the new certificate, will retry in the background. (Next refresh will be at 2020-02-13 14:29:22.860563\u0026#43;00:00.) Environment certificates W: Missing certificate for domain www.\u0026lt;PLATFORM_ENVIRONMENT\u0026gt;-\u0026lt;PLATFORM_PROJECT\u0026gt;.\u0026lt;REGION\u0026gt;.platformsh.site W: Missing certificate for domain \u0026lt;PLATFORM_ENVIRONMENT\u0026gt;-\u0026lt;PLATFORM_PROJECT\u0026gt;.\u0026lt;REGION\u0026gt;.platformsh.site One reason that this can happen has to do with the limits of Let’s Encrypt itself, which caps off at 64 characters for URLS. If your TLS certificates are not being provisioned, it’s possible that the names of your branches are too long, and the environment’s generated URL goes over that limit. At this time, generated URLs have the following pattern: \u0026lt;PLATFORM_ENVIRONMENT\u0026gt;-\u0026lt;PLATFORM_PROJECT\u0026gt;.\u0026lt;REGION\u0026gt;.platformsh.site PLATFORM_ENVIRONMENT = PLATFORM_BRANCH \u0026#43; 7 character hash PLATFORM_PROJECT = 13 characters REGION = 2-4 characters, depending on the region platformsh.site = 15 characters extra characters (. \u0026amp; -) = 4 characters This breakdown leaves you with 21-23 characters to work with naming your branches (PLATFORM_BRANCH) without going over the 64 character limit, dependent on the region. Since this pattern for generated URLs will remain similar, but could change slightly over time, it’s our recommendation to use branch names with a maximum length between 15 and 20 characters.",
        "section": "Configure routes",
        "subsections": " Let\u0026rsquo;s Encrypt TLS configuration  min_version strict_transport_security Client authenticated TLS   Automated SSL certificate renewal using Cron Let\u0026rsquo;s Encrypt limits and branch names  ",
        "image": "",
        "url": "/configuration/routes/https.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "93169d1b8f69ad2ffa3d7078ea48ab26",
        "title": "Java featured frameworks",
        "description": "",
        "text": " Hibernate Hibernate ORM is an object-relational mapping tool for the Java programming language. It provides a framework for mapping an object-oriented domain model to a relational database. Hibernate handles object-relational impedance mismatch problems by replacing direct, persistent database accesses with high-level object handling functions. Hibernate Best Practices Jakarta EE/ Eclipse MicroProfile Eclipse MicroProfile is a semi-new community dedicated to optimizing the Enterprise Java mission for microservice-based architectures. Now Enterprise Java has been standardized under the Eclipse Foundation as Jakarta EE . Jakarta EE/ Eclipse MicroProfile Best Practices Templates Apache Tomee Thorntail KumuluzEE Helidon Open Liberty Payara Payara Micro is an Open Source, lightweight Java EE (Jakarta EE) microservices deployments. Templates Payara Micro References Article Links Search Source NoSQL Source JPA Source Hello World Source Quarkus QuarkusIO , the Supersonic Subatomic Java, promises to deliver small artifacts, extremely fast boot time, and lower time-to-first-request. Templates Quarkus References Article Links Panache MongoDB Source Command Mode Application Source Hibernate Search With Elasticsearch Source PostgreSQL With Panache Source PostgreSQL with JPA Source Hello World Source Spring The Spring Framework provides a comprehensive programming and configuration model for modern Java-based enterprise applications - on any kind of deployment platform. Platform.sh is flexible, and allows you to use Spring Framework in several flavors such as Spring MVC and Spring Boot . Spring Best Practices Templates Spring Boot MySQL Spring Boot MongoDB References Article Link Spring Data MongoDB Reactive Source Spring Webflux Source Spring Data Redis Source Spring with Gradle Source Spring Data ElasticSearch and Spring Data Sorl Elasticsearch and Solr Spring MVC and Spring Data MongoDB Source Spring Boot and Spring Data JPA Source Tomcat Apache Tomcat is an open-source implementation of the Java Servlet, JavaServer Pages, Java Expression Language and WebSocket technologies. Note: By default, Spring Boot provides an embedded Apache Tomcat build. Therefore, if you want to use Tomcat with Spring, check the Spring section. Templates Tomcat Micronaut Micronaut is a modern, JVM-based, full-stack framework for building modular, easily testable microservice and serverless applications. Templates micronaut",
        "section": "Java",
        "subsections": " Hibernate Jakarta EE/ Eclipse MicroProfile  Templates   Payara  Templates References   Quarkus  Templates References   Spring  Templates References   Tomcat  Templates   Micronaut  Templates    ",
        "image": "",
        "url": "/languages/java/frameworks.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "3ff5ef8902e64cdc39a6e4a5a4aa05a3",
        "title": "Local development",
        "description": "",
        "text": " Now that you have a project on Platform.sh, it would be helpful to run the same build process on your local machine so that you can develop and test new features before pushing them. This guide will take you through the steps of connecting remotely to your services and building your application locally. These steps assume that you have already: Signed up for a free trial account with Platform.sh. Started either a template project or pushed your own code to Platform.sh. If you have not completed these steps by now, click the links and do so before you begin. Get started!",
        "section": "Introduction",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/developing/local-development.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "f1ae5aa6b6be802f122ea30d75cf86f5",
        "title": "Name",
        "description": "",
        "text": " The name is the unique identifier of the application. Platform.sh supports multiple applications within a project, so each application must have a unique name within a project. The name may only be composed of lower case alpha-numeric characters (a-z0-9). Warning: Changing the name of your application after it has been deployed will destroy all storage volumes and result in the loss of all persistent data. This is typically a Very Bad Thing to do. It could be useful under certain circumstances in the early stages of development but you almost certainly don’t want to change it on a live project. This name is used in the .platform/routes.yaml file to define the HTTP upstream (by default php:http). For instance, if you called your application app you will need to use app:http in the upstream field. You can also use this name in multi-application relationships.",
        "section": "Configure your application",
        "subsections": "",
        "image": "",
        "url": "/configuration/app/name.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "7a8b1a9a655fb8e138742eb8038513d7",
        "title": "Open a free trial account",
        "description": "",
        "text": " The best way to understand a tool is to use it. That’s why Platform.sh offers a free one month trial. Visit the Platform.sh accounts page and fill out your information to set up your trial account. Alternatively, you can sign up using an existing GitHub, Bitbucket, or Google account. If you choose this option, you will be able to set a password for your Platform.sh account later. Back I\u0026#39;ve set up my free trial account",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/free-trial.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "ee34d2b27ae3be96f8d1ff03aa161cd6",
        "title": "Open a free trial account",
        "description": "",
        "text": " The best way to understand a tool is to use it. That’s why Platform.sh offers a free one month trial.Visit the Platform.sh accounts page and fill out your information to set up your trial account. Alternatively, you can sign up using an existing GitHub, Bitbucket, or Google account. If you choose this option, you will be able to set a password for your Platform.sh account later. Back I\u0026#39;ve made a free trial account",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/template/free-trial.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "df0211a5f4bc39c19839d83d8413b0ce",
        "title": "Platform.sh Dedicated",
        "description": "",
        "text": " Platform.sh Dedicated is a robust, redundant layer on top of Platform.sh Professional. It is well-suited for those who like the Platform.sh development experience but need more resources and redundancy for their production environment. It is available only with an Enterprise contract. Platform.sh Dedicated consists of two parts: The Development Environment and the Dedicated Cluster. The Development Environment The Development Environment is a normal Platform.sh Grid account, with all of the capabilities and workflows of Platform.sh Professional. The one difference is that the master branch will not be associated with a domain and thus will never be “production”. The Dedicated Cluster The Dedicated Cluster is a three-Virtual Machine redundant configuration provisioned by Platform.sh for each customer. Every service is replicated across all three virtual machines in a failover configuration (as opposed to sharding), allowing a site to remain up even if one of the VMs is lost entirely. The build process for your application is identical for both the Development Environment and the Dedicated Cluster. However, because the VMs are provisioned by Platform.sh, not as a container, service configuration must be done by Platform.sh’s Customer Success team. By and large the same flexibility is available but only via filing a support ticket.",
        "section": "Dedicated",
        "subsections": " The Development Environment The Dedicated Cluster  ",
        "image": "",
        "url": "/dedicated/overview.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "b9e86538ace39448a5d7ff6d29489b3b",
        "title": "Project configuration",
        "description": "",
        "text": " You can access the project-wide configuration settings by selecting the project from your list of projects, then click the Settings tab at the top of the screen. General From the first page of the project settings, General, you can update the project name, or navigate to other project settings options on the left side of the screen. Access The Access screen allows you to manage users’ access on your project. You can invite new users to your project by clicking the Add button and entering their email address, or modify permissions of existing users by clicking the Edit link when hovering over the user. Note: Currently, permissions changes that grant or revoke SSH access to an environment take effect only after the next time that environment is deployed. Selecting a user will allow you to either edit that user’s permissions or delete the user’s access to the project entirely. If you check the Project admin box, this user will be an administrator of the project and will have fulll access on all environments. If you uncheck the box, you’ll have the option of adjusting the user’s permissions on each environment. Note: The Account owner is locked and you can’t change its permissions. Domains The Domains screen allows you to manage your domains that your project will be accessible at. More information on how to setup your domain . Note: Platform.sh expects an ASCII representation of the domain here. In case you want to use an internationalized domain name you can use the conversion tool provided by Verisign to convert your IDN domain to ASCII. Certificates The Certificates screen allows you to manage your project’s TLS certificates that enable HTTPS. You can view current certificates by hovering over one on the list and clicking the View link that appears, or you can add a new certificate by clicking the Add button a the top of the page. All projects get TLS certificates provided by Let’s Encrypt automatically. In most cases no user action is required. You will only need to add certificates on this page if you are using TLS certificates provided by a third party. Deploy Key The Deploy Key page provides the SSH key that Platform.sh will use when trying to access external private Git repository during the build process. This is useful if you want to reuse some code components across multiple projects and manage those components as dependencies of your project. Variables The Variables screen allows you to define the variables that will be available project-wide - that is, in each environment. It also allows you define variables that will be available during the build process.",
        "section": "Management console",
        "subsections": " General Access Domains Certificates Deploy Key Variables  ",
        "image": "",
        "url": "/administration/web/configure-project.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "0a622b40cc128ffe7b6d8edc89168152",
        "title": "Project templates",
        "description": "",
        "text": " You can initialize your projects using any of our pre-made template repositories. You can click the Deploy on Platform.sh button to launch a new project using a template, or you can visit and clone the repository and push to an empty project you have created using the CLI or in the management console. C#/.NET Core View the C#/.NET Core documentation . ASP.NET Core ASP.NET Core This template builds the ASP.NET Core framework for Platform.sh. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. ASP.NET Core is an open-source and cross-platform .NET framework for building modern cloud-based web applications. Services: .NET 2.2 MariaDB 10.2 Redis 5.0 View the repository on GitHub. Go View the Go documentation . Basic Go Basic Go This template provides the most basic configuration for running a custom Go project. Go is a statically typed, compiled language with an emphasis on easy concurrency and network services. Services: Go 1.14 MariaDB 10.2 View the repository on GitHub. Beego Beego This template builds the Beego framework for Platform.sh. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. Beego is a popular web framework written in Go. Services: Go 1.14 MariaDB 10.2 View the repository on GitHub. Echo Echo This template builds the Echo framework for Platform.sh. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. Echo is a lightweight, minimalist web framework written in Go. Services: Go 1.14 MariaDB 10.2 View the repository on GitHub. Gin Gin This template builds the Gin framework for Platform.sh. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. Gin is a lightweight web framework written in Go that emphasizes performance. Services: Go 1.14 MariaDB 10.2 View the repository on GitHub. Hugo Hugo This template provides a basic Hugo skeleton. All files are generated at build time, so at runtime only static files need to be served. Hugo is a static site generator written in Go, using Go\u0026#39;s native template packages for formatting. Services: Go 1.14 View the repository on GitHub. Mattermost Mattermost This template builds Mattermost on Platform.sh, configuring the deployment through user-defined environment variables. Mattermost is an open-source messaging framework written in Go and React. Services: Go 1.14 PostgreSQL 12 Elasticsearch 7.2 View the repository on GitHub. Java View the Java documentation . Apache Tomcat Apache Tomcat This project provides a starter kit for Apache Tomcat hosted on Platform.sh. Apache Tomcat is an open-source implementation of the Java Servlet, JavaServer Pages, Java Expression Language and WebSocket technologies. Services: Java 8 Maven Eclipse MicroProfile Apache Tomcat View the repository on GitHub. Apache TomEE Apache TomEE This project provides a starter kit for Apache TomEE Eclipse MicroProfile projects hosted on Platform.sh. Apache TomEE is the Eclipse MicroProfile implementation that uses several Apache Project flavors such as Apache Tomcat, Apache OpenWebBeans and so on. Services: Java 8 Maven Eclipse MicroProfile Apache TomEE View the repository on GitHub. Helidon Helidon This project provides a starter kit for Helidon Eclipse MicroProfile projects hosted on Platform.sh. Helidon is a collection of Java libraries for writing microservices that run on a fast web core powered by Netty. Helidon is designed to be simple to use, with tooling and examples to get you going quickly. Since Helidon is just a collection of libraries running on a fast Netty core, there is no extra overhead or bloat. Services: Java 8 Maven Eclipse MicroProfile Helidon View the repository on GitHub. Jenkins Jenkins This project provides a starter kit for Jenkins projects hosted on Platform.sh. Jenkins is an open source automation server written in Java. Jenkins helps to automate the non-human part of the software development process, with continuous integration and facilitating technical aspects of continuous delivery. Services: Java 8 Jenkins View the repository on GitHub. Jetty Jetty Eclipse Jetty provides a Web server and javax.servlet container, plus support for HTTP/2, WebSocket, OSGi, JMX, JNDI, JAAS and many other integrations. These components are open source and available for commercial use and distribution. Eclipse Jetty is used in a wide variety of projects and products, both in development and production. Jetty can be easily embedded in devices, tools, frameworks, application servers, and clusters. Services: Java 8 Maven Eclipse Jetty View the repository on GitHub. KumuluzEE KumuluzEE This project provides a starter kit for KumuluzEE Eclipse MicroProfile projects hosted on Platform.sh. KumuluzEE is a lightweight framework for developing microservices using standard Java, Java EE / Jakarta EE technologies and migrating existing Java applications to microservices. KumuluzEE packages microservices as standalone JARs. KumuluzEE microservices are lightweight and optimized for size and start-up time. Services: Java 8 Maven Eclipse MicroProfile KumuluzEE View the repository on GitHub. Micronaut Micronaut This project provides a starter kit for Micronaut projects hosted on Platform.sh. Micronaut is a modern, JVM-based, full-stack framework for building modular, easily testable microservice and serverless applications. Services: Java 8 Maven Micronaut View the repository on GitHub. Open Liberty Open Liberty This project provides a starter kit for Open Liberty Eclipse MicroProfile projects hosted on Platform.sh. Open Liberty is a highly composable, fast to start, dynamic application server runtime environment. Services: Java 8 Maven Eclipse MicroProfile Open Liberty View the repository on GitHub. Payara Micro Payara Micro This project provides a starter kit for Payara Micro projects hosted on Platform.sh. Payara Micro is an Open Source, lightweight Java EE (Jakarta EE) microservices deployments. Services: Java 8 Maven Eclipse MicroProfile Payara Micro View the repository on GitHub. Quarkus Quarkus QuarkusIO, the Supersonic Subatomic Java, promises to deliver small artifacts, extremely fast boot time, and lower time-to-first-request. Services: Java 8 Maven Eclipse MicroProfile Quarkus View the repository on GitHub. Spring Boot, Gradle, Mysql Spring Boot, Gradle, Mysql This project provides a starter kit for Spring Boot Gradle with MySQL projects hosted on Platform.sh. The Spring Framework is an application framework and inversion of control container for the Java platform. Services: Java 8 Gradle Spring Boot Oracle MySQL 8.0 View the repository on GitHub. Spring Boot, Maven, Mysql Spring Boot, Maven, Mysql This project provides a starter kit for Spring Boot Maven with MySQL projects hosted on Platform.sh. The Spring Framework is an application framework and inversion of control container for the Java platform. Services: Java 8 Maven Spring Boot Oracle MySQL 8.0 View the repository on GitHub. Spring MVC, Maven, MongoDB Spring MVC, Maven, MongoDB This project provides a starter kit for Spring MVC Maven with MongoDB projects hosted on Platform.sh. The Spring Framework is an application framework and inversion of control container for the Java platform. Services: Java 8 Maven Spring MVC MongoDB View the repository on GitHub. Spring, Kotlin, Maven Spring, Kotlin, Maven This project provides a starter kit for Spring Boot Maven with Kotlin projects hosted on Platform.sh. The Spring Framework is an application framework and inversion of control container for the Java platform. Services: Java 8 View the repository on GitHub. Thorntail Thorntail This project provides a starter kit for Thorntail Eclipse MicroProfile projects hosted on Platform.sh. Thorntail offers an innovative approach to packaging and running Java EE applications by packaging them with just enough of the server runtime to  java -jar  your application. It\u0026#39;s MicroProfile compatible, too. Services: Java 8 Maven Eclipse MicroProfile Thorntail View the repository on GitHub. xwiki xwiki This project provides a starter kit for XWiki projects hosted on Platform.sh. XWiki is a free wiki software platform written in Java with a design emphasis on extensibility. XWiki is an enterprise wiki. It includes WYSIWYG editing, OpenDocument based document import/export, semantic annotations and tagging, and advanced permissions management. Services: Java 8 View the repository on GitHub. Lisp View the Lisp documentation . Lisp Hunchentoot Lisp Hunchentoot This template is a simple Lisp Hunchentoot web server on Platform.sh. It includes a minimalist application for demonstration, but you are free to alter it as needed. Hunchentoot is a web server written in Common Lisp and at the same time a toolkit for building dynamic websites. Services: Lisp 1.5 View the repository on GitHub. Node.js View the Node.js documentation . Express Express This template builds the Express framework for Platform.sh. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. Express is a minimalist web framework written in Node.js. Services: Node.js 10 MariaDB 10.2 View the repository on GitHub. Gatsby Gatsby This template builds a simple application using Gatsby hosted on Platform.sh. Gatsby is a free and open source framework based on React that helps developers build blazing fast websites and apps. Services: Node.js View the repository on GitHub. Gatsby with Wordpress Gatsby with Wordpress This template builds a multi-app project using Gatsby as its frontend and a Wordpress to store content. Gatsby is a free and open source framework based on React that helps developers build statically-generated websites and apps, and WordPress is a blogging and lightweight CMS written in PHP. Services: Node.js 12 PHP 7.3 MariaDB 10.4 View the repository on GitHub. Koa Koa This template builds a Koa project on Platform.sh. Koa is a lightweight web microframework for Node.js. Services: Node.js 10 MariaDB 10.2 View the repository on GitHub. Node.js Node.js This template builds a simple application using the Node.js built-in `http` web server. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. Node.js is an open-source JavaScript runtime built on Chrome\u0026#39;s V8 JavaScript engine. Services: Node.js 10 MariaDB 10.4 View the repository on GitHub. Probot Probot This template builds a simple GitHub App using Probot. Probot is a framework for building GitHub Apps in Node.js. Services: Node.js 12 View the repository on GitHub. strapi strapi This template builds a Strapi backend for Platform.sh. It does not include a frontend application, but you can add one of your choice and access Strapi by defining it in a relationship in your frontend\u0026#39;s .platform.app.yaml file. Strapi is a Headless CMS framework written in Node.js. Services: Node.js 12 PostgreSQL 11 View the repository on GitHub. PHP View the PHP documentation . Backdrop Backdrop This template builds a Backdrop site, with the entire site committed to Git. Backdrop is a PHP-based CMS, originally forked from Drupal 7. Services: PHP 7.3 MariaDB 10.4 View the repository on GitHub. Basic PHP Basic PHP This template provides the most basic configuration for running a custom PHP project. PHP is a high-performance scripting language especially well suited to web development. Services: PHP 7.3 MariaDB 10.2 View the repository on GitHub. Drupal 8 Drupal 8 This template builds Drupal 8 using the  Drupal Recommended  Composer project. It also includes configuration to use Redis for caching, although that must be enabled post-install in `.platform.app.yaml`. Drupal is a flexible and extensible PHP-based CMS framework. Services: PHP 7.4 MariaDB 10.4 Redis 5.0 View the repository on GitHub. Drupal 8 Multisite Drupal 8 Multisite This template builds Drupal 8 in a multisite configuration using the  Drupal Recommended  Composer project. It also includes configuration to use Redis for caching, although that must be enabled post-install per-site. Drupal is a flexible and extensible PHP-based CMS framework capable of hosting multiple sites on a single code base. Services: PHP 7.4 MariaDB 10.4 Redis 5.0 View the repository on GitHub. Drupal 9 Drupal 9 This template builds Drupal 9 Beta using the  Drupal Recommended  Composer project. It also includes configuration to use Redis for caching, although that must be enabled post-install in `.platform.app.yaml`. Drupal is a flexible and extensible PHP-based CMS framework. Services: PHP 7.4 MariaDB 10.4 Redis 5.0 View the repository on GitHub. GovCMS 8 GovCMS 8 This template builds the Australian government\u0026#39;s GovCMS Drupal 8 distribution using the Drupal Composer project for better flexibility. It also includes configuration to use Redis for caching, although that must be enabled post-install in .platform.app.yaml. GovCMS is a Drupal distribution built for the Australian government, and includes configuration optimized for managing government websites. Services: PHP 7.2 MariaDB 10.2 Redis 5.0 View the repository on GitHub. Laravel Laravel This template provides a basic Laravel skeleton. It comes pre-configured to use a MariaDB database and Redis for caching and sessions. Laravel is an opinionated, integrated rapid-application-development framework for PHP. Services: PHP 7.3 MariaDB 10.2 Redis 5.0 View the repository on GitHub. Magento 2 Community Edition Magento 2 Community Edition This template builds Magento 2 CE on Platform.sh. It includes additional scripts to customize Magento to run effectively in a build-and-deploy environment. Magento is a fully integrated ecommerce system and web store written in PHP. This is the Open Source version. Services: PHP 7.2 MariaDB 10.2 Redis 3.2 View the repository on GitHub. Mautic Mautic TThis template provides a basic Mautic installation. Mautic is an Open SOurce marketing automation tool built on Symfony. Services: PHP 7.2 MariaDB 10.4 RabbitMQ 3.7 View the repository on GitHub. Nextcloud Nextcloud This template builds Nextcloud on Platform.sh. Nextcloud is a PHP-based groupware server with installable apps, file synchronization, and federated storage. An adminstrative user will be created automatically. See the deploy log after the project is installed for the name and password. Services: PHP 7.4 MariaDB 10.4 Redis 5.0 View the repository on GitHub. Opigno Opigno This template builds the Opigno Drupal 8 distribution using the Drupal Composer project for better flexibility. It also includes configuration to use Redis for caching, although that must be enabled post-install in .platform.app.yaml. Opigno is a Learning Management system built as a Drupal distribution. Services: PHP 7.3 MariaDB 10.4 Redis 5.0 View the repository on GitHub. Pimcore Pimcore Pimcore Digital Platform for Enterprises Services: PHP 7.4 MariaDB 10.2 Redis 5.0 View the repository on GitHub. Sculpin Sculpin This template provides a basic Sculpin skeleton. All files are generated at build time, so at runtime only static files need to be served. Sculpin is a static site generator written in PHP and using the Twig templating engine. Services: PHP 7.3 View the repository on GitHub. Symfony 3 Symfony 3 This template provides a basic Symfony 3 skeleton. It is configured for Production mode by default so the usual Symfony  welcome  page will not appear. Symfony is a high-performance loosely-coupled PHP web development framework. Version 3 is the legacy support version. Services: PHP 7.2 MariaDB 10.2 View the repository on GitHub. Symfony 4 Symfony 4 This template provides a basic Symfony 4 skeleton. It is configured for Production mode by default so the usual Symfony  welcome  page will not appear. That can be adjusted in .platform.app.yaml. Symfony is a high-performance loosely-coupled PHP web development framework. Services: PHP 7.3 MariaDB 10.4 View the repository on GitHub. Symfony 5 Symfony 5 This template provides a basic Symfony 5 skeleton. It is configured for Production mode by default so the usual Symfony  welcome  page will not appear. That can be adjusted in .platform.app.yaml. Symfony is a high-performance loosely-coupled PHP web development framework. Services: PHP 7.3 MariaDB 10.4 View the repository on GitHub. TYPO3 TYPO3 This template provides a basic TYPO3 installation. TYPO3 is a PHP-based Content Management System Services: PHP 7.4 MariaDB 10.4 Redis 5.0 View the repository on GitHub. Wordpress Wordpress This template builds WordPress on Platform.sh using the johnbolch/wordpress  Composer Fork  of WordPress. Plugins and themes should be managed with Composer exclusively. WordPress is a blogging and lightweight CMS written in PHP. Services: PHP 7.3 MariaDB 10.2 View the repository on GitHub. Python View the Python documentation . Basic Python 2 Basic Python 2 This template provides the most basic configuration for running a custom Python 2.7 project. It launches a Python application directly rather than using a runner. Python is a general purpose scripting language often used in web development. Services: Python 2 MariaDB 10.2 Redis 5 View the repository on GitHub. Basic Python 3 Basic Python 3 This template provides the most basic configuration for running a custom Python 3.7 project. It launches a Python application directly rather than using a runner. Python is a general purpose scripting language often used in web development. Services: Python 3 MariaDB 10.2 Redis 5 View the repository on GitHub. Django 1 Django 1 This template builds Django 1 on Platform.sh, using the gunicorn application runner. New projects should be built using Django 2, but this project is a reference for existing migrating sites. Django is a Python-based web application framework with a built-in ORM. Version 1 is the legacy support version. Services: Python 2 PostgreSQL 10 View the repository on GitHub. Django 2 Django 2 This template builds Django 2 on Platform.sh, using the gunicorn application runner. Django is a Python-based web application framework with a built-in ORM. Services: Python 3.7 PostgreSQL 10 View the repository on GitHub. Django 3 Django 3 This template builds Django 3 on Platform.sh, using the gunicorn application runner. Django is a Python-based web application framework with a built-in ORM. Services: Python 3.7 PostgreSQL 10 View the repository on GitHub. Flask Flask This template builds a Flask project on Platform.sh, run natively without a separate runner. Flask is a lightweight web microframework for Python. Services: Python 3.7 MariaDB 10.2 Redis 5.0 View the repository on GitHub. MoinMoin MoinMoin This template builds a Moin Moin wiki on Platform.sh. The project doesn\u0026#39;t include Moin Moin itself; rather, it includes build and deploy scripts that will download Moin Moin on the fly. Moin Moin is a Python-based Wiki system that uses flat files on disk for storage. Services: Python 2 View the repository on GitHub. Pelican Pelican This template provides a basic Pelican skeleton. All files are generated at build time, so at runtime only static files need to be served. Pelican is a static site generator written in Python and using Jinja for templating. Services: Python 3.7 View the repository on GitHub. Pyramid Pyramid This template builds Pyramid on Platform.sh. It includes some basic example code to demonstrate how to connect to the database. Pyramid is a web framework written in Python. Services: Python 3.7 View the repository on GitHub. Python 3 running UWSGI Python 3 running UWSGI This template provides the most basic configuration for running a custom Python 3.7 project. It launches the application using the UWSGI application runner. Python is a general purpose scripting language often used in web development. Services: Python 3.7 MariaDB 10.2 Redis 5.0 View the repository on GitHub. Wagtail Wagtail This template builds the Wagtail CMS on Platform.sh, using the gunicorn application runner. Wagtail is a web CMS built using the Django framework for Python. Services: Python 3.7 PostgreSQL 10 View the repository on GitHub. Ruby View the Ruby documentation . Ruby on Rails Ruby on Rails This template builds Ruby on Rails 5 on Platform.sh. It includes a bridge library that will auto-configure most databases and services. Rails is an opinionated rapid application development framework written in Ruby. Services: Ruby 2.6 Postgresql 11 View the repository on GitHub.",
        "section": "Development",
        "subsections": " C#/.NET Core Go Java Lisp Node.js PHP Python Ruby  ",
        "image": "",
        "url": "/development/templates.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "ff6cf53b92a5848537f00d89a719d8e9",
        "title": "Sponsored community sites",
        "description": "",
        "text": " Platform.sh provides sponsored hosting for Free Software projects and tech community events and organizations as part of our effort to support the Free Software community. Eligibility A non-trivial software project released under a Free and Open Source license (*GPL, BSD, MIT, Apache, etc.) A community-oriented conference or similar event run either by the community or a related non-profit corporation. For-profit events and organizations are not eligible. An offline IT community organization, such as a local user group for a Free and Open Source project. An online non-profit community project that supports one of the above. The Platform.sh Developer Relations team manages the program and collectively has final say on what projects are eligible for sponsorship. Platform.sh will periodically reevaluate sponsored sites to ensure they remain eligible, and may terminate the free service at its own will. Platform.sh will provide a month notice to allow users to migrate in these cases. Offering An eligible project is entitled to one (1) Standard instance, with the standard 3 environments and 5 GB of storage. Any number of users may be added as needed for the project at no additional cost. The site is to be used exclusively for content supporting and promoting the project. (Brochureware site, documentation, user forums, etc. are all acceptable.) There is no limit on the number of domains or application containers (subject to resource limits of the Standard Plan) that a project may use, although the project is expected to register and manage its own domain(s). Platform.sh will provision SSL certificates for free (as may be limited by the Let’s Encrypt API) but the hosted site may provide their own. A project is under no obligation to use the application container of their project. (Eg, a PHP project is free to use Jekyll to produce a static site using Platform.sh, a Python project is free to host a PHPBB site, etc.) Expectations In return, eligible projects are expected to include an HTML snippet conspicuously on each page of the site. That may be in a sidebar or footer, with text no smaller than the standard body text size for the site. The image may be scaled via CSS as appropriate as long as the text is fully readable. The badge markup will be provided by Platform.sh when setting up the site and will include a link to Platform.sh and an analytics tracking value so we know where links are coming from. It does not include any cookies. Platform.sh may from time to time update the widget’s contents and the sponsored site will update the contents in a timely fashion. The sponsored site may ask, and Platform.sh shall accept variations of the widget as long as these fall within the branding guidelines of Platform.sh. Platform.sh may announce on its own media presence both online and offline that it hosts the said project, and kindly ask the sponsored site communicate at least once over its usual social media presence (Twitter, Mailing list, etc) the sponsored hosting. If the eligible project is a registered non-profit organization in the US or Germany, it is also expected to provide paperwork to register an in-kind donation to the organization of the equivalent price of the service ($50 USD/month or the EUR equivalent), for tax purposes. Sponsored projects will receive the same level of support as any other self-service hosted customer site. Contact If you are an eligible Free Software project or community group, contact our Developer Relations team and let us know that you are interested.",
        "section": "Pricing",
        "subsections": " Eligibility Offering Expectations Contact  ",
        "image": "",
        "url": "/overview/pricing/sponsored.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "28b7c8d84715f165ec4f9784b859ebf6",
        "title": "Start with a template",
        "description": "",
        "text": " Welcome to Platform.sh! Getting started is as easy as opening a free trial account and initializing a template project . There are no requirements on your part at this point. This guide will take you from zero to hero - from first glance to a deployed application entirely from your browser. Get started!",
        "section": "Introduction",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/template.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "f9eddd35ac682933bc7570fd91fc54c4",
        "title": "Structure",
        "description": "",
        "text": " Every application you deploy on Platform.sh is built as a virtual cluster, containing a set of containers. The master branch of your Git repository is always deployed as the production cluster. Any other branch can be deployed as a development cluster. By default, you can have up to three live development clusters at once, but you can buy more on a per-project basis. There are three types of containers within your cluster: one Router one or more Application containers zero or more Service containers All of those containers are managed by three special files in your Git repository: .platform/routes.yaml .platform/services.yaml .platform.app.yaml In most cases, that means your repository will look like this: yourproject/ .git/ .platform/ services.yaml routes.yaml .platform.app.yaml \u0026lt;your application files\u0026gt; Router There is always exactly one Router per cluster. The Router of a cluster is a single nginx process. It is configured by the routes.yaml file. It maps incoming requests to the appropriate Application container and provides basic caching of responses, if so configured. It has no persistent storage. Service Service containers are configured by the services.yaml file. There may be zero or more Service containers in a cluster, depending on the services.yaml file. The code for a Service is provided by Platform.sh in a pre-built container image, along with a default configuration. Depending on the service it may also include user-provided configuration in the services.yaml file. Examples of services include MySQL/MariaDB, Elasticsearch, Redis, and RabbitMQ. Application There always must be one Application container in a cluster, but there may be more. Each Application container corresponds to a .platform.app.yaml file in the repository. If there are 3 .platform.app.yaml files, there will be three Application containers. Application containers hold the code you provide via your Git repository. Application containers are always built off of one of the Platform.sh-provided language-specific images, such as “PHP 5.6”, “PHP 7.2”, or “Python 3.7”. It is also possible to have multiple Application containers running different languages or versions. For typical applications, there is only one .platform.app.yaml file, which is generally placed at the repository root.",
        "section": "The big picture",
        "subsections": " Router Service Application  ",
        "image": "",
        "url": "/overview/structure.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "b6ba5f5c2756291ed287b05c7efe0395",
        "title": "Symfony Frequently Asked Questions (FAQ)",
        "description": "",
        "text": " How do I store my session files? If you get the following error: failed: Read-only file system (30) in /app/app/cache/dev/classes.php line 420 that’s because Symfony is trying to write into: /var/lib/php5/ which is read-only. The solution is to mount a sessions folder into Platform.sh and write sessions in that folder. Simply edit your .platform.app.yaml and add a mounts there: mounts:... app/sessions :source:localsource_path:sessions...Then, add this line at the top of your app_dev.php: ini_set(\u0026#39;session.save_path\u0026#39;, __DIR__.\u0026#39;/../app/sessions\u0026#39; ); Why does my newly cloned Symfony install throw errors? You may encounter the WSOD (white screen of death) when you first clone a new Symfony project from your platform. This is likely because of missing dependencies. You will need to install composer first and then run the following command: cd my_project_name/ composer install Why do I get ‘Permission denied’ in a deploy hook? If you get the following error during a deploy hook: Launching hook \u0026#39;app/console cache:clear\u0026#39;. /bin/dash: 1: app/console: Permission denied This means that you might have committed the executable file (in this case app/console) without the execute bit set. Run this to fix the problem: chmod a\u0026#43;x app/console git add app/console git commit -m  Fix the console script execute permission. ",
        "section": "Symfony - Getting started",
        "subsections": " How do I store my session files? Why does my newly cloned Symfony install throw errors? Why do I get \u0026lsquo;Permission denied\u0026rsquo; in a deploy hook?  ",
        "image": "",
        "url": "/frameworks/symfony/faq.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "8eef2783c62c78fd72d3592be2ce5fe6",
        "title": "Tethered",
        "description": "",
        "text": " Tethered Local The simplest way to run a project locally is to use a local web server, but keep all other services on Platform.sh and connect to them over an SSH tunnel. This approach requires very little setup, but depending on the speed of your connection and how I/O intensive your application is may not be performant enough to use regularly. It will also require an active Internet connection, of course. Quick Start In your application directory run platform tunnel:open \u0026amp;\u0026amp; export PLATFORM_RELATIONSHIPS= $(platform tunnel:info --encode) . This will open an SSH tunnel to your current Platform.sh environment and expose a local environment variable that mimics the relationships array on Platform.sh. You can now run your application locally (for example by running php -d variables_order=EGPCS -S localhost:8001 for PHP), assuming it is configured to read its configuration from the Platform.sh environment variables. Note that other Platform.sh environment configuration such as the routes or application secret value will still not be available. Also be aware that the environment variable exists only in your current shell. If you are starting multiple local command shells you will need to rerun the export command above in each of them. Local web server For the local web server the approach will vary depending on your language. For a self-serving language (Go or Node.js), simply run the program locally. For PHP, you may install your own copy of Nginx (or Apache) and PHP-FPM, or simply use the built-in PHP web server. Be aware however that by default the PHP web server will ignore environment variables by default. You will need to explicitly instruct it to read them, like so: php -S -d variables_order=EGPCS localhost:8001. That will start a basic web server capable of running PHP, serving the current directory, on port 8001, using available environment variables. See the PHP manual for more information. For other languages it is recommended that you install your own copy of Nginx or Apache. A virtual machine or Docker image is also a viable option. SSH tunneling Now that the code is running, it needs to connect it to its services. For that, open an SSH tunnel to the current project. $ platform tunnel:open SSH tunnel opened on port 30000 to relationship: redis SSH tunnel opened on port 30001 to relationship: database Logs are written to: ~/.platformsh/tunnels.log List tunnels with: platform tunnels View tunnel details with: platform tunnel:info Close tunnels with: platform tunnel:close Now you can connect to the remote database normally, as if it were local. $ mysql --host=127.0.0.1 --port=30001 --user=\u0026#39;user\u0026#39; --password=\u0026#39;\u0026#39; --database=\u0026#39;main\u0026#39; The specific port that each service uses is not guaranteed, but is unlikely to change unless you add an additional service or connect to multiple projects at once. In most cases it’s safe to add a local-configuration file for your application that connects to, in this case, localhost:30001 for the SQL database and localhost:30000 for Redis. After the tunnel(s) are opened, you can confirm their presence: platform tunnel:list You can show more information about the open tunnel(s) with: platform tunnel:info and you can close tunnels with: platform tunnel:close Note: The platform tunnel:open command requires the pcntl and posix PHP extensions. Run php -m | grep -E \u0026#39;posix|pcntl\u0026#39; to check if they’re there. If you don’t have these extensions installed, you can use the platform tunnel:single command to open one tunnel at a time. This command also lets you specify a local port number. Local environment variables Alternatively, you can read the relationship information directly from Platform.sh and expose it locally in the same form. From the command line, run: export PLATFORM_RELATIONSHIPS= $(platform tunnel:info --encode)  That will create a PLATFORM_RELATIONSHIPS environment variable locally that looks exactly the same as the one you’d see on Platform.sh, but pointing to the locally mapped SSH tunnels. Whatever code you have that looks for and decodes the relationship information from that variable (which is what runs on Platform.sh) will detect it and use it just as if you were running on Platform.sh. Note that the environment variable is set globally so you cannot use this mechanism to load mutiple tethered Platform.sh projects at the same time. If you need to run multiple tethered environments at once you will have to read the relationships information for each one from the application code, like so: \u0026lt;?php if ($relationships_encoded = shell_exec(\u0026#39;platform tunnel:info --encode\u0026#39;)) { $relationships = json_decode(base64_decode($relationships_encoded, TRUE), TRUE); // ... } {%- language name= Python , type= py  -%} import json import base64 import subprocess encoded = subprocess.check_output([\u0026#39;platform\u0026#39;, \u0026#39;tunnel:info\u0026#39;, \u0026#39;--encode\u0026#39;]) if (encoded): json.loads(base64.b64decode(relationships).decode(\u0026#39;utf-8\u0026#39;)) # ...",
        "section": "Set up your local development environment",
        "subsections": " Quick Start Local web server SSH tunneling  Local environment variables    ",
        "image": "",
        "url": "/development/local/tethered.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "5d3c9dd44a61cd3d93654846f6ba5431",
        "title": "TYPO3 Frequently Asked Questions (FAQ)",
        "description": "",
        "text": " Why are there warnings in the install tool? The TYPO3 install tool doesn’t yet fully understand when you are working on a cloud envirionment and may warn you that some folders are not writable. Don’t worry, your TYPO3 installation will be fully functional. How do I add extensions? TYPO3 extension can easily be added using composer. Just use the platform.sh CLI tool to run composer, as follows: platform get \u0026lt;project id\u0026gt; -e \u0026lt;branch name\u0026gt; composer require typo3-ter/[extension name] git add composer.* git commit git push",
        "section": "TYPO3 - Getting started",
        "subsections": " Why are there warnings in the install tool? How do I add extensions?  ",
        "image": "",
        "url": "/frameworks/typo3/faq.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "9ade93b8dcb9b5af7838d604e9a5262a",
        "title": "User administration",
        "description": "",
        "text": " Every Platform.sh user has a role which controls access and improves security on your project. Different roles are authorized to do different things with your applications, environments and users. You can use your collection of Roles to manage how users interact with Platform.sh. User roles At the project level: Project Administrator - A project administrator can change settings and execute actions on any environment. Project Viewer - A project reader can view all environments within a project but cannot execute any actions on them. A Project Reader can have a specific role on different environments. At the environment level: Environment Administrator - An environment administrator can change settings and execute actions on this environment. Environment Contributor - An environment contributor can push code to this environment and branch the environment. Environment Viewer - An environment reader can only view this environment. Important: After a user is added to (or deleted from) an environment, it will be automatically redeployed, after which the new permissions will be fully updated. When adding users at the project level, however, redeployments do not occur automatically, and you will need to trigger a redeployments to update those settings for each environment using the CLI command platform redeploy. Otherwise, user access will not be updated on those environments until after the next build and deploy commit. When a development team works on a project, the team leader can be the project administrator and decide which roles to give his team members. One team member can contribute to one environment, another member can administer a different environment and the customer can be a reader of the master environment. If you want your users to be able to see everything (Reader), but only commit to a specific branch, change their permission level on that environment to “Contributor”. SSH Access Control: An environment contributor can push code to the environment and has SSH access to the environment. You can change this by specifying user types with SSH access. Note: The project owner - the person licensed to use Platform.sh - doesn’t have special powers. A project owner usually has a project administrator role. Manage user permissions at the project level From your list of projects, select the project where you want to view or edit user permissions. At this point, you will not have selected a particular environment. Click the Settings tab at the top of the page, then click the Access tab on the left to show the project-level users and their roles. The Access tab shows project-level users and their roles. Selecting a user will allow you either to edit that user’s permissions or delete the user’s access to the project entirely. Add a new user by clicking on the Add button. If you select the “Viewer” role for the user, you’ll have the option of adjusting the user’s permissions at the environment level. From this view, you can assign the user’s access. Selecting them to become a “Project admin” will give them “Admin” access to every environment in the project. Alternatively, you can give the user “Admin”, “Viewer”, “Contributor”, or “No Access” to each environment separately. If you select the “Viewer” role for the user, you’ll have the option of adjusting the user’s permissions at the environment level. Once this has been done, if the user does not have a Platform.sh account, they will receive an email asking to confirm their details and register an account name and a password. In order to push and pull code (or to SSH to one of the project’s environments) the user will need to add an SSH key. If the user already has an account, they will receive an email with a link to the project. Manage user permissions at the environment level From within a project select an environment from the ENVIRONMENT pull-down menu. Click the Settings tab at the top of the screen and then click the Access tab on the left hand side. The Access tab shows environment-level users and their roles. Selecting a user will allow you either to edit that user’s permissions or delete the user’s access to the environment entirely. Add a new user by clicking on the Add button. Note: Remember the user will only be able to access the environment once it has been rebuilt (after a git push). Manage users with the CLI You can user the Platform.sh command line client to fully manage your users and integrate this with any other automated system. Available commands: user:add Add a user to the project user:delete Delete a user user:list (users) List project users user:role View or change a user’s role For example, the following command would add the ‘admin’ role to alice@example.com in the current project. platform user:add This will present you with an interactive wizard that will allow you to choose precisely what rights you want to give the new user. $ platform user:add Email address: alice@example.com The user\u0026#39;s project role can be \u0026#39;viewer\u0026#39; (\u0026#39;v\u0026#39;) or \u0026#39;admin\u0026#39; (\u0026#39;a\u0026#39;). Project role [V/a]: The user\u0026#39;s environment-level roles can be \u0026#39;viewer\u0026#39;, \u0026#39;contributor\u0026#39;, or \u0026#39;admin\u0026#39;. development environment role [V/c/a]: sprint1 environment role [V/c/a]: hot-fix environment role [V/c/a]: master environment role [V/c/a]: pr-2 environment role [V/c/a]: pr-3 environment role [V/c/a]: Summary: Email address: alice@example.com Project role: viewer development: viewer sprint1: viewer hot-fix: viewer pr-2: viewer pr-3: viewer Adding users can result in additional charges. Are you sure you want to add this user? [Y/n] User alice@example.com created Once this has been done, the user will receive an email asking her to confirm her details and register an account name and a password. To give Alice the ‘contributor’ role on the environment ‘development’, you could run: platform user:role alice@example.com --level environment --environment development --role contributor Use platform list to get the full list of commands. Transfer ownership If you want to transfer ownership of a project to a different user, first invite that user as a project administrator and then submit a support ticket from the current project owner to ask for the transfer. This action will automatically transfer the subscription charges to the new owner.",
        "section": "Administration",
        "subsections": " User roles Manage user permissions at the project level Manage user permissions at the environment level Manage users with the CLI Transfer ownership  ",
        "image": "",
        "url": "/administration/users.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "e72ac4ff37210cb6b603c3ebfed4df5f",
        "title": "Using Redis with WordPress",
        "description": "",
        "text": " There are a number of Redis libraries for WordPress, only some of which are compatible with Platform.sh. We have tested and recommend devgeniem/wp-redis-object-cache-dropin , which requires extremely little configuration. Requirements Add a Redis service First you need to create a Redis service. In your .platform/services.yaml file, add the following: rediscache:type:redis:5.0That will create a service named rediscache, of type redis, specifically version 5.0. Expose the Redis service to your application In your .platform.app.yaml file, we now need to open a connection to the new Redis service. Under the relationships section, add the following: relationships:redis: rediscache:redis The key (left side) is the name that will be exposed to the application in the PLATFORM_RELATIONSHIPS variable . The right hand side is the name of the service we specified above (rediscache) and the endpoint (redis). If you named the service something different above, change rediscache to that. Add the Redis PHP extension Because the Redis extension for PHP has been known to have BC breaks at times, we do not bundle a specific verison by default. Instead, we provide a script to allow you to build your desired version in the build hook. See the PHP-Redis page for a simple-to-install script and instructions. Add the Redis library If using Composer to build WordPress, you can install the WP-Redis library with the following Composer command: composer require devgeniem/wp-redis-object-cache-dropin Then commit the resulting changes to your composer.json and composer.lock files. Configuration To enable the WP-Redis cache the object-cache.php file needs to be copied from the downloaded package to the wp-content directory. Add the following line to the bottom of your build hook: cp -r wp-content/wp-redis-object-cache-dropin/object-cache.php web/wp/wp-content/object-cache.php It should now look something like: hooks:build:| set -ebashinstall-redis.sh5.1.1cp-rwp-content/wp-redis-object-cache-dropin/object-cache.phpweb/wp/wp-content/object-cache.phpNext, place the following code in the wp-config.php file, somewhere before the final require_once(ABSPATH . \u0026#39;wp-settings.php\u0026#39;); line. \u0026lt;?php if (!empty($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;]) \u0026amp;\u0026amp; extension_loaded(\u0026#39;redis\u0026#39;)) { $relationships = json_decode(base64_decode($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;]), true); $relationship_name = \u0026#39;redis\u0026#39;; if (!empty($relationships[$relationship_name][0])) { $redis = $relationships[$relationship_name][0]; define(\u0026#39;WP_REDIS_CLIENT\u0026#39;, \u0026#39;pecl\u0026#39;); define(\u0026#39;WP_REDIS_HOST\u0026#39;, $redis[\u0026#39;host\u0026#39;]); define(\u0026#39;WP_REDIS_PORT\u0026#39;, $redis[\u0026#39;port\u0026#39;]); } } That will define 3 constants that the WP-Redis extension will look for in order to connect to the Redis server. If you used a different name for the relationship above, change $relationship_name accordingly. This code will have no impact when run on a local development environment. That’s it. There is no Plugin to enable through the WordPress administrative interface. Commit the above changes and push. Verifying Redis is running Run this command in a SSH session in your environment redis-cli -h redis.internal info. You should run it before you push all this new code to your repository. This should give you a baseline of activity on your Redis installation. There should be very little memory allocated to the Redis cache. After you push this code, you should run the command and notice that allocated memory will start jumping.",
        "section": "Wordpress",
        "subsections": " Requirements  Add a Redis service Expose the Redis service to your application Add the Redis PHP extension Add the Redis library   Configuration  Verifying Redis is running    ",
        "image": "",
        "url": "/frameworks/wordpress/redis.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "f8f002328663e62ad4ef9f18391abdbb",
        "title": "YAML",
        "description": "",
        "text": " YAML (“YAML Ain’t Markup Language”) is a human-readable data file format, well suited to human-edited configuration files. Nearly all aspects of your project’s build and deploy pipeline are controlled via YAML files. YAML is a whitespace-sensitive format that is especially good at key/value type configuration, such as that used by Platform.sh. There are many good YAML tutorials online, and the format is reasonably self-documenting. We especially recommend: GravCMS’s YAML tutorial Learn YAML in Y Minutes The following is only a cursory look at YAML itself. The tutorials above will provide a more in-depth introduction. Basic YAML A YAML file is a text file that ends in .yaml. (Some systems use an alternative .yml extension, but Platform.sh uses the four-letter extension.) It consists primarily of key value pairs, and supports nesting. For example: name:\u0026#39;app\u0026#39;type:\u0026#39;php:7.4\u0026#39;build:flavor:\u0026#39;composer\u0026#39;disk:1024This example defines a key name with value app, a key type with value php:7.1, a key disk with a value 1024, and a key build that is itself a nested set of key/value pairs, of which there is only one: flavor, whose value is composer. Informally, nested values are often referenced using a dotted syntax, such as build.flavor, and that format is used in this documentation in various places. Keys are always strings, and may be quoted or not. Values may be strings, numbers, booleans, or further nested key/value pairs. Alphanumeric strings may be quoted or not. More complex strings (with punctuation, etc.) must be quoted. Numbers should not be quoted. The boolean values true and false should never be quoted. For quoted values, both single quotes (\u0026#39;) and double quotes ( ) are valid. Double quotes, however, will interpolate common escape characters such as and so forth. For that reason using single quotes is generally recommended unless you want escape characters to be processed rather than taken literally. In general the order of keys in a YAML file does not matter. Neither do blank lines. Indentation may be with any number of spaces, as long as it is consistent throughout the file. Platform.sh examples by convention use four-space indentation. Multi-line strings In case of long, multi-line strings, the | character tells the YAML parser that the following, indented lines are all part of the same string. That is, this: hooks:build:| set -ecpa.txtb.txtcreates a nested property hooks.build, which has the value set a.txt b.txt. (That is, a string with a line break in it.) That is useful primarily for hooks, which allow the user to enter small shell scripts within the YAML file. Includes YAML allows for special “tags” on values that change their meaning. These tags may be customized for individual applications so may vary from one system to another. The main Platform.sh “local tag” is !include, which allows for external files to be logically embedded within the YAML file. The referenced file is always relative to the YAML file’s directory. string The string type allows an external file to be inlined in the YAML file as though it had been entered as a multi-line string. For example, given this file on disk named build.sh: set -e cp a.txt b.txt Then the following two YAML fragments are exactly equivalent: hooks:build:| set -ecpa.txtb.txthooks:build:!includetype:stringpath:build.shThat is primarily useful for breaking longer build scripts or inlined configuration files out to a separate file for easier maintenance. binary The binary type allows an external binary file to be inlined in the YAML file. The file will be base64 encoded. For example: properties:favicon:!includetype:binarypath:favicon.icowill reference the favicon.ico file, which will be provided to Platform.sh’s management system. yaml Finally, the yaml type allows an external YAML file to be inlined into the file as though it had been typed in directly. That can help simplify more complex files, such a .platform.app.yaml file with many highly-customized web.locations blocks. The yaml type is the default, meaning it may reference a file inline without specifying a type. For example, given this file on disk named main.yaml: the following three location definitions are exactly equivalent: Another custom tag available is !archive, which specifies a value is a reference to a directory on disk, relative to the location of the YAML file. Essentially it defines the value of key as “this entire directory”. Consider this services.yaml fragment: mysearch:type:solr:8.0disk:1024configuration:conf_dir:!archive solr/conf In this case, the mysearch.configuration.conf_dir value is not the string “solr/conf”, but the contents of the solr/conf directory (relative to the services.yaml file). On Platform.sh, that is used primarily for service definitions in services.yaml to provide a directory of configuration files for the service (such as Solr in this case). Platform.sh will use that directive to copy the entire specified directory into our management system so that it can be deployed with the specified service. Anchors YAML supports internal named references, known as “anchors.” They can be referenced using an “alias.” That allows you to have a large block of YAML that gets repeated multiple times in different places within a single file without having to copy-paste the whole block. An anchor is defined by appending \u0026amp;name to a segment, where “name” is some unique identifier. For example: relationships:\u0026amp;relsdatabase:\u0026#39;mysqldb:db1\u0026#39;cache:\u0026#39;rediscache:redis\u0026#39;search:\u0026#39;searchserver:elasticsearch\u0026#39;This block defines an anchor called rels, the contents of which is the 3 key/value pairs for database, cache, and search. An anchor can be referenced using an alias like *name, which will inject the anchored value into the file at that point. That is, the following two snippets are logically equivalent: foo:\u0026amp;foothing:stuffmany:{\u0026#39;stuff\u0026#39;,\u0026#39;here\u0026#39;}bar:*foofoo:thing:stuffmany:{\u0026#39;stuff\u0026#39;,\u0026#39;here\u0026#39;}bar:thing:stuffmany:{\u0026#39;stuff\u0026#39;,\u0026#39;here\u0026#39;}By default, aliases will inject their child contents entirely. If you want to overwrite a specific child key of an anchor there is a different alias syntax you must use: foo:\u0026amp;foothing:stuffmany:{\u0026#39;stuff\u0026#39;,\u0026#39;here\u0026#39;}bar:\u0026lt;\u0026lt;:*foothing:otherWhich is equivalent to: foo:thing:stuffmany:{\u0026#39;stuff\u0026#39;,\u0026#39;here\u0026#39;}bar:thing:othermany:{\u0026#39;stuff\u0026#39;,\u0026#39;here\u0026#39;}Be aware that aliases have sometimes non-obvious requirements around their whitespace formatting. In particular, when aliasing a anchor into a YAML array the alias reference must be at the same indentation level as any overrides. That is: - \u0026amp;mylistlist:ofvalues:here- \u0026lt;\u0026lt;: *mylist # These two lines must start at the same indentation.values:thereAnchor example Anchors and aliases are mainly useful when you want to repeat a given block of configuration. For example, the following snippet will define three identical worker instances within a .platform.app.yaml file: workers:queue1:\u0026amp;runnersize:\u0026#39;S\u0026#39;commands:start:pythonqueue-worker.pyvariables:env:type:\u0026#39;worker\u0026#39;queue2:*runnerqueue3:*runner",
        "section": "Configuration",
        "subsections": " Basic YAML Multi-line strings Includes  string binary yaml !archive   Anchors  Anchor example    ",
        "image": "",
        "url": "/configuration/yaml.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "d7299aacdddd49ea1f964e5c96b378a8",
        "title": "(Optional) Configure a third-party TLS certificate",
        "description": "",
        "text": " Platform.sh automatically provides standard TLS certificates issued by Let’s Encrypt to all production instances. No further action is required to use TLS-encrypted connections beyond specifying HTTPS routes in your routes.yaml file. Alternatively, you may provide your own third party TLS certificate from the TLS issuer of your choice at no charge from us. Please consult your TLS issuer for instructions on how to generate an TLS certificate. A custom certificate is not necessary for development environments. Platform.sh automatically provides wildcard certificates that cover all *.platform.sh domains, including development environments. Note: The private key should be in the old style, which means it should start with BEGIN RSA PRIVATE KEY. If it starts with BEGIN PRIVATE KEY that means it is bundled with the identifier for key type. To convert it to the old-style RSA key: openTLS rsa -in private.key -out private.rsa.key Adding a custom certificate through the management console You can add a custom certificate via the Platform.sh management console . In the management console for the project go to Settings and click Certificates on the left hand side. You can add a certificate with the Add button at the top of the page. You can then add your private key, public key certificate and optional certificate chain. Adding a custom certificate through the CLI Example: platform domain:add secure.example.com --cert=/etc/TLS/private/secure-example-com.crt --key=/etc/TLS/private/secure-example-com.key See platform help domain:add for more information. Success!: Your site should now be live, and accessible to the world (as soon as the DNS propagates). If something is not working see the troubleshooting guide for common issues. If that doesn’t help, feel free to contact support.",
        "section": "Going Live - Steps",
        "subsections": "   Adding a custom certificate through the management console Adding a custom certificate through the CLI    ",
        "image": "",
        "url": "/golive/steps/tls.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "43df3375130e2cd56d5ee8d06ffa5824",
        "title": "Backup and restore",
        "description": "",
        "text": " Backups are triggered directly via the management console or via the CLI. The backup creates a complete snapshot of the environment. It includes all persistent data from all running services (MySQL, Solr,…) and any files stored on the mounted volumes. Backups You need to have the “admin” role in order to create a backup of an environment. Backups on Platform.sh Professional are retained for at least 7 days. They will be purged between 7 days and 6 months, at Platform.sh’s discretion. Please see the data retention page for more information. Note: We advise you to make backups of your live environment before merging an environment to the live environment, or each time you increase the storage space of your services. Using the CLI: $ platform backup:create Please be aware that triggering a backup will cause a momentary pause in site availability so that all requests can complete, allowing the backup to be taken against a known consistent state. The total interruption is usually only 15 to 30 seconds and any requests during that time are held temporarily, not dropped. Restore You will see the backup in the activity feed of your environment in the Platform.sh management console. You can trigger the restore by clicking on the restore link. You can also restore the backup to a different environment using the CLI. You can list existing backups with the CLI as follows: $ platform backups Finding backups for the environment master \u0026#43;---------------------\u0026#43;------------\u0026#43;----------------------\u0026#43; | Created | % Complete | Backup name | \u0026#43;---------------------\u0026#43;------------\u0026#43;----------------------\u0026#43; | 2015-06-19 17:11:42 | 100 | 2ca4d90639f706283fee | | 2015-05-28 15:05:42 | 100 | 1a1fbcb9943849706ee6 | | 2015-05-21 14:38:40 | 100 | 7dbdcdb16f41f9e1c061 | | 2015-05-20 15:29:58 | 100 | 4997900d2804d5b2fc39 | | 2015-05-20 13:31:57 | 100 | c1f2c976263bec03a10e | | 2015-05-19 14:51:18 | 100 | 71051a8fe6ef78bca0eb | You can then restore a specific backup with the CLI as follows: $ platform backup:restore 2ca4d90639f706283fee Or even restore the backup to a different branch with the CLI as follows: $ platform backup:restore --target=RESTORE_BRANCH 2ca4d90639f706283fee For this to work, it’s important to act on the active branch on which the backup was taken. Restoring a backup from develop when working on the staging branch is impossible. Switch to the acting branch and set your --target as above snippet mentions. If no branch already exists, you can specify the parent of the branch that will be created to restore your backup to as follows: $ platform backup:restore --branch-from=PARENT_BRANCH 2ca4d90639f706283fee Note: You need “admin” role to restore your environment from a backup. Be aware that the older US and EU regions do not support restoring backups to different environments. If your project is on one of the older regions you may file a support ticket to ask that a backup be restored to a different environment for you, or migrate your project to one of the new regions that supports this feature. Backups and downtime A backup does cause a momentary pause in service. We recommend running during non-peak hours for your site. Automated backups Backups are not triggered automatically on Platform.sh Professional. Backups may be triggered by calling the CLI from an automated system such as Jenkins or another CI service, or by installing the CLI tool into your application container and triggering the backup via cron. Automated backups using Cron Note: Automated backups using cron requires you to get an API token and install the CLI in your application container. We ask that you not schedule a backup task more than once a day to minimize data usage. Once the CLI is installed in your application container and an API token configured you can add a cron task to run once a day and trigger a backup. The CLI will read the existing environment variables in the container and default to the project and environment it is running on. In most cases such backups are only useful on the master production environment. A common cron specification for a daily backup on the master environment looks like this: crons:backup:spec:\u0026#39;0 5 * * *\u0026#39;cmd:| if [  $PLATFORM_BRANCH  = master ]; thenplatformbackup:create--yes--no-waitfiThe above cron task will run once a day at 5 am (UTC), and, if the current environment is the master branch, it will run platform backup:create on the current project and environment. The --yes flag will skip any user-interaction. The --no-wait flag will cause the command to complete immediately rather than waiting for the backup to complete. Note: It is very important to include the --no-wait flag. If you do not, the cron process will block and you will be unable to deploy new versions of the site until the backup creation process is complete. Retention Please see our Data Retention Page .",
        "section": "Administration",
        "subsections": " Backups Restore Backups and downtime Automated backups  Automated backups using Cron Retention    ",
        "image": "",
        "url": "/administration/backup-and-restore.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "646460e528f26d680702036756ff2d5c",
        "title": "Build \u0026 Deploy",
        "description": "",
        "text": " Every time you push to a live branch (a git branch with an active environment attached to it) or activate an environment for a branch, there are two main processes that happen: Build and Deploy.” The build process looks through the configuration files in your repository and assembles the necessary containers. The deploy process makes those containers live, replacing the previous versions, with virtually no interruption in service. Always Be Compiling Interpreted languages like PHP or Node.js may not seem like they have to be compiled, but with modern package management tools like Composer or npm, as well as the growing use of CSS preprocessors such as Sass, most modern web applications need a “build” step between their source code and their production execution code. At Platform.sh, we aim to make that easy. The build step includes the entire application container—from language version to build tools to your code—rebuilt every time. That build process is under your control and runs on every Git push. Every Git push is a validation not only of your code, but of your build process. Whether that’s installing packages using Composer or Bundler, compiling TypeScript or Sass, or building your application code in Go or Java, your build process is vetted every time you push. Whenever possible, you should avoid committing build assets to your repository that can be regenerated at build time. Depending on your application, that may include 3rd party libraries and frameworks, generated and optimized CSS and JS files, generated source code, etc. The following two constraints make sure you have fast, repeatable builds: The build step should be environment-independent. This is paramount to ensure development environments are truly perfect copies of production. This means you can not connect to services (like the database) during the build. The final built application must be read-only. If your application requires writing to the filesystem, you can specify the directories that require Read/Write access. These should not be directories that have code, because that would be a security risk. Building the application After you push your code, the first build step is to validate your configuration files (i.e. .platform.app.yaml, .platform/services.yaml, and .platform/routes.yaml). The Git server will issue an error if validation fails, and nothing will happen on the server. Note: While most projects have a single .platform.app.yaml file, Platform.sh supports multiple applications in a single project. It will scan the repository for .platform.app.yaml files in subdirectories and each directory containing one will be built as an independent application. A built application will not contain any directories above the one in which it is found. The system is smart enough not to rebuild applications that have already been built, so if you have multiple applications, only changed applications will be rebuilt and redeployed. The live environment is composed of multiple containers—both for your application(s) and for the services it depends on. It also has a virtual network connecting them, as well as a router to route incoming requests to the appropriate application. Based on your application type the system will select one of our pre-built container images and run the following: First, any dependencies specified in the .platform.app.yaml file are installed. Those include tools like Sass, Gulp, Drupal Console, or any others that you may need. Then, depending on the “build flavor” specified in the configuration file, we run a series of standard commands. The default for PHP containers, for example, is simply to run composer install. Finally, we run the “build hook” from the configuration file. The build hook comprises one or more shell commands that you write to finish creating your production code base. That could be compiling Sass files, running a Gulp or Grunt script, rearranging files on disk, compiling an application in a compiled language, or whatever else you want. Note that, at this point, all you are able to access is the file system; there are no services or other databases available. Once all of that is completed, we freeze the file system and produce a read-only container image. That image is the final build artifact: a reliable, repeatable snapshot of your application, built the way you want, with the environment you want. Because container configuration (both for your application and its underlying services) is exclusively based on your configuration files, and your configuration files are managed through Git, we know that a given container has a 1:1 relationship with a Git commit. That means builds are always repeatable. It also means that if we detect that there are no changes that would affect a given container, we can skip the build step entirely and reuse the existing container image, saving a great deal of time. In practice, the entire build process usually takes less than a minute. Deploying the application Deploying the application also has several steps, although they’re much quicker. First, we pause all incoming requests and hold them so that there’s no interruption of service. Then we disconnect the current containers from their file system mounts, if any, and connect the file systems to the new containers instead. If it’s a new branch and there is no existing file system, we clone it from the parent branch. We then open networking connections between the various containers, but only those that were specified in the configuration files (using the relationships key). No configuration, no connection. That helps with security, as only those connections that are actually needed even exist. The connection information for each service is available in an application as environment variables. Now that we have working, running containers there are two more steps to run. First, if there is a “start” command specified in your .platform.app.yaml file to start the application, we run that. (With PHP, this is optional.) Then we run your deploy hook. Just like the build hook, the deploy hook is any number of shell commands you need to prepare your application. Usually this includes clearing caches, running database migrations, and so on. You have complete access to all services here as your application is up and running, but the file system where your code lives will now be read-only. Finally, once the deploy hooks are done, we open the floodgates and let incoming requests through to your newly deployed application. You’re done! In practice, the deploy process takes only a few seconds, plus whatever time is required for your deploy hook, if any.",
        "section": "The big picture",
        "subsections": " Always Be Compiling Building the application Deploying the application  ",
        "image": "",
        "url": "/overview/build-deploy.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "cc8e6c04be55eb4698bbee443553be4b",
        "title": "Changelog",
        "description": "",
        "text": " Look here for all the most recent additions to Platform.sh. 2020 June 2020 Node.js 14: We now support Node.js 14 . April 2020 We now offer Xdebug on PHP containers. Custom activity scripts are now available, in alpha. March 2020 Go 1.14: We now support Go 1.14 . Ruby 2.7: We now support Ruby 2.7 . .NET Core: We now support .NET Core 3.1 . Memcached 1.6: We now support Memcached 1.6 . Solr 8.4: We now support Solr 8.4 . February 2020 Memcached 1.5: We now support Memcached 1.5 . Character set and collation are now configurable on MySQL/MariaDB . January 2020 RabbitMQ: We now support RabbitMQ virtual host configuration 2019 December 2019 Python 3.8: We now support Python 3.8 . Node.js 12: We now support Node.js 12 . RabbitMQ 3.8: We now support RabbitMQ 3.8 . October 2019 PHP 7.4: We now support PHP 7.4 . October 2019 Projects can now have larger application containers in development environments. September 2019 “Dark mode” now available in the Console. Go 1.13: We now support Go 1.13 . July 2019 Elasticsearch 7.2: We now support Elasticsearch 7.2 . Elasticsearch 5.2 and 5.4 support is now deprecated Kafka 2.2: We now support Kafka 2.2 . Java 13: We now support Java 13 . June 2019 Java: We support and documented the use of Java runtimes 8, 11, and 12, that includes examples that use the Java Config Reader . Headless Chrome: Users can now define a Headless Chrome service to access a service container with a headless browser, which can be used for automated UI testing. May 2019 InfluxDB: We now support InfluxDB 1.7. Solr 7 \u0026amp; 8: We now support Solr 7.7 and 8.0. April 2019 Network storage service: Users can now define a Network storage service for sharing files between containers. Kafka message queue service: Users can now define a Kafka service for storing, reading and analysing streaming data. Management Console: Images and wording updated throughout entire documentation alongside Management Console release. March 2019 Ruby 2.6: A new version of Ruby is now available. Go 1.12: We now support Go 1.12 . Elasticsearch 6.5: We now support Elasticsearch 6.5 . January 2019 RabbitMQ 3.7: We now support RabbitMQ 3.7 . Solr 7: We now support Solr 7.6 . Varnish: We now offer Varnish 5.2 and 6.0. 2018 December 2018 Elasticsearch 5.4: We now offer Elasticsearch 5.4 . Improved Bash support: Bash history on application containers now persists between logins. PHP 7.3: We now support PHP 7.3 . PostgreSQL 10.0 and 11.0: We now support PostgreSQL 10.0 and 11.0 with an automated upgrade path. Ruby 2.5 out of beta: We now fully support Ruby 2.5 . October 2018 Redis updates: Redis 4.0 and 5.0 are now supported. Go language support: Go is now a fully supported language platform. September 2018 Python 3.7 support: We now support Python 3.7 . August 2018 New public Canadian region: Our new Canadian region is now open for business. July 2018 Security and Compliance: We have created a new “Security and Compliance” section to help customers address common questions relating to GDPR, Data Collection, Data Retention, Encryption, and similar topics. June 2018 Node.js 10: We now offer Node.js version 10 . All releases in the 10.x series will be included in that container. MongoDB 3.6: We now offer MongoDB 3.2, 3.4, and 3.6 . Note that upgrading from MongoDB 3.0 requires upgrading through all intermediary versions. March 2018 Web Application Firewall (WAF): Platform.sh is securing your applications and you don’t need to change anything. Read more on our blog post . February 2018 post_deploy hook added: Projects can now run commands on deploy but without blocking new requests . 2017 December 2017 New project subdomains: The routes generated for subdomains and literal domains in development environments will now use . instead of translating them to ---, for projects created after this date. !include tag support in YAML files: All YAML configuration files now support a generic !include tag that can be used to embed one file within another. Extended mount definitions: A new syntax has been added for defining mount points that is more self-descriptive and makes future extension easier. Blocking older TLS versions: It is now possible to disable support for HTTPS requests using older versions of TLS. TLS 1.0 is known to be insecure in some circumstances and some compliance standards require a higher minimum supported version. {all} placeholder for routes: A new placeholder is available in routes.yaml files that matches all configured domains. GitLab source code integration: Synchronize Git repository host on GitLab to Platform.sh. November 2017 PHP 7.2 supported: With the release of PHP 7.2.0, Platform.sh now offers PHP 7.2 containers on Platform Professional. September 2017 Health notifications: Low-disk warnings will now trigger a notification via email, Slack, or PagerDuty. August 2017 Worker instances: Applications now support worker instances . July 2017 Node.js 8.2: Node.js 8.2 is now available. June 2017 Memcache 1.4: Memcache 1.4 is now available as a caching backend. Custom static headers in .platform.app.yaml: Added support for setting custom headers for static files in .platform.app.yaml. See the example for more information. May 2017 Code-driven variables in .platform.app.yaml: Added support for setting environment variables via .platform.app.yaml . Python 3.6, Ruby 2.4, Node.js 6.10: Added support for updated versions of several languages. April 2017 Support for automatic SSL certificates: All production environments are now issued an SSL certificate automatically through Let’s Encrypt. See the routing documentation for more information. MariaDB 10.1: MariaDB 10.1 is now available (accessible as mysql:10.1). Additionally, both MariaDB 10.0 and 10.1 now use the Barracuda file format with innodb_large_prefix enabled, which allows for much longer indexes and resolves issues with some UTF-8 MB use cases. March 2017 Elasticsearch 2.4 and 5.2 with support for plugins: Elasticsearch 2.4 and 5.2 are now available. Both have a number of optional plugins avaialble. See the Elasticsearch documentation for more information. InfluxDB 1.2: A new service type is available for InfluxDB 1.2, a time-series database. See the InfluxDB documentation for more information. February 2017 HHVM 3.15 and 3.18: Two new HHVM versions are now available. January 2017 Support for Multiple MySQL databases and restricted users: MySQL now supports multiple databases, and restricted users per MySQL service. See the MySQL documentation for details or read our blog post . Support for Persistent Redis services: Added a redis-persistent service that is appropriate for persistent key-value data. The redis service is still available for caching. See the Redis documentation for details. Support Apache Solr 6.3 with multiple cores: Added an Apache 6.3 service, which can be configured with multiple cores. See the Solr documentation for details. Support for HTTP/2: Any site configured with HTTPS will now automatically support HTTP/2. Read more on our blog post . 2016 December 2016 Support Async PHP: Deploy applications like ReactPHP and Amp which allow PHP to run as a single-process asynchronous process. Read more on our blog post . Pthreads: Multithreaded PHP: Our PHP 7.1 containers are running PHP 7.1 ZTS, and include the Pthreads extension. Read more on our blog post . PHP 7.1: Service is documented here . Support .environment files: This file will get sourced as a bash script by the system when a container boots up, as well as on all SSH logins. Feature is documented here . Support web.commands.start for PHP: That option wasn’t available for PHP as PHP only has one applicable application runner, PHP-FPM. It is now available for PHP. Read more on our blog post . November 2016 Customizable build flavor: Added a none build flavor which will not run any specific command during the build process. Use it if your application requires a custom build process which can be defined in your build hook. Read more in our blog post . October 2016 PostgreSQL 9.6: Service is documented here . PostgreSQL extensions: Read more in our blog post . Node.js 6.8: Language is documented here . September 2016 Python 2.7 \u0026amp; 3.5: Language is documented here . Ruby 2.3: Language is documented here . August 2016 Support Gitflow: Read more in our blog post . July 2016 Block Httpoxy security vulnerability: We bypass the Httpoxy security vulnerability by blocking the Proxy header from incoming HTTP headers. Read more in our blog post . Remove default configuration files: We are removing the default configuration files that were previously used if your project didn’t include one. You now need to include configuration files to deploy your applications on Platform.sh. Read more in our blog post . June 2016 June update is summarized in our blog post . New PLATFORM_PROJECT_ENTROPY variable: New variable which has a random value, stable throughout the project’s life. It can be used for Drupal hash salt for example (in our Drupal 8 example ). It is documented here Extend PLATFORM_RELATIONSHIPS variable: Expose the hostname and IP address of each service in the PLATFORM_RELATIONSHIPS environment variable. Services updates: Update MongoDB client to 3.2.7, Node.js to 4.4.5, Blackfire plugin to 1.10.6, Nginx to 1.11.1. May 2016 May update is summarized in our blog post . Pre-warms Composer cache before executing Composer: The composer build flavor now pre-warms the Composer cache before executing Composer. New image processing packages (advancecomp, jpegoptim, libjpeg-turbo-progs, optipng, pngcrush): Various image processing packages were added: advancecomp, jpegoptim, libjpeg-turbo-progs, optipng, pngcrush. Security updates: Including imagetragick, glibc issue, various Java, OpenSSL and OpenSSH issues, along with some Git CLI vulnerabilities. April 2016 White label capabilities (Magento Enterprise Cloud Edition): Support for Platform.sh white label offering. First launch at Magento Imagine 2016 in Las Vegas of Magento Enterprise Cloud Edition . March 2016 CloudWatt deployment: Platform.sh is now available on Cloudwatt Orange Business Services hosted infrastructure. Read more in our blog post . January 2016 Redis 3.0: Service is documented here . 2015 December 2015 Node.js 0.12, 4.4 \u0026amp; 6.2: Read more in our blog post November 2015 Java Ant \u0026amp; Maven build scripts: Java Ant and Maven build scripts is supported for PHP 5.6 and up. Your application can pull and use most of Java dependency. Read more in our blog post October 2015 MariaDB/MySQL 10.0: Service is documented here . MongoDB 3.0: Service is documented here . September 2015 PHP 5.4, 5.5 \u0026amp; 5.6: Read more in our blog post RabbitMQ 3.5: Service is documented here . Read more in our blog post HHVM 3.9 \u0026amp; 3.12: Read more in our blog post July 2015 Documentation 3.0 release: Read more in our blog post . June 2015 Bitbucket integration: This add-on allows you to deploy any branch or pull request on a fully isolated Platform.sh environment with a dedicated URL. Read more in Bitbucket’s blog post . PostgreSQL 9.3: Service is documented here . May 2015 UI 2.0 release: Read more in our blog post . February 2015 Blackfire integration: PHP applications come pre-installed with the Blackfire Profiler developed by SensioLabs . Read more in our blog post . January 2015 Redis 2.8: Service is documented here . 2014 November 2014 Read more about this release in our blog post . HTTP caching per route: Support for HTTP caching at the web server level, finely configurable on a per-route basis. Custom PHP configurations: Support for tweaking the PHP configuration, by enabling / disabling extensions and shipping your own php.ini. Build dependencies: Support for specifying build dependencies, i.e. PHP, Python, Ruby or Node.js tools (like sass, grunt, uglifyjs and more) that you want to leverage to build your PHP application. Elasticsearch 0.90, 1.4 \u0026amp; 1.7: Service is documented here . October 2014 Automated protective block: Platform.sh provides a unique approach to protect your applications from known security issues. An automated protective blocking system which works a bit like an antivirus: it compares the code you deploy on Platform.sh with a database of signatures of known security issues in open source projects. This feature is documented here . Read more in our blog post . Solr 4.10: Service is documented here . July 2014 MariaDB/MySQL 5.5: Service is documented here . Solr 3.6: Service is documented here .",
        "section": "Platform.sh",
        "subsections": " 2020 2019 2018 2017 2016 2015 2014  ",
        "image": "",
        "url": "/other/changelog.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "18989c6838541b94952b0f2f93ef8362",
        "title": "Cloudflare configuration",
        "description": "",
        "text": " One of the main features that a modern DNS provider needs to have in order to work well with Platform.sh is colloquially known as “Cname Flattening”. This solves the problem of being able to point your “root domain” (example.com) to a domain name (CNAME) rather than an IP address (A record). This post explains it well. In order to correctly point DNS to your Platform.sh project, you’ll need at the very least the master environment CNAME, in other words the domain of your site before you add a custom domain on the management console for that project (or otherwise in the CLI). This is the value you would get from Step 2 of the pre-launch checklist . Assuming that you are using both a www. subdomain as well as the bare domain, you’ll want to point both of those DNS entries to the same place. Whether you choose the bare domain version or the www subdomain doesn’t make any practical difference, as they both will reach Platform.sh and be handled correctly. Enable “Full SSL” option in the Cloudflare admin Cloudflare also makes it very simple to use their free TLS/SSL service to secure your site via HTTPS, while also being behind their CDN if you so choose. If you decide to use Cloudflare’s CDN functionality in addition to their DNS service, you should be sure to choose the “Full SSL” option in the Cloudflare admin. This means that traffic to your site is encrypted from the client (browser) to Cloudflare’s servers using their certificate, and also between Cloudflare’s servers and your project hosting here at Platform.sh, mostly like using your project’s Let’s Encrypt certificate. # Cloudflare\u0026#39;s Full SSL option https https User \u0026lt;---------------\u0026gt; Cloudflare \u0026lt;-------------\u0026gt; Platform.sh The other option known as “Flexible SSL” will cause issues if you intend to redirect all traffic to HTTPS. The “Flexible SSL” option will use Cloudflare’s TLS/SSL certificate to encrypt traffic between your users and the CDN, but will pass requests from the CDN back to your project at Platform.sh via HTTP. This can make it easy for sites that don’t have a TLS/SSL certificate to begin ofering their users a more secure experience, by at the least eliminating the unencrypted attack vector on the the “last mile” to the user’s browser. # Cloudflare\u0026#39;s Flexible SSL option https http User \u0026lt;---------------\u0026gt; Cloudflare \u0026lt;-------------\u0026gt; Platform.sh This will cause all traffic from Cloudflare to your project to be redirected to HTTPS, which will set off an endless loop as HTTPS traffic will be presented as HTTP to your project no matter what. In short: Always use “Full SSL” unless you have a very clear reason to do otherwise",
        "section": "Content Delivery Networks",
        "subsections": " Enable \u0026ldquo;Full SSL\u0026rdquo; option in the Cloudflare admin  ",
        "image": "",
        "url": "/golive/cdn/cloudflare.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "45584cb66ba776d1feda9da54211c588",
        "title": "Composer manager",
        "description": "",
        "text": " Drupal 7 does not natively support installing packages via Composer. Although there is a Drupal Composer project for Drupal 7 , many projects are still built using a vanilla Drupal download or with Drush Make. For sites built without Drupal Composer that still want to use modules that have Composer dependences, the most widely used option is the Composer Manager module. Because of the read-only file system, however, there are specific configuration parameters necessary on Platform.sh. 1. Install and patch Composer Manager Install the Composer Manager module in the manner appropriate for your site. There are also 2 patch files needed that have not been committed to the module yet. If you’re installing it via Drush Make, add the appropriate lines to your .make file: projects[composer_manager][version] =  1.8  projects[composer_manager][patch][] =  https://www.drupal.org/files/issues/composer_manager-2620348-3.patch  projects[composer_manager][patch][] =  https://www.drupal.org/files/issues/composer_manager-relative_realpath-2864297-5.patch  If you’re checking the entire codebase into Git, do so with Composer Manager as well. Then download and apply the two patches in the issues above and commit the result. It’s important to uncheck the two “Automatically…” options at the config page on admin/config/system/composer-manager/settings. If checked, Drupal tries to update the composer folder. Since this isn’t a writable mount, installation of composer based modules will fail due to these writing permissions. 2. Configure file locations Composer Manager works by using a Drush command to aggregate all module-provided composer.json files into a single file, which can then be installed via a normal Composer command. Both the generated file and the resulting vendor directory must be in the application portion of the file system, that is, not in a writable file mount. As that is not the default configuration for Composer Manager it will need to be changed. Add the following lines to your settings.php file: $conf[\u0026#39;composer_manager_vendor_dir\u0026#39;] = \u0026#39;../composer/vendor\u0026#39;; $conf[\u0026#39;composer_manager_file_dir\u0026#39;] = \u0026#39;../composer\u0026#39;; The above lines will direct Composer Manager to put the generated composer.json file into ../composer/, and to autoload Composer-based packages from the ../composer/vendor directory. The paths are relative to the Drupal root. You may use another location if desired provided that they are not in a writable file mount, and provided the vendor directory is a sibling of wherever the composer.json file will be. Then, manually create the composer directory and place a .gitignore file inside it, containing the following, and commit it to Git: # Exclude Composer dependencies. /vendor 3. Update the build hook Create a build hook in your .platform.app.yaml file that will install Composer dependencies: hooks:build:| # Install Composer dependencies.cdcomposercomposerinstall--no-interaction--optimize-autoloader--no-devReplace composer with whatever the path to the composer directory is. Note that if using the drupal build flavor with Drush Make, then the composer directory may have been moved into the same directory as your modules, like public/sites/default. It can be moved back via another line in the build hook: hooks:build:| # Move the Composer directory (back) to the application root.mvpublic/sites/default/composercomposer# Install Composer dependencies.cdcomposercomposerinstall--no-interaction--optimize-autoloader--no-devThe composer install command may also be further customized as desired. 4. Generate and commit the composer.* files locally From the Drupal root on your local system, run drush composer-json-rebuild to generate the aggregated composer.json file. Then, change to the composer directory and run composer install yourself, to generate the composer.lock file and download all dependencies. The composer.json and composer.lock files must be committed to the repository. The vendor directory must not be. That way, during build the Composer command in the build hook will download the exact version of all dependent packages listed in the composer.lock file, which helps keep builds consistent and predictable. Any time a new module with Composer dependencies is added, or a new version of a dependent library is available, repeat step 4 only.",
        "section": "Getting Started",
        "subsections": " 1. Install and patch Composer Manager 2. Configure file locations 3. Update the build hook 4. Generate and commit the composer.* files locally  ",
        "image": "",
        "url": "/frameworks/drupal7/composer-manager.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "b399d3e553aaf012dab49f7bd4293100",
        "title": "Configure routes",
        "description": "",
        "text": " Platform.sh allows you to define the routes used in your environments. A route describes how an incoming HTTP request is going to be processed by Platform.sh. The routes are defined using .platform/routes.yaml file in your Git repository. If you don’t have one, use the commands below to create it: $ mkdir .platform $ touch .platform/routes.yaml Route templates The YAML file is composed of a list of routes and their configuration. A route can either be an absolute URL or a URL template that looks like: http://www.{default}/ or https://{default}/blog where {default} will be substituted by the default fully qualified domain name configured in the project. So if your default domain is example.com, these routes will be resolved to http://www.example.com/ and https://example.com/blog in the master environment. Platform.sh will also generate a domain for every active development environment. It will receive a domain name based on the region, project ID, branch name, and a per-environment random string. The domain name itself is not guaranteed stable, although the pattern is consistent. Note: Platform.sh supports running multiple applications per environment. The .platform/routes.yaml file defines how to route requests to different applications. Route configuration Each route can be configured separately. It has the following properties type can be: upstream serves an application It will then also have an upstream property which will be the name of the application (as defined in .platform.app.yaml), followed by “:http” (see examples below). redirect redirects to another route It will then be followed by a to property, this defines a HTTP 301 redirect to any URL or another route (see examples below). cache controls caching behavior of the route . ssi controls whether Server Side Includes are enabled. For more information: see SSI . redirects controls redirect rules associated with the route. Note: For the moment, the value of upstream is always in the form: \u003capplication-name\u003e:http. \u003capplication-name\u003e is the name defined in .platform.app.yaml file. :php is a deprecated application endpoint; use :http instead. In the future, Platform.sh will support multiple endpoints per application. Route limits Although there is no fixed limit on the number of routes that can be defined, there is a cap on the size of generated route information. This limitation comes from the Linux kernel, which caps the size of environment variables. The kernel limit on environment variables is 32 pages. Each page is 4k on x86 processors, resulting in a maximum environment variable length of 131072 bytes. If your routes.yaml file would result in too large of a route information value it will be rejected. The full list of generated route information is often much larger than what is literally specified in the routes.yaml file. For example, by default all HTTPS routes will be duplicated to create an HTTP redirect route. Also, the {all} placeholder will create two routes (one HTTP, one HTTPS) for each domain that is configured. As a general rule we recommend keeping the defined routes under 100. Should you find your routes.yaml file rejected due to an excessive size the best alternative is to move any redirect routes to the application rather than relying on the router, or collapsing them into a regular expression-based redirect within a route definition. Let’s Encrypt also limits an environment to 100 configured domains. If you try to add more than that some of them will fail to get an SSL certificate. Routes examples Here is an example of a basic .platform/routes.yaml file:  https://{default}/ :type:upstreamupstream: app:http  https://www.{default}/ :type:redirectto: https://{default}/ In this example, we will route both the apex domain and the www subdomain to an application called “app” , the www subdomain being redirected to the apex domain using an HTTP 301 Moved Permanently response. In the following example, we are not redirecting from the www subdomain to the apex domain but serving from both:  https://{default}/ :type:upstreamupstream: app:http  https://www.{default}/ :type:upstreamupstream: app:http Route placeholders You can configure any number of domains on a project when you are ready to make it live. Only one of them may be set as the “default” domain. In the routes.yaml file a route can be defined either literally or using one of two special placeholders: {default} and {all}. The magic value {default} will be replaced with the production domain name configured as the default on your account in the production branch. In a non-production branch it will be replaced with the project ID and environment ID so that it is always unique. The magic value {all} will be replaced by all of the domain names configured on the account. That is, if two domains example1.com and example2.com are configured, then a route named https://www.{all}/ will result in two routes in production: https://www.example1.com and https://www.example2.com. That can be useful in cases when a single application is serving two different websites simultaneously. In a non-production branch it will be replaced with the project ID and environment ID for each domain, in the same fashion as a static route below. If there are no domains defined on a project (as is typical in development before launch) then the {all} placeholder will behave exactly like the {default} placeholder. It’s also entirely possible to use an absolute URL in the route. In that case, it will be used as-is in a production environment. On a development environment it will be mangled to include the project ID and environment name. For example:  https://www.example.com/ :type:upstreamupstream: app:http  https://blog.example.com/ :type:upstreamupstream: blog:http In this case, there are two application containers app and blog. In a production environment, they would be accessible at www.example.com and blog.example.com, respectively. On a development branch named sprint, however, they would be accessible at URLs something like: https://www.example.com.sprint-7onpvba-tvh56f275i3um.eu-2.platformsh.site/ https://blog.example.com.sprint-7onpvba-tvh56f275i3um.eu-2.platformsh.site/ If your project involves only a single apex domain with one app or multiple apps under subdomains, it’s generally best to use the {default} placeholder. If you are running multiple applications on different apex domains then you will need to use a static domain for all but one of them. Please note that when there are two routes sharing the same HTTP scheme, domain, and path, where the first route is using the {default} placeholder and the other is using the {all} placeholder, the route using {default} takes precedence. Route identifiers All routes defined for an environment are available to the application in its PLATFORM_ROUTES environment variable, which contains a base64-encoded JSON object. This object can be parsed by the language of your choice to give your application access to the generated routes. When routes are generated, all placeholders will be replaced with appropriate domains names, and depending on your configuration, additional route entries may be generated (e.g. automatic HTTP to HTTPS redirects). To make it easier to locate routes in a standardized fashion, you may specify an id key on each route which remains stable across environments. You may also specify a single route as primary, which will cause it to be highlighted in the web management console but will have no impact on the runtime environment. Consider this routes.yaml configuration example:  https://site1.{default}/ :type:upstreamupstream:'site1:http' https://site2.{default}/ :type:upstreamid:'the-second'upstream:'site2:http'This example defines two routes, on two separate subdomains, pointing at two separate app containers. (However, they could just as easily be pointing at the same container for our purposes). On a branch named test, the route array in PHP would look like this: Array ( [https://site1.test-t6dnbai-abcdef1234567.us-2.platformsh.site/] =\u003e Array ( [primary] =\u003e 1 [id] =\u003e [type] =\u003e upstream [upstream] =\u003e site1 [original_url] =\u003e https://site1.{default}/ // ... ) [https://site2.test-t6dnbai-abcdef1234567.us-2.platformsh.site/] =\u003e Array ( [primary] =\u003e [id] =\u003e the-second [type] =\u003e upstream [upstream] =\u003e site2 [original_url] =\u003e https://site2.{default}/ // ... ) [http://site1.test-t6dnbai-abcdef1234567.us-2.platformsh.site/] =\u003e Array ( [to] =\u003e https://site1.test-t6dnbai-abcdef1234567.us-2.platformsh.site/ [original_url] =\u003e http://site1.{default}/ [type] =\u003e redirect [primary] =\u003e [id] =\u003e ) [http://site2.test-t6dnbai-abcdef1234567.us-2.platformsh.site/] =\u003e Array ( [to] =\u003e https://site2.test-t6dnbai-abcdef1234567.us-2.platformsh.site/ [original_url] =\u003e http://site2.{default}/ [type] =\u003e redirect [primary] =\u003e [id] =\u003e ) ) (Some keys omitted for space.) Note that the site2 HTTPS route has an id specified as the-second while other routes have no ID. Futhermore, because we did not specify a primary route, the first non-redirect route defined is marked as the primary route by default. In each case, the original_url specified in the configuration file is accessible if desired. That makes it straightforward to look up the domain of a particular route, regardless of what branch it’s on, from within application code. For example, the following PHP function will retrieve the domain for a specific route id, regardless of the branch it’s on. function get_domain_for(string $id) { foreach ($routes as $domain =\u003e $route) { if ($route['id'] == $id) { return $domain; } } } That can be used, for example, for specifically allowing inbound requests, a feature of many PHP frameworks. Route attributes Route attributes are an arbitrary key/value pair attached to a route. This metadata does not have any impact on Platform.sh, but will be available in the route definition structure in $PLATFORM_ROUTES.  http://{default}/ :type:upstreamupstream: app:http attributes: foo :  bar Attributes will appear in the routes data for PHP like so: [https://site1.test-t6dnbai-abcdef1234567.us-2.platformsh.site/] =\u003e Array ( [primary] =\u003e 1 [id] =\u003e [type] =\u003e upstream [upstream] =\u003e site1 [original_url] =\u003e https://site1.{default}/ [attributes] =\u003e Array ( [foo] =\u003e bar ) // ... ) These extra attributes may be used to “tag” routes in more complex scenarios that can then be read by your application. Configuring routes on the management console Routes can also be configured using the management console in the routes section of the environment settings. If you have edited the routes via the management console, you will have to git pull the updated .platform/routes.yaml file from us. CLI Access You can get a list of the configured routes of an environment by running platform environment:routes. If you need to see more detailed info, such as cache and ssi, use platform route:get Wildcard routes Platform.sh supports wildcard routes, so you can map multiple subdomains to the same application. This works both for redirect and upstream routes. You can simply prefix the route with a star (*), for example *.example.com, and HTTP request to www.example.com, blog.example.com, us.example.com will all get routed to the same endpoint. For your master environment, this would function as a catch-all domain once you added the parent domain to the project settings. For development environments, we will also be able to handle this. Here is how: Let’s say we have a project on the EU cluster whose ID is “vmwklxcpbi6zq” and we created a branch called “add-theme”. It’s environment name will be similar to add-theme-def123. The generated apex domain of this environment will be add-theme-def123-vmwklxcpbi6zq.eu.platform.sh. If we have a http://*.{default}/ route defined, the generated route will be http://*.add-theme-def123-vmwklxcpbi6zq.eu.platform.sh/. This means you could put any subdomain before the left-most . to reach your application. HTTP request to both http://foo.add-theme-def123-vmwklxcpbi6zq.eu.platform.sh/ and http://bar.add-theme-def123-vmwklxcpbi6zq.eu.platform.sh/ URLs will be routed to your application properly. However, request to http://*.add-theme-def123-vmwklxcpbi6zq.eu.platform.sh/ will not be routed since it is not a legitimate domain name. Be aware, however, that we do not support Let’s Encrypt wildcard certificates (they would need DNS validation). That means if you want to use a wildcard route and protect it with HTTPS you will need to provide a custom TLS certificate . Note: In projects created before November 2017 the . in subdomains was replaced with a triple dash (---). It was switched to preserve . to simplify SSL handling and improve support for longer domains. If your project was created before November 2017 then it will still use --- to the left of the environment name. If you wish to switch to dotted-domains please file a support ticket and we can do that for you. Be aware that doing so may change the domain name that your production domain name should CNAME to. WebSocket routes To use WebSocket on a route, cache must be disabled because WebSocket is incompatible with buffering, which is a requirement of caching on our router. Here is an example to define a route that serves WebSocket:  https://{default}/ws :type:upstreamupstream: app:http cache:enabled:false",
        "section": "Configuration",
        "subsections": " Route templates Route configuration Route limits Routes examples Route placeholders Route identifiers Route attributes Configuring routes on the management console CLI Access Wildcard routes WebSocket routes  ",
        "image": "",
        "url": "/configuration/routes.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "6438412924c2bd66b579d62667929e9a",
        "title": "Connect to services",
        "description": "",
        "text": " Now that you have a local copy of your application code, you can make changes to the project without pushing to Platform.sh each time to test them. Instead you can locally build your application using the CLI, even when its functionality depends on a number of services. Note: If your application does not contain any services, you do not need to open a tunnel and can proceed to the next step. Open an SSH tunnel to connect to your services Open local SSH tunnels to your environment’s services. platform tunnel:open Export environment variables Platform.sh utilizes environment variables called Relationships within the application container. These store the credentials needed to connect to individual services. In order to connect with them remotely using the SSH tunnel you will need to mimic the same environment variables locally. export PLATFORM_RELATIONSHIPS= $(platform tunnel:info --encode)  In order to use these credentials to connect to your services, it may also be necessary to install the clients for those services are locally. Additionally, if your application also needs access to the PORT environment variable, you can mock the variable used in a Platform.sh environment with export PORT=8888 If you are using a Config Reader library with the application, it will also be necessary to mock two additional variables export PLATFORM_APPLICATION_NAME=\u0026lt;.platform.app.yaml name, i.e. app\u0026gt; export PLATFORM_BRANCH=\u0026lt;branch being built, i.e. dev\u0026gt; Verify You can visualize the open tunnels for your application with the command platform tunnel:list The tunnel will close itself after a timeout period of inactivity, but you can also do so with the command platform tunnel:close Now that you have created an SSH tunnel to your services, build your application locally. Back I\u0026#39;ve opened an SSH tunnel into my services",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/developing/local-development/connect-services.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "5fca82741ee8914b75e2544f75403423",
        "title": "Create a new project",
        "description": "",
        "text": " In the previous step, you set up a free trial account with Platform.sh. Now you have access to the Platform.sh management console . From here you can create projects, adjust account settings, and a lot more that you will explore throughout these Getting Started guides. Since you do not yet have any projects on Platform.sh, the first thing you will see when you sign in is a workflow for creating a new project. Project type You will first be given the option of creating a New Project that is empty ( one that you can migrate your own code base to ), or to Use a Template. Select the Use a Template option, and then click Next. Details Give your project a name like My First Project. In the next field, you have the option to configure the project’s region, which corresponds with the data center where your project will live. Select the region that most closely matches where most of your traffic will come from and click Next. Template Now you will be able to see the large collection of Platform.sh’s supported templates. There are several types of templates available, including simple language-specific examples, ready-to-use frameworks you can build upon, and setapplications you can start using immediately after installation. If you are more comfortable with a particular language, click the dropdown labeled All languages. Select a language and the template list will update. Select a template and click Next. Note: You can find the source code for all Platform.sh templates in the GitHub Templates Organization. Plan \u0026amp; Pricing Under your free trial, your project will be created under a “Development” plan size. The management console will tell you how many users, development environments, and the price of that plan after your trial has completed. After you have read through the features, click Continue. With these few selections Platform.sh will create the project and build the template in less than two minutes. Back I\u0026#39;ve created a new project with a template",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/template/create-project.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "a47be1b5ba7b9e91da1de1ac98313fa5",
        "title": "Create environment",
        "description": "",
        "text": " Platform.sh supports the deployment of isolated development environments from your branches, complete with exact copies of all of your data. This is useful for testing changes in isolation before merging them. Create branch, make changes, push to Platform.sh Create and checkout a branch for your new feature. platform branch dev This command will create a new branch dev from master, as well as a local dev branch for you to work on. dev will be a clone of master, including an exact-copy of all of its data and files. Make changes to your code, commit them, and push to the Platform.sh remote. git add . git commit -m  Commit message.  git push platform dev Note: If you create a new branch with Git (i.e. git checkout -b new-feature), when you push its commits to Platform.sh that branch will not automatically build. new-feature is an inactive environment, because Platform.sh does not assume that every branch should be associated with an active environment, since your plan will limit the number of active environments you are allowed. If you want to activate the new-feature environment after it has been pushed, you can do so with the command platform environment:activate new-feature Verify After Platform.sh has built and deployed the environment, verify that it has been activated by visiting its new url: platform url The command will provide a list of the generated routes for the application according to how the routes were configured. The structure will be: Enter a number to open a URL [0] https://\u0026lt;branch name\u0026gt;-\u0026lt;hash of branch name\u0026gt;-\u0026lt;project ID\u0026gt;.\u0026lt;region\u0026gt;.platformsh.site/ [1] https://www.\u0026lt;branch name\u0026gt;-\u0026lt;hash of branch name\u0026gt;-\u0026lt;project ID\u0026gt;.\u0026lt;region\u0026gt;.platformsh.site/ The above URLs represent the upstream (0) and redirect (1) routes for the most common routes.yaml configuration. Back I\u0026#39;ve created a development environment",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/developing/dev-environments/create-environment.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "7f3a1a50032fe827888ec42d287d9d28",
        "title": "Development environments",
        "description": "",
        "text": " While you’re developing your application, you will at some point create some new features for it. Typically you’re going to develop that feature on a separate branch in your Git repository, run some tests, and then merge that feature into your production application. This is where the stress comes in and where breaking your live site becomes a real worry. Platform.sh removes this stress considerably by providing live development environments for the features you’re working on. This guide assumes that you have already: Signed up for a free trial account with Platform.sh. Started either a template project or pushed your own code to Platform.sh. If you have not completed these steps by now, click the links and do so before you begin. Get started!",
        "section": "Introduction",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/developing/dev-environments.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "702e7331732912988d7075d3566d0496",
        "title": "Exporting data",
        "description": "",
        "text": " Platform.sh aims to be a great host, but we never want to lock you in to our service. Your code and your data belong to you, and you should always be able to download your site’s data for local development, backup, or to “take your data elsewhere”. Downloading code Your application’s code is maintained in Git. Because Git is a distributed system it is trivial to download your entire code history with a simple git clone or platform get command. Downloading files Your application runs on a read-only file system, so it cannot be edited. That means there’s nothing to download from most of it that isn’t already in your Git repository. The only files to download are from any writable file mounts you may have defined in your .platform.app.yaml file. The easiest way to download those is using the rsync tool. For instance, suppose you have a mounts section that defines one web-accessible directory and one non-web-accessible directory: mounts:\u0026#39;web/uploads\u0026#39;:source:localsource_path:uploads\u0026#39;private\u0026#39;:source:localsource_path:privateUsing the CLI The CLI provides a useful mount command for accessing mount data. platform mount:list Downloading a mount is then as simple as running the following: platform mount:download Using rsync To use rsync to download each directory, we can use the following commands. The platform ssh --pipe command will return the SSH URL for the current environment as an inline string that rsync can recognize. To use a non-default environment, use the -e switch after --pipe. Note that the trailing slash on the remote path means rsync will copy just the files inside the specified directory, not the directory itself. rsync -az `platform ssh --pipe`:/app/private/ ./private/ rsync -az `platform ssh --pipe`:/app/web/uploads ./uploads/ Note: If you’re running rsync on MacOS, you should add --iconv=utf-8,utf-8-mac to your rsync call. See the rsync documentation for more details on how to adjust the download process. Download data from services The mechanism for downloading from each service (such as your database) varies. For services designed to hold non-persistent information (such as Redis or Solr) it’s generally not necessary to download data as it can be rebuilt from the primary data store. To download data from persistent services ( MySQL , PostgreSQL , MongoDB , or InfluxDB ), see each service’s page for instructions.",
        "section": "Tutorials",
        "subsections": " Downloading code Downloading files  Using the CLI Using rsync   Download data from services  ",
        "image": "",
        "url": "/tutorials/exporting.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "13602a80bb29527d60e82bc0751512ac",
        "title": "Frameworks",
        "description": "",
        "text": " Full Drush support and Composer-based builds make handling dependencies and builds for PHP frameworks as simple as committing your composer.json to your project. Drupal Drupal is an open-source content management framework written in PHP. Since Composer comes pre-installed on Platform.sh, Drupal can be installed and updated completely using Composer. The default build flavor for PHP application runs composer install during build, handling all of your dependencies automatically. Drupal 7 Best Practices Drupal 8 Best Practices Community Support Drupal FAQs, how-to guides and other tutorials right on Platform.sh Community . Drupal on Platform.sh Community Templates Drupal 7 Drupal 7 (Vanilla) Drupal 8 Drupal 8 (Multisite variant) Opigno GovCMS8 eZ Platform eZ Platform is a CMS based on the Symfony full-stack framework. eZ Platform comes pre-configured for use with Platform.sh for versions 1.13 and later, all it takes is mapping a few environment variables to an existing project. Consult the caching, configuration, and local development best practices for eZ Platform and Fastly integration for more information. eZ Platform Best Practices Example Projects Note: Template projects (repositories in the platformsh-templates GitHub organization) are actively maintained by the Platform.sh team. Any other example projects come with less support, and remain in public repositories as proof-of-concepts. eZ Platform Symfony Symfony is a web application framework written in PHP. Like Drupal, Symfony projects can utilize native Composer to build applications and manage dependencies. Symfony Best Practices Community Support Symfony FAQs, how-to guides and other tutorials right on Platform.sh Community . Symfony on Platform.sh Community Templates Symfony 3 Symfony 4 TYPO3 TYPO3 is an open-source CMS written in PHP. Utilized Platform.sh native Composer to handle builds and maintain dependencies. TYPO3 Best Practices Example Projects Note: Template projects (repositories in the platformsh-templates GitHub organization) are actively maintained by the Platform.sh team. Any other example projects come with less support, and remain in public repositories as proof-of-concepts. TYPO3 Wordpress Wordpress is a PHP content management system. Platform.sh recommends using the composer-based installation method for Wordpress. Wordpress Best Practices Community Support All your Wordpress FAQs, plus how-to guides and tutorials right on Platform.sh Community . Wordpress on Platform.sh Community Templates Wordpress",
        "section": "PHP",
        "subsections": " Drupal  Community Support Templates   eZ Platform  Example Projects   Symfony  Community Support Templates   TYPO3  Example Projects   Wordpress  Community Support Templates    ",
        "image": "",
        "url": "/languages/php/frameworks.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "7ebe9ded6d61c1e6df9aca4b06c91069",
        "title": "Getting Started",
        "description": "",
        "text": " Drupal 8 and Composer The recommended way to deploy Drupal 8 on Platform.sh is to use Composer. Composer is the PHP package management suite, and is now supported by Drupal 8 (and Drupal 7 in a pinch). There is an unofficial but well-supported Composer flavor of Drupal 8 called Drupal Composer that we recommend. If you use the Drupal 8 Example Repository or select the Drupal 8 option when creating a new project from a template, that’s what you will be using. You can also create your own project directly from that repository and add the Platform.sh-specific configuration files. Note that you will also need to add the Drupal.org Composer repositories to composer.json if you are not working from our template. If you use Drupal Composer, note that any 3rd party modules, themes, or PHP libraries you install, as well as Drupal core itself, will not be checked into your repository. They are specifically excluded from Git by a .gitignore file, as they will be re-downloaded when you run composer install or composer update. Rather than downloading modules or themes using wget or FTP, you can add them using composer. For example, to add the devel module you would run this command: $ composer require drupal/devel And then commit just the changes to composer.json and composer.lock to your repository. That also means that to get a working copy of your site locally you will need to run composer install to download all of the necessary libraries and modules. We also strongly recommend installing the Platform.sh Config Reader library, which simplifies access to the Platform.sh environment. The rest of this documentation assumes you have it installed. $ composer require platformsh/config-reader Note: When using Composer, your docroot where most of Drupal lives will be called web, but the vendor directory will be outside of that directory in contrast to how a standard Drupal download .tar.gz file is organized. The config export directory will also be outside of the web root. This is normal, expected, and more secure. File organization Your repository should be laid out as follows: composer.json composer.lock config/ sync/ \u003cthis is where exported configuration will go\u003e drush/ .git/ .gitignore .platform/ routes.yaml services.yaml .platform.app.yaml scripts/ web index.php ... (other Drupal core files) modules/ contrib/ \u003cempty until composer runs\u003e custom/ \u003cyour custom modules here\u003e themes/ contrib/ \u003cempty until composer runs\u003e custom/ \u003cyour custom themes here\u003e sites/ default/ settings.php settings.platformsh.php Changes to settings.php Platform.sh exposes database configuration, as well as other configuration values such as a hash salt, to PHP as environment variables available either via $_ENV or getenv(). That means you’ll need to tell Drupal how to get that information. Additionally, Drupal needs to be told where the config export directory is, where the private files directory is (which is outside of the web root), and so on. The easiest way to access that information is via a small configuration add-on we provide. See our recommended settings.php file , which includes a file called settings.platformsh.php . The latter maps all Platform.sh-provided environment values to Drupal settings, either the Drupal database array or the global $settings object. If run on a non-Platform.sh server this file does nothing so it is safe to always include. If you need to add additional Platform.sh-specific configuration, such as to enable a Redis server for caching, we recommend also putting it into settings.platformsh.php. Vanilla Drupal 8 If you prefer, Drupal 8 can also be installed “vanilla” from Drupal.org, with the entire site checked into the repository. While not recommended it is fully supported. At the end of the day Platform.sh doesn’t care where your files come from, just that you tell the system where they are! You will still need to put the Drupal docroot in a subdirectory of your repository. We recommend web for consistency but any directory name will do. If using a vanilla Drupal install, your repository should look something like this: .git/ .gitignore config/ sync/ .platform/ routes.yaml services.yaml .platform.app.yaml web/ index.php ... (other Drupal core files) core/ modules/ sites/ sites/ default/ settings.php settings.platformsh.php Note the settings.php and settings.platformsh.php files. Both should be identical to the ones used for a Composer-based site. Also note that the config/sync directory is still outside the docroot. That is recommended for all Drupal installs generally, and is configured by the settings.php file. Configuring Platform.sh for Drupal The ideal .platform.app.yaml file will vary from project to project, and you are free to customize yours as needed. A recommended baseline Drupal 8 configuration is listed below, and can also be found in our Drupal 8 template project . # This file describes an application. You can have multiple applications# in the same project.## See https://docs.platform.sh/user_guide/reference/platform-app-yaml.html# The name of this app. Must be unique within a project.name:'app'# The runtime the application uses.type:'php:7.4'# The relationships of the application with services or other applications.## The left-hand side is the name of the relationship as it will be exposed# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand# side is in the form `\u003cservice name\u003e:\u003cendpoint name\u003e`.relationships:database:'db:mysql'redis:'cache:redis'# The size of the persistent disk of the application (in MB).disk:2048# The 'mounts' describe writable, persistent filesystem mounts in the application.mounts:'/web/sites/default/files':source:localsource_path:'files''/tmp':source:localsource_path:'tmp''/private':source:localsource_path:'private''/.drush':source:localsource_path:'drush''/drush-backups':source:localsource_path:'drush-backups''/.console':source:localsource_path:'console'# Configuration of the build of this application.build:flavor:composer# The hooks executed at various points in the lifecycle of the application.hooks:# The build hook runs after Composer to finish preparing up your code.build:| set -ebashinstall-redis.sh4.3.0# The deploy hook runs after your application has been deployed and started.deploy:| set -ephp./drush/platformsh_generate_drush_yml.phpcdwebdrush-ycache-rebuilddrush-yupdatedbdrush-yconfig-import# The configuration of app when it is exposed to the web.web:# Specific parameters for different URL prefixes.locations:'/':# The folder from which to serve static assets, for this location.## This is a filesystem path, relative to the application root.root:'web'# How long to allow static assets from this location to be cached.## Can be a time in seconds, or -1 for no caching. Times can be# suffixed with  s  (seconds),  m  (minutes),  h  (hours),  d # (days),  w  (weeks),  M  (months, as 30 days) or  y  (years, as# 365 days).expires:5m# Whether to forward disallowed and missing resources from this# location to the application.## Can be true, false or a URI path string.passthru:'/index.php'# Deny access to static files in this location.allow:false# Rules for specific URI patterns.rules:# Allow access to common static Deny direct access to configuration Allow access to all files in the public files directory.allow:trueexpires:5mpassthru:'/index.php'root:'web/sites/default/files'# Do not execute PHP scripts.scripts:falserules:# Provide a longer TTL (2 weeks) for aggregated CSS and JS files.'^/sites/default/files/(css|js)':expires:2w# The configuration of scheduled execution.crons:drupal:spec:'*/20 * * * *'cmd:'cd web ; drush core-cron'",
        "section": "Featured frameworks",
        "subsections": " Drupal 8 and Composer  File organization Changes to settings.php   Vanilla Drupal 8 Configuring Platform.sh for Drupal  ",
        "image": "",
        "url": "/frameworks/drupal8.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "9dde4f32b7dc0685d7cca3d1af46ca85",
        "title": "Going Live - Steps",
        "description": "",
        "text": " Going live on Platform.sh is a simple two or three step process. You can either use the Platform.sh management console or the CLI to configure your project for production. Once you have gone through it once the whole process usually takes a couple of minutes. Note: The order of operations is not really important, but if you are migrating a site from an existing provider, you should first configure the domain on the Platform.sh side, and only then switch DNS over. 1. Change your plan to a production plan If you are on a Development plan, you cannot add a domain. You will need to upgrade your subscription to a production plan. Go to your account , click on the small wheel next to you project’s name and click on edit. You can also access information about the project’s plan under “Billing”, and then by selecting the project from your list of projects. You can make changes to the project by clicking ‘Upgrade Plan’. You can make changes to the type of plan, the number of environments, amount of storage and number of users here. When you make changes, it will update the monthly price you will be paying. Click Upgrade plan to save the new settings. You can find more information on pricing on the pricing page . 2. (CDN version) Configure your DNS provider If you are serving the site through a CDN, configure your DNS provider to point at your CDN account. The address or CNAME to set for that will vary with the CDN provider. Refer to their documentation or to the CDN guide . 2. (Non-CDN version) Configure your DNS provider Configure your DNS provider to point your domain to your Platform.sh Master environment domain name. The way to do so will vary somewhat depending on your registrar, but nearly all registrars should allow you to set a CNAME. Some will call it an Alias or similar alternate name, but either way the intent is to say “this domain should always resolve to… this other domain”. You can access the CNAME target by running platform environment:info edge_hostname. That is the host name by which Platform.sh knows your environment. Add a CNAME record from your desired domain (www.example.com) to the value of the edge_hostname. If you have multiple domains you want to be served by the same application you will need to add a CNAME record for each of them. Note that depending on your registrar and the TTL you set, it could take anywhere from 15 minutes to 72 hours for the DNS change to fully propagate across the Internet. If you are using an apex domain (example.com), see the additional information about Apex domains and CNAME records . 3. (Non-CDN version) Set your domain in Platform.sh Note: If using a CDN, skip this step. The CDN should already have been configured in advance to point to Platform.sh as its upstream. This step will tell the Platform.sh edge layer where to route requests for your web site. You can do this through the CLI with platform domain:add example.com or using the managment console . You can add multiple domains to point to your project. Each domain can have its own custom SSL certificate, or use the default one provided. If you require access to the site before the domain name becomes active you can create a hosts file entry on your computer and point it to the IP address that resolves when you access your master project branch. To get the IP address, first run platform environment:info edge_hostname. That will print the “internal” domain name for your project. Run ping \u003cthat domain name\u003e to get its IP address. In OS X and Linux you can add that IP to your /etc/hosts file. In Windows the file is named You will need to be a admin user to be able to change that file. So in OS X you will usually run something like sudo vi /etc/hosts. After adding the line the file will look something like: Alternatively there is also an add-on for Firefox and Google Chrome that allow you to dynamically switch DNS IP addresses without modifying your hosts file. Firefox LiveHosts add-on Google Chrome LiveHosts add-on Note: Do not put the IP address you see here, but the one you got from the ping command. Also, remember to remove this entry after you have configured DNS! Sometimes it can take Let’s Encrypt a couple of minutes to provision the certificate the first time. This is normal, and only means the first deploy after enabling a domain may take longer than usual. Setting the CNAME record with your DNS provider first helps to minimize that disruption. 4. Bonus steps (Optional) Configure health notifications While not required, it’s strongly recommended that you set up health notifications to advise you if your site is experiencing issues such as running low on disk space. Notifications can be sent via email, Slack, or PagerDuty. Configure production cron tasks It’s strongly recommended that you set up automatic backups and automatic certificate renewal cron tasks. You will first need to set up an API token and install the CLI as part of the build hook. Then you can easily configure the appropriate cron tasks. The following snippet is generally sufficient but see the the links above for more details, and please modify the cron schedules listed to match your use case. crons:backup:# Take a backup automatically every night at 3 am (UTC).spec:'0 3 * * *'cmd:| if [  $PLATFORM_BRANCH  = master ]; thenplatformbackup:create--yes--no-waitfirenewcert:# Force a redeploy at 8 am (UTC) on the 14th and 28th of every month.spec:'0 8 14,28 * *'cmd:| if [  $PLATFORM_BRANCH  = master ]; thenplatformredeploy--yes--no-waitfi",
        "section": "Going live",
        "subsections": " 1. Change your plan to a production plan 2. (CDN version) Configure your DNS provider 2. (Non-CDN version) Configure your DNS provider 3. (Non-CDN version) Set your domain in Platform.sh 4. Bonus steps (Optional)  Configure health notifications Configure production cron tasks    ",
        "image": "",
        "url": "/golive/steps.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "37db2162f2d40f81c95d41bdee7fb3fa",
        "title": "Headless Chrome",
        "description": "",
        "text": " Headless Chrome is a headless browser that can be configured on projects like any other service on Platform.sh. You can interact with the headless-chrome service container using Puppeteer, a Node.js library that provides an API to control Chrome over the DevTools Protocol. Puppeteer can be used to generate PDFs and screenshots of web pages, automate form submission, and test your project’s UI. You can find out more information about using Puppeteer on GitHub or in their documentation . Supported versions Grid Dedicated 73 None available Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : { service :  headless , ip :  169.254.73.96 , hostname :  3rxha4e2w4yv36lqlypy7qlkza.headless.service._.eu-3.platformsh.site , cluster :  moqwtrvgc63mo-master-7rqtwti , host :  headless.internal , rel :  http , scheme :  http , type :  chrome-headless:73 , port : 9222}Requirements Puppeteer requires at least Node.js version 6.4.0, while using the async and await examples below requires Node 7.6.0 or greater. Using the Platform.sh Config Reader library requires Node.js 10 or later. Other languages It will be necessary to upgrade the version of Node.js in other language containers before using Puppeteer. You can use Node Version Manager or NVM to change or update the version available in your application container by following the instructions in the Alternate Node.js install documentation. Usage example In your .platform/services.yaml: headlessbrowser:type:chrome-headless:73 In your .platform.app.yaml: relationships:chromeheadlessbrowser: headlessbrowser:http  Note: You will need to use the chrome-headless type when defining the service # .platform/services.yamlservice_name:type:chrome-headless:version and the endpoint http when defining the relationship # .platform.app.yamlrelationships:relationship_name:“service_name:http” Your service_name and relationship_name are defined by you, but we recommend making them distinct from each other. After configuration, include Puppeteer as a dependency in your package.json: {  dependencies : {  puppeteer :  ^1.14.0  } } Using the Node.js Config Reader library, you can retrieve formatted credentials for connecting to headless Chrome with Puppeteer: const platformsh = require(\u0026#39;platformsh-config\u0026#39;); let config = platformsh.config(); const credentials = config.credentials(\u0026#39;chromeheadlessbrowser\u0026#39;); and use them to define the browserURL parameter of puppeteer.connect() within an async function: exports.takeScreenshot = async function (url) { try { // Connect to chrome-headless using pre-formatted puppeteer credentials const formattedURL = config.formattedCredentials(\u0026#39;chromeheadlessbrowser\u0026#39;, \u0026#39;puppeteer\u0026#39;); const browser = await puppeteer.connect({browserURL: formattedURL}); ... return browser } catch (e) { return Promise.reject(e); } }; Puppeteer allows your application to create screenshots , emulate a mobile device , generate PDFs , and much more. You can find some useful examples of using headless Chrome and Puppeteer on Platform.sh on the Community Portal: How to take screenshots using Puppeteer and Headless Chrome How to generate PDFs using Puppeteer and Headless Chrome",
        "section": "Configure services",
        "subsections": " Supported versions Relationship Requirements  Other languages   Usage example  ",
        "image": "",
        "url": "/configuration/services/headless-chrome.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "44c7c836fce4755546271a721b6f6849",
        "title": "HTTP cache",
        "description": "",
        "text": " Platform.sh supports HTTP caching at the server level. Caching is enabled by default, but is only applied to GET and HEAD requests. The cache can be controlled using the cache key in your .platform/routes.yaml file. If a request is cacheable, Platform.sh builds a cache key from several request properties and stores the response associated with this key. When a request comes with the same cache key, the cached response is reused. When caching is on… you can configure cache behaviour for different location blocks in your .platform.app.yaml; the router will respect whatever cache headers are sent by the application; cookies will bypass the cache; responses with the Cache-Control header set to Private, No-Cache, or No-Store are not cached. Basic usage The HTTP cache is enabled by default, however you may wish to override this behaviour. To configure the HTTP cache, add a cache key to your route in .platform/routes.yaml. You may like to start with the defaults: https://{default}/:type:upstreamupstream:app:httpcache:enabled:truedefault_ttl:0cookies:[\u0026#39;*\u0026#39;]headers:[\u0026#39;Accept\u0026#39;,\u0026#39;Accept-Language\u0026#39;]Example In this example, requests will be cached based on the URI, the Accept header, Accept-Language header, and X-Language-Locale header; Any response that lacks a Cache-Control header will be cached for 60 seconds; and the presence of any cookie in the request will disable caching of that response. https://{default}/:type:upstreamupstream:app:httpcache:enabled:trueheaders:[\u0026#39;Accept\u0026#39;,\u0026#39;Accept-Language\u0026#39;,\u0026#39;X-Language-Locale\u0026#39;]cookies:[\u0026#39;*\u0026#39;]default_ttl:60How it works The cache key If a request is cacheable, Platform.sh builds a cache key from several request properties and stores the response associated with this key. When a request comes with the same cache key, the cached response is reused. There are two parameters that let you control this key: headers and cookies. The default value for these keys are the following: cache:enabled:truecookies:[\u0026#39;*\u0026#39;]headers:[\u0026#39;Accept\u0026#39;,\u0026#39;Accept-Language\u0026#39;]Duration The cache duration is decided based on the Cache-Control response header value. If no Cache-Control header is in the response, then the value of default_ttl key is used. Conditional requests Conditional requests using If-Modified-Since and If-None-Match are both supported. Our web server does not honor the Pragma request header. Cache revalidation When the cache is expired (indicated by Last-Modified header in the response) the web server will send a request to your application with If-Modified-Since header. If the If-None-Match header is sent in the conditional request when Etag header is set in the cached response, your application can extend the validity of the cache by replying HTTP 304 Not Modified. Flushing The HTTP cache does not support a complete cache flush, however, you can invalidate the cache by setting cache:false. Cache configuration properties enabled Turns the cache on or off for a route. Type: Boolean Required: Yes Values true: enable the cache for this route [default, but only if the cache key is not actually specified] false: disable the cache for this route headers Adds specific header fields to the cache key, enabling caching of separate responses for those headers. For example, if the headers key is the following, Platform.sh will cache a different response for each value of the Accept HTTP request header only: cache:enabled:trueheaders:[ Accept ] Type: List Values: [\u0026#39;Accept\u0026#39;, \u0026#39;Accept-Language\u0026#39;]: Cache on Accept \u0026amp; Accept-Language [default] Header behaviors The cache is only applied to GET and HEAD requests. Some headers trigger specific behaviours in the cache. Header field Cache behavior Cache-Control Responses with the Cache-Control header set to Private, No-Cache, or No-Store are not cached. All other values override default_ttl. Vary A list of header fields to be taken into account when constructing the cache key. Multiple header fields can be listed, separted by commas. The Cache key is the union of the values of the Header fields listed in Vary header, and whatever’s listed in the routes.yaml file. Set-Cookie Not cached Accept-Encoding, Connection, Proxy-Authorization, TE, Upgrade Not allowed, and will throw an error Cookie Not allowed, and will throw an error. Use the cookies value, instead. Pragma Ignored A full list of HTTP headers is available on Wikipedia . cookies A list of allowed cookie names to include values for in the cache key. All cookies will bypass the cache when using the default ([\u0026#39;*\u0026#39;]) or if the Set-Cookie header is present. For example, for the cache key to depend on the value of the foo cookie in the request. Other cookies will be ignored. cache:enabled:truecookies:[ foo ] Type: List Values: [\u0026#39;*\u0026#39;]: any request with a cookie will bypass the cache [default] []: Ignore all cookies [\u0026#39;cookie_1\u0026#39;,\u0026#39;cookie_2\u0026#39;]: A list of allowed cookies to include in the cache key. All other cookies are ignored. A cookie value may also be a regular expression. An entry that begins and ends with a / will be interpreted as a PCRE regular expression to match the cookie name. For example: cache:enabled:truecookies:[\u0026#39;/^SS?ESS/\u0026#39;]Will cause all cookies beginning with SESS or SSESS to be part of the cache key, as a single value. Other cookies will be ignored for caching. If your site uses a session cookie as well as 3rd party cookies, say from an analytics service, this is the recommended approach. default_ttl Defines the default time-to-live for the cache, in seconds, for non-static responses, when the response does not specify one. The cache duration is decided based on the Cache-Control response header value. If no Cache-Control header is in the response, then the value of default_ttl is used. If the application code returns a Cache-Control header or if your .platform.app.yaml file is configured to set a cache lifetime, then this value is ignored in favor of the application headers. The default_ttl only applies to non-static responses, that is, those generated your application. To set a cache lifetime for static resources configure that in your .platform.app.yaml file. All static assets will have a Cache-Control header with a max age defaulting to 0 (which is the default for expires in the .platform.app.yaml). Type: integer Values: 0: Do not cache [default]. This prevents caching, unless the response specifies a Cache-Control header value. Debugging Platform.sh adds an X-Platform-Cache header to each request which show whether your request is a cache HIT, MISS or BYPASS. This can be useful when trying to determine whether it is your application, the HTTP cache, or another proxy or CDN which is not behaving as expected. If in doubt, disable the cache using cache:false. Advanced caching strategies Cache per route If you need fine-grained caching, you can set up caching rules for several routes separately: https://{default}/:type:upstreamupstream:app:httpcache:enabled:truehttps://{default}/foo/:type:upstreamupstream:app:httpcache:enabled:falsehttps://{default}/foo/bar/:type:upstreamupstream:app:httpcache:enabled:trueWith this configuration, the following routes are cached: https://{default}/ https://{default}/foo/bar/ https://{default}/foo/bar/baz/ And the following routes are not cached: https://{default}/foo/ https://{default}/foo/baz/ Note: Regular expressions in routes are not supported. Allowing only specific cookies Some applications use cookies to invalidate cache responses, but expect other cookies to be ignored. This is a simple case of allowing only a subset of cookies to invalidate the cache. cache:enabled:truecookies:[ MYCOOKIE ]Cache HTTP and HTTPS separately using the Vary header Set the Vary header to X-Forwarded-Proto custom request header to render content based on the request protocol (i.e. HTTP or HTTPS). By adding Vary: X-Forwarded-Proto to the response header, HTTP and HTTPS content would be cached separately. Cache zipped content separately Use Vary: Accept-Encoding to serve different content depending on the encoding. Useful for ensuring that gzipped content is not served to clients that can’t read it.",
        "section": "Configure routes",
        "subsections": " Basic usage Example How it works  The cache key Duration Conditional requests Cache revalidation Flushing   Cache configuration properties  enabled headers cookies default_ttl   Debugging Advanced caching strategies  Cache per route Allowing only specific cookies Cache HTTP and HTTPS separately using the Vary header Cache zipped content separately    ",
        "image": "",
        "url": "/configuration/routes/cache.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "0a9e34befd7f9cc9e291e8a1690857c9",
        "title": "Import your own code",
        "description": "",
        "text": " Welcome to Platform.sh! Importing your own code to Platform.sh is as easy as installing the CLI and configuring your application with a few YAML files. When code examples are provided in the guide, click the language of your application. If you consult those examples and a few templates as you go along, your code will be up and running on Platform.sh in no time. Get started!",
        "section": "Introduction",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "dbb0e89777d8d38b2e967257e06d456b",
        "title": "One site or many",
        "description": "",
        "text": " Platform.sh supports running multiple “application containers” in a single project. That can be extremely powerful in some cases, but if misused can lead to unnecessary maintenance difficulty and excessive costs. The way to determine what setup is appropriate for your use case is to think of your project as a collection of services, some of which you’ve written yourself. That is, put “your code” and “the database” on the same level. (That is essentially true from the Platform.sh perspective.) Does your project consist of multiple “your code” pieces, but they all are parts of the same project? Or are they discrete applications that conceptually exist independently of each other? Discrete projects If your applications are discrete systems that are only incidentally related (such as because you wrote both of them), make them separate projects. That will provide the most flexible development workflow. It will also be cheaper, as running multiple applications in a single project requires at least a Medium plan, which costs more than two Standard plans. Discrete projects are appropriate if: You want to deploy new releases of each application independently of the others. The projects are for different customers/clients. The projects do not need deep internal knowledge of each other’s data. Different teams will be working on different applications. You want to develop true-microservices, where each microservice is fully stand-alone process with its own data. If you are uncertain how your needs map to projects, it probably means they should be separate, discrete projects. Clustered applications A clustered application is one where your project requires multiple “app services”, written by you, but are all part of the same conceptual project. That is, removing one of the app services would render the others broken. In a clustered application, you either have multiple .platform.app.yaml files in different directories with separate code bases that deploy separately or you have a single application that spawns one or more worker instances that run background processes. (See the link for details on how to set those up.) A Clustered application requires at least a Medium plan. With a clustered application, you often will not need multiple service instances. The MySQL, MariaDB , and Solr services support defining multiple databases on a single service, which is significantly more efficient than defining multiple services. Redis , Memcached , Elasticsearch , and RabbitMQ natively support multiple bins, or queues, or indexes (the terminology varies) defined by the client application as part of the request so they need no additional configuration on Platform.sh, although they may need application configuration. Clustered applications are appropriate if: You want one user-facing application and an entirely separate admin-facing application that are both operating on the same data. You want to have a user-facing application and a separate worker process (either the same code or separate) that handles background tasks. You want a single conceptual application written in multiple programming languages, such as a PHP frontend with Node.js background worker. Multi-site applications Some Content Management Systems or other applications support running multiple logical “sites” off of a single code base. Those will usually work on Platform.sh depending on the configuration details of the application but are generally not recommended. Often their multi-site logic is dependant on the domain name of the incoming request, which on Platform.sh will vary by branch. They also often recommend running multiple databases, which while supported just fine on Platform.sh makes the setup process for each site more difficult. Leveraging multi-site capabilities of an application are appropriate if, and only if: There is only a single team working on all of the “sites” involved. All “sites” should be updated simultaneously as a single unit. Each individual site is relatively low traffic, such that the aggregate traffic is appropriate for your plan size. All sites really do use the same codebase with no variation, just different data. If any of those is not the case, discrete projects will be a better long term plan.",
        "section": "Best practices",
        "subsections": " Discrete projects Clustered applications Multi-site applications  ",
        "image": "",
        "url": "/bestpractices/oneormany.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "613a2e6671a11c75801a0de3a535e6d6",
        "title": "Performance tuning Java",
        "description": "",
        "text": " There are a number of settings that can be adjusted for each application to optimize its performance on Platform.sh. Memory limits The JVM generally requires specifying a maximum memory size it is allowed to use, using the Xmx parameter. That should be set based on the available memory on the application container, which will vary with its size. To extract the container-scaled value on the command line, use $(jq .info.limits.memory /run/config.json). You should also set the ExitOnOutOfMemoryError. When you enable this option, the JVM exits on the first occurrence of an out-of-memory error. Platform.sh will restart the application automatically. These are the recommended parameters for running a Java application. Thus, the command to use to start a Java application is: java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:\u0026#43;ExitOnOutOfMemoryError //The rest of the arguments and the jar file. Garbage collection When migrating the application to a cloud environment, it is often essential to analyze the Garbage Collector’s log and behavior. For this, there are two options: Placing the log into the Platform.sh /var/log/app.log file (which captures STDOUT). Creating a log file specifically for the GC. To use the STDOUT log, you can add the parameter -XX: \u0026#43; PrintGCDetails, E.g.: java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:\u0026#43;ExitOnOutOfMemoryError -XX:\u0026#43;PrintGCDetails //The rest of the arguments and the jar file. Java supports a number of different garbage collection strategies. Which one is optimal for your application will vary depending on your available memory, Java version, and application profile. Determining which is best for your application is out of scope, but the main options and how to enable them are: Name Command Flag Description Serial Garbage Collector -XX:\u0026#43;UseSerialGC This is the simplest GC implementation, as it basically works with a single thread. Parallel Garbage Collector -XX:\u0026#43;UseParallelGC Unlike Serial Garbage Collector, this uses multiple threads for managing heap space. But it also freezes other application threads while performing GC. CMS Garbage Collector -XX:\u0026#43;USeParNewGC The Concurrent Mark Sweep (CMS) implementation uses multiple garbage collector threads for garbage collection. It’s for applications that prefer shorter garbage collection pauses, and that can afford to share processor resources with the garbage collector while the application is running. G1 Garbage Collector -XX:\u0026#43;UseG1GC Garbage First, G1, is for applications running on multiprocessor machines with large memory space. The default strategy on Java 9 and later is G1. The GC strategy to use can be set in the start line with: Serial java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:\u0026#43;ExitOnOutOfMemoryError -XX:\u0026#43;PrintGCDetails -XX:\u0026#43;UseSerialGC //The rest of the arguments and the jar file. Parallel Garbage Collector java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:\u0026#43;ExitOnOutOfMemoryError -XX:\u0026#43;PrintGCDetails -XX:\u0026#43;UseParallelGC //The rest of the arguments and the jar file. CMS Garbage Collector java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:\u0026#43;ExitOnOutOfMemoryError -XX:\u0026#43;PrintGCDetails -XX:\u0026#43;USeParNewGC //The rest of the arguments and the jar file. G1 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:\u0026#43;ExitOnOutOfMemoryError -XX:\u0026#43;PrintGCDetails -XX:\u0026#43;UseG1GC //The rest of the arguments and the jar file. Java 8 Optimization Ideally, all applications should run the latest LTS release of the JVM at least. That is currently Java 11. Java 11 has a number of performance improvements, particularly on container-based environments such as Platform.sh. However, in many cases, this is not possible. If you are still running on Java 8 there are two additional considerations. The default garbage collector for Java 8 is Parallel GC. In most cases G1 will offer better performance. We recommend enabling it, as above. Furthermore, there is the UseStringDeduplication flag which works to eliminate duplicate Strings within the GC process. That flag can save between 13% to 30% of memory, depending on application. However, this can impact on the pause time of your app. java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:\u0026#43;UseG1GC -XX:\u0026#43;UseStringDeduplication -XX:\u0026#43;ExitOnOutOfMemoryError -XX:\u0026#43;PrintGCDetails References Java Memory Commands How to Migrate my Java application to Platform.sh Garbage Collector Log Introduction to Garbage Collection Tuning",
        "section": "Java",
        "subsections": " Memory limits Garbage collection  Serial Parallel Garbage Collector CMS Garbage Collector G1   Java 8 Optimization References  ",
        "image": "",
        "url": "/languages/java/tuning.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "e88f1bf9c1df6ed5f5b9c384ec4e281d",
        "title": "Platform.sh Dedicated cluster specifications",
        "description": "",
        "text": " Platform.sh Dedicated clusters are launched into a Triple Redundant configuration consisting of 3 virtual machines (VMs). This is an N+1 configuration that is sized to withstand the total loss of any one of the 3 members of the cluster without incurring any downtime. Each instance hosts the entire application stack, allowing this architecture superior fault tolerance to traditional N-Tier installations. Moreover, the Cores assigned to production are solely for production. Storage Each Dedicated cluster comes with 50GB of storage per environment by default. This storage is intended for customer data - databases, search indexes, user uploaded files, etc. - and can be subdivided in any way that the customer wishes. 50GB is only the default amount; more storage can be added easily as a line item in the contract and can be added at anytime that the project requires: at contract renewal or at any point in the term. Default storage is based on the default SSD block-storage offering for each cloud. Extra provisioned IOPS can be discussed with your sales representative. Accessing services Your application will be able to connect to each service by referencing the exact same environment variables as a Grid environment. While the configuration of the service will be performed by our team, the application configuration is the same and your code should be the same. See the services documentation for service-specific details. Note that not all services and languages are available in a Dedicated environment.",
        "section": "Dedicated",
        "subsections": " Storage Accessing services  ",
        "image": "",
        "url": "/dedicated/architecture.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "e0ef22ff09aeff7b1299d16840505e38",
        "title": "Platform.sh environments",
        "description": "",
        "text": " Platform.sh helps a coder with the development workflow by making it easy to manage multiple environments, including the Master environment which runs the production website. It’s precisely like a “development” or a “staging” server, except they are created on the fly, and they are absolutely identical copies of their parent environments. An environment is tied to a Git branch, plus all the services that are serving that branch. You can see that as a complete working website. With Bitbucket and GitHub integrations you can even get a “development server” for each and every pull request. You can have branches that are not tied to a running instance of your application; these are what we call “inactive environments”. Master environment Every Platform.sh project starts with a Master environment which corresponds to the Master branch in Git. If you subscribed to a production plan, this environment is your live site and can be mapped to a domain name and a custom SSL certificate. Note: Your project must have a master branch: it will not function properly without one. Hierarchy Platform.sh brings the concept of a hierarchy between your environments. Each new environment you create is considered a child of the parent environment from which it was branched. Each child environment can sync code and/or data down from its parent, and merge code up to its parent. These are used for development, staging, and testing. When you create a branch or child environment through the Platform.sh management console the branch it was made from will be treated as the parent. If you create a branch through your local Git checkout and push it to Platform.sh, or synchronize a branch from a 3rd party such as GitHub or Bitbucket, its parent will default to the master branch. Any environment’s parent can be changed using the Platform.sh CLI with the following command: platform environment:info parent NEW_PARENT In this case, the current environment (the branch you’re on) will be set to have NEW_PARENT as its parent environment. The environment to reparent can be set explicitly with the -e option: platform environment:info -e feature-x parent NEW_PARENT Workflows Since you can organize your environments as you want, you have complete flexibility to create your own workflows. There are no rules you must follow when branching the master environment. You simply need a structure that best fits your workflow: Agile: a child environment per sprint. Each story in the sprint can have its own environment as a child of the sprint environment. Developer-centric: one QA environment and a few development environments (per developer, per task…). Testing: an operational test environment, a user test environment and a few unit test environments. Hotfix: one environment for every bug, security, or hotfix that needs deployment. Here is an example of a possible Agile workflow. The administrator creates a Sprint environment and gives each of the developers permission to create new feature environments. Another approach is that the administrator could create an environment for each developer. As a feature is completed, the administrator can review the work by accessing the website of the feature environment. The new feature is then merged back into the Sprint environment. The remaining features will sync with the Sprint environment to ensure their working environment is up-to-date with the latest code. When the objectives of the sprint are complete, the administrator can then make a backup of the live site, then merge the Sprint environment into the live (Master) environment. The administrator can then synchronize the next sprint’s environment with data from the live (Master) environment to repeat and continue the development process. Naming conventions Platform.sh provides great flexibility on the way you can organize and work with your development environments. To improve readability and productivity, it’s important to think carefully about how to name and structure those environments. The name should represent the purpose of the environment. Is it a Staging site to show to your client? Is it an implementation of a new feature? Is it a hotfix? If you use Agile, for example, you could create hierarchical environments and name them like this: Sprint1 Feature1 Feature2 Feature3 Sprint2 Feature1 Feature2 ... If you prefer splitting your environments per developer and having a specific environment per task or per ticket, you could use something like this: Staging Developer1 Ticket-526 Ticket-593 Developer2 Ticket-395 ...",
        "section": "Management console",
        "subsections": " Master environment Hierarchy Workflows Naming conventions  ",
        "image": "",
        "url": "/administration/web/environments.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "af754b130a377bea960ed8e2f6ec4e9e",
        "title": "Requirements for the CLI",
        "description": "",
        "text": " Now that you have created your free trial account, you are able to push your application to Platform.sh once you have installed the CLI, but there are a few requirements that must be met first. Git Git is the open source version control system that is utilized by Platform.sh. Any change you make to your Platform.sh project will need to be committed via Git. You can see all the Git commit messages of an environment in the Environment Activity feed of the management console for each project you create. Before getting started, make sure you have Git installed on your computer. SSH key pair Once your account has been set up and the CLI is installed, Platform.sh needs one additional piece of information about your computer so that you can access your projects from the command line. If you are unfamiliar with how to generate an RSA public and private key, there are excellent instructions in the documentation about how to do so . Add your SSH key to your account Now that you have the requirements out of the way, place your SSH key onto Platform.sh so that you can communicate with your projects from your computer using the management console. Log in to your account Access SSH key settings in the management console From the management console, move to the top right hand corner of the screen and click the dropdown menu to the left of the settings gear box icon. In the menu, click on Account. The next page will normally list all of your projects, which at this point will be empty if you’re just starting out. Click on the Account Settings link at the top of the page, then click the SSH keys tab to the left of your account information. Add your SSH key to your account At this point you won’t see anything listed in the body of the page, because you don’t have SSH configured with Platform.sh yet. Click the \u0026#43; Add public key button in the top right hand corner of the screen. This will open up another window with two fields. Name the key with something memorable, like home-computer, and in the field below that, paste the content of the public key you created in the previous step. When you have finished, click Save to save the key. That’s it! Now that you have met the requirements and configured an SSH key, Platform.sh can authenticate your computer and you can interact with your project from the command line. Next, you will need to install the Platform.sh CLI so that you can import your code to a project. Back I\u0026#39;ve added my public SSH key",
        "section": "Getting started",
        "subsections": "   Git SSH key pair Add your SSH key to your account    ",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/cli-requirements.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "4636e25c794d2ede787bcaeaf6199672",
        "title": "Resource and incident monitoring",
        "description": "",
        "text": " All of our Dedicated clusters are monitored 24/7 to ensure uptime and to measure server metrics such as available disk space, memory and disk usage, and several dozen other metrics that give us a complete picture of the health of your application’s infrastructure. Alerting is set up on these metrics, so if any of them goes outside of normal bounds an operations engineer can react accordingly to maintain the uptime and performance of your cluster. These alerts are sent to our support and operations teams, and are not directly accessible to the customer. Monitoring systems Platform.sh uses well-known open source tooling to collect metrics and to alert our staff if any of these metrics goes out of bounds. That includes the use of Munin for collecting time-series data on server metrics, and dashboarding of these metrics so that our team can monitor trends over time. It also includes use Nagios as a point in time alerting system for our operations staff. These tools are internal Platform.sh tools only. A third-party availability monitoring system is configured for every Dedicated project. The customer can be subscribed to email alerts upon request. Application performance monitoring Platform.sh does not provide application-level performance monitoring. However, we strongly recommend that customers leverage application monitoring themselves. Platform.sh is a Blackfire.io reseller. You can contact your sales representative to get a quote for whatever size cluster is running your application. Platform.sh also supports New Relic APM . After you have signed up with New Relic and gotten your license key, open a support ticket so that it can be installed on your project. New Relic infrastructure monitoring is not supported. Availability incident handling procedure Automated monitoring may trigger alerts that will page the on-call engineer, or the end-user may file an urgent priority ticket. PagerDuty will page the on-call using several methods. The on-call engineer responds to the alerts and begins to triage the issue. Cloud infrastructure issues are handled by the customer success team. Application problems are escalated to an application support specialist if an agreement is part of the customer subscription. Otherwise, they are returned to the user and may be downgraded. When a Urgent/High issue is escalated it will page the on-call application support specialist. Application support may also escalate infrastructure issues back as Urgent/High.",
        "section": "Platform.sh Dedicated",
        "subsections": " Monitoring systems Application performance monitoring Availability incident handling procedure  ",
        "image": "",
        "url": "/dedicated/overview/monitoring.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "b0be6b730e93a77709ec53ebca895b12",
        "title": "SCA",
        "description": "",
        "text": " In accordance with Article 14(1) of the Commission Delegated Regulation (EU) 2018/389, Platform.sh has made changes in order to comply with and implement Strong Customer Authentication (SCA) for customers using payment methods from the EU. That article states: Payment service providers shall apply strong customer authentication when a payer creates, amends, or initiates for the first time, a series of recurring transactions with the same amount and with the same payee. SCA is part of the second Payment Services Directive (PSD2) , acting as a regulatory requirement to reduce fraud and to make online transactions more secure. The law went into affect September 14, 2019, and European card holders will be required to go through an additional re-authentication step within the Platform.sh management console starting October 1, 2019 in order to authenticate recurring payments with your payment institution. Prior to October 1, 2019, Platform.sh projects associated with an EU credit card will see a banner at the top of the management console. That banner will direct you to authenticate your payment information settings. The process after you click “Authenticate” will vary according to your payment institution. In most cases, having your phone number registered with that institution will be sufficient to enable 2FA with them from here. After October 1, 2019, the SCA banner in the management console will become a warning for you to update your payment settings. There will be a grace period in the first few weeks following that change, but when that period has ended projects that have not set up payment authentication through the console will be suspended, so it is important for you to do so as soon as possible.",
        "section": "Security and compliance",
        "subsections": "",
        "image": "",
        "url": "/security/sca.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "5d645502da3c569d07ef810d7bf6bde4",
        "title": "Security \u0026 data privacy",
        "description": "",
        "text": " Updates \u0026amp; upgrades Platform.sh updates the core software of the Dedicated Cluster (operating system, web server, PHP, MySQL, etc.) periodically, and after any significant security vulnerability is disclosed. These updates are deployed automatically with no additional work required by the user. We attempt to maintain parity with your development environment, but we do not guarantee absolute parity of point versions of your Dedicated environments with their corresponding development environments. I.e, your development environment may have a PHP container running 5.6.30, but your production environment may lag behind at 5.6.22. We can upgrade point releases on request and will always upgrade the underlying software in the event of security release. Updates to application software (PHP code, Javascript, etc.) are the responsibility of the customer. Project isolation All Dedicated Clusters are single-tenant. The three VMs are exclusively used by a single customer and each Dedicated cluster is launched into its own isolated network (VPC on AWS, equivalent on other providers). The network is firewalled to incoming connections; only ports 22 (SSH), 80 (HTTP), 443 (HTTPS), 2221 (SFTP) are opened to incoming traffic. There are no exceptions for this rule, so any incoming web service requests, ETL jobs, or otherwise will need to transact over one of these protocols. Outgoing TCP traffic is not firewalled. Outgoing UDP traffic is disallowed. The Development Environment deploys each branch as a series of containers hosted on a shared underlying VM. Many customers will generally share the same VM. However, all containers are allowed to connect only to other containers in their same environment, and even then only if an explicit “relationship” has been defined by the user via configuration file. Security incident handling procedure Should Platform.sh become aware of a security incident — such as an active or past hacking attempt, virus or worm, or data breach — senior personnel including the CTO will be promptly notified. Our security incident procedures include isolating the affected systems, collecting forensic evidence for later analysis including a byte-for-byte copy of the affected system, and finally restoring normal operations. Once normal service is restored we perform a root cause analysis to determine exactly what happened. A Reason for Outage report may be provided to the customer upon request that summarizes the incident, cause, and steps taken. Platform.sh will cooperate with relevant law enforcement, and inform law enforcement in the event of an attempted malicious intrusion. Depending on the type of incident the root cause analysis may be conducted by law enforcement rather than Platform.sh personnel. Platform.sh will endeavor to notify affected customers within 24 hours in case of a personal data breach and 72 hours in case of a project data breach. Under the European General Data Protection Regulation (GPDR), Platform.sh is required to notify our supervising authority within 72 hours of a discovered breach that may result in risk to the rights and freedoms of individuals. Our supervising authority is the French Commission Nationale de l’Informatique et des Libertés . Audit trail As part of the security incident process we record a log of all steps taken to identify, isolate, and respond to the incident. This log may include: A byte-for-byte copy of the affected systems How the intrusion was detected The steps taken to contain the intrusion Any contact with 3rd parties, including law enforcement Any conclusions reached regarding the root cause Encryption AWS AWS EBS Volumes are encrypted on Platform.sh Dedicated sites are fully encrypted. Keys are managed by AWS’s KMS (Key Management Service). AWS automatically rotates these keys every three years. In some cases, temporary storage (eg swap) is stored on unencrypted local storage volumes. Azure By default, data is encrypted using Microsoft Managed Keys for Azure Blobs, Tables, Files and Queues.",
        "section": "Platform.sh Dedicated",
        "subsections": " Updates \u0026amp; upgrades Project isolation Security incident handling procedure Audit trail Encryption  AWS Azure    ",
        "image": "",
        "url": "/dedicated/overview/security.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "a18c123f65c496ecad5d8efb610128e1",
        "title": "Technical Requirements: Git and SSH",
        "description": "",
        "text": " Git Git is the open source version control system that is utilized by Platform.sh. Any change you make to your Platform.sh project will need to be committed via Git. You can see all the Git commit messages of an environment in the activity feed of the management console. Before getting started, make sure you have it installed on your computer to be able to interact with Platform.sh. See also: Install Git Learn more about Git SSH You connect to your Platform.sh Git repository and to your applications and services using SSH. SSH requires two RSA keys : A private key kept secret by the user A public key stored within the Platform.sh account These keys are called the public-private keypair and usually look like random lines of characters, like this: A private key: -----BEGIN RSA PRIVATE KEY----- MIIEowIBAAKCAQEAtpw0S4DwDVj2q04mhiIMkhvrYU7Z6hRiNbTFsqg3X7x/uYS/ dcNrSvT82j/jSeYQP3Dsod9GERW\u0026#43;dmOuLaFNeiqOStZi6jRSWo41hCOWOFbpBum3 ra1n6nUO1wa/7O5wbgzhUOfnim77oOK0UgkqPArBCNXiNFTUJAvRyVmCtvJOyrqz ...(20 more lines of this garbage)... cPjJ/wKBgGd3eZIBK6Ak92u65HYXgY9EcX3vBNP4NsF087uxV4YfrM18KlGf5I87 QGerp3VKaGe0St3ot57GlwCAQUJAf1mit8qDTi0I8MhBe7q2lstXkBvde7GY1gKx Kng4ohG6xHZ/OvC9tq7/THwAvleaxgLZN5GyXfAqNylDdZ0LtSjl -----END RSA PRIVATE KEY----- A public key (one very long line): ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC2nDRLgPANWParTiaGIgySG\u0026#43;thTtnqFGI1tMWyqDdfvH\u0026#43;5hL91w2tK9PzaP\u0026#43;NJ5hA/cOyh30YRFb52Y64toU16Ko5K1mLqNFJajjWEI5Y4VukG6betrWfqdQ7XBr/s7nBuDOFQ5\u0026#43;eKbvug4rRSCSo8CsEI1eI0VNQkC9HJWYK28k7KurMdTN7X/Z/4vknM4/Rm2bnMk2idoORQgomeZS1p3GkG8dQs/c0j/b4H7azxnqdcCaR4ahbytX3d49BN0WwE84C\u0026#43;ItsnkCt1g5tVADPrab\u0026#43;Ywsm/FTnGY3cJKKdOAHt7Ls5lfpyyug2hNAFeiZF0MoCekjDZ2GH2xdFc7AX/ your_email_address@example.com You will need a SSH public/private keypair in order to interact with Platform.sh. Your public key is uploaded to your Platform.sh user account, and it then governs authentication for Git, SSH sessions (shell access), and other tools that connect to your Platform.sh project. GitHub has a good walk-through of creating an SSH keypair on various operating systems.",
        "section": "Development",
        "subsections": " Git SSH  ",
        "image": "",
        "url": "/development/tools.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "09f532a2ca00d9080bf73a5fa9b588fd",
        "title": "Type",
        "description": "",
        "text": " The type key defines the base container image that will be used to run the application. There is a separate base container image for each primary language for the application, often in multiple versions. Supported types Available languages and their supported versions include: Language runtime Supported version C#/.Net Core dotnet 2.0, 2.1, 2.2, 3.1 Elixir elixir 1.9 Go golang 1.11, 1.12, 1.13, 1.14 Java java 11, 12, 8, 13 Lisp lisp 1.5 Node.js nodejs 6, 8, 10, 12, 14 PHP php 7.2, 7.3, 7.4 Python python 2.7, 3.5, 3.6, 3.7, 3.8 Ruby ruby 2.3, 2.4, 2.5, 2.6, 2.7 Example configuration type:\u0026#39;php:7.4\u0026#39; Runtime The .platform.app.yaml file also supports a runtime key that allows selected customizations to the language runtime. As those possibilities vary by language, please see the appropriate language documentation. PHP",
        "section": "Configure your application",
        "subsections": " Supported types Example configuration Runtime  ",
        "image": "",
        "url": "/configuration/app/type.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "f5e039061ce2e2b11cf4c777ad7c0fba",
        "title": "Untethered local",
        "description": "",
        "text": " It’s possible to run your entire site locally on your computer. That is more performant as there’s no extra latency to connect to a remote database and doesn’t require an active Internet connection to work. However, it does require running all necessary services (databases, search servers, etc.) locally. These can be set up however you prefer, although Platform.sh recommends using a virtual machine to make it easier to share configuration between developers. If you already have a development workflow in place that works for you, you can keep using it with virtually no changes. To synchronize data from an environment on Platform.sh, consult the documentation for each service . Each service type has its own native data import/export process and Platform.sh does not get in the way of that. It’s also straightforward to download user files from your application using rsync.",
        "section": "Set up your local development environment",
        "subsections": "",
        "image": "",
        "url": "/development/local/untethered.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "589897e6fc3b1bd4b80bb95e4dde918c",
        "title": "Upgrade plan",
        "description": "",
        "text": " “Development” plan projects cannot be assigned a domain name, so you will not be able to go live until you upgrade to at least a Standard plan. This can be done using the management console. Development plans come with four environments: three development and one “future” production environment, which is the master branch. For example, “Small” plan sizes provide a production environment, but restrict your application to the use of a single service (i.e. a database). On your project, click the “Go live” button in the top right hand corner of your project preview image. This will allow you to edit the project’s plan, and it can also be reached from your “Account” page by clicking “Edit” from the vertical dot dropdown for your project. Select the plan size that is appropriate for the needs of your application. This is also the page where you can increase the number of development environments, and the amount of storage. Make your changes and then click “Update plan” at the bottom of the page. Your application will redeploy. Back I\u0026#39;ve upgraded my plan size",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/next-steps/going-live/upgrade-plan.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "c9f731512314f55f0583ad2e06d08a5c",
        "title": "Working with Drush in Drupal 8",
        "description": "",
        "text": " Drush is a command-line shell and scripting interface for Drupal, a veritable Swiss Army knife designed to make life easier for those who spend their working hours hacking away at the command prompt. Drush commands can, for example, be used to clear the Drupal cache, run module and database updates, revert features, perform database imports and dumps, and a whole lot more. You can reference the full set of Drush commands at Drush.org If you have never used Drush before, you can learn more about it on the Drush GitHub Repository Platform.sh’s Drupal templates have Drush installed. Drush commands can be run in build, deploy, and post_deploy hooks, although remember that as the database is not available at build time many Drush commands will not work at build time. In addition, you can use the Platform.sh CLI to set up Drush aliases easily for all of your project’s environments. See the section below on use drush aliases . Note: Platform’s CLI requires Drush 6 or higher. Installing Drush Use the Platform.sh-provided Drupal examples If you started your project from one of Platform.sh’s Drupal templates then Drush is already installed and configured. There is nothing else for you to do. Install Drush in custom projects using Composer This is the recommended approach. Run this command in the project’s repository root folder: $ composer require drush/drush Commit the composer.json and composer.lock files and push. Drush will then be available at vendor/bin/drush, in the exact same version on your local system and on Platform.sh. Install Drush in custom projects using Build Dependencies Platform.sh supports installing some as system-level tools as build dependencies . To install Drush using this method, use the following in .platform.app.yaml dependencies:php: drush/drush :  ^8.0 Accessing Drush within the project For Drush to be available on the command line, it must be added to the project’s $PATH. Add a new file named .environment to the root of your your project’s git repository with this code: # Statements in this file will be executed (sourced) by the shell in SSH # sessions, in deploy hooks, in cron jobs, and in the application\u0026#39;s runtime # environment. This file must be placed in the root of the application, not # necessarily the git repository\u0026#39;s root. In case of multiple applications, # each application can have its own .environment file. # Allow executable app dependencies from Composer to be run from the path. if [ -n  $PLATFORM_APP_DIR  -a -f  $PLATFORM_APP_DIR /composer.json ] ; then bin=$(composer config bin-dir --working-dir= $PLATFORM_APP_DIR  --no-interaction 2\u0026gt;/dev/null) export PATH= ${PLATFORM_APP_DIR}/${bin:-vendor/bin}:${PATH}  fi Install Drush locally You can install drush globally with Composer. This does not add it to your project. $ composer global require drush/drush At the end of the installation, you should be able to run: $ drush Use drush aliases Create Drush aliases Drush aliases make it easy to manage your development websites. Here’s an example of a Drush alias file . The Platform.sh CLI generates Drush aliases for you automatically when you run platform get [project_id]. To see the aliases that are created, run platform drush-aliases and you should get output similar to that below: $ platform drush-aliases Aliases for My Site (tqmd2kvitnoly): @my-site._local @my-site.master @my-site.staging @my-site.sprint1 Recreating Drush aliases To recreate existing aliases, or after pushing a new branch via git to create the new alias, run: platform drush-aliases -r",
        "section": "Getting Started",
        "subsections": " Installing Drush  Use the Platform.sh-provided Drupal examples Install Drush in custom projects using Composer Install Drush in custom projects using Build Dependencies   Accessing Drush within the project Install Drush locally Use drush aliases  Create Drush aliases Recreating Drush aliases    ",
        "image": "",
        "url": "/frameworks/drupal8/drush.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "910b8a2be805fd45e86c829cc1486b84",
        "title": "Build site locally",
        "description": "",
        "text": " Now that you’ve opened tunnels into your services, you’ll have access to all of your data in your environment. All that’s left now is to actually build the site. Build the site From the repository root, run the command platform build The Platform CLI will first ask you for the source directory and the build destination, then it will use your .platform.app.yaml file to execute the build process locally . This will create a _www directory in the project root that is a symlink to the currently active build, which is now located in .platform/local/builds. Verify Move to the build destination (i.e. cd _www) and then run a local web server to verify the build. PHP Python Ruby php -d variables_order=EGPCS -S localhost:8001 python3 -m http.server 8000 ruby -run -e httpd . -p 8000 Applications written in Node.js, Go and Java can be configured to listen on a port locally, so it will only be necessary to execute the program directly. Cleanup That’s it! Now you can easily spin up a local build of your application and test new features with full access to all of the data in your services. When you are finished, shut down the web server and then close the tunnel to your services: platform tunnel:close Now you know how to connect to your services on Platform.sh and perform a local build during development. Back I\u0026#39;ve built my application locally",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/developing/local-development/build-locally.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "1a6db47c5770be902d43a43ac3444955",
        "title": "Build, Deploy, Done!",
        "description": "",
        "text": " Once you have configured the template application in the previous step, Platform.sh will build your project for you. If you created a blank project, be sure to set up your SSH keys before trying to upload your files. Explore the management console When the build screen has cleared, Platform.sh will return you to the management console. Since you now have a project on your account, a version of this page will be what you see each time you visit the console. You will start on the main page for your new project, My First Project. From here, you can control the settings of this project and monitor its status. In the Environments box, click on Master. Check the build status Take a minute to notice some the information available on this page. Overview In this box the Master environment, which is a live environment built from the master branch of your application code, will have a status of Building. Once that status has updated to Active, the build is complete and the application has deployed. Environment Activity In this block, you can see what you have done so far has two initial entries: My First Project was created, and the template profile you chose was initialized on the environment. Done! That’s it! Once the build status has changed to Active, your application has been deployed on Platform.sh. You can view the template by clicking on the link that is now visible for the Master environment under the Overview box. It will open another tab in your browser to your new live site! In these few steps you created a free trial account, configured a template application on a project and deployed it using the management console entirely from your browser. Using the Platform.sh CLI , however, you get even more control over your project configurations, including the ability to migrate your own applications to Platform.sh. Move onto the next step to install it. Back I\u0026#39;ve deployed a template application",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/template/check-status.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "a77c46cb0716f05cb976ca30e4105f96",
        "title": "CLI (Command line interface)",
        "description": "",
        "text": " The CLI is the official tool to use and manage your Platform.sh projects directly from your terminal. Anything you can do within the management console can be done with the CLI. Behinds the scenes it uses both the Git interface and our REST API. The source code of the CLI is hosted on GitHub . Find detailed information on setting up a local development environment . Installation You can install the CLI easily using this command: curl -sS https://platform.sh/cli/installer | php You can find the system requirements and more information in the installation instructions on GitHub . Authentication The Platform.sh CLI will authenticate you with Platform.sh and show your projects. Just type this command to start: platform You will be asked to log in via a browser. When you are logged in, a list of your projects appears, along with some tips for getting started. Your command-line tools are now ready to use with Platform.sh. Note: Please consult the full documentation on CLI Authentication on the public CLI Github repository for further details. Usage The CLI uses Platform.sh API to trigger commands (Branch, Merge…) on your projects. It’s also very useful when you work locally since it can simulate a local build of your codebase as if you were pushing a change to Platform.sh. Once you have the CLI installed, run platform list to see all of the available commands. You can preface any command with help to see more information on how to use that command. $ platform help domain:add Command: domain:add Description: Add a new domain to the project Usage: domain:add [--project[= ... ]] [--cert= ... ] [--key= ... ] [--chain= ... ] [name] Arguments: name The name of the domain Options: --project The project ID --cert The path to the certificate file for this domain. --key The path to the private key file for the provided certificate. --chain The path to the certificate chain file or files for the provided certificate. (multiple values allowed) --help (-h) Display this help message --quiet (-q) Do not output any message --verbose (-v|vv|vvv) Increase the verbosity of messages --version (-V) Display this application version --yes (-y) Answer  yes  to all prompts --no (-n) Answer  no  to all prompts --shell (-s) Launch the shell CLI features Additional settings to control the operation of the Platform.sh CLI can be managed in the configuration file (.platform/local/project.yaml) or environment variables. See the README for the CLI for details . Auto-selecting your project When your shell’s working directory is inside a local checkout of your project repository, the CLI will autodetect your project ID and environment so you don’t need to list them as parameters each time. In your home directory, for example, you need to provide the project ID as an argument each time: $ platform project:info --project=acdefghijkl --environment=staging You can instead get the same result with just: $ cd myproject $ platform project:info You can also set a preferred project ID with the environment variables PLATFORM_PROJECT, PLATFORM_BRANCH and PLATFORM_APPLICATION_NAME. export PLATFORM_PROJECT=acdefghijkl; export PLATFORM_BRANCH=staging; platform project:info Autocomplete on the command line Once installed, the platform CLI tool provides tab auto-completion for commands, options, and even some values (your projects, valid regions). Note: Your system must include the bash-completion package or equivalent. This is not available by default on OSX, but can be installed via brew. Check your home directory and ensure that the file ~/.platformsh/autocompletion.sh is being included by your shell. platform self:install will attempt a reinstall of this utility if it’s needed. Installing the CLI on Windows 10 There are multiple ways to install the CLI on Windows 10. Platform.sh recommends using Bash for Windows (Windows Subsystem for Linux). Installing Bash for Windows You can install Bash to use the CLI on a Windows 10, 64-bit machine. The Windows 10 Anniversary Update is needed to support Git. To install Bash on Windows 10 Anniversary Edition you need to: Activate the Developer Mode in “Update \u0026 Security” in Windows Settings. This will prompt you to restart your computer. Activate the “Windows Subsystem for Linux (Beta) , under “Turn Windows features on or off” in the Programs and Features section of the Control Panel. Once again, you will need to restart your computer. In the Start Menu, search for the program “bash.exe”, which will prompt you to install it from the Windows Store. Bash is now installed. You can read more on WindowsCentral . Upon starting Bash, you will be asked to choose a username. According to the article, it doesn’t have to be the same as your current username. However, if the username don’t exist, the Linux system might not be able to create the Linux directory (depending on your permissions level). It is therefore recommended you use the same username for Linux as your Windows machine (provided your Windows user name isn’t “Admin”, as that will not be allowed). Once Bash for Windows is installed, you can install the Platform.sh CLI with the same command as above: curl -sS https://platform.sh/cli/installer | php",
        "section": "Development",
        "subsections": " Installation Authentication Usage CLI features Auto-selecting your project Autocomplete on the command line Installing the CLI on Windows 10  Installing Bash for Windows    ",
        "image": "",
        "url": "/development/cli.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "34d6bd5038a2e3e0dda87a632e86e933",
        "title": "Configure DNS",
        "description": "",
        "text": " The next step is to configure your DNS provider to point to the domain of your master environment on Platform.sh. You can access the CNAME target from your terminal by using the CLI and the command platform environment:info edge_hostname Add a CNAME record from your desired domain to the value of the edge_hostname. Depending on your registrar, this value may be called an “Alias” or something similar. If your application is going to serve multiple domains, you will need to add a CNAME record for each of them. You can find out more information about using an apex domain and CNAME records in the Going Live documentation . Depending on your registrar and the TTL you set for the domain, it may take up to 72 hours for the DNS change to fully propagate across the Internet. Back I have configured my DNS provider",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/next-steps/going-live/configure-dns.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "726b3bcbc3ca6d46f1c0e79916d7d938",
        "title": "Configure services",
        "description": "",
        "text": " Platform.sh allows you to completely define and configure the topology and services you want to use on your project. Unlike other PaaS services, Platform.sh is batteries included which means that you don’t need to subscribe to an external service to get a cache or a search engine. And that those services are managed. When you back up your project, all of the services are backed-up. Services are configured through the .platform/services.yaml file you will need to commit to your Git repository. This section describes specifics you might want to know about for each service.” If you don’t have a .platform folder, you need to create one: mkdir .platform touch .platform/services.yaml Here is an example of a services.yaml file: database1:type:mysql:10.1disk:2048database2:type:postgresql:9.6disk:1024Configuration Name The name you want to give to your service. You are free to name each service as you wish (lowercase alphanumeric only). Note: Because we support multiple services of the same type (you can have 3 different MySQL instances), changing the name of the service in services.yaml will be interpreted as destroying the existing service and creating a new one. This will make all the data in that service disappear forever. Remember to always back up your environment in which you have important data before modifying this file. Type The type of your service. It’s using the format type:version. If you specify a version number which is not available, you’ll see this error when pushing your changes: Validating configuration files. E: Error parsing configuration files: - services.mysql.type: 'mysql:5.6' is not a valid service type. Service types and their supported versions include: Service type Supported version Headless Chrome chrome-headless 73 Elasticsearch elasticsearch 6.5, 7.2 InfluxDB influxdb 1.2, 1.3, 1.7 Kafka kafka 2.1, 2.2, 2.3, 2.4 MariaDB mariadb 10.0, 10.1, 10.2, 10.3, 10.4 Memcached memcached 1.4, 1.5, 1.6 MongoDB mongodb 3.0, 3.2, 3.4, 3.6 Network Storage network-storage 1.0 Oracle MySQL oracle-mysql 5.7, 8.0 PostgreSQL postgresql 9.6, 10, 11, 12 RabbitMQ rabbitmq 3.5, 3.6, 3.7, 3.8 Redis redis 3.2, 4.0, 5.0 Solr solr 3.6, 4.1, 6.3, 6.6, 7.6, 7.7, 8.0, 8.4 Varnish varnish 5.6, 6.0 Disk The disk attribute is the size of the persistent disk (in MB) allocated to the service. For example, the current default storage amount per project is 5GB (meaning 5120MB) which you can distribute between your application (as defined in .platform.app.yaml) and each of its services. For memory-resident-only services such as memcache or redis, the disk key is not available and will generate an error if present. Note: Currently we do not support downsizing the persistent disk of a service. Size By default, Platform.sh will allocate CPU and memory resources to each container automatically. Some services are optimized for high CPU load, some for high memory load. By default, Platform.sh will try to allocate the largest “fair” size possible to all services, given the available resources on the plan. That is not always optimal, however, and you can customize that behavior on any service or on any application container. See the application sizing page for more details. Service timezones All services have their system timezone set to UTC by default. In most cases that is the best option. For some applications it’s possible to change the application timezone, which will affect only the running application itself. MySQL - You can change the per-connection timezone by running SQL SET time_zone = \u003ctimezone\u003e;. PostgreSQL - You can change the timezone of current session by running SQL SET TIME ZONE \u003ctimezone\u003e;. Using the services In order for a service to be available to an application in your project (Platform.sh supports not only multiple backends but also multiple applications in each project) you will need to refer to it in the .platform.app.yaml file which configures the relationships between applications and services. Endpoints All services offer one or more endpoints. An endpoint is simply a named set of credentials that can be used to gives access to other applications and services in your project to that service. Only some services support multiple user-defined endpoints. If you do not specify one then one will be created with a standard defined name, generally the name of the service type (e.g., mysql or solr). An application container, defined by a .platform.app.yaml file, always exposes and endpoint named http to allow the router to forward requests to it. When defining relationships in a configuration file you will always address a service as \u003cservicename\u003e:\u003cendpoint\u003e. See the appropriate service page for details on how to configure multiple endpoints for each service that supports it. Connecting to a service Once a service is running and exposed as a relationship, its appropriate credentials (host name, username if appropriate, etc.) will be exposed through the PLATFORM_RELATIONSHIPS environment variable. The structure of each is documented on the appropriate service’s page, along with sample code for how to connect to it from your application. Note that different applications manage configuration differently so the exact code will vary from one application to another. Be aware that the keys in the PLATFORM_RELATIONSHIPS structure are fixed but the values they hold may change on any deployment or restart. Never hard-code connection credentials for a service into your application. You should re-check the environment variable every time your script or application starts. Access to the database or other services is only available from within the cluster. For security reasons they cannot be accessed directly. However, they can be accessed over an SSH tunnel. There are two ways to do so. (The example here uses MariaDB but the process is largely identical for any service.) Obtaining service credentials In either case, you will also need the service credentials. For that, run platform relationships. That will give output similar to the following: redis:- service: rediscacheip:246.0.82.19cluster:jyu7waly36ncj-master-7rqtwtihost:redis.internalrel:redisscheme:redisport:6379database:- username: userscheme:mysqlservice:mysqldbip:246.0.80.37cluster:jyu7waly36ncj-master-7rqtwtihost:database.internalrel:mysqlpath:mainquery:is_master:truepassword:''port:3306That indicates that the database relationship can be accessed at host database.internal, user user, and an empty password. The path key contains the database name, main. The other values can be ignored. Note: When using the default endpoint on MySQL/MariaDB, the password is usually empty. It will be filled in if you define any custom endpoints. As there is only the one user and port access is tightly restricted anyway the lack of a password does not create a security risk. Open an SSH tunnel directly The first option is to open an SSH tunnel for all of your services. You can do so with the Platform.sh CLI, like so: $ platform tunnel:open SSH tunnel opened on port 30000 to relationship: redis SSH tunnel opened on port 30001 to relationship: database Logs are written to: ~/.platformsh/tunnels.log List tunnels with: platform tunnels View tunnel details with: platform tunnel:info Close tunnels with: platform tunnel:close The tunnel:open command will connect all relationships defined in the .platform.app.yaml file to local ports, starting at 30000. You can then connect to those ports on localhost using the program of your choice. In this example, we would connect to localhost:30001, database name main, with username user and an empty password. The platform tunnels command will list all open tunnels: +-------+---------------+-------------+-----------+--------------+ | Port | Project | Environment | App | Relationship | +-------+---------------+-------------+-----------+--------------+ | 30000 | a43m75zns6k4c | master | [default] | redis | | 30001 | a43m75zns6k4c | master | [default] | database | +-------+---------------+-------------+-----------+--------------+ Using an application tunnel Alternatively, many database applications (such as MySQL Workbench and similar tools) support establishing their own SSH tunnel. Consult the documentation for your application for how to enter SSH credentials, including telling it where your SSH private key is. (Platform.sh does not support password-based SSH authentication.) To get the values to use, the easiest way is to run platform ssh --pipe. That will return a command line that can be used to connect over SSH, from which you can pull the appropriate information. For example: jyu7waly36ncj-master-7rqtwti--app@ssh.us.platform.sh In this case, the username is jyu7waly36ncj-master-7rqtwti--app and the host is ssh.us.platform.sh. Note that the host will vary per region, and the username will vary per-environment. In this example, we would configure our database application to setup a tunnel to ssh.us.platform.sh as user jyu7waly36ncj-master-7rqtwti--app, and then connect to the database on host database.internal, username user, empty password, and database name main.",
        "section": "Configuration",
        "subsections": " Configuration  Name Type Disk Size   Service timezones Using the services Endpoints Connecting to a service  Obtaining service credentials Open an SSH tunnel directly Using an application tunnel    ",
        "image": "",
        "url": "/configuration/services.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "7f0dfb316b5ab9f92ea424cf1598742a",
        "title": "Content Delivery Networks",
        "description": "",
        "text": " Platform.sh Dedicated plans include a Fastly CDN account by default, which will be managed by Platform.sh. Our experience has shown that effective caching can mean a huge difference in the perceived performance of an application by its users, and that placing the caches closer to your users (wherever they may be) is the best solution currently available. Self-Service Grid plans do not include a CDN by default, but you are welcome to configure one yourself. See our guidelines for when and if to use a CDN for HTTP caching. We have partnerships with a variety of CDN vendors depending on your application’s needs. Our recommended CDN provider is Fastly . DNS management The distributed nature of most CDNs means that for proper functioning, any domains that you intend to make use of the CDN will be required to use CNAME records for pointing the DNS entries. Pointing the root domain (example.com) at a CNAME record is not possible for all DNS hosts, so you will need to confirm this functionality or migrate to a new DNS host. CloudFlare has a more detailed writeup of the challenges of root CNAMEs. In the event that you and your team choose a pure Fastly solution, this is negated by their providing a set of Anycast IP addresses for you. This allows you to create A records for your root domain that will point to Fastly’s CDN. Initial setup For Enterprise-Dedicated plans, CDN setup is handled by Platform.sh as part of your onboarding. After the application is stood up on its Dedicated VMs we can begin the collaborative process of provisioning the CDN and configuring DNS and caching setup. We provide CDN services for both staging and production. For self-service Grid plans, the setup can be done at any time by the customer. Cache configuration Depending on which CDN is decided as part of the pre-sales analysis, there may be varying levels of flexibility with regard to caching and ongoing cache invalidation. This should be discussed between your sales representative and senior technical members of your team if there are concerns with CDN configuration and functionality. If using Fastly as a CDN, it is possible to provide either custom VCL snippets or a full custom VCL file. Platform.sh will grant customers access to do so upon request. However, be aware that downtime caused by custom VCL configuration will not be covered by the SLA, just as application code in your repository is not covered by the SLA. TLS encryption Security and the related topic of encryption of data are fundamental principles here at Platform.sh, and as such we provide TLS certificates in the default Enterprise-Dedicated package. This allows for encryption of all traffic between your users and your application. By default we will provision a shared certificate with the chosen CDN vendor. If you opt for the Global Application Cache, we will provision certificates for both the site subdomain (www) and the asset/CDN subdomain. We use wildcard certificates to secure production, staging, and any other subdomains simultaneously. If you need Extended Validation TLS certificates you will need to provide your own from an issuer of your choice that we can install for you. If you need to provide your own TLS certificate, place the certificate, the unencrypted private key, and the necessary certificate chain supplied by your TLS provider in your application’s private directory (not web accessible), and then open a ticket to let our team know to install it. Platform.sh Enterprise-Dedicated supports a single TLS certificate on the origin. Support for multiple certificates is offered only through a CDN such as CloudFront or Fastly. Self-signed certificates can optionally be used on the origin for development purposes or for enabling TLS between the CDN and origin. All TLS certificates used with CloudFront MUST be 2048 bit certificates. Larger sizes will not work. Web Application Firewall \u0026 Anti-DDoS All Platform.sh-hosted sites, either Grid or Dedicated, live on infrastructure provided by major cloud vendors. These vendors include their own Level 3 DDoS protection that is sufficient for the vast majority of cases. Customers are welcome to put their own WAF in front of a Dedicated cluster or add other security measures not included in the offering. The router cache When using a CDN the Platform.sh router’s HTTP cache becomes redundant. In most cases it’s best to disable it outright. Modify your route in .platform/routes.yaml like so to disable the cache:  https://{default}/ :type:upstreamupstream: app:http cache:# Disable the HTTP cache on this route. It will be handled by the CDN instead.enabled:falsePreventing direct access When using a CDN, you might not want users to access your Platform.sh origin directly. There are three ways to secure your origin. Password protected HTTP Authentication You can password protect your project using HTTP access control . Make sure that you generate a password of sufficient strength. You can then share the password with your CDN provider. Make sure the CDN adds a header to authenticate correctly to your origin. Add a custom header to the origin request with the base64 encoded username:password. For example: Aladdin:OpenSesame would become Authorization: Basic QWxhZGRpbjpPcGVuU2VzYW1l. Be aware that this approach will apply the same user and password to all development environments, too. You can have developers enter credentials through their browser, or override the access control setting for each child environment. Note: This is the recommended approach for CloudFlare. Allowing and denying IP addresses If your CDN does not support adding headers to the request to origin, you can allow the IP addresses of your CDN. Note: You WILL have to update your configuration when your CDN updates their IP addresses. List of IP ranges for: CloudFlare Fastly Be aware that this approach will apply the same IP restrictions to all development environments, too. To remove it from development environments, you will need to disable it on each environment or else create a single child of master where it is disabled, and them make all development branches off of that environment. Client authenticated TLS If your CDN offers this option, an alternative way of securing the connection is client authenticated TLS . note: Please remember to permit your developers to access the origin by creating your own certificate or else they won’t be able to access the project url directly. (see below) CloudFlare has a very good article on what client authenticated TLS is, and how to set this up. To activate authenticated TLS follow the following steps: Download the correct certificate from your CDN provider. CloudFlare Caveat! an attacker could make a Cloudflare account to bypass your origin restriction. For CloudFlare, using the HTTP access control described above is the recommended way of securing your origin. Fastly Make sure you have a .crt file. If you have have .pem file, simply rename it to cdn.crt Add the cdn.crt to your git repository Add the relevant configuration to your .platform.app.yaml file tls: client_authentication:  require  client_certificate_authorities: - !include type: string path: cdn.crt Note: The steps above are generally similar but can vary for different CDN providers. Contact your CDN provider’s support department for specific assistance.",
        "section": "Going live",
        "subsections": " DNS management Initial setup Cache configuration TLS encryption Web Application Firewall \u0026amp; Anti-DDoS The router cache Preventing direct access  Password protected HTTP Authentication Allowing and denying IP addresses Client authenticated TLS    ",
        "image": "",
        "url": "/golive/cdn.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "d7452b54e01892fffca3eae8ff36cf7b",
        "title": "Custom sizing",
        "description": "",
        "text": " By default, Platform.sh will automatically select appropriate resource sizes (CPU and memory) for a container when it’s deployed, based on the plan size and the number of other containers in the cluster. The more containers in a project the fewer resources each one gets, and vice versa, with similar containers getting similar resources. Note: These are advanced settings and should only be used by experienced Platform.sh users. 99.9% of the time our default container sizes are the correct choice for best performance. Usually that’s fine, but sometimes it’s undesirable. You may, for instance, want to have a queue worker container that you know has low memory and CPU needs, so it’s helpful to give that one fewer resources and another container more. Or a given service may be very heavily used in your architecture so it needs all the resources it can take. In those cases you can provide sizing hints to the system on a per-service basis. Every application container as well as every service in .platform/services.yaml supports a size key, which instructs the system how many resources to allocate to it. The exact CPU and memory allocated will depend on the application or service type, and we may adjust these values over time to better optimize resource usage. Legal values for the size key are AUTO (the default), S, M, L, XL, 2XL, 4XL. Note that in a development environment this value is ignored and always set to S. It will only take effect in a production deployment (a master branch with an associated domain). If the total resources requested by all apps and services is larger than what the plan size allows then a production deployment will fail with an error. How do I make a background processing container smaller to save resources? Simply set the size key to S to ensure that the container gets fewer resources, leaving more to be allocated to other containers. name:processingtype:nodejs:6.11size:S...",
        "section": "Configure your application",
        "subsections": " How do I make a background processing container smaller to save resources?  ",
        "image": "",
        "url": "/configuration/app/size.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "a9015effbb9bd28b57ead22d637175fb",
        "title": "Data collection",
        "description": "",
        "text": " As part of our normal business operations we do collect various pieces of data. In GDPR terms: Article 4: Our accounts system contains some (routine) Article 4 items (name, address, phone, etc.) in order to allow us to bill your account appropriately. This information can be verified, changed, and deleted by logging into your account . Article 9: We don’t capture and store any Article 9 special identifiers (such as race, religion, sexual orientation, or other attributes that are irrelevant to our business). Article 30: The only Article 30 items we keep are IP address and Log files. These reside on AWS/Azure/Orange (depending on your hosting), and may be sent to Sentry.io when there are crashes. Application logs Application logs are those generated by the host application or application server (such as PHP-FPM). They are immutable to Customers to prevent tampering. These logs are secured behind key-based SSH so that only the Customer and our relevant teams have access. System logs Platform.sh records routine system logs. We do not access Customer-specific system logs or the customer environment unless requested to do so to help solve a problem. In the future, we will be rolling out better log segregation to allow a Customer to get easier access to their own logs for diagnostic purposes. Access logs There are two main types of access logs: web and SSH. Web access logs Application access logs are immutable to Customers to prevent tampering. These logs are secured behind key-based SSH so that only the Customer and our relevant teams have access. SSH access logs SSH access logs are securely stored in our infrastructure and not accessible to customers. They can be accessed by Platform.sh support personnel as part of an audit if requested. Access by customers and Platform.sh support personnel to customer environments is logged. However, we only log the connection itself, not what was done during the session, as that would be a violation of customer privacy. Vendor data sharing We have identified and mapped all data we collect and share with vendors (such as AWS, Azure, and Orange). We know what we capture and where it goes. All of our vendors have been vetted for security and GDPR compliance. We have enacted contract amendments and Data Processing Agreements (DPAs) where applicable.",
        "section": "Security and compliance",
        "subsections": " Application logs System logs Access logs  Web access logs SSH access logs   Vendor data sharing  ",
        "image": "",
        "url": "/security/data-collection.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "1cb46400325c3c9c33bb934b162ffed2",
        "title": "Deployment",
        "description": "",
        "text": " Deploying to Production and Staging The production branch of your Git repository is designated for production, and a staging branch is designated for staging. Any code merged to those branches will automatically trigger a rebuild of the production and staging environments, respectively, in the Dedicated Cluster. Any defined users or environment variables will also be propagated to the Dedicated Cluster as well. Note that there is no automatic cloning of data from the Dedicated Cluster to the Development Environment the way there is between branches in the Development Environment. Production data may still be replicated to the Development Environment manually. The master branch is still available but will have no impact on either the production or staging environments. Deploys of the master branch will not trigger a rebuild of the Dedicated Cluster environments. A common model is to use the master branch as a pre-integration branch before merging code to staging, such as at the end of a sprint. Deployment process When deploying to the Dedicated Cluster the process is slightly different than when working with Platform.sh on the Grid. The new application image is built in the exact same fashion as for Platform.sh Professional. Any active background tasks on the cluster, including cron tasks, are terminated. The cluster (production or staging) is closed, meaning it does not accept new requests. Incoming requests will receive an HTTP 500 error. The application image on all three servers is replaced with the new image. The deploy hook is run on one, and only one, of the three servers. The cluster is opened to allow new requests. The deploy usually takes approximately 30-90 seconds, although that is highly dependent on how long the deploy hook takes to run. During the deploy process the cluster is unavailable. However, nearly all Platform.sh Dedicated instances are fronted by a Content Delivery Network (CDN). Most CDNs can be configured to allow a “grace period”, that is, requests to the origin that fail will be served from the existing cache, even if that cache item is stale. We strongly recommend configuring the CDN with a grace period longer than a typical deployment. That means anonymous users should see no interruption in service at all. Authenticated traffic that cannot be served by the CDN will still see a brief interruption. Deployment philosophy Platform.sh values consistency over availability, acknowledging that it is nearly impossible to have both. Because the deploy hook may make database changes that are incompatible with the previous code version it is unsafe to have both old and new code running in parallel (on different servers), as that could result in data loss. We believe that a minute of planned downtime for authenticated users is preferable to a risk of race conditions resulting in data corruption, especially with a CDN continuing to serve anonymous traffic uninterrupted. That brief downtime applies only to changes pushed to the production branch. Deployments to staging or to a development branch have no impact on the production environment and will cause no downtime.",
        "section": "Platform.sh Dedicated cluster specifications",
        "subsections": " Deploying to Production and Staging Deployment process Deployment philosophy  ",
        "image": "",
        "url": "/dedicated/architecture/deploying.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "6d163cc59d4928838cda44688d3ffed4",
        "title": "Developing on Platform.sh",
        "description": "",
        "text": "Once an application has been migrated to Platform.sh, there's plenty more features that will help improve your development life cycle. You can build your site locally, remotely connect to your services, and test new features on a live site all with Platform.sh.",
        "section": "Introduction",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/developing.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "97f66866006367ccf3beeca603b04376",
        "title": "Environment configuration",
        "description": "",
        "text": " You can access an environment’s settings by selecting that environment from the Select Environments pull-down menu at the top of the page or by clicking that environment within the Environments graphic on the right side. Click the Settings tab at the top of the screen. General The General screen allows you to extend the behavior of a specific environment. Environment name The first setting allows you to modify the name of the environment and view its parent environment. Status From the Status tab, you can activate or deactivate an environment. The Deactivate \u0026amp; Delete Data action will Deactivate the environment. Unless is is re-activated, it will no longer deploy and it will not be accessible from the web or via SSH. Destroy all services running on this environment. Delete all data specific to the environment. If the environment is reactivated, it will sync data from its parent environment. Once the environment is deactivated, the Git branch will remain on Platform.sh in the inactive environment. To delete the branch as well, you need to execute the following: git push origin :BRANCH-NAME Note: Deleting the Master environment is forbidden. Outgoing emails From this tab, you can allow your application to send emails via a SendGrid SMTP proxy. Changing this setting will temporarily list the environment’s status as “Building”, as the project re-builds with the new setting. Once it has re-deployed, it will appear once again as “Active” in your settings. Search engine visibility From this tab, you can tell search engines to ignore the site entirely, even if it is publicly visible. X-Robots-Tag By default, Platform.sh includes an additional X-Robots-Tag header on all non-production environments: X-Robots-Tag: noindex, nofollow That tells search engines to not index sites on non-production environments entirely nor traverse links from those sites, even if they are publicly visible. That keeps non-production sites out of search engine indexes that would dilute the SEO of the production site. To disable that feature for a non-production environment, use the Platform.sh CLI command below: platform environment:info restrict_robots false Or to disable it for a specific environment other than the one that is currently checked out, execute the following: platform environment:info -e ENVNAME restrict_robots false where ENVNAME is the name of the environment. On a production instance (the master branch, after a domain has been assigned) the search-blocker is disabled and your application can serve a robots.txt file as normal. However, you must ensure that the file is in your project’s web root (the directory where the / location maps to) and your application is configured to serve it. See the location section in .platform.app.yaml . HTTP access control You should not expose your development environments to the whole wide world. Platform.sh allows you to simply implement access control, either by login/password (the equivalent to .htaccess) or by filtering IP addresses or a network using the CIDR format . That is, 4.5.6.7 and 4.5.6.0/8 are both legal formats. Note: Changing access control will trigger a new deploy of the current environment. However, the changes will not propagate to child environments until they are manually redeployed. These settings get inherited by branches below the one you are on. That means if you create a staging environment, and you create branches from this one, they will all inherit the same authentication information and you only have to set-it up once. You can also setup authentication with the CLI using the following command platform environment:http-access which also allows you to read the current setup. This eases the integration of CI jobs with Platform.sh as you will not need to hardcode the values in the CI. You can allow or deny access to specific IPs or IP ranges. First switch the access control section to ON. Then add one or more IPs or CIDR IP masks, followed by allow or deny. See the example below. Note that allow entries should come before deny entries in case both of them would match. For example, the following configuration will only allow the 1.2.3.4 IP to access your website. 1.2.3.4/32 allow 0.0.0.0/0 deny Access The Access screen allows you to manage the users’ access on your project. You can invite new users to a specific environment by clicking the Add button and entering their email address, or modify permissions of existing users by clicking the Edit link when hovering the user. Note: Currently, permission changes that grant or revoke SSH access to an environment take effect only after the next time that environment is deployed. Selecting a user will allow you to either edit or remove access to that environment. You can also manage access to users on multiple environments using the project configuration screen. Variables The Variables screen allows you to define the variables that will be available on a specific environment. Routes The Routes screen describes the configuration features that define the routes of your application. Routes cannot be edited here, but it provides a simple routes configuration example for your project’s .platform/routes.yaml file. Consult the documentation for more information about properly configuring Routes for your project.",
        "section": "Management console",
        "subsections": " General  Environment name Status Outgoing emails Search engine visibility X-Robots-Tag HTTP access control   Access Variables Routes  ",
        "image": "",
        "url": "/administration/web/configure-environment.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "93f4343df884eff652b0ab0924905c16",
        "title": "eZ Platform",
        "description": "",
        "text": " eZ Platform is a Composer-based PHP CMS, and as such fits well with the Platform.sh model. As a Symfony-based application its setup is very similar to Symfony. eZ Platform comes pre-configured for use with Platform.sh in version 1.13 and later. Version 2.5 and later is recommended. Those are the only versions that are supported. Appropriate Platform.sh configuration files are included in the eZ Platform application itself, but of course may be modified to suit your particular site if needed. Cache and sessions By default, eZ Platform is configured to use a single Redis instance for both the application cache and session storage. You may optionally choose to use a separate Redis instance for session storage in case you have a lot of authenticated traffic (and thus there would be many session records). To do so, uncomment the redissession entry in the .platform/services.yaml file and the corresponding relationship in the .platform.app.yaml file. The bridge code that is provided with eZ Platform 1.13 and later will automatically detect the additional Redis service and use it for session storage. On a Dedicated instance, we strongly recommend using two separate Redis instances for Cache and Sessions. The service and relationship names that ship with the default Platform.sh configuration in eZ Platform should be used as-is. To ensure the development environment works like Production, uncomment the redissession entry in the .platform/services.yaml file and the corresponding relationship in the .platform.app.yaml file. The bridge code that is provided with eZ Platform 1.13 and later will automatically detect the additional Redis service and use it for session storage. By default, on Dedicated instances we will configure both Cache and Session storage in “persistent” mode, so that data is not lost in case of a system or process restart. That reduces the potential for cache stampede issues or inadvertently logging people out. Modifying an existing eZ Platform project If you have an existing eZ Platform project that was upgraded from a previous version, or want to resynchronize with the latest recommended configuration, please see the eZ Platform official repository . In particular, see: The .platform.app.yaml file, which automatically builds eZ Platform in dev mode or production mode depending on your defined project-level variables. The .platform directory The platformsh.php configuration file, which does the work of mapping Platform.sh environment variables into eZ Platform. It also will automatically enable Redis-based cache and session support if detected. Local Development with eZ Platform 2.x and later eZ Systems provide a tool called eZ Launchpad for local development on top of a Docker stack. It improves Developer eXperience and reduces complexity for common actions by simplifying your interactions with Docker containers. eZ Launchpad is ready to work with Platform.sh. It serves as a wrapper that allows you to run console commands from within the container without logging into it explicitly. For example to run bin/console cache:clear inside the PHP container do: ~/ez sfrun cache:clear eZ Launchpad installation eZ Launchpad’s approach is to stay as decoupled as possible from your development machine and your remote hosting whether you are Linux or Mac OSX. To install run: curl -LSs https://ezsystems.github.io/launchpad/install_curl.bash | bash Then you can start to use it to initialize your eZ Platform project on top Docker. ~/ez init or create the Docker stack based on an existing project git clone yourproject.git application cd application ~/ez create You will find more details on the eZ Launchpad documentation . At this time you will have a working eZ Platform application with many services including Varnish, Solr, Redis etc. Platform.sh integration To generate the key files for Platform.sh (.platform.app.yaml and .platform) run: ~/ez platformsh:setup eZ Launchpad will generate the files for you and you are then totally free to fine tune them. Solr specificity Solr is fully functional with eZ Launchpad but it is not enabled by default on Platform.sh. You will have to set it up manually following the current documenation here: https://github.com/ezsystems/ezplatform/blob/master/.platform/services.yaml#L37. Actions needed are: Generate the Solr configuration thanks to the script provided by eZ Systems. Put the result in the .platform at the root of your project. Add the service in the .platform/services.yaml. Add the relationship in the .platform.app..yaml. Environment variables (optional) eZ Launchpad allows you to define environment variables in the provisioning/dev/docker-compose.yml file. You may use that to set Platform.sh variables to match Platform.sh environments so that you can keep your environment behavior in sync. Such variables have to be set in the engine container. # provisioning/dev/docker-compose.ymlengine:environment:- ASIMPLEVARIABLE=avalue- PLATFORM_RELATIONSHIPS=A_BASE64_ENCODED_VALUELocal development with Platform.sh Thanks to eZ Launchpad you are able to be work 100% locally: untethered . We have the whole project working offline on our local machine. Note: Platform.sh also provides a smooth SSH tunnels integration described in the tethered page. Local services are provided by the Docker stack but there are minimum day-to-day tasks that you might need with Platform.sh. The main ones are: Downstream database synchronization: Getting it from the remote to the local. Downstream file storage synchronization: Getting it from the remote to the local. To help you with that, Platform.sh provides a CLI that you probably already have. If you don’t, see the install guide . Combined together, eZ Launcphad and Platform.sh CLI make those actions straight forward and simple. Database and storage synchronization platform db:dump --gzip -f ezplatform.sql.gz -d data/ -y platform mount:download -m ezplatform/web/var --target=ezplatform/web/var/ -y ~/ez/importdata The two first lines get the remote database and storage from the remote environment and stores it locally in data/. The third tells to eZ Launchpad to import those data in the Docker stack. Note: The storage (images and files) synchronization is optional. eZ Platform provides a placeholder generator mechanism which allows you to forget about the real images for your local.",
        "section": "Featured frameworks",
        "subsections": " Cache and sessions Modifying an existing eZ Platform project Local Development with eZ Platform 2.x and later  eZ Launchpad installation Platform.sh integration Local development with Platform.sh    ",
        "image": "",
        "url": "/frameworks/ez.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "8c18f1de7b0d457621f4d862c950b32a",
        "title": "InfluxDB (Database service)",
        "description": "",
        "text": " InfluxDB is a time series database optimized for high-write-volume use cases such as logs, sensor data, and real-time analytics. It exposes an HTTP API for client interaction. See the InfluxDB documentation for more information. Supported versions Grid Dedicated 1.2 1.3 1.7 None available Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : {  cluster :  rjify4yjcwxaa-master-7rqtwti ,  host :  influxdb.internal ,  hostname :  mycuti5glfqyt322jjhcfahrpi.influxdb.service._.eu-3.platformsh.site ,  ip :  169.254.180.153 ,  port : 8086,  rel :  influxdb ,  scheme :  http ,  service :  influxdb ,  type :  influxdb:1.7  } Usage example In your .platform/services.yaml: timedb:type:influxdb:1.7disk:256 In your .platform.app.yaml: relationships:influxtimedb: timedb:influxdb  Note: You will need to use the influxdb type when defining the service # .platform/services.yamlservice_name:type:influxdb:versiondisk:256 and the endpoint influxdb when defining the relationship # .platform.app.yamlrelationships:relationship_name:“service_name:influxdb” Your service_name and relationship_name are defined by you, but we recommend making them distinct from each other. You can then use the service in a configuration file of your application with something like: \u0026lt;?php // This assumes a fictional application with an array named $settings. if (getenv(\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;)) { $relationships = json_decode(base64_decode($relationships), TRUE); // For a relationship named \u0026#39;influxtimedb\u0026#39; referring to one endpoint. if (!empty($relationships[\u0026#39;influxtimedb\u0026#39;])) { foreach ($relationships[\u0026#39;influxtimedb\u0026#39;] as $endpoint) { $settings[\u0026#39;influxdb_host\u0026#39;] = $endpoint[\u0026#39;host\u0026#39;]; $settings[\u0026#39;influxdb_port\u0026#39;] = $endpoint[\u0026#39;port\u0026#39;]; break; } } } Exporting data InfluxDB includes its own export mechanism . To gain access to the server from your local machine open an SSH tunnel with the Platform.sh CLI: platform tunnel:open That will open an SSH tunnel to all services on your current environment, and produce output something like the following: SSH tunnel opened on port 30000 to relationship: influxtimedb The port may vary in your case. Then, simply run InfluxDB’s export commands as desired. influx_inspect export -compress",
        "section": "Configure services",
        "subsections": " Supported versions Relationship Usage example Exporting data  ",
        "image": "",
        "url": "/configuration/services/influxdb.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "a1b09f1f5b13f42ffa8e096570485ece",
        "title": "Install the CLI",
        "description": "",
        "text": " In the previous steps you checked that the requirements on your computer were met and configured an SSH key on your Platform.sh account. Now all we have to do is install the CLI and you can access your projects from the command line. Install the CLI In your terminal run the following command depending on your operating system: Installing on OSX or Linux curl -sS https://platform.sh/cli/installer | php Installing on Windows curl https://platform.sh/cli/installer -o cli-installer.php php cli-installer.php Authenticate and Verify Once the installation has completed, you can run the CLI in your terminal with the command platform Take a moment to view some of the available commands with the command platform list Now that you have installed the CLI and it is communicating with Platform.sh, you can configure and push your project to Platform.sh. Back I\u0026#39;ve installed the CLI",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/cli-install.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "9cbb9988fbeeb9cf6f649e71650b799b",
        "title": "Lando",
        "description": "",
        "text": " Using Lando for local development Lando is a container-based local development toolchain that plays nicely with Platform.sh. It is maintained by Tandem , a 3rd party agency, but is a viable option for most Platform.sh projects. See the Lando documentation for installing and setting up Lando on your system. Lando will ask you to create a .lando.yml file in your application root, which functions similarly to the .platform.app.yaml file. (Note the different file extension.) It is safe to check this file into your Git repository as Platform.sh will simply ignore it. If your application is one of those with a specific “recipe” available from Lando, you can use that directly in your .lando.yml file. It can be customized further as needed for your application, and some customizations are specific to certain applications. .lando.yml configuration In particular, we recommend: # Name the application the same as in your .platform.app.yaml.name:app# Use the recipe appropriate for your application.recipe:drupal8config:# Lando defaults to Apache. Switch to nginx to match Platform.sh.via:nginx# Set the webroot to match your .platform.app.yaml.webroot:web# Lando defaults to the latest MySQL release, but Platform.sh uses MariaDB.# Specify the version to match what\u0026#39;s in services.yaml.database:mariadb:10.1Downloading data from Platform.sh into Lando In most cases downloading data from Platform.sh and loading it into Lando is straightforward. If you have a single MySQL database then the following two commands, run from your application root, will download a compressed database backup and load it into the local Lando database container. platform db:dump --gzip -f database.sql.gz lando db-import database.sql.gz Rsync can download user files easily and efficiently. See the exporting tutorial for information on how to use rsync. Then you need to update your sites/default/settings.local.php to configure your codebase to connect to the local database that you just imported: /* Working in local with Lando */ if (getenv(\u0026#39;LANDO\u0026#39;) === \u0026#39;ON\u0026#39;) { $lando_info = json_decode(getenv(\u0026#39;LANDO_INFO\u0026#39;), TRUE); $settings[\u0026#39;trusted_host_patterns\u0026#39;] = [\u0026#39;.*\u0026#39;]; $settings[\u0026#39;hash_salt\u0026#39;] = \u0026#39;CHANGE THIS TO SOME RANDOMLY GENERATED STRING\u0026#39;; $databases[\u0026#39;default\u0026#39;][\u0026#39;default\u0026#39;] = [ \u0026#39;driver\u0026#39; =\u0026gt; \u0026#39;mysql\u0026#39;, \u0026#39;database\u0026#39; =\u0026gt; $lando_info[\u0026#39;database\u0026#39;][\u0026#39;creds\u0026#39;][\u0026#39;database\u0026#39;], \u0026#39;username\u0026#39; =\u0026gt; $lando_info[\u0026#39;database\u0026#39;][\u0026#39;creds\u0026#39;][\u0026#39;user\u0026#39;], \u0026#39;password\u0026#39; =\u0026gt; $lando_info[\u0026#39;database\u0026#39;][\u0026#39;creds\u0026#39;][\u0026#39;password\u0026#39;], \u0026#39;host\u0026#39; =\u0026gt; $lando_info[\u0026#39;database\u0026#39;][\u0026#39;internal_connection\u0026#39;][\u0026#39;host\u0026#39;], \u0026#39;port\u0026#39; =\u0026gt; $lando_info[\u0026#39;database\u0026#39;][\u0026#39;internal_connection\u0026#39;][\u0026#39;port\u0026#39;], ]; }",
        "section": "Set up your local development environment",
        "subsections": " .lando.yml configuration Downloading data from Platform.sh into Lando  ",
        "image": "",
        "url": "/development/local/lando.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "ac44f2f6e4f3015808a97000be48b7d7",
        "title": "Merge into production",
        "description": "",
        "text": " Now that you’ve had the chance to verify that your application built and deployed correctly on your development environment, you’re ready to merge it into your production site. Platform.sh provides backup features that protect against any unforeseen consequences of your merges, keeping a historical copy of all of your code and data. Note: The --project flag is not needed if you are running the platform command from within your local repository. Create a backup Before you merge the dev feature into master, create a backup of the master environment. The backup will preserve both the code and all of its data. platform backup --project \u0026lt;project id\u0026gt; Select master as the environment you want to back up. Merge feature into production git checkout master git merge dev git push When the build process completes, verify that your changes have been merged. platform url Restore a backup If you would like to restore the code and data to the time of your backup, use the command platform backup:restore --project \u0026lt;project id\u0026gt; Back I\u0026#39;ve merged the new feature",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/developing/dev-environments/merge.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "f81e5b46acd926264d0a4e1ccfd225a8",
        "title": "Migrating an existing Drupal 7 site to Platform.sh",
        "description": "",
        "text": " Once you’ve setup the code for your site as a Platform.sh project, you will need to upload your existing database and files directories as well to complete the site. Import database With Drush (preferred) You can use drush aliases to import your existing local database into Platform. The aliases here are examples. Use the CLI’s platform drush-aliases command to find your own aliases. drush @platform._local sql-dump \u0026gt; backup_database.sql You can also sanitize your database prior to import it into Platform.sh by running: drush @platform._local sql-sanitize When you’re ready, export your local database and then import it into your remote Platform.sh environment. drush @platform._local sql-dump \u0026gt; local_database.sql drush @platform.master sql-cli \u0026lt; local_database.sql When the process completes, you can visit the URL of your development environment and test that the database has been properly imported. Without Drush Export your database in an SQL file or in a compressed file. Copy it via SSH to the remote environment on Platform into the /app/tmp folder which is writable: scp database.sql [SSH-URL]:/app/tmp Log in to the environment via SSH and import the database: ssh [SSH-URL] web@[PROJECT-ID]-master--php:~$ mysql -h database.internal main \u0026lt; tmp/database.sql Import files With Drush You can use Drush site aliases to import your existing local files. $ drush rsync @platform._local:%files @platform.master:%files You will destroy data from [SSH-URL]:././sites/default/files and replace with data from ~/Sites/platform/sites/default/files/ Do you really want to continue? (y/n): y Note: Drush will verify that you are copying and overwriting the proper “files” folders, so double-check that information before you type y to continue. This step may take some time, but when the process completes, you can visit the URL of your master environment and test that the files have properly been imported. Without Drush Go to your Drupal root on your local machine and synchronize the files folder to your remote Platform environment: $ rsync -r sites/default/files/. [SSH-URL]:public/sites/default/files/ Note: The local files path may depend of your installation. The path in URL may vary depending on what your .platform.app.yaml file specifies as the root path and files mount. Directly from server to platform.sh If the files folder is too large to fit on your computer, you can transfer them directly from server to server. If you have a firewall between the origin server and platform.sh, you can use agent-forwarding to enable a direct connection: $ ssh -A -t [USER]@[ORIGIN-SERVER] ssh -A -t [SSH-URL] $ rsync -a --delete [USER]@[ORIGIN-SERVER]:/var/www/drupal/sites/default/files/ public/sites/default/files Note: If you are using a Mac OS computer, you might experience issues where files with non-ascii characters in them don’t work after transfer because Mac OS X uses decomposed form (like “a \u0026#43; ¨ = ä”, a form known as NFD), not the usual composed form (“ä”, a form known as NFC used everywhere else). One workaround is to use the direct server-to-server transfer method mentioned above.",
        "section": "Getting Started",
        "subsections": " Import database  With Drush (preferred) Without Drush   Import files  With Drush Without Drush    ",
        "image": "",
        "url": "/frameworks/drupal7/migrating.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "3c1cc86d71e1be82729b7561be5e8dfc",
        "title": "Migrating to Platform.sh",
        "description": "",
        "text": " Moving an already-built site to Platform.sh is generally straightforward. For the most part, the only part that will vary from one framework to another is the details of the Platform.sh configuration files. See the Featured Frameworks section or our Project Templates for more project-specific documentation. Preparation First, assemble your Git repository as appropriate, on your master branch. Be sure to include the Platform.sh configuration files, as you will not be able to push the repository to Platform.sh otherwise! For some applications, such as Drupal you will need to dump configuration to files before proceeding. You will also need to provide appropriate configuration to read the credentials for your services at runtime and integrate them into your application’s configuration. The details of that integration will vary between systems. Be sure to see the appropriate project templates for our recommended configuration. Go Templates Java Templates Node.js Templates PHP Templates Python Templates In the management console, click \u0026#43; Add project to create a new Platform.sh project. When asked to select a template pick “Create a blank project”. Push your code When creating a new project, the management console will provide two commands to copy and paste similar to the following: git remote add platform nodzrdripcyh6@git.us.platform.sh:nodzrdripcyh6.git git push -u platform master The first will add a Git remote for the Platform.sh repository named platform. The name is significant as the Platform.sh CLI will look for either platform or origin to be the Platform.sh repository, and some commands may not function correctly otherwise. The second will push your repository’s master branch to the Platform.sh master branch. Note that a project must always start with a master branch, or deploys to any other environment will fail. When you push, a new environment will be created using your code and the provided configuration files. The system will flag any errors with the configuration if it can. If so, correct the error and try again. Import your database You will need to have a dump or backup of the database you wish to start from. The process is essentially the same for each type of persistent data service. See the MySQL , PostgreSQL , or MongoDB documentation as appropriate. Import your files Content files (that is, files that are not intended as part of your code base so are not in Git) can be uploaded to your mounts using the Platform.sh CLI or by using rsync. You will need to upload each directory’s files separately. Suppose for instance you have the following file mounts defined: mounts:\u0026#39;web/uploads\u0026#39;:source:localsource_path:uploads\u0026#39;private\u0026#39;:source:localsource_path:privateWhile using the CLI and rsync are the most common solutions for uploading files to mounts, you can also use SCP . Platform.sh CLI The easiest way to import files to your project mounts is by using the Platform.sh CLI mount:upload command. To upload to each of directories above, we can use the following commands. platform mount:upload --mount web/uploads --source ./uploads platform mount:upload --mount private --source ./private rsync You can also use rsync to upload each directory. The platform ssh --pipe command will return the SSH URL for the current environment as an inline string that rsync can recognize. To use a non-default environment, use the -e switch after --pipe. Note that the trailing slash on the remote path means rsync will copy just the files inside the specified directory, not the directory itself. rsync -az ./private `platform ssh --pipe`:/app/private/ rsync -az ./web/uploads `platform ssh --pipe`:/app/web/uploads Note: If you’re running rsync on MacOS, you should add --iconv=utf-8-mac,utf-8 to your rsync call. See the rsync documentation for more details on how to adjust the upload process.",
        "section": "Tutorials",
        "subsections": " Preparation Push your code Import your database Import your files  Platform.sh CLI rsync    ",
        "image": "",
        "url": "/tutorials/migrating.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "621e172e4ec322e85de660d2cc247d72",
        "title": "Onboarding process",
        "description": "",
        "text": " On-Boarding a new Dedicated client is a three phase process that begins the moment your contract is closed with your sales representative. As part of the onboarding service, you will continue to work with the Solutions Engineer who was present during the technical discovery and analysis during the pre-sales phase. During the entire on-boarding process, your Solutions Engineer is available to assist with questions and prioritize any tickets that may be submitted through the help desk. Phase Meetings Description Setup Introduction Hand-off The Solutions Engineer briefly reviews the development workflow that was discussed during the sales process. A dedicated Slack channel for your team’s onboarding process will be set up and members of your team invited. This will allow for a quicker feedback loop during the onboarding process. The Solutions Engineer will introduce you to the Platform Dedicated workflow process, provision resources, and hand them off to the client. Development Developer Workflow Consultation The customer has access to all the resources necessary to develop, migrate, and test the project on the Platform.sh infrastructure - development, staging, and production. If necessary, developers may request an additional training session (held by the Solutions Engineer) to discuss the development workflow and best practices in detail. Once the application is live on your staging environment, a staging CDN distribution will be created so that proper configuration of your CDN can begin. Go-Live Pre-launch Debrief Customer notifies their Solutions Engineer through a support ticket of the intention to go live. The Solutions Engineer may also reach out to the customer a few days before the approximate cut-over date if one was provided. The Solutions Engineer reviews the infrastructure, notes any risks, and discusses the go live process. The production configuration of your CDN will be created with the configuration mirroring that of your staging distribution, and will be configured to pull from your production environment.",
        "section": "Platform.sh Dedicated",
        "subsections": "",
        "image": "",
        "url": "/dedicated/overview/onboarding.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "009340ee0a2d47e658a82896690cc72b",
        "title": "Performance tuning",
        "description": "",
        "text": " Once your application is up and running it still needs to be kept fast. Platform.sh offers a wide degree of flexibility in how PHP behaves, but that does mean you may need to take a few steps to ensure your site is running optimally. The following recommendations are guidelines only. They’re also listed in approximately the order we recommend investigating them, although your mileage may vary. Upgrade to PHP 7.2\u0026#43; There is very little purpose to trying to optimize a PHP application on PHP 5. PHP 7 is generally twice as fast and uses half as much memory as PHP 5, making it unquestionably the first step to take when trying to make a PHP-based site run faster. To change your PHP version, simply change the type key in your .platform.app.yaml to the desired PHP version. As always, test it on a branch first before merging to master. Ensure that the router cache is properly configured Although not PHP-specific, a common source of performance issues is a misconfigured cache. The most common issue is not allowing the right cookies as part of the router cache. Some cookies, such as session cookies, need to be allowed, whereas others, such as marketing and analytics cookies, usually shouldn’t be allowed to be part of the cache key. See the router cache documentation, and the cookie entry specifically. You will also need to ensure that your application is sending the correct cache-control header. The router cache will obey whatever cache headers your application sends, so send it good ones. Static assets cache headers are set using the expires key in .platform.app.yaml. See the web.locations documentation for more details. Optimize the FPM worker count PHP-FPM reserves a fixed number of simultaneous worker processes to handle incoming requests. If more simultaneous requests are received than the number of workers then some requests will wait. The default worker count is deliberately set rather conservative but can be improved in many cases. See the PHP-FPM sizing page for how to determine and set a more optimal value. Enable preloading PHP 7.4 and later supports preloading code files into shared memory once at server startup, bypassing the need to include or autoload them later. Depending on your application doing so can result in significant improvements to both CPU and memory usage. If using PHP 7.4, see the PHP Preload instructions for how to configure it on Platform.sh and consult your application’s documentation to see if they have any recommendations for an optimal preload configuration. If you are not using PHP 7.4, this is a good reason to upgrade. Configure opcache PHP 5.5 and later include an opcache that is enabled at all times, as it should be. It may still need to be tuned, however. The opcache can be configured using php.ini values, which in this case are best set using the variables block in .platform.app.yaml. Note: If using opcache preloading on PHP 7.4 or later, configure that first and let the application run for a while before tuning the opcache itself as the preload script may change the necessary configuration here. The most important values to set are: opcache.max_accelerated_files: The max number of files that the opcache may cache at once. If this is lower than the number of files in the application it will begin thrashing and become less effective. opcache.memory_consumption: The total memory that the opcache may use. If the application is larger than this the cache will start thrashing and become less effective. To determine how many files you have, run this command from the root of your application: find . -type f -name \u0026#39;*.php\u0026#39; | wc -l That will report the number of files in your file tree that end in .php. That may not be perfectly accurate (some applications have PHP code in files that don’t end in .php, it may not catch generated files that haven’t been generated yet, etc.) but it’s a reasonable approximation. Set the opcache.max_accelerated_files option to a value slightly higher than this. Note that PHP will automatically round the value you specify up to the next highest prime number, for reasons long lost to the sands of time. Determining an optimal opcache.memory_consumption is a bit harder, unfortunately, as it requires executing code via a web request to get adequate statistics. Fortunately there is a command line tool that will handle most of that. Change to the /tmp directory (or any other non-web-accessible writable directory) and install CacheTool . It has a large number of commands and options but we’re only interested in the opcache status for FastCGI command. The really short version of downloading and using it would be: cd /tmp curl -sO http://gordalina.github.io/cachetool/downloads/cachetool.phar php cachetool.phar opcache:status --fcgi=$SOCKET The --fcgi=$SOCKET option tells the command how to connect to the PHP-FPM process on the server through the Platform.sh-defined socket. That command will output something similar to the following: \u0026#43;----------------------\u0026#43;---------------------------------\u0026#43; | Name | Value | \u0026#43;----------------------\u0026#43;---------------------------------\u0026#43; | Enabled | Yes | | Cache full | No | | Restart pending | No | | Restart in progress | No | | Memory used | 29.65 MiB | | Memory free | 34.35 MiB | | Memory wasted (%) | 0 b (0%) | \u0026#43;----------------------\u0026#43;---------------------------------\u0026#43; | Cached scripts | 1528 | | Cached keys | 2609 | | Max cached keys | 32531 | | Start time | Mon, 18 Jun 2018 18:19:32 \u0026#43;0000 | | Last restart time | Never | | Oom restarts | 0 | | Hash restarts | 0 | | Manual restarts | 0 | | Hits | 8554 | | Misses | 1594 | | Blacklist misses (%) | 0 (0%) | | Opcache hit rate | 84.29247142294 | \u0026#43;----------------------\u0026#43;---------------------------------\u0026#43; The most important values for now are the Memory used, Memory free, and Oom restarts (Out Of Memory Restarts). If the Oom restarts number is high (meaning more than a handful) it means you don’t have enough memory allocated to the opcache. In this example the opcache is using about half of the 64 MB given to it by default, which is fine. If Memory free is too low or Oom Restarts too high, set a higher value for the memory consumption. Remember to remove the cachetools.phar file once you’re done with it. Your .platform.app.yaml file will end up including a block similar to: variables:php:\u0026#39;opcache.max_accelerated_files\u0026#39;:22000\u0026#39;opcache.memory_consumption\u0026#39;:96(Memory consumption is set in megabytes.) Optimize your code It’s also possible that your own code is doing more work than it needs to. Profiling and optimizing a PHP application is a much larger topic than will fit here, but Platform.sh recommends enabling Blackfire.io on your project to determine what slow spots can be found and addressed. The web agency Pixelant has also published a log analyzer tool for Platform.sh . It works only for PHP scripts, but offers good visualizations and insights into the operation of your site that can suggest places to further optimize your configuration and provide guidance on when it’s time to increase your plan size. (Please note that this tool is maintained by a 3rd party, not by Platform.sh.)",
        "section": "PHP",
        "subsections": " Upgrade to PHP 7.2+ Ensure that the router cache is properly configured Optimize the FPM worker count Enable preloading Configure opcache Optimize your code  ",
        "image": "",
        "url": "/languages/php/tuning.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "2e1393762fdce12bafc0b2c1b2c269a5",
        "title": "Platform.sh development environments",
        "description": "",
        "text": " Architecture (Development Environments) Default limits The Development Environment for a Dedicated project provides a production and staging branch linked to the Dedicated Cluster, a master branch, and ten (10) additional active environments. This number can be increased if needed for an additional fee. The default storage for Dedicated contracts is 50GB per environment (production, staging, and each development environment) - this comprises total storage for your project and is inclusive of any databases, uploaded files, writable application logging directories, search index cores, and so on. The storage amount for your development environment will reflect the amount in your Enterprise contract. A project may have up to six (6) users associated with it at no additional charge. Additional users may be added for an additional fee. These users will have access to both the Development Environment and the Dedicated Cluster. Larger developments environments By default, all containers in development environments are “Small” sized, as they have limited traffic needs. For more resource-intensive applications this size can be increased for an additional fee.",
        "section": "Platform.sh Dedicated cluster specifications",
        "subsections": " Architecture (Development Environments) Default limits Larger developments environments  ",
        "image": "",
        "url": "/dedicated/architecture/development.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "ae98ba09f46790676990d7923db8b500",
        "title": "Pricing",
        "description": "",
        "text": " Platform.sh is the Idea-to-Cloud Application Platform—the end to end solution to develop and deploy web apps and sites. We offer a free trial period so you can test the service and see how great it is. You can see our full pricing information at: https://platform.sh/pricing/ All Platform.sh plans include: four Environments (3 for staging/development, 1 for the live site). one Developer license 5GB of Storage per environment multiple Backend services (MySQL, PostgreSQL, Elasticsearch, Redis, Solr..) support You can switch between plans (downgrade or upgrade) freely, but note that reducing storage is currently not supported for technical reasons. If you need to reduce storage, please create a support ticket. You will always be billed the prorated rate of your plan over the period it was used. You may cancel your plan at any time and you will only be billed for the actual period used. For Elite, Enterprise and Agency Plans you can pay by purchase order. For all other plans you need to add a credit card to your account. We offer a free trial period so you can test the service and see how great it is. If ever you need more time to evaluate Platform.sh, please contact our sales representatives. They can issue you an extra voucher to prolong your test. Prices below are listed in US Dollars. You will be billed in US Dollars, Euros, or British Pounds depending on where your billing address is. For a list of current prices please refer to https://platform.sh/pricing Euro Prices are presented excluding VAT. In your bill, as appropriate we will include the correct VAT rate. Extras All extra consumption is prorated to the time it was actually used. For example, if you added an extra developer for 10 days you would be billed around $3 extra at the end of the month (based on the then-current price of an extra developer seat). Extra developers Adding a developer to your project will add a monthly per project per user fee unless you have an agency or an enterprise account. Extra environments You can add extra staging/development environments to any plan by multiples of 3. For example, if you want to have 12 staging environments you would pay additional $63 per month on top of your basic plan price. Extra storage You can add additional storage at $2.50 per 5GB per staging/development environment. For example, if you have the default plan (with 3 staging environments) and you add 10GB (for a total of 15GB per environment), you would pay an extra $15 a month. If you added 3 extra environments (for a total of 6 staging environments) and you added 10GB (for a total of 15GB per environment), you would pay an extra $30 a month. Development The basic plan (Development) starts at $10 per month, and includes 4 environments: 3 staging/development and 1 future production). You can not map a custom domain name to a development plan Development environments have less resources than production environments. Production The live environment (master) of a production plan has more resources than the development environments of the project. https://platform.sh/pricing lists the resources available per plan (these are always only the production environment resources) the development environment have their own resources, and are not counted towards the limit. You can map domain names to your master environment. SSL support is always included. Multiple Applications in a single project All Platform.sh plans support multiple applications in a single cluster, but they share the global resources of the cluster. The resources of a Standard plan are not sufficient to run more than one application in the same cluster if there is also a MySQL database as a Service. Useful multi-apps start at Medium. A Medium plan, for example, can support 3 Apps with a MySQL instance and a Redis instance. If you wonder if a specific setup would fit in a plan, don’t hesitate to contact our support. Dedicated Instances For a price lower than traditional managed hosting, you get included development and staging environments, as well as triple redundancy on every element of the stack with: 99.99% Uptime Guaranteed 24/7 White Glove On-boarding and Support Please contact our sales department to discuss how we can help you. Agencies We offer three tiers for agencies with many perks. Free user licenses A free site for your own agency Up to 10% customer lifetime referral fees and 15% discounts Access to an agency speciﬁc “Small” price plan Free Medium or Large plan website for your own agency site Free Small plan for every Enterprise project sold …and more! Learn more and join today… German Cloud Pricing The prices for Germany are currently set at 10% above the EU and US plan prices. Thus, a “Production Standard” environment on the Sovereign German Cloud will be $55 instead of $50. Our estimation page (which you can reach by clicking on your account dashboard on the edit link for a project) does reflect these new options. If you have any questions don’t hesitate to contact our sales department .",
        "section": "The big picture",
        "subsections": " Extras  Extra developers Extra environments Extra storage   Development Production  Multiple Applications in a single project   Dedicated Instances Agencies German Cloud Pricing  ",
        "image": "",
        "url": "/overview/pricing.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "0c8b91d0dcd51e175732e47489963a0e",
        "title": "Redirects",
        "description": "",
        "text": " Managing redirection rules is a common requirement for web applications, especially in cases where you do not want to lose incoming links that have changed or been removed over time. You can manage redirection rules on your Platform.sh projects in two different ways, which we describe here. If neither of these options satisfy your redirection needs, you can still implement redirects directly from within your application, which if implemented with the appropriate caching headers would be almost as efficient as using the configuration options provided by Platform.sh. Whole-route redirects Using whole-route redirects, you can define very basic routes in your .platform/routes.yaml file whose sole purpose is to redirect. A typical use case for this type of route is adding or removing a www. prefix to your domain, as the following example shows: https://{default}/:type:redirectto:https://www.{default}/Partial redirects In the .platform/routes.yaml file you can also add partial redirect rules to existing routes: https://{default}/:# [...]redirects:expires:1dpaths:\u0026#39;/from\u0026#39;:to:\u0026#39;https://example.com/\u0026#39;\u0026#39;^/foo/(.*)/bar\u0026#39;:to:\u0026#39;https://example.com/$1\u0026#39;regexp:trueThis format is more rich and works with any type of route, including routes served directly by the application. Two keys are available under redirects: expires: optional, the duration the redirect will be cached. Examples of valid values include 3600s, 1d, 2w, 3m. paths: the paths to apply redirections to. Each rule under paths is defined by its key describing the expression to match against the request path and a value object describing both the destination to redirect to with detail on how to handle the redirection. The value object is defined with the following keys: to: required, a relative URL - \u0026#39;/destination\u0026#39;, or absolute URL - \u0026#39;https://example.com/\u0026#39;. regexp: optional, defaults to false. Specifies whether the path key should be interpreted as a PCRE regular expression. In the following example, a request to https://example.com/foo/a/b/c/bar would redirect to https://example.com/a/b/c: https://{default}/:type:upstreamredirects:paths:\u0026#39;^/foo/(.*)/bar\u0026#39;:to:\u0026#39;https://example.com/$1\u0026#39;regexp:trueNote that special arguments in the to statement are also valid when regexp is set to true: $is_args will evaluate to ? or empty string $args will evaluate to the full query string if any $arg_foo will evaluate to the value of the query parameter foo $uri will evaluate to the full URI of the request. prefix: optional, specifies whether we should redirect both the path and all its children or just the path itself. Defaults to true, but not supported if regexp is true. For example, https://{default}/:type:upstreamredirects:paths:\u0026#39;/from\u0026#39;:to:\u0026#39;https://{default}/to\u0026#39;prefix:truewith prefix set to true, /from will redirect to /to and /from/another/path will redirect to /to/another/path. If prefix is set to false then /from will trigger a redirect, but /from/another/path will not. append_suffix: optional, determines if the suffix is carried over with the redirect. Defaults to true, but not supported if regexp is true or if prefix is false. If we redirect with append_suffix set to false, for example, then the following https://{default}/:type:upstreamredirects:paths:\u0026#39;/from\u0026#39;:to:\u0026#39;https://{default}/to\u0026#39;append_suffix:falsewould result in /from/path/suffix redirecting to just /to. If append_suffix was left on its default value of true, then /from/path/suffix would have redirected to /to/path/suffix. code: optional, HTTP status code. Valid status codes are 301, 302, 307, and 308. Defaults to 302. https://{default}/:type:upstreamredirects:paths:\u0026#39;/from\u0026#39;:to:\u0026#39;https://example.com/\u0026#39;code:308\u0026#39;/here\u0026#39;:to:\u0026#39;https://example.com/there\u0026#39;In this example, redirects from /from would use a 308 HTTP status code, but redirects from /here would default to 302. expires: optional, the duration the redirect will be cached for. Defaults to the expires value defined directly under the redirects key, but at this level we can fine-tune the expiration of individual partial redirects: https://{default}/:type:upstreamredirects:expires:1dpaths:\u0026#39;/from\u0026#39;:to:\u0026#39;https://example.com/\u0026#39;\u0026#39;/here\u0026#39;:to:\u0026#39;https://example.com/there\u0026#39;expires:2wIn this example, redirects from /from would be set to expire in one day, but redirects from /here would expire in two weeks. Application-driven redirects If neither of the above options satisfy your redirection needs, you can still implement redirects directly in your application. If sent with the appropriate caching headers, this is nearly as efficient as implementing the redirect through one of the two configurations described above. Implementing application-driven redirects depends on your own code or framework and is beyond the scope of this documentation.",
        "section": "Configure routes",
        "subsections": " Whole-route redirects Partial redirects Application-driven redirects  ",
        "image": "",
        "url": "/configuration/routes/redirects.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "08c931477e4c4a3a1814ad94169096ba",
        "title": "Using Redis with Drupal 8.x",
        "description": "",
        "text": " If you are using the Platform.sh-provided Drupal 8 template, most of this work is already done for you. All you need to do is uncomment a the Redis relationship in .platform.app.yaml after your site is installed and Redis-based caching should “just work”. If you are working from an older repository or migrating a pre-built site to Platform.sh, see the instructions below. Requirements Add a Redis service First you need to create a Redis service. In your .platform/services.yaml file, add or uncomment the following: rediscache:type:redis:5.0That will create a service named rediscache, of type redis, specifically version 5.0. Expose the Redis service to your application In your .platform.app.yaml file, we now need to open a connection to the new Redis service. Under the relationships section, add the following: relationships:redis: rediscache:redis The key (left side) is the name that will be exposed to the application in the PLATFORM_RELATIONSHIPS variable . The right hand side is the name of the service we specified above (rediscache) and the endpoint (redis). If you named the service something different above, change rediscache to that. Add the Redis PHP extension Because the Redis extension for PHP has been known to have BC breaks at times, we do not bundle a specific verison by default. Instead, we provide a script to allow you to build your desired version in the build hook. See the PHP-Redis page for a simple-to-install script and instructions. Add the Drupal module You will need to add the Redis module to your project. If you are using Composer to manage your Drupal 8 site (which we recommend), simply run: composer require drupal/redis Then commit the resulting changes to your composer.json and composer.lock files. Note that the Redis module does not need to be enabled in Drupal except for diagnostic purposes. The configuration below is sufficient to leverage its functionality. Configuration To make use of the Redis cache you will need to set some Drupal variables. The configuration is a bit more complex than can be easily represented in Platform.sh’s environment variables configuration, so using settings.php directly is the recommended approach. Place the following at the end of settings.platformsh.php. Note the inline comments, as you may wish to customize it further. Also review the README.txt file that comes with the redis module, as it has a great deal more information on possible configuration options. For instance, you may wish to not use Redis for the persistent lock if you have a custom module that needs locks to persist for more than a few seconds. The example below is intended as a “most common case”. (Note: This example assumes Drupal 8.2 or later.) Note: If you do not already have the Platform.sh Config Reader library installed and referenced at the top of the file, you will need to install it with composer require platformsh/config-reader and then add the following code before the block below: \u0026lt;?php $platformsh = new if (!$platformsh-\u0026gt;inRuntime()) { return; } \u0026lt;?php // Set redis configuration. if ($platformsh-\u0026gt;hasRelationship(\u0026#39;redis\u0026#39;) \u0026amp;\u0026amp; !drupal_installation_attempted() \u0026amp;\u0026amp; extension_loaded(\u0026#39;redis\u0026#39;)) { $redis = $platformsh-\u0026gt;credentials(\u0026#39;redis\u0026#39;); // Set Redis as the default backend for any cache bin not otherwise specified. $settings[\u0026#39;cache\u0026#39;][\u0026#39;default\u0026#39;] = \u0026#39;cache.backend.redis\u0026#39;; $settings[\u0026#39;redis.connection\u0026#39;][\u0026#39;host\u0026#39;] = $redis[\u0026#39;host\u0026#39;]; $settings[\u0026#39;redis.connection\u0026#39;][\u0026#39;port\u0026#39;] = $redis[\u0026#39;port\u0026#39;]; // Apply changes to the container configuration to better leverage Redis. // This includes using Redis for the lock and flood control systems, as well // as the cache tag checksum. Alternatively, copy the contents of that file // to your project-specific services.yml file, modify as appropriate, and // remove this line. $settings[\u0026#39;container_yamls\u0026#39;][] = \u0026#39;modules/contrib/redis/example.services.yml\u0026#39;; // Allow the services to work before the Redis module itself is enabled. $settings[\u0026#39;container_yamls\u0026#39;][] = \u0026#39;modules/contrib/redis/redis.services.yml\u0026#39;; // Manually add the classloader path, this is required for the container cache bin definition below // and allows to use it without the redis module being enabled. \u0026#39;modules/contrib/redis/src\u0026#39;); // Use redis for container cache. // The container cache is used to load the container definition itself, and // thus any configuration stored in the container itself is not available // yet. These lines force the container cache to use Redis rather than the // default SQL cache. $settings[\u0026#39;bootstrap_container_definition\u0026#39;] = [ \u0026#39;parameters\u0026#39; =\u0026gt; [], \u0026#39;services\u0026#39; =\u0026gt; [ \u0026#39;redis.factory\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; ], \u0026#39;cache.backend.redis\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;arguments\u0026#39; =\u0026gt; [\u0026#39;@redis.factory\u0026#39;, \u0026#39;@cache_tags_provider.container\u0026#39;, \u0026#39;@serialization.phpserialize\u0026#39;], ], \u0026#39;cache.container\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;factory\u0026#39; =\u0026gt; [\u0026#39;@cache.backend.redis\u0026#39;, \u0026#39;get\u0026#39;], \u0026#39;arguments\u0026#39; =\u0026gt; [\u0026#39;container\u0026#39;], ], \u0026#39;cache_tags_provider.container\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;arguments\u0026#39; =\u0026gt; [\u0026#39;@redis.factory\u0026#39;], ], \u0026#39;serialization.phpserialize\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; ], ], ]; } The example.services.yml file noted above will also use Redis for the lock and flood control systems. The redis module is able to use Redis as a queue backend, however, that should not be done on an ephemeral Redis instance as that could result in lost items when the Redis service instance is restarted or fills up. If you wish to use Redis for the queue we recommend using a separate persistent Redis instance. See the Redis documentation page for more information. Verifying Redis is running Run this command in a SSH session in your environment redis-cli -h redis.internal info. You should run it before you push all this new code to your repository. This should give you a baseline of activity on your Redis installation. There should be very little memory allocated to the Redis cache. After you push this code, you should run the command and notice that allocated memory will start jumping. Clear SQL cache tables Once you’ve confirmed that your site is using Redis for caching, you can and should purge any remaining cache data in the MySQL database as it is now just taking up space. TRUNCATE any table that begins with cache except for cache_form. Despite its name cache_form is not part of the cache system proper and thus should not be moved out of SQL.",
        "section": "Getting Started",
        "subsections": " Requirements  Add a Redis service Expose the Redis service to your application Add the Redis PHP extension Add the Drupal module   Configuration  Verifying Redis is running Clear SQL cache tables    ",
        "image": "",
        "url": "/frameworks/drupal8/redis.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "02c57db0ac1a6df1b24d9946022cdcd9",
        "title": "Backups",
        "description": "",
        "text": " Platform.sh takes a byte-for-byte snapshot of Dedicated production environments every six (6) hours. They are retained on a sliding scale, so more recent time frames have more frequent backups. Time frame Backup retention Days 1-3 Every backup Days 4-6 One backup per day Weeks 2-6 One backup per week Weeks 8-12 One bi-weekly backup Weeks 12-22 One backup per month Platform.sh Dedicated creates the backup using snapshots to encrypted elastic block storage (EBS) volumes. An EBS snapshot is immediate, but the time it takes to write to the simple storage service (S3) depends on the volume of changes. Recovery Point Objective (RPO) is 6 hours (maximum time to last backup). Recovery Time Objective (RTO) depends on the size of the storage. Large EBS volumes take more time to restore. These backups are only used in cases of catastrophic failure and can only be restored by Platform.sh. A ticket must be opened by the customer to request a restoration. The restoration process may take a few hours, depending on the infrastructure provider in use. In the ticket, specify if you want backups of files, MySQL, or both. Uploaded files will be placed in an SSH-accessible directory on the Dedicated Cluster. MySQL will be provided as a MySQL dump file on the server. You may restore these to your site at your leisure. (We will not proactively overwrite your production site with a backup; you are responsible for determining a “safe” time to restore the backup, or for selectively restoring individual files if desired.) Customers are welcome to make their own backups using standard tools (mysqldump, rsync, etc.) at their own leisure.",
        "section": "Platform.sh Dedicated",
        "subsections": "",
        "image": "",
        "url": "/dedicated/overview/backups.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "689d8aa9f60b16a6ddb7716c5f61e120",
        "title": "Configure your application",
        "description": "",
        "text": " You control your application and the way it will be built and deployed on Platform.sh via a single configuration file, .platform.app.yaml, located at the root of your application folder inside your Git repository. The .platform.app.yaml file is extremely flexible. Depending on your needs it could be less than 10 lines long or over 100. The only required keys are name, type, disk, and at least one “instance definition”, either a web or worker block. All others are optional. Your application code can generate one or more application instances. Web instances can be accessed from the outside world, while workers cannot and just run a persistent background process. Otherwise they are very similar. Different configuration properties can be applied to individual web and worker instances, or globally to all of them. In the most typical case, with one web instance and no workers, it’s common to just list each of the configuration directives below as a top-level property. However, they can also be specified within the web or worker blocks to apply to just those instances. The following properties apply only at the global level, and cannot be replicated inside an instance definition. name (required) - Sets the unique name of the application container. type (required) - Sets the container base image to use, including application language. timezone - Sets the timezone of cron tasks in application container. build, dependencies, and hooks - Control how the application gets compiled. Note that this compilation happens before the application is copied into different instances, so any steps here will apply to all web and worker instances. cron - Defines scheduled tasks for the application. Cron tasks will, technically, run as part of the web instance regardless of how many workers are defined. source.root - This nested value specifies the path where all code for the application lives. It defaults to the directory where the .platform.app.yaml file is defined. It is rarely needed except in advanced configurations. The following properties can be set at the top level of the .platform.app.yaml file and apply to all application instances, or set within a given instance definition and apply just to that one. If set in both places then the instance-specific one will take precedence, and completely replace the global one. That is, if you want to make a change to just one sub-property of one of the following keys you need to replicate the entire block. size - Sets an explicit sizing hint for the application. relationships - Defines connections to other services and applications. access - Restricts SSH access with more granularity than the management console. disk and mounts (required) - Defines writable file directories for the application. variables - Sets environment variables that control application behavior. firewall - Defines outbound firewall rules for the application. The .platform.app.yaml file needs at least one of the following to define an instance, but may define both. web - Controls how the web application is served. worker - Defines alternate copies of the application to run as background processes. Available resources Each web or worker instance is its own running container, which takes its own resources. The size key allows some control over how many resources each container gets and if omitted the system will select one of a few fixed sizes for each container automatically. All application and service containers are given resources out of a common pool defined by your plan size. That means the more containers you define, the fewer resources each one will get and you may need to increase your plan size. Compression Platform.sh does not compress any dynamic responses generated by your application due to a well known security issue . While your application can compress its own response, doing so when the response includes any user-specific information, including a session cookie, opens up an attack vector over SSL/TLS connections. For that reason we recommend against compressing any generated responses. Requests for static files that are served directly by Platform.sh are compressed automatically using either gzip or brotli compression if: The request headers for the file support gzip or brotli. The file is served directly from disk by Platform.sh, not passed through your application. The file would be served with a cache expiration time in the future. The file type is one of: html, javascript, json, pdf, postscript, svg, css, csv, plain text, or XML. Additionally, if a file with a “.gz” or “.br” extension exists that will be served instead for the appropriate compression type regardless of the file type. That is, a request for styles.css that accepts a gzipped file (according to the request headers) will automatically return the contents of styles.css.gz if it exists. This approach supports any file type and offers some CPU optimization, especially if the cache lifetime is short. Example configuration An example of a minimalist .platform.app.yaml file for PHP, heavily commented, is below: # .platform.app.yaml# The name of this application, which must be unique within a project.name:'app'# The type key specifies the language and version for your application.type:'php:7.0'# On PHP, there are multiple build flavors available. Pretty much everyone# except Drupal 7 users will want the composer flavor.build:flavor:composer# The relationships of the application with services or other applications.# The left-hand side is the name of the relationship as it will be exposed# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand# side is in the form `\u003cservice name\u003e:\u003cendpoint name\u003e`.relationships:database:'mysqldb:mysql'# The hooks that will be triggered when the package is deployed.hooks:# Build hooks can modify the application files on disk but not access any services like databases.build:| rm web/app_dev.php# Deploy hooks can access services but the file system is now read-only.deploy:| app/console --env=prod cache:clear# The size of the persistent disk of the application (in MB).disk:2048# The 'mounts' describe writable, persistent filesystem mounts in the application.# The keys are directory paths relative to the application root. The values are a# mount definition. In this case, `web-files` is just a unique name for the mount.mounts:'web/files':source:localsource_path:'web-files'# The configuration of the application when it is exposed to the web.web:locations:'/':# The public directory of the application relative to its root.root:'web'# The front-controller script which determines where to send# non-static requests.passthru:'/app.php'# Allow uploaded files to be served, but do not run scripts.# Missing files get mapped to the front controller above.'/files':root:'web/files'scripts:falseallow:truepassthru:'/app.php' Note: This configuration file is specific to one application. If you have multiple applications inside your Git repository (such as a RESTful web service and a front-end, or a main web site and a blog), you need .platform.app.yaml at the root of each application. See the Multi-app documentation. Upgrading from previous versions of the configuration file. Although we make an effort to always maintain backward compatibility in the .platform.app.yaml format, we do from time to time upgrade the file and encourage you to upgrade as well.",
        "section": "Configuration",
        "subsections": " Available resources Compression Example configuration Upgrading from previous versions of the configuration file.  ",
        "image": "",
        "url": "/configuration/app.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "24a0f103d7c4174594f3d60822bddb4f",
        "title": "Create a new project",
        "description": "",
        "text": " With the Platform.sh CLI now installed and configured to communicate with your account, you can create a new project from the command line and connect it to your application. Create an empty project Type the command platform create in your terminal. The CLI will then ask you to set up some initial project configurations: Project title: You need a unique name for each project, so title this one My CLI Project. Region: In general you will choose the region that is closest to where most of your site’s traffic is coming from. Here, go ahead and begin typing us-2.platform.sh and the CLI will auto-complete the rest for you. Plan: Select the development plan for your trial project. Environments: The master branch will become the Master environment, the live production environment for your application. Additionally, other branches may be activated as fully running environments for developing new features. More on that later . This value selects the maximum number of development environments the project will allow. You can change this value later at any time. For now, press Enter to select the default number of environments. Storage: You can modify the amount of storage your application can use from the CLI and from the management console, as well as upgrade that storage later once your project starts growing. For now, press Enter to select the default amount of storage. When the CLI has finished creating a project, it will output your project ID. This is the primary identifier for making changes to your projects, and you will need to use it to set Platform.sh as the remote for your repository in the next step. You can also retrieve the project ID with the command platform project:list, which lists all of your projects and their IDs in a table. Set Platform.sh as remote for your application Next you will need to connect to the remote project in order to push your code to Platform.sh. If you have not already initialized your project directory as a Git repository, you will first need to do so git init Then you can set Platform.sh as a remote with the command platform project:set-remote \u0026lt;project ID\u0026gt; That’s it! You have now created an empty project and connected your repository to that project using the CLI. Move on now to the next step to start configuring your repository to deploy on Platform.sh. Back I\u0026#39;ve created a project",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/create-project.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "48d3eea65894ade75808c6e3159467c7",
        "title": "Cron timezone",
        "description": "",
        "text": " All Platform.sh containers default to running in UTC time. Applications and application runtimes may elect to use a different timezone but the container itself runs in UTC. That includes the spec parameter for cron tasks that are defined by the application. That is generally fine but sometimes it’s necessary to run cron tasks in a different timezone. Setting the system timezone for cron tasks The timezone property sets the timezone for which the spec property of any cron tasks defined by the application will be interpreted. Its value is one of the tz database region codes such as Europe/Paris or America/New_York. This key will apply to all cron tasks defined in that file. This entry is only meaningful on cron specs that specify a particular time of day, rather than a “time past each hour”. For example, 25 1 * * * would run every day at 1:25 am in the timezone specified. Setting an application runtime timezone The application runtime timezone can also be set, although the mechanism varies a bit by the runtime. PHP runtime - You can change the timezone by providing a custom php.ini . Node.js runtime - You can change the timezone by starting the server with env TZ=\u0026#39;\u0026lt;timezone\u0026gt;\u0026#39; node server.js. Python runtime - You can change the timezone by starting the server with env TZ=\u0026#39;\u0026lt;timezone\u0026gt;\u0026#39; python server.py. Java runtime - You can change the timezone by starting the server with env TZ=\u0026#39;\u0026lt;timezone\u0026gt;\u0026#39; java -jar .... An alternative to setting an environment variable is setting the JVM argument user.timezone. This JVM argument takes precedence over the environment variable TZ. For example, you can use the flag -D when running the application: java -jar -Duser.timezone=GMT or java -jar -Duser.timezone= Asia/Kolkata  Setting the application timezone will only affect the application itself, not system operations such as log files. Note: In the vast majority of cases it’s best to leave all timezones in UTC and store user data with an associated timezone instead.",
        "section": "Configure your application",
        "subsections": " Setting the system timezone for cron tasks Setting an application runtime timezone  ",
        "image": "",
        "url": "/configuration/app/timezone.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "4d05bae40d9b853cc53632f9ea3a06b4",
        "title": "Customize settings.php",
        "description": "",
        "text": " For applications using the drupal build flavor (those based on our Drupal 7 example ), Platform.sh automatically generates a settings.php file if not present and will always generate a settings.local.php file. This allows the Drupal site to be connected to MySQL without any additional configuration. If you wish to customize either file, we recommend instead using the example files provided in our Drupal 7 project template. There are two: settings.php and settings.platformsh.php . The former will automatically include the latter, and all Platform.sh-specific configuration is found in the settings.platformsh.php file. It will also automatically include a settings.local.php file if found so it will not conflict with your local development workflow. Note: You should never commit a settings.local.php file to your repository. If you need to add additional configuration that is specific to Platform.sh, such as connecting to additional services like Redis or Solr , those changes should go in the settings.platformsh.php file.",
        "section": "Getting Started",
        "subsections": "",
        "image": "",
        "url": "/frameworks/drupal7/customizing-settings-php.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "69f9345e04cb90015861b6a12ec23a63",
        "title": "Data retention",
        "description": "",
        "text": " Platform.sh logs and stores all sorts of data as a normal part of its business. This information is retained as needed for business purposes and old data is purged. The retention time varies depending on the type of data stored. Account information Information relating to customer accounts (login information, billing information, etc.) is retained for as long as the account is active with Platform.sh. Customers may request that their account be deleted and all related data be purged by filing an issue ticket. System logs System level access and security logs are maintained by Platform.sh for diagnostic purposes. These logs are not customer-accessible. These logs are retained for at least 6 months and at most 1 year. General system level logs are retained for at least 30 days and at most 1 year. Payment processing logs Logs related to payment processing are retained for at least 3 months and at most 1 year. This is consistent with PCI recommendations. Application logs Application logs on each customer environment are retained with the environment. Individual log files are truncated at 100 MB, regardless of their age. See the accessing logs page for instructions on how to access them. When an environment is deleted its application logs are deleted as well. Grid Backups Application backups running on the Grid (e.g. If you subscribe to a Platform.sh Professional plan) are retained for at least 7 days. They will be purged between 7 days and 6 months, at Platform.sh’s discretion. Dedicated backups Backups for applications running on a Dedicated instance will follow the schedule documented on our dedicated backups page. Tombstone backups When a project is deleted Platform.sh takes a final backup of active environments as well as the Git repository holding user code. This final backup is to allow Platform.sh to recover a recently-deleted project in case of accident. These “tombstone” backups are retained for between 7 days and 6 months. Analytics Platform.sh uses Google Analytics on various web pages, and therefore Google Analytics will store collected data for a period of time. We have configured our Google Analytics account to store data for 14 months from the time you last accessed our site, which is the minimum Google allows.",
        "section": "Security and compliance",
        "subsections": " Account information System logs Payment processing logs Application logs Grid Backups Dedicated backups Tombstone backups Analytics  ",
        "image": "",
        "url": "/security/data-retention.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "1807729f93a685bf2af4ae9b6011922b",
        "title": "Docksal",
        "description": "",
        "text": " Using Docksal for local development Docksal is a docker based local development tool that plays nicely with Platform.sh. It is maintained by a community of developers and is a viable option for most Platform.sh projects. See the Docksal documentation for installing and setting up Docksal on your system. Docksal will ask you to create a .docksal directory in your application root, which functions similarly to the .platform.app.yaml file. It is safe to check this directory into your Git repository as Platform.sh will simply ignore it. Docksal Fin (fin) is a command line tool for controlling Docksal and used for interacting with a Docksal project. For more information on the use of the fin command, type the help subcommand to get all available commands and options. fin help Using Platform.sh CLI within Docksal The SECRET_PLATFORMSH_CLI_TOKEN must be set to use the Platform.sh CLI within your Docksal project. This is an API Token found with your Platform.sh account and can be generated by going to the API Tokens page and clicking the Create API Token link. This will allow for you to interact with your Platform.sh account from within the CLI container. fin config set --global SECRET_PLATFORMSH_CLI_TOKEN=XXX Pulling a Platform.sh project The Docksal CLI ships with the Platform.sh CLI tool. To use the tool and pull a project locally, make sure you have uploaded your SSH key to your Platform.sh account. Once that is done and a SECRET_PLATFORMSH_CLI_TOKEN has been added using the above step, you can set up your project with the following instructions. Note: Replace PROJECT_ID with your project’s ID, which can found within the Platform.sh dashboard. Replace PROJECT_DIRECTORY with the name of the local directory you’d like the project cloned into. If you do not already have Platform.sh CLI installed locally, you can use the one in the CLI image. The advantage of this would mean that the tool would never have to be installed locally and therefore is one less dependency. fin run-cli \u0026#39;platform get PROJECT_ID -e master PROJECT_DIRECTORY\u0026#39; If you already have Platform.sh CLI installed locally, you can use that instead. platform get PROJECT_ID -e master PROJECT_DIRECTORY Initializing a Platform.sh project To start a new Docksal project, initialize the configuration with the fin config generate command and specify the docroot flag. fin config generate --docroot=web fin project start The web directory is one of the many different items that can be set for the document. If this is different or changes over time running the following will fix this. fin config set docroot=XXX # Replacing XXX with the new document root. Customizing a Platform.sh project By default, Docksal comes configured with a PHP 7.1 container, an Apache 2.4 web container, and a MySQL 5.6 database container. Additional versions are available in the images and you can set the desired versions by setting following variables within your .docksal/docksal.env file. # Apache Versions 2.2 / 2.4 #WEB_IMAGE=\u0026#39;docksal/web:2.1-apache-2.2\u0026#39; WEB_IMAGE=\u0026#39;docksal/web:2.1-apache-2.4\u0026#39; # MySQL Version: 5.6 / 5.7 / 8.0 #DB_IMAGE=\u0026#39;docksal/db:1.2-mysql-5.6\u0026#39; DB_IMAGE=\u0026#39;docksal/db:1.2-mysql-5.7\u0026#39; #DB_IMAGE=\u0026#39;docksal/db:1.2-mysql-8.0\u0026#39; # PHP Versions Available 5.6 / 7.0 / 7.1 / 7.2 #CLI_IMAGE=\u0026#39;docksal/cli:2.5-php5.6\u0026#39; #CLI_IMAGE=\u0026#39;docksal/cli:2.5-php7.0\u0026#39; CLI_IMAGE=\u0026#39;docksal/cli:2.5-php7.1\u0026#39; #CLI_IMAGE=\u0026#39;docksal/cli:2.5-php7.2\u0026#39; You can further create and customize a .docksal/docksal.yml file within your project. This is a docker-compose file and can be customized as needed for your application, as some customizations are specific to certain applications. See Docksal documentation on extending stock images . Downloading MySQL data from Platform.sh into Docksal In most cases, downloading data from Platform.sh and loading it into your project is straightforward. The following commands, run from your application root, will download a compressed database backup and load it into the local Docksal database container. fin platform db:dump --gzip -f /tmp/database.sql.gz fin exec \u0026#39;zcat \u0026lt; /tmp/database.sql.gz | mysql -u user -puser -h db default\u0026#39; Connecting Projects to the Database After importing your database into the project the next step is connecting to the database server. The following information can be used for setting up a connection for your application. Key Value DB Name default Username user Password user Host db Port 3306 See the exporting tutorial for information on how to use rsync.",
        "section": "Set up your local development environment",
        "subsections": " Using Platform.sh CLI within Docksal Pulling a Platform.sh project Initializing a Platform.sh project Customizing a Platform.sh project Downloading MySQL data from Platform.sh into Docksal  Connecting Projects to the Database    ",
        "image": "",
        "url": "/development/local/docksal.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "fd096e5b720aedc538db711f4e5f85da",
        "title": "Getting help",
        "description": "",
        "text": " If you’re facing any issue with Platform.sh, you can submit a support ticket from the Platform.sh management console. You will be redirected to a list of your support tickets, and you can click to open a ‘New ticket’ at the top of the page. File your issues in the fields for the new ticket and Submit. You can also open a support ticket directly from your user account . You are more than welcome to hop-on to our public Slack chat channel . And you are always invited to drop us a line at our contact form .",
        "section": "The big picture",
        "subsections": "",
        "image": "",
        "url": "/overview/getting-help.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "179af012a85ecc13da4e7e9137b2e002",
        "title": "Going Live - Troubleshooting",
        "description": "",
        "text": " If all steps above have been followed and the site still does not resolve (after waiting for the DNS update to propagate), here are a few simple self-help steps to take before contacting support. Verify DNS On the command line with OS X or Linux (or using the Linux subsystem for Windows) type host www.example.com: The response should be something like: www.example.com is an alias for master-t2xxqeifuhpzg.eu.platform.sh. master-t2xxqeifuhpzg.eu.platform.sh has address 54.76.136.188 If it is not either you have not configured correctly your DNS server, or the DNS configuration did not propagate yet. As a first step you can try and remove your local DNS cache. You can also try to set your DNS server to the Google public DNS server (8.8.8.8/8.8.4.4) to see if the issue is with the DNS server you are using. Try to run ping www.example.com (with you own domain name) if the result is different from what you got form the host www.example.com you might want to verify your /etc/hosts file (or its windows equivalent), you might have left there an entry from testing. Verify SSL On the command line with OS X or Linux (or using the Linux subsystem for Windows) type curl -I -v https://example.com (again using your own domain): The response should be long. Look for error messages. They are usually explicit enough. Often the problem will be with a mismatch between the certificate and the domain name. Verify your application On the command line type platform logs app and see there are no clear anomalies there. Do the same with platform logs error Something still wrong ? Contact support We are here to help. Please include as much detail as possible (we will be able to provide quicker help).",
        "section": "Going live",
        "subsections": " Verify DNS Verify SSL Verify your application Something still wrong ? Contact support  ",
        "image": "",
        "url": "/golive/troubleshoot.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "00e511700dd45ace2bc6e6e2bf0794b7",
        "title": "Hibernate",
        "description": "",
        "text": " Hibernate ORM is an object-relational mapping tool for the Java programming language. It provides a framework for mapping an object-oriented domain model to a relational database. Hibernate handles object-relational impedance mismatch problems by replacing direct, persistent database accesses with high-level object handling functions. Services The configuration reader library for Java is used in these examples, so be sure to check out the documentation for installation instructions and the latest version. MySQL MySQL is an open-source relational database technology. Define the driver for MySQL , and the Java dependencies. Then determine the SessionFactory client programmatically: import org.hibernate.Session; import org.hibernate.SessionFactory; import org.hibernate.Transaction; import org.hibernate.cfg.Configuration; import sh.platform.config.Config; import sh.platform.config.Hibernate; public class HibernateApp { public static void main(String[] args) { Config config = new Config(); Configuration configuration = new Configuration(); configuration.addAnnotatedClass(Address.class); final Hibernate credential = config.getCredential( database , Hibernate::new); final SessionFactory sessionFactory = credential.getMySQL(configuration); try (Session session = sessionFactory.openSession()) { Transaction transaction = session.beginTransaction(); //... transaction.commit(); } } } Note: You can use the same MySQL driver for MariaDB as well if you wish to do so. MariaDB MariaDB is an open-source relational database technology. Define the driver for MariaDB , and the Java dependencies. Then determine the SessionFactory client programmatically: import org.hibernate.Session; import org.hibernate.SessionFactory; import org.hibernate.Transaction; import org.hibernate.cfg.Configuration; import sh.platform.config.Config; import sh.platform.config.Hibernate; public class HibernateApp { public static void main(String[] args) { Config config = new Config(); Configuration configuration = new Configuration(); configuration.addAnnotatedClass(Address.class); final Hibernate credential = config.getCredential( database , Hibernate::new); final SessionFactory sessionFactory = credential.getMariaDB(configuration); try (Session session = sessionFactory.openSession()) { Transaction transaction = session.beginTransaction(); //... transaction.commit(); } } } PostgreSQL PostgreSQL is an open-source relational database technology. Define the driver for PostgreSQL , and the Java dependencies. Then determine the SessionFactory client programmatically: import org.hibernate.Session; import org.hibernate.SessionFactory; import org.hibernate.Transaction; import org.hibernate.cfg.Configuration; import sh.platform.config.Config; import sh.platform.config.Hibernate; public class HibernateApp { public static void main(String[] args) { Config config = new Config(); Configuration configuration = new Configuration(); configuration.addAnnotatedClass(Address.class); final Hibernate credential = config.getCredential( database , Hibernate::new); final SessionFactory sessionFactory = credential.getPostgreSQL(configuration); try (Session session = sessionFactory.openSession()) { Transaction transaction = session.beginTransaction(); //... transaction.commit(); } } }",
        "section": "Featured frameworks",
        "subsections": " Services  MySQL MariaDB PostgreSQL    ",
        "image": "",
        "url": "/frameworks/hibernate.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "837460cf59fb1618fa1dfb808c2df277",
        "title": "Kafka (Message queue service)",
        "description": "",
        "text": " Apache Kafka is an open-source stream-processing software platform. It is a framework for storing, reading and analyzing streaming data. See the Kafka documentation for more information. Supported versions Grid Dedicated 2.1 2.2 2.3 2.4 None available Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : {  cluster :  rjify4yjcwxaa-master-7rqtwti ,  host :  kafka.internal ,  hostname :  wsxz7kjwfkb3j6eh2sdzfubrry.kafka.service._.eu-3.platformsh.site ,  ip :  169.254.252.225 ,  port : 9092,  rel :  kafka ,  scheme :  kafka ,  service :  kafka ,  type :  kafka:2.2  } Usage example In your .platform/services.yaml: queuekafka:type:kafka:2.4disk:512 In your .platform.app.yaml: relationships:kafkaqueue: queuekafka:kafka  Note: You will need to use the kafka type when defining the service # .platform/services.yamlservice_name:type:kafka:versiondisk:256 and the endpoint kafka when defining the relationship # .platform.app.yamlrelationships:relationship_name:“service_name:kafka” Your service_name and relationship_name are defined by you, but we recommend making them distinct from each other. You can then use the service in a configuration file of your application with something like: Java Python Ruby package sh.platform.languages.sample; import org.apache.kafka.clients.consumer.Consumer; import org.apache.kafka.clients.consumer.ConsumerConfig; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.producer.Producer; import org.apache.kafka.clients.producer.ProducerConfig; import org.apache.kafka.clients.producer.ProducerRecord; import org.apache.kafka.clients.producer.RecordMetadata; import sh.platform.config.Config; import sh.platform.config.Kafka; import java.time.Duration; import java.util.HashMap; import java.util.Map; import java.util.function.Supplier; public class KafkaSample implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); try { // Get the credentials to connect to the Kafka service. final Kafka kafka = config.getCredential( kafka , Kafka::new); Map\u0026lt;String, Object\u0026gt; configProducer = new HashMap\u0026lt;\u0026gt;(); configProducer.putIfAbsent(ProducerConfig.CLIENT_ID_CONFIG,  animals ); final Producer\u0026lt;Long, String\u0026gt; producer = kafka.getProducer(configProducer); // Sending data into the stream. RecordMetadata metadata = producer.send(new ProducerRecord\u0026lt;\u0026gt;( animals ,  lion )).get(); logger.append( Record sent with to partition  ).append(metadata.partition()) .append(  with offset metadata = producer.send(new ProducerRecord\u0026lt;\u0026gt;( animals ,  dog )).get(); logger.append( Record sent with to partition  ).append(metadata.partition()) .append(  with offset metadata = producer.send(new ProducerRecord\u0026lt;\u0026gt;( animals ,  cat )).get(); logger.append( Record sent with to partition  ).append(metadata.partition()) .append(  with offset // Consumer, read data from the stream. final HashMap\u0026lt;String, Object\u0026gt; configConsumer = new HashMap\u0026lt;\u0026gt;(); configConsumer.put(ConsumerConfig.GROUP_ID_CONFIG,  consumerGroup1 ); configConsumer.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,  earliest ); Consumer\u0026lt;Long, String\u0026gt; consumer = kafka.getConsumer(configConsumer,  animals ); ConsumerRecords\u0026lt;Long, String\u0026gt; consumerRecords = consumer.poll(Duration.ofSeconds(3)); // Print each record. consumerRecords.forEach(record -\u0026gt; { logger.append( Record: Key   \u0026#43; record.key()); logger.append(  value   \u0026#43; record.value()); logger.append(  partition   \u0026#43; record.partition()); logger.append(  offset   \u0026#43; }); // Commits the offset of record to broker. consumer.commitSync(); return logger.toString(); } catch (Exception exp) { throw new RuntimeException( An error when execute Kafka , exp); } } } from json import dumps from json import loads from kafka import KafkaConsumer, KafkaProducer from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the Kafka service. credentials = config.credentials(\u0026#39;kafka\u0026#39;) try: kafka_server = \u0026#39;{}:{}\u0026#39;.format(credentials[\u0026#39;host\u0026#39;], credentials[\u0026#39;port\u0026#39;]) # Producer producer = KafkaProducer( bootstrap_servers=[kafka_server], value_serializer=lambda x: dumps(x).encode(\u0026#39;utf-8\u0026#39;) ) for e in range(10): data = {\u0026#39;number\u0026#39; : e} producer.send(\u0026#39;numtest\u0026#39;, value=data) # Consumer consumer = KafkaConsumer( bootstrap_servers=[kafka_server], auto_offset_reset=\u0026#39;earliest\u0026#39; ) consumer.subscribe([\u0026#39;numtest\u0026#39;]) output = \u0026#39;\u0026#39; # For demonstration purposes so it doesn\u0026#39;t block. for e in range(10): message = next(consumer) output \u0026#43;= str(loads(message.value.decode(\u0026#39;UTF-8\u0026#39;))[ number ]) \u0026#43; \u0026#39;, \u0026#39; # What a real implementation would do instead. # for message in consumer: # output \u0026#43;= loads(message.value.decode(\u0026#39;UTF-8\u0026#39;))[ number ] return output except Exception as e: return e ## With the ruby-kafka gem # Producer require  kafka  kafka = Kafka.new([ kafka.internal:9092 ], client_id:  my-application ) kafka.deliver_message( Hello, World! , topic:  greetings ) # Consumer kafka.each_message(topic:  greetings ) do |message| puts message.offset, message.key, message.value end (The specific way to inject configuration into your application will vary. Consult your application or framework’s documentation.)",
        "section": "Configure services",
        "subsections": " Supported versions Relationship Usage example  ",
        "image": "",
        "url": "/configuration/services/kafka.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "4a5c71ddcdd2c3a97bcafcf4359a9b05",
        "title": "Management console",
        "description": "",
        "text": " Platform.sh provides a responsive management console which allows you to interact with your projects and manage your environments. Everything you can do with the management console you can also achieve with the CLI (Command Line Interface) . Environment List From your project’s main page, each of the environments are available from the pull-down menu ENVIRONMENT at the top of the page. There is also a graphic view of your environments on the right hand side, where you can view your environments as a list or as a project tree. The name of the environment is struck out if it’s been disabled. If it has an arrow next to it, this means the environment has children. Environments Once you select an environment, the management console can give you a great deal of information about it. Activity Feed The management console displays all the activity happening on your environments. You can filter messages per type. Header Within a project’s environment, the management console exposes 4 main actions and 4 drop-down command options that you can use to interface with your environments. Branch Branching an environment means creating a new branch in the Git repository, as well as an exact copy of that environment. The new branch includes code, all of the data that is stored on disk (database, Solr indexes, uploaded files, etc.), and also a new copy of the running services (and their configuration) that the application needs. This means that when you branch an environment, you also branch the complete infrastructure. During a branch, three things happen: A new branch is created in Git. The application is rebuilt on the new branch, if necessary. The new branch is deployed. After clicking Branch a dialog box will appear that will provide commands to execute future merges from the command line using the Platform.sh CLI. Merge Merging an environment means introducing the code changes from a branch to its parent branch and redeploying the parent. During a merge: The code changes are merged via Git to the parent branch. The application is rebuilt on the parent branch, if necessary. The parent branch is deployed. Rebuilding the application is not necessary if the same code was already built (for any environment): in this case you will see the message Slug already built for this tree id, skipping. After clicking Merge a dialog box will appear that will provide commands to execute future merges from the command line using the Platform.sh CLI. Sync Synchronization performs a merge from a parent into a child environment, and then redeploys that environment. You have the option of performing a Sync on only the code, replacing the data (i.e. databases) of that environment from its parent, or both. These options are provided in a separate dialog box that will appear when you click the Sync button, along with the Platform.sh CLI commands that perform the same action. Be aware that sync uses the Snapshot mechanism and will have the same caveats. Be aware that sync uses the Backup mechanism and will have the same caveats. Note that Sync is only available if your branch has no unmerged commits, and can be fast-forwarded. It is good practice to take a backup of your environment before performing a synchronization. Backup Creating a backup for an environment means saving a copy of the database so that it can be restored. You will see the backup in the activity feed of you environment in the Platform.sh management console where you can trigger the restore by clicking on the restore link. After clicking Backup a dialog box will appear that will provide commands to execute future merges from the command line using the Platform.sh CLI. You can also use the CLI with: $ platform environment:backup to create a backup, and $ platform environment:restore to restore an existing backup. URLs The URLs pull-down exposes the domains that can be used to access application environments from the web. GIT The Git pull-down displays the commands to use to clone the codebase via Git. CLI The CLI pull-down displays the commands to get your project set up locally with the Platform.sh CLI. SSH The SSH pull-down display the commands to access your project over SSH. Configuration settings From the management console you can also view information about how your routes, services, and applications are currently configured for the environment. At the top of the page, click the “Services” tab. Applications Select the application container on the left to show more detailed information for it on the right. The “Overview” tab gives you metadata information regarding the application. It tells you what size container it has been configured for, the amount of persistent disk, the number of active workers and cron jobs, as well as the command to ssh into that container. Each cron job associated with the application is listed with its frequency, the last time it was run, it’s status, and its command. The “Configuration” tab provides an overview of the application’s configuration pulled from its .platform.app.yaml file. Services Each service has a tab on the left, so select the one you are interested in. The overview tab gives you metadata information regarding the service. It tells you what size container it has been configured for and the amount of persistent disk given to it in your services.yaml file. The “Configuration” tab provides an overview of the service configuration that has been pulled from the services.yaml file. Routes Each route will appear when you select the Routes tab on the left and describe its type and whether caching and SSI have been enabled for it.",
        "section": "Administration",
        "subsections": " Environment List Environments  Activity Feed Header Configuration settings    ",
        "image": "",
        "url": "/administration/web.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "d00747ffdd16c906126219f9efbad199",
        "title": "Next steps",
        "description": "",
        "text": " In this guide you learned how to create and activate live feature environments, test them, and merge them into production safely using backups . Don’t stop now! There are many more features that make Platform.sh helpful to developers. Developing on Platform.sh The next guide shows how to set up your development workflow to benefit from Platform.sh. Local development Remotely connect to services and build your application locally during development. Additional Resources External Integrations Configure Platform.sh to mirror every push and pull request on GitHub, Gitlab, and Bitbucket. Going Live Set up your site for production, configure domains, and go live! Platform.sh Community Portal Check out how-tos, tutorials, and get help for your questions about Platform.sh. Platform.sh Blog Read news and how-to posts all about working with Platform.sh. Back",
        "section": "Getting started",
        "subsections": "   Developing on Platform.sh Additional Resources    ",
        "image": "",
        "url": "/gettingstarted/developing/dev-environments/next-steps.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "e861d84a0ca9ef0dd2ed9962461025de",
        "title": "Next steps",
        "description": "",
        "text": " In this guide you opened an SSH tunnel to your Platform.sh project and built your application locally. Don’t stop now! There are far more features that make Platform.sh profoundly helpful to developers that you have left to explore. Developing on Platform.sh Consult these additional resources to help improve your development life cycle. Development environments Activate development branches and test new features before merging into production. Additional Resources External Integrations Configure Platform.sh to mirror every push and pull request on GitHub, Gitlab, and Bitbucket. Going Live Set up your site for production, configure domains, and go live! Platform.sh Community Portal Check out how-tos, tutorials, and get help for your questions about Platform.sh. Platform.sh Blog Read news and how-to posts all about working with Platform.sh. Back",
        "section": "Getting started",
        "subsections": "   Developing on Platform.sh Additional Resources    ",
        "image": "",
        "url": "/gettingstarted/developing/local-development/next-steps.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "c1f9f10db9df5ee536f34a6d4e93a013",
        "title": "Next steps",
        "description": "",
        "text": "Now that your application is up and running, here are some additional pieces of information that will help you leverage every bit of technology Platform.sh has to offer.",
        "section": "Introduction",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/next-steps.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "9876ce8aa32050ca2db85566db1aff57",
        "title": "Next Steps: Requirements for the CLI",
        "description": "",
        "text": " With the management console you can start new projects from templates just as you did in the previous steps, but deploying your own applications requires you to also use the Platform.sh CLI . Before you install it there are a few requirements that must be met first. Git Git is the open source version control system used by Platform.sh. Any change you make to your Platform.sh project will need to be committed via Git. You can see all the Git commit messages of an environment in the Environment Activity feed of the management console for each project you create. Before getting started, make sure you have Git installed on your computer. SSH key pair Once your account has been set up and the CLI is installed, Platform.sh needs one additional piece of information about your computer so that you can access your projects from the command line. If you are unfamiliar with how to generate an SSH public and private key, there are instructions in the documentation about how to do so . Add your SSH public key to your account Add your SSH public key to your Platform.sh account so that you can communicate with your projects using the CLI. Access SSH key settings in the management console From the management console, move to the top right hand corner of the screen and click the dropdown menu to the left of the settings gear box icon. In the menu, click on Account. This next page lists all of your active projects, which now includes My First Project. Click on the Account Settings link at the top of the page, then click the SSH keys tab to the left of your account information. Add your SSH public key to your account Click the \u0026#43; Add public key button in the top right hand corner of the screen. This will open up another window with two fields. Name the key with something memorable, like home-computer, and in the field below that, paste the content of your public key. When you have finished, click Save to save the key. That’s it! Now that you have met the requirements and configured an SSH key, all that’s left is to install the Platform.sh CLI so you can interact with your projects from the command line. Back I\u0026#39;ve added my public SSH key",
        "section": "Getting started",
        "subsections": "   Git SSH key pair Add your SSH public key to your account    ",
        "image": "",
        "url": "/gettingstarted/gettingstarted/template/cli-requirements.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "b23fc77d8afd2f22b8cc81959e3700d3",
        "title": "PHP.ini settings",
        "description": "",
        "text": " There are two ways to customize php.ini values for your application. The recommended method is to use the variables property of .platform.app.yaml to set ini values using the php prefix. For example, to increase the PHP memory limit you’d put the following in .platform.app.yaml: variables:php:memory_limit:256MIt’s also possible to provide a custom php.ini file in the repository in the root of the application (where your .platform.app.yaml file is). ; php.ini ; Increase PHP memory limit memory_limit = 256M Another example is to set the timezone of the PHP runtime (though, the timezone settings of containers/services would remain in UTC): variables:php: date.timezone :  Europe/Paris or ; php.ini ; Set PHP runtime timezone date.timezone =  Europe/Paris  Environment-specific php.ini configuration directives can be provided via environment variables separately from the application code. See the note in the Environment variables section. Disabling functions A common recommendation for securing a PHP installation is to disable certain built-in functions that are frequently used in remote attacks. By default, Platform.sh does not disable any functions as they all do have some legitimate use in various applications. However, you may wish to disable them yourself if you know they are not needed. For example, to disable pcntl_exec and pcntl_fork (which are not usable in a web request anyway): variables:php: disable_functions :  pcntl_exec,pcntl_fork Common functions to disable include: create_function - create_function has no useful purpose since PHP 5.3 and should not be used, ever. It has been effectively replaced by anonymous functions. exec,passthru,shell_exec,system,proc_open,popen - These functions all allow a PHP script to run a bash shell command. That is rarely used by web applications, although build scripts may need them. pcntl_exec,pcntl_fork,pcntl_setpriority - The pcntl_* functions (including those not listed here) are responsible for process management. Most of them will cause a fatal error if used within a web request. Cron tasks or workers may make use of them, however. Most are safe to disable unless you know that you are using them. curl_exec,curl_multi_exec - These functions allow a PHP script to make arbitrary HTTP requests. Note that they are frequently used by other HTTP libraries such as Guzzle, in which case you should not disable them. show_source - This function shows a syntax highlighted version of a named PHP source file. That is rarely useful outside of development. Naturally if your application does make use of any of these functions, it will fail if you disable them. In that case, do not disable them. Default php.ini settings The default values for some frequently-modified php.ini settings are listed below. memory_limit=128M post_max_size=64M upload_max_filesize=64M display_errors=On This value is on by default to ease setting up a project on Platform.sh. We strongly recommend providing a custom error handler in your application or setting this value to Off before you make your site live. zend.assertions=-1 Assertions are optimized out of existence and have no impact at runtime. You should have assertions set to 1 for your local development system. opcache.memory_consumption=64 This is the number of megabytes available for the opcache. Large applications with many files may want to increase this value. opcache.validate_timestamps=On The opcache will check for updated files on disk. This is necessary to support applications that generate compiled PHP code from user configuration. If you are certain your application does not do so then you can disable this setting for a small performance boost. Warning: We do not limit what you can put in your php.ini file, but many settings can break your application. This is a facility for advanced users.",
        "section": "PHP",
        "subsections": " Disabling functions Default php.ini settings  ",
        "image": "",
        "url": "/languages/php/ini.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "be1c958468274a09d81d789f9a94a006",
        "title": "Platform.sh Third-Party Resources",
        "description": "",
        "text": " This is a Big List of known third party resources for Platform.sh. These resources are not vetted by Platform.sh, but may be useful for people working with the platform. Blogs “ The future of the PHP PaaS is here: Our journey to Platform.sh  , by Marcus Hausammann An introduction to Platform.sh from Chris Ward Guides Getting started \u0026amp; workflow Set up your Mac for Platform.sh using MAMP by @owntheweb How Platform.sh can simplify your contribution workflow on GitHub by Mickaël Andrieu from Akeneo A guide in French on deploying to Platform.sh by Thomas Asnar [FR] Nacho Digital has a guide on moving an existing site to Platform.sh All the stuff you need for a pro-dev-flow using platform.sh as your deploy target again by https://www.thinktandem.io Working with Platform.sh How to connect to your MySQL database using Sequel Pro How to set up XDebug Official Symfony documentation on deploying to Platform.sh Official Sylius documentation on deploying to Platform.sh How to install Apache Tika on Platform.sh How to store complete logs at AWS S3 by Contextual Code Automated SSL Certificates Export on Platform.sh by Contextual Code A Platform.sh region migration tool by Contextual Code Drupal Modifying distribution make files for Platform.sh Platform.sh Drupal 8 Development Workflow by @JohnatasJMO Syslogging is not supported on Platform.sh, instead, you can Log using Monolog to keep log files out of the database (and/or use whatever processors \u0026amp; handlers you want) Magento Deploying Magento 2 with Redis on Platform.sh by @rafaelcgstz Sylius The Sylius documentation has a solid set of instructions for setting up Sylius with Platform.sh. Examples Platform.sh lists maintained examples on its Github page, with some cross-referencing from http://docs.platform.sh. Examples listed below could work fine, or may be out-of-date or unmaintained. Use at your own risk. NodeJS Framework Credit Date added MEAN stack @OriPekelman May 2017 Python Framework Credit Date added Python Flask using gunicorn @etoulas May 2017 Odoo Open Source ERP and CRM @OriPekelman May 2017 PHP Framework Credit Date added Akeneo example @maciejzgadzaj May 2017 API Platform with a ReactJS client admin @GuGuss May 2017 Backdrop example @gmoigneu May 2017 Headless Drupal 8 with Angular @GuGuss May 2017 Headless Drupal 8 with React.js @systemseed Aug 2018 Joomla example @gmoigneu May 2017 Laravel example @JGrubb May 2017 Moodle example @JGrubb May 2017 Mouf framework example The Coding Machine May 2017 Flow Framework support package Dominique Feyer Jul 2017 Neos CMS support package Dominique Feyer Jul 2017 Silex example @JGrubb May 2017 Silverstripe example @gmoigneu May 2017 Thunder example maintained by the MD Systems team May 2017 WooCommerce example @Liip May 2017 Grav example Mike Crittenden August 2017 Ruby Framework Credit Date added Jekyll example @JGrubb May 2017 Rust Framework Credit Date added Rust with Rocket and webasm Royall Spence July 2018 Integrations Integrate GitLab with Platform.sh using Gitlab-CI , by @Axelerant Running Behat tests from CircleCI to a Platform.sh environment , by Matt Glaman Platform.sh’s original (unsupported) scripts for GitLab https://gist.github.com/pjcdawkins/0b3f7a6da963c129030961f0947746c4. Platform.sh now supports Gitlab natively. An adapter from platform.sh webhook to slack incoming webhook that can be hosted on a platform.sh app https://github.com/hanoii/platformsh2slack How to call the NewRelic API on deploy (by @christopher-hopper) A helper utility for running browser based tests on CircleCI against a Platform.sh environment. https://github.com/xendk/dais Tools \u0026amp; development MySQL disk space monitor https://github.com/galister/platformsh_mysqlmon Create deploy commands you can run from composer , using Symfony A small tool from Hanoii https://github.com/hanoii/drocal Script to sync a Drupal site from Production to Local https://github.com/pjcdawkins/platformsh-sync Matt Pope’s Platform.sh automated mysql and files backup script Development environments Beetbox , a pre-provisioned L*MP stack for Drupal and other frameworks, with Platform.sh CLI integration A Docker image with the Platform.sh CLI on it https://github.com/maxc0d3r/docker-platformshcli Some tips on using Platform.sh with DrupalVM https://github.com/geerlingguy/drupal-vm/issues/984 Vagrant with Ansible for Platform.sh, opinionated towards Drupal, by @mglaman. Ansible Playbook for setting up Vagrant and VirtualBox for use with a Platform.sh project PixelArt’s Platform.sh CLI role",
        "section": "Tutorials",
        "subsections": " Blogs Guides  Getting started \u0026amp; workflow Working with Platform.sh Drupal Magento Sylius   Examples  NodeJS Python PHP Ruby Rust   Integrations Tools \u0026amp; development  Development environments Ansible    ",
        "image": "",
        "url": "/tutorials/third-party.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "bfdf2db69e1bf1a8e36868a9183e196f",
        "title": "Scalability",
        "description": "",
        "text": " Part of the original design goal of Platform.sh’s Triple Redundant Architecture was to ensure scalability in times of load spikes outside of the bounds of the original traffic specs. Because the cluster is configured as an N\u0026#43;1 architecture, we can respond to legitimate traffic events by removing a node from the cluster, upsizing it, returning it into rotation, and then repeating the process on the next node in turn. Scaling Process and Procedure The scaling process is not automatic and requires manual effort. It may be initiated in two ways. On customer request via a ticket. We strongly recommend notifying us ahead of time if you know a large traffic event is coming (a major product launch, Black Friday, etc.) We cannot guarantee a turnaround time on a resizing unless given prior notice. High load incidents detected by our monitoring system. If the load is diagnosed to be due to a bot or crawler that we are able to block, we will attempt to block it. This prevents unnecessary scaling, which prevents unnecessary costs to you. If it is not a bot or is not blockable, then we will begin the upscaling process detailed above. Be advised that this process may take up to 60-90 minutes depending on the diagnostic steps needed. We will open a support ticket to notify you of such changes, but we will not wait for your response before upscaling your cluster. The uptime of your application is our top priority and reactive scaling events are part of how we ensure that we meet the obligations of our Service Level Agreement. You may opt-out of the upsizing service if you wish, but outages caused by high-traffic will not be considered to violate the Service Level Agreement.",
        "section": "Platform.sh Dedicated cluster specifications",
        "subsections": " Scaling Process and Procedure  ",
        "image": "",
        "url": "/dedicated/architecture/scalability.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "2b03d4587f37d8fe4745bda9ca5942a4",
        "title": "Server Side Includes",
        "description": "",
        "text": " Server side includes is a powerful mechanism by which you can at the same time leverage caching and serve dynamic content. You can activate or deactivate SSI on a per-route basis in your .platform/routes.yaml for example:  https://{default}/ :type:upstreamupstream: app:http cache:enabled:falsessi:enabled:true https://{default}/time.php :type:upstreamupstream: app:http cache:enabled:trueIt allows you to include in your HTML response directives that will make the server “fill-in” parts of the HTML respecting the caching you setup. For example you could in a dynamic non-cached page include a block that would have been cached for example in the /index.php page we would have: \u0026lt;?php echo date(DATE_RFC2822); ?\u0026gt; \u0026lt;!--#include virtual= time.php  --\u0026gt; and in time.php we had \u0026lt;?php header( Cache-Control: max-age=600 ); echo date(DATE_RFC2822); And you visit the home page you will see, as you refresh the page, the time on the top will continue to change, while the one on the bottom will only change every 600 seconds. For more on SSI functionality see the nginx documentation .",
        "section": "Configure routes",
        "subsections": "",
        "image": "",
        "url": "/configuration/routes/ssi.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "1f11414305f90c1ab394928de7df7b7d",
        "title": "Set up your local development environment",
        "description": "",
        "text": " While Platform.sh is great as a tool for hosting an application during both development and production, it’s naturally not the ideal place to edit code. You can’t, in fact, as the file system is read-only (as it should be). The proper place to edit your code is on your computer. You must have an SSH key already configured on your account, and have both Git and the Platform.sh CLI installed before continuing. Download the code If you don’t already have a local copy of your project’s code, run platform get to download one. You can also run platform projects to list all of the projects in your account. ~/htdocs $ platform projects Your projects are: +---------------+----------------------------+------------------------------------------------+ | ID | Name | URL | +---------------+----------------------------+------------------------------------------------+ | [project-id] | New Platform Project | https://eu.platform.sh/#/projects/[project-id] | +---------------+----------------------------+------------------------------------------------+ Get a project by running platform get [id]. List a project's environments by running platform environments. Now you can download the code using platform get [project-id] [folder-name]: ~/htdocs $ platform get [project-id] my-project Cloning into 'my-project/repository'... remote: counting objects: 11, done. Receiving objects: 100% (11/11), 1.36 KiB | 0 bytes/s, done. Checking connectivity... done. You should now have a repository folder, based on what you used for [folder-name] in the platform get command above. You will also notice a new directory in your project, .platform/local, which is excluded from Git. This directory contains builds and any local metadata about your project needed by the CLI. Building the site locally Run the platform build command to run through the same build process as would be run on Platform.sh. That will produce a _www directory in your project root that is a symlink to the currently active build in the .platform/local/builds folder. It should be used as the document root for your local web server. ~/htdocs/my-project $ platform build Building application myapp (runtime type: php) Beginning to build ~/htdocs/my-project/project.make. drupal-7.38 downloaded. drupal patched with install-redirect-on-empty-database-728702-36.patch. Generated PATCHES.txt file for drupal platform-7.x-1.3 downloaded. Running post-build hooks Symlinking files from the 'shared' directory to sites/default Build complete for application myapp Web root: ~/htdocs/my-project/_www ~/htdocs/my-project $ Be aware, of course, that the platform build command will run locally, and so require whatever appropriate runtime or other tools you specify. It may also result in packages referenced in your dependendencies block being installed on your local computer. If that is undesireable, a local virtual machine will let you create an enclosed local development environment that won’t affect your main system. Running the code Platform.sh supports whatever local development environment you wish to use. There is no dependency on any particular tool so if you already have a local development workflow you’re comfortable with you can keep using it without changes. That’s the “ untethered ” option. For quick changes, you can also run your code locally but use the services hosted on Platform.sh. That is, your site is “ tethered ” to Platform.sh. While this approach requires installing less on your system it can be quite slow as all communication with the database or cache server will need to travel from your computer to Platform.sh’s servers. Specific documentation is also available for the local development tools Lando and Docksal , which support most applications that Platform.sh supports.",
        "section": "Development",
        "subsections": " Download the code Building the site locally Running the code  ",
        "image": "",
        "url": "/development/local.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "ef5e8c58797815555eff6508ecb15a90",
        "title": "Set your domain",
        "description": "",
        "text": " You will need to configure your registered domain on your Platform.sh project before going live, and you can do that either through the management console or by using the CLI. Through the management console Now that you have changed your project to a production plan, you can click the same “Go live” button at the top of the project page. Alternatively, you can click “Settings” at the top of the page, and then visit the “Domains” section on the left. Click the “Add\u0026#43;” button in the top right hand corner of the page, enter your registered domain and select if you want it to be the default domain for the project. You can add multiple domains to a project, but only one can be set as the default. When you’re finished, click “Add domain”, and the project will once again redeploy to apply your changes. Using the CLI You can also add a domain to your project using the Platform.CLI. From a terminal window, type the command platform domain:add example.com --project \u0026lt;project ID\u0026gt; The CLI will validate your registered domain and provision Let’s Encrypt certificates for it. Back I have configured my registered domain",
        "section": "Getting started",
        "subsections": " Through the management console Using the CLI  ",
        "image": "",
        "url": "/gettingstarted/next-steps/going-live/set-domain.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "0ea7fa3e24cff48f1088f37b4aaa70d0",
        "title": "Single sign-on (SSO)",
        "description": "",
        "text": " Platform.sh allows you to set up mandatory SSO with a third-party identity provider (IdP) for all your users. Your SSO provider can be enabled for a specific email domain, for example @example.com. Every user with a matching email address will have to log in or register on Platform.sh using your SSO provider. Such users will not be able to use an alternative provider, or register a password, or change their email address. Mitigation controls If you deactivate a user on your identity provider, they will not be able to log in or register on Platform.sh. If the user is already logged in to Platform.sh, they will be automatically deactivated after their access token has expired (generally after 1 hour). A deactivated user will no longer be able to use SSH, Git, or other Platform.sh APIs. Service users If you have a service user with an email address under your SSO domain, e.g. machine-user@example.com, you can allow that user so that they will not be required to authenticate through your identity provider. Please open a support ticket if you need to allow a user. SSO providers Google Tier availability This feature is only available to Enterprise customers. Compare the Platform.sh tiers on our pricing page, or contact our sales team for more information. Enforce your users to authenticate with your OpenID Connect provider. Please open a support ticket to enable SSO with your OpenID Connect provider. OpenId Connect Tier availability This feature is only available to Elite customers. Compare the Platform.sh tiers on our pricing page, or contact our sales team for more information. Enforce your users to authenticate with Google. Please open a support ticket to enable Google SSO.",
        "section": "Administration",
        "subsections": " Mitigation controls Service users SSO providers  Google OpenId Connect    ",
        "image": "",
        "url": "/administration/sso.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "cbde876fb2cc1fa9b4df4e59d5483039",
        "title": "Using Memcached with Drupal 8.x",
        "description": "",
        "text": " Platform.sh recommends using Redis for caching with Drupal 8 over Memcached, as Redis offers better performance when dealing with larger values as Drupal tends to produce. However, Memcached is also available if desired and is fully supported. Requirements Add a Memcached service First you need to create a Memcached service. In your .platform/services.yaml file, add or uncomment the following: cacheservice:type:memcached:1.4That will create a service named cacheservice, of type memcached, specifically version 1.4. Expose the Memcached service to your application In your .platform.app.yaml file, we now need to open a connection to the new Memcached service. Under the relationships section, add the following: relationships:cache: cacheservice:memcached The key (left side) is the name that will be exposed to the application in the PLATFORM_RELATIONSHIPS variable . The right hand side is the name of the service we specified above (cacheservice) and the endpoint (memcached). If you named the service something different above, change cacheservice to that. Add the Memcached PHP extension You will need to enable the PHP Memcached extension. In your .platform.app.yaml file, add the following right after the type block: # Additional extensionsruntime:extensions:- memcachedAdd the Drupal module You will need to add the Memcache module to your project. If you are using Composer to manage your Drupal 8 site (which we recommend), simply run: composer require drupal/memcache Then commit the resulting changes to your composer.json and composer.lock files. Note: You must commit and deploy your code before continuing, then enable the module. The memcache module must be enabled before it is configured in the settings.platformsh.php file. Configuration The Drupal Memcache module must be configured via settings.platformsh.php. Place the following at the end of settings.platformsh.php. Note the inline comments, as you may wish to customize it further. Also review the README.txt file that comes with the memcache module, as it has a more information on possible configuration options. For instance, you may want to consider using memcache for locking as well and configuring cache stampede protection. The example below is intended as a “most common case”. \u0026lt;?php if (getenv(\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;) \u0026amp;\u0026amp; extension_loaded(\u0026#39;memcached\u0026#39;)) { $relationships = json_decode(base64_decode(getenv(\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;)), TRUE); // If you named your memcached relationship something other than  cache , set that here. $relationship_name = \u0026#39;cache\u0026#39;; if (!empty($relationships[$relationship_name])) { // This is the line that tells Drupal to use memcached as a backend. // Comment out just this line if you need to disable it for some reason and // fall back to the default database cache. $settings[\u0026#39;cache\u0026#39;][\u0026#39;default\u0026#39;] = \u0026#39;cache.backend.memcache\u0026#39;; foreach ($relationships[$relationship_name] as $endpoint) { $host = sprintf( %s:%d , $endpoint[\u0026#39;host\u0026#39;], $endpoint[\u0026#39;port\u0026#39;]); $settings[\u0026#39;memcache\u0026#39;][\u0026#39;servers\u0026#39;][$host] = \u0026#39;default\u0026#39;; } } // By default Drupal starts the cache_container on the database. The following // code overrides that. // Make sure that the $class_load-\u0026gt;addPsr4 is pointing to the right location of // the memcache module. The value below should be correct if memcache was installed // using Drupal Composer. $memcache_exists = class_exists(\u0026#39;Memcache\u0026#39;, FALSE); $memcached_exists = class_exists(\u0026#39;Memcached\u0026#39;, FALSE); if ($memcache_exists || $memcached_exists) { \u0026#39;modules/contrib/memcache/src\u0026#39;); // If using a multisite configuration, adapt this line to include a site-unique // value. $settings[\u0026#39;memcache\u0026#39;][\u0026#39;key_prefix\u0026#39;] = getenv(\u0026#39;PLATFORM_ENVIRONMENT\u0026#39;); // Define custom bootstrap container definition to use Memcache for cache.container. $settings[\u0026#39;bootstrap_container_definition\u0026#39;] = [ \u0026#39;parameters\u0026#39; =\u0026gt; [], \u0026#39;services\u0026#39; =\u0026gt; [ \u0026#39;database\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;factory\u0026#39; =\u0026gt; \u0026#39;arguments\u0026#39; =\u0026gt; [\u0026#39;default\u0026#39;], ], \u0026#39;settings\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;factory\u0026#39; =\u0026gt; ], \u0026#39;memcache.settings\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;arguments\u0026#39; =\u0026gt; [\u0026#39;@settings\u0026#39;], ], \u0026#39;memcache.factory\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;arguments\u0026#39; =\u0026gt; [\u0026#39;@memcache.settings\u0026#39;], ], \u0026#39;memcache.backend.cache.container\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;factory\u0026#39; =\u0026gt; [\u0026#39;@memcache.factory\u0026#39;, \u0026#39;get\u0026#39;], \u0026#39;arguments\u0026#39; =\u0026gt; [\u0026#39;container\u0026#39;], ], \u0026#39;lock.container\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;arguments\u0026#39; =\u0026gt; [\u0026#39;container\u0026#39;, \u0026#39;@memcache.backend.cache.container\u0026#39;], ], \u0026#39;cache_tags_provider.container\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;arguments\u0026#39; =\u0026gt; [\u0026#39;@database\u0026#39;], ], \u0026#39;cache.container\u0026#39; =\u0026gt; [ \u0026#39;class\u0026#39; =\u0026gt; \u0026#39;arguments\u0026#39; =\u0026gt; [\u0026#39;container\u0026#39;, \u0026#39;@memcache.backend.cache.container\u0026#39;, \u0026#39;@lock.container\u0026#39;, \u0026#39;@memcache.config\u0026#39;, \u0026#39;@cache_tags_provider.container\u0026#39;], ], ], ]; } }",
        "section": "Getting Started",
        "subsections": " Requirements  Add a Memcached service Expose the Memcached service to your application Add the Memcached PHP extension Add the Drupal module   Configuration  ",
        "image": "",
        "url": "/frameworks/drupal8/memcached.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "f2243791752080ebe4b295db34326cbd",
        "title": "Vouchers",
        "description": "",
        "text": " Applying a voucher to your project If you receive a Platform.sh voucher code, you can redeem it as follows: Go to your Account Settings, logging in if necessary, via the link in the top-right corner of these docs, or via this link: https://accounts.platform.sh/user . In the left navigation click on “VOUCHERS” On the page click on “Add a voucher code” Enter the code and click on the “ADD CODE” button Et voilà! Your account will now be credited with additional dollars pounds or euros. If you are assessing Platform.sh for your organization and think that you could benefit from a little more oomf in your test project why not contact us to request a voucher? You can tell us more at: https://platform.sh/contact/",
        "section": "Management console",
        "subsections": " Applying a voucher to your project  ",
        "image": "",
        "url": "/administration/web/vouchers.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "dece15b1c992422b272d240405960133",
        "title": " Working with Drush in Drupal 7",
        "description": "",
        "text": " Drush is a command-line shell and scripting interface for Drupal, a veritable Swiss Army knife designed to make life easier for those who spend their working hours hacking away at the command prompt. You can use the CLI to set up Drush aliases, to easily run Drush commands on specific remote Platform.sh environments. See the documentation on Drush in Drupal 8 for installation, drush aliases, and other general information. The installation procedure is the same for both Drupal 7 and 8. Drush make Platform.sh can automatically build your Drupal 7 site using Drush make files. This allows you to easily test specific versions, apply patches and keep your site up to date. It also keeps your working directory much cleaner as since it only contains your custom code. Your make file can be called: project.make or drupal-org.make. A basic make file looks like this: api = 2 core = 7.x ; Drupal core. projects[drupal][type] = core projects[drupal][version] = 7.67 projects[drupal][patch][] =  https://drupal.org/files/issues/install-redirect-on-empty-database-728702-36.patch  ; Drush make allows a default sub directory for all contributed projects. defaults[projects][subdir] = contrib ; Platform indicator module. projects[platform][version] = 1.4 When building as a profile, you need a make file for Drupal core called: project-core.make: api = 2 core = 7.x projects[drupal][type] = core Generate a make file from an existing site If you want to generate a make file from your existing site, you can run: $ drush make-generate project.make This will output a make file containing all your contributed modules, themes and libraries. Note: Make generate command Apply patches You can apply contributed patches to your modules, themes or libraries within your project.make: projects[features][version] =  2.2  projects[features][patch][] =  https://www.drupal.org/files/issues/alter_overrides-766264-45.patch  You can also apply self-hosted patches. Simply create a PATCHES folder at the root of your repository and add the patch as follow: projects[uuid][version] =  1.0-alpha5  projects[uuid][patch][] =  PATCHES/fix-non-uuid-entity-load.patch  Work with a DEV version When you are using a module that is in a DEV version, the best practice is to always target a specific commit ID so that you’re always building the same “version” of the module: ; CKEditor module: version 7.x-1.15\u0026#43;2-dev projects[ckeditor][download][revision] =  b29372fb446b547825dc6c30587eaf240717695c  projects[ckeditor][download][type] =  git  projects[ckeditor][download][branch] =  7.x-1.x  projects[ckeditor][type] =  module ",
        "section": "Getting Started",
        "subsections": " Drush make  Generate a make file from an existing site Apply patches Work with a DEV version    ",
        "image": "",
        "url": "/frameworks/drupal7/drush.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "3f3246dd84a0486dab2e0258351a3488",
        "title": "Configuring projects",
        "description": "",
        "text": " In the previous step, you created a new project on Platform.sh using the CLI. Now, there are a few configuration steps left that will help Platform.sh know what to do with your application during builds and deployments. Consult a template alongside this guide As you go through this guide, example files will be provided that will give you a good impression of how to configure applications on Platform.sh in the programming language they use. However, since they are simple examples and your own application may require more detailed configuration than those examples address, it is recommended that you take a look at our maintained templates for additional guidance. Select a language and choose one or more templates that most closely resemble your application and keep the template in another tab as you continue through this guide. Using these two resources together is the fastest way to correctly configure your project for Platform.sh. C#/.Net Core C#/.Net Core templates Platform.sh offers project templates for a number of C#/.Net Core applications. They can be used as a reference for importing your web application. Available templates: ASP.NET Core Go Go templates Platform.sh offers project templates for a number of Go applications. They can be used as a reference for importing your web application. Available templates: Basic Go Beego Echo Gin Hugo Mattermost Java Java templates Platform.sh offers project templates for a number of Java applications. They can be used as a reference for importing your web application. Available templates: Apache Tomcat Apache TomEE Helidon Jenkins Jetty KumuluzEE Micronaut Open Liberty Payara Micro Quarkus Spring Boot, Gradle, Mysql Spring Boot, Maven, Mysql Spring MVC, Maven, MongoDB Spring, Kotlin, Maven Thorntail xwiki Lisp Lisp templates Platform.sh offers project templates for a number of Lisp applications. They can be used as a reference for importing your web application. Available templates: Lisp Hunchentoot Node.js Node.js templates Platform.sh offers project templates for a number of Node.js applications. They can be used as a reference for importing your web application. Available templates: Express Gatsby Gatsby with Wordpress Koa Node.js Probot strapi PHP PHP templates Platform.sh offers project templates for a number of PHP applications. They can be used as a reference for importing your web application. Available templates: Backdrop Basic PHP Drupal 8 Drupal 8 Multisite Drupal 9 GovCMS 8 Laravel Magento 2 Community Edition Mautic Nextcloud Opigno Pimcore Sculpin Symfony 3 Symfony 4 Symfony 5 TYPO3 Wordpress Python Python templates Platform.sh offers project templates for a number of Python applications. They can be used as a reference for importing your web application. Available templates: Basic Python 2 Basic Python 3 Django 1 Django 2 Django 3 Flask MoinMoin Pelican Pyramid Python 3 running UWSGI Wagtail Ruby Ruby templates Platform.sh offers project templates for a number of Ruby applications. They can be used as a reference for importing your web application. Available templates: Ruby on Rails Create empty configuration files You will notice that each of the templates above contain the following structure around their application code: . ├── .platform │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml └── \u0026lt; application code \u0026gt; In order to successfully deploy to Platform.sh you must add three YAML files: A .platform/routes.yaml file, which configures the routes used in your environments. That is, it describes how an incoming HTTP request is going to be processed by Platform.sh. A .platform/services.yaml file, which configures the services that will be used by the application. Connecting to Platform.sh’s maintained services only requires properly writing this file. While this file must be present, if your application does not require services it can remain empty. At least one .plaform.app.yaml file, which configures the application itself. It provides control over the way the application will be built and deployed on Platform.sh. When you set Platform.sh as a remote for your repository in the previous step, the CLI automatically created the hidden configuration directory .platform for you. The next steps will explore in more detail what each configuration files must include, but for now create empty files in their place. touch .platform/routes.yaml touch .platform/services.yaml touch .platform.app.yaml (Optional) Follow the Project Setup Wizard instructions in the management console All of the steps in this guide are also available in the Project Setup Wizard in your management console. Once you have created your project, the Wizard will appear at the top of your project page with detailed steps to help you properly configure your applications on Platform.sh. With the empty configuration files in place, you will need to specify your service configuration in .platform/services.yaml. Back I\u0026#39;ve created empty configuration files",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/project-configuration.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "0052c864679be1866aee2996f38b5e79",
        "title": "Data deletion",
        "description": "",
        "text": " Data deletion is handled via our backend providers. When a volume is released back to the provider, the provider will perform a wipe on the data utilizing either NIST 800-88 or DoD 5220.22-M depending upon the offering. This wipe is done immediately before reuse. All projects, except those hosted on Orange Cloud for Business, utilize encrypted volumes. The encryption key is destroyed when we release the volume back to the provider, adding another layer of protection. Media destruction Media destruction is handled via our backend providers. When the provider decommissions media it undergoes destruction as outlined in NIST 800-88. Data subject removal Data subject deletion requests where Platform is the controller are handled via a support ticket . For contracts designating Platform as the processor, deletion requests should be sent to the controller and we will forward any that we receive. Our product is a Platform as a Service. Platform does not directly edit customer data to ensure data confidentiality, security, and integrity. All data deletion requests for customer data must be handled by the concerned data controller. Resources AWS Security Whitepaper Azure Data Retention Google Cloud Platform Compliance Information Interoute Compliance Information Orange Cloud for Business certifications",
        "section": "Security and compliance",
        "subsections": " Media destruction Data subject removal Resources  ",
        "image": "",
        "url": "/security/data-deletion.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "b00b374267ecfbd49485fc6252c71b60",
        "title": "Delete a project",
        "description": "",
        "text": " To delete a Platform.sh project, including all data, code, and active environments: Go to your Account Settings, logging in if necessary, via the link in the top-right corner of these docs, or via this link: https://accounts.platform.sh/user . Locate the project you wish to delete in the project list. Hover the gear icon on the project and select “Delete”. Confirm your intent to delete the project. You will only be billed for the portion of a month during which the project was active. If you delete a project part way through the month the cost of the project will be prorated accordingly. A user account with no projects associated with it will have no charges.",
        "section": "Management console",
        "subsections": "",
        "image": "",
        "url": "/administration/web/delete.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "6d4ddc88e7d09f890c91f29939c408a4",
        "title": "Differences from Platform.sh on the Grid",
        "description": "",
        "text": " When using Platform.sh Dedicated, a few configuration options and tools function differently from Platform.sh on the Grid, aka the Development Environment. PHP Platform.sh Dedicated comes with pdo, apcu, curl, gd, imagick, ldap, mcrypt, mysqli, redis, soap, and opcache extensions enabled by default. In addition, we can enable enchant, gearman, geoip, gmp, http, pgsql, pinba, pspell, recode, tidy, xdebug, oci8 (PHP5.6 only), or any extension with a pre-existing package in the Debian Apt repository if desired. Please request such extensions via a ticket. Custom php.ini files are not supported on Platform.sh Dedicated. However, all PHP options that can be changed at runtime are still available. For example the memory limit can be changed using ini_set(\u0026#39;memory_limit\u0026#39;,\u0026#39;1024M\u0026#39;); PHP options that can we can change via support ticket include: max_execution_time max_input_time max_input_vars memory_limit post_max_size request_order upload_max_filesize Xdebug Platform.sh runs a second PHP-FPM process on all Dedicated clusters that has Xdebug enabled, but is only used if a request includes the appropriate Xdebug header. That means it’s safe to have Xdebug “always on”, as it will be ignored on most requests. To obtain the key you will need to file a ticket to have our support team provide it for you. Staging and Production have separate keys. Set that key in the Xdebug helper for your browser, and then whenever you have Xdebug enabled the request will use the alternate development PHP-FPM process with Xdebug. Cron tasks may be interrupted by deploys On Platform.sh Grid projects, a running cron task will block a deployment until it is complete. On Platform.sh Dedicated, however, a deploy will terminate a running cron task. Specifically, when a deploy to either Production or Staging begins, any active cron tasks are sent a SIGTERM message so that they can terminate gracefully if needed. If they are still running 2 seconds later a SIGKILL message will be sent to forcibly terminate the process. For that reason, it’s best to ensure your cron tasks can receive a SIGTERM message and terminate gracefully. Configuration \u0026amp; change management Some configuration parameters for Dedicated clusters cannot be managed via the YAML configuration files, and for those parameters you will need to open a support ticket to have the change applied. Further, the .platform/routes.yaml and .platform/services.yaml files do not automatically apply. Those will apply on the development environments but not on the staging and production instances. Any existing service upgrades or new service additions to staging and production will require a support ticket. It is possible to run different configurations for some (but not all) options between staging and production, such as cron tasks. By default we will make configuration changes to both instances unless you request otherwise. Specifically: Cron commands Worker instances Service versions and configuration (everything in .platform/services.yaml) Route, domain, and redirect configuration (everything in .platform/routes.yaml) Application container version Additional PHP extensions Web server configuration (the web.locations section of .platform.app.yaml) Cron Cron tasks may run up to once per minute. (They are limited to once every 5 minutes on Platform.sh Grid.) Cron tasks are always interpreted in UTC time. Logs Logs are available on the Dedicated Cluster at a different path than on Platform.sh Grid. Specifically, the can be found in: /var/log/platform/\u0026lt;application-name\u0026gt;/ This folder contains the application, cron, error and deployment logs.",
        "section": "Platform.sh Dedicated",
        "subsections": " PHP  Xdebug   Cron tasks may be interrupted by deploys Configuration \u0026amp; change management  Cron   Logs  ",
        "image": "",
        "url": "/dedicated/overview/grid.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "7d3060421a1a4550c36ef08e86fdb1b3",
        "title": "Example",
        "description": "",
        "text": " In this short section we will give you a very simple, typical example. More involved use-cases (such as site with many domains or multiple applications are simply variations on this). Suppose your project ID is abc123 in the US region, and you’ve registered mysite.com. You want www.mysite.com to be the “real” site and mysite.com to redirect to it. Configure routes.yaml First, configure your routes.yaml file like so:  https://www.{default}/ :type:upstreamupstream: app:http  https://{default}/ :type:redirectto: https://www.{default}/ That will result in two domains being created on Platform.sh: master-def456-abc123.eu-2.platformsh.site and www---master-def456-abc123.eu-2.platformsh.site. The former will automatically redirect to the latter. In the routes.yaml file, {default} will automatically be replaced with master-def456-abc123.eu-2.platformsh.site. In domain prefixes (like www), the . will be replaced with ---. Set your domain Now, add a single domain to your Platform.sh project for mysite.com. Using the CLI type: platform domain:add mysite.com You can also use the management console for that. As soon as you do, Platform.sh will no longer serve master-def456-abc123.eu-2.platformsh.site at all. Instead, {default} in routes.yaml will be replaced with mysite.com anywhere it appears when generating routes to respond to. You can still access the original internal domain by running platform environment:info edge_hostname -e master. Configure your DNS provider On your DNS provider, you would create two CNAMEs: mysite.com should be an ALIAS/CNAME/ANAME to master-def456-abc123.eu-2.platformsh.site. www.mysite.com should be a CNAME to master-def456-abc123.eu-2.platformsh.site. Note: Both point to the same name. See the note above regarding how different registrars handle dynamic apex domains. Result Here’s what will now happen under the hood. Assume for a moment that all caches everywhere are empty. An incoming request for mysite.com will result in the following: Your browser asks the DNS network for mysite.com\u0026#39;s DNS A record (the IP address of this host). It responds with “it’s an alias for www.master-def456-abc123.eu-2.platformsh.site” (the CNAME) which itself resolves to the A record with IP address 1.2.3.4 (Or whatever the actual address is). By default DNS requests by browsers are recursive, so there is no performance penalty for using CNAMEs. Your browser sends a request to 1.2.3.4 for domain mysite.com. Your router responds with an HTTP 301 redirect to www.mysite.com (because that’s what routes.yaml specified). Your browser looks up www.mysite.com and, as above, gets an alias for www.master-def456-abc123.eu-2.platformsh.site, which is IP 1.2.3.4. Your browser sends a request to 1.2.3.4 for domain www.mysite.com. Your router passes the request through to your application which in turn responds with whatever it’s supposed to do. On subsequent requests, your browser will know to simply connect to 1.2.3.4 for domain www.mysite.com and skip the rest. The entire process takes only a few milliseconds.",
        "section": "Going live",
        "subsections": " Configure routes.yaml Set your domain Configure your DNS provider Result  ",
        "image": "",
        "url": "/golive/example.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "bdb721610052260e3a58e1a0bbb7aa27",
        "title": "Install the CLI",
        "description": "",
        "text": " With all of the requirements met, install the CLI to start developing with Platform.sh. Install the CLI In your terminal run the following command: Installing on OSX or Linux curl -sS https://platform.sh/cli/installer | php Installing on Windows curl https://platform.sh/cli/installer -o cli-installer.php php cli-installer.php Authenticate and Verify Once the installation has completed, you can run the CLI in your terminal with the command platform Note: If you opened your free trial account using another login (i.e. GitHub), you will not be able to authenticate with this command until you setup your account password with Platform.sh in the console. You should now be able to see a list of your Platform.sh projects, including the template you made in this guide. You can copy its project ID hash, and then download a local copy of the repository with the command platform get \u0026lt;project ID\u0026gt; With a local copy, you can create branches, commit to them, and push your changes to Platform.sh right away! git push platform master Take a minute to explore some of the commands available with the CLI by using the command platform list. That’s it! Now that you have the management console set up and the CLI installed on your computer, you’re well on your way to exploring all of the ways that Platform.sh can improve your development workflow. Back I\u0026#39;ve installed the CLI",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/template/cli-install.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "cbf5ecf034e398d561aa46492ecb961d",
        "title": "Jakarta EE/Eclipse MicroProfile",
        "description": "",
        "text": " Eclipse MicroProfile is a community dedicated to optimizing the Enterprise Java mission for microservice-based architectures. The goal is to define a microservices application platform that is portable across multiple runtimes. Currently, the leading players in this group are IBM, Red Hat, Tomitribe, Payara, the London Java Community (LJC), and SouJava. Java Enterprise Edition (Java EE) is an umbrella that holds specifications and APIs with enterprise features, like distributed computing and web services. Widely used in Java, Java EE runs on reference runtimes that can be anything from microservices to application servers that handle transactions, security, scalability, concurrency, and management for the components it’s deploying. Now, Enterprise Java has been standardized under the Eclipse Foundation with the name Jakarta EE . Services The configuration reader library for Java is used in these examples, so be sure to check out the documentation for installation instructions and the latest version. MongoDB You can use Jakarta NoSQL / JNoSQL to use MongoDB with your application by first determining the MongoDB client programmatically. import com.mongodb.MongoClient; import jakarta.nosql.document.DocumentCollectionManager; import jakarta.nosql.document.DocumentCollectionManagerFactory; import org.jnosql.diana.mongodb.document.MongoDBDocumentConfiguration; import sh.platform.config.Config; import sh.platform.config.MongoDB; import javax.annotation.PostConstruct; import javax.enterprise.context.ApplicationScoped; import javax.enterprise.inject.Disposes; import javax.enterprise.inject.Produces; @ApplicationScoped class DocumentManagerProducer { private DocumentCollectionManagerFactory managerFactory; private MongoDB mongoDB; @PostConstruct public void init() { Config config = new Config(); this.mongoDB = config.getCredential( database , MongoDB::new); final MongoClient mongoClient = mongoDB.get(); MongoDBDocumentConfiguration configuration = new MongoDBDocumentConfiguration(); this.managerFactory = configuration.get(mongoClient); } @Produces public DocumentCollectionManager getManager() { return managerFactory.get(mongoDB.getDatabase()); } public void destroy(@Disposes DocumentCollectionManager manager) { this.manager.close(); } } Apache Solr You can use Jakarta NoSQL / JNoSQL to use Solr with your application by first determining the Solr client programmatically. import jakarta.nosql.document.DocumentCollectionManager; import jakarta.nosql.document.DocumentCollectionManagerFactory; import org.apache.solr.client.solrj.impl.HttpSolrClient; import org.jnosql.diana.solr.document.SolrDocumentConfiguration; import sh.platform.config.Config; import sh.platform.config.Solr; import javax.annotation.PostConstruct; import javax.enterprise.context.ApplicationScoped; import javax.enterprise.inject.Disposes; import javax.enterprise.inject.Produces; @ApplicationScoped class DocumentManagerProducer { private DocumentCollectionManagerFactory managerFactory; @PostConstruct public void init() { Config config = new Config(); Solr solr = config.getCredential( database , Solr::new); final HttpSolrClient httpSolrClient = solr.get(); SolrDocumentConfiguration configuration = new SolrDocumentConfiguration(); this.managerFactory = configuration.get(httpSolrClient); } @Produces public DocumentCollectionManager getManager() { return managerFactory.get( collection ); } public void destroy(@Disposes DocumentCollectionManager manager) { this.manager.close(); } } Elasticsearch You can use Jakarta NoSQL / JNoSQL to use Elasticsearch with your application by first determining the Elasticsearch client programmatically. import jakarta.nosql.document.DocumentCollectionManager; import jakarta.nosql.document.DocumentCollectionManagerFactory; import org.elasticsearch.client.RestHighLevelClient; import org.jnosql.diana.elasticsearch.document.ElasticsearchDocumentConfiguration; import sh.platform.config.Config; import sh.platform.config.Elasticsearch; import javax.annotation.PostConstruct; import javax.enterprise.context.ApplicationScoped; import javax.enterprise.inject.Disposes; import javax.enterprise.inject.Produces; @ApplicationScoped class DocumentManagerProducer { private DocumentCollectionManagerFactory managerFactory; @PostConstruct public void init() { Config config = new Config(); Elasticsearch elasticsearch = config.getCredential( database , Elasticsearch::new); final RestHighLevelClient client = elasticsearch.get(); ElasticsearchDocumentConfiguration configuration = new ElasticsearchDocumentConfiguration(); this.managerFactory = configuration.get(client); } @Produces public DocumentCollectionManager getManager() { return managerFactory.get( collection ); } public void destroy(@Disposes DocumentCollectionManager manager) { this.manager.close(); } } Redis You can use Jakarta NoSQL / JNoSQL to use Redis with your application by first determining the Redis client programmatically. import jakarta.nosql.keyvalue.BucketManager; import org.jnosql.diana.redis.keyvalue.RedisBucketManagerFactory; import org.jnosql.diana.redis.keyvalue.RedisConfiguration; import redis.clients.jedis.JedisPool; import sh.platform.config.Config; import sh.platform.config.Redis; import javax.annotation.PostConstruct; import javax.enterprise.context.ApplicationScoped; import javax.enterprise.inject.Disposes; import javax.enterprise.inject.Produces; @ApplicationScoped class BucketManagerProducer { private static final String BUCKET =  olympus ; private RedisBucketManagerFactory managerFactory; @PostConstruct public void init() { Config config = new Config(); Redis redis = config.getCredential( redis , Redis::new); final JedisPool jedisPool = redis.get(); RedisConfiguration configuration = new RedisConfiguration(); managerFactory = configuration.get(jedisPool); } @Produces public BucketManager getManager() { return managerFactory.getBucketManager(BUCKET); } public void destroy(@Disposes BucketManager manager) { manager.close(); } } MySQL MySQL is an open-source relational database technology, and Jakarta EE supports a robust integration with it: JPA . The first step is to choose the database that you would like to use in your project. Define the driver for MySQL and the Java dependencies. Then determine the DataSource client programmatically: import sh.platform.config.Config; import sh.platform.config.JPA; import javax.annotation.PostConstruct; import javax.enterprise.context.ApplicationScoped; import javax.enterprise.inject.Disposes; import javax.enterprise.inject.Produces; import javax.persistence.EntityManager; import javax.persistence.EntityManagerFactory; @ApplicationScoped class EntityManagerConfiguration { private EntityManagerFactory entityManagerFactory; private EntityManager entityManager; @PostConstruct void setUp() { Config config = new Config(); final JPA credential = config.getCredential( postgresql , JPA::new); entityManagerFactory = credential.getMySQL( jpa-example ); this.entityManager = entityManagerFactory.createEntityManager(); } @Produces @ApplicationScoped EntityManagerFactory getEntityManagerFactory() { return entityManagerFactory; } @Produces @ApplicationScoped EntityManager getEntityManager() { return entityManager; } void close(@Disposes EntityManagerFactory entityManagerFactory) { entityManagerFactory.close(); } void close(@Disposes EntityManager entityManager) { entityManager.close(); } } Note: You can use the same MySQL driver for MariaDB as well if you wish to do so. MariaDB MariaDB is an open-source relational database technology, and Jakarta EE supports a robust integration with it: JPA . The first step is to choose the database that you would like to use in your project. Define the driver for MariaDB and the Java dependencies. Then determine the DataSource client programmatically: import sh.platform.config.Config; import sh.platform.config.JPA; import javax.annotation.PostConstruct; import javax.enterprise.context.ApplicationScoped; import javax.enterprise.inject.Disposes; import javax.enterprise.inject.Produces; import javax.persistence.EntityManager; import javax.persistence.EntityManagerFactory; @ApplicationScoped class EntityManagerConfiguration { private EntityManagerFactory entityManagerFactory; private EntityManager entityManager; @PostConstruct void setUp() { Config config = new Config(); final JPA credential = config.getCredential( postgresql , JPA::new); entityManagerFactory = credential.getMariaDB( jpa-example ); this.entityManager = entityManagerFactory.createEntityManager(); } @Produces @ApplicationScoped EntityManagerFactory getEntityManagerFactory() { return entityManagerFactory; } @Produces @ApplicationScoped EntityManager getEntityManager() { return entityManager; } void close(@Disposes EntityManagerFactory entityManagerFactory) { entityManagerFactory.close(); } void close(@Disposes EntityManager entityManager) { entityManager.close(); } } PostgreSQL PostgreSQL is an open-source relational database technology, and Jakarta EE supports a robust integration with it: JPA . The first step is to choose the database that you would like to use in your project. Define the driver for PostgreSQL and the Java dependencies. Then determine the DataSource client programmatically: import sh.platform.config.Config; import sh.platform.config.JPA; import javax.annotation.PostConstruct; import javax.enterprise.context.ApplicationScoped; import javax.enterprise.inject.Disposes; import javax.enterprise.inject.Produces; import javax.persistence.EntityManager; import javax.persistence.EntityManagerFactory; @ApplicationScoped class EntityManagerConfiguration { private EntityManagerFactory entityManagerFactory; private EntityManager entityManager; @PostConstruct void setUp() { Config config = new Config(); final JPA credential = config.getCredential( postgresql , JPA::new); entityManagerFactory = credential.getPostgreSQL( jpa-example ); entityManager = entityManagerFactory.createEntityManager(); } @Produces @ApplicationScoped EntityManagerFactory getEntityManagerFactory() { return entityManagerFactory; } @Produces @ApplicationScoped EntityManager getEntityManager() { return entityManager; } void close(@Disposes EntityManagerFactory entityManagerFactory) { entityManagerFactory.close(); } void close(@Disposes EntityManager entityManager) { entityManager.close(); } } Transaction To any Eclipse Microprofile or any non-JTA application is essential to point out, CDI does not provide transaction management implementation as part of its specs. Transaction management is left to be implemented by the programmer through the interceptors, such as the code below. import javax.annotation.Priority; import javax.inject.Inject; import javax.interceptor.AroundInvoke; import javax.interceptor.Interceptor; import javax.interceptor.InvocationContext; import javax.persistence.EntityManager; import javax.persistence.EntityTransaction; import javax.transaction.Transactional; @Transactional @Interceptor @Priority(Interceptor.Priority.APPLICATION) public class TransactionInterceptor { @Inject private EntityManager manager; @AroundInvoke public Object manageTransaction(InvocationContext context) throws Exception { final EntityTransaction transaction = manager.getTransaction(); transaction.begin(); try { Object result = context.proceed(); transaction.commit(); return result; } catch (Exception exp) { transaction.rollback(); throw exp; } } } Furthermore, Apache Delta Spike has a post for treating this problem. Templates Apache Tomee Thorntail Payara Micro KumuluzEE Helidon Open Liberty Quarkus Tomcat",
        "section": "Featured frameworks",
        "subsections": " Services  MongoDB Apache Solr Elasticsearch Redis MySQL MariaDB PostgreSQL   Transaction Templates  ",
        "image": "",
        "url": "/frameworks/jakarta.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "7243925f0793ac942dcc633580b13c07",
        "title": "Memcached (Object cache)",
        "description": "",
        "text": " Memcached is a simple in-memory object store well-suited for application level caching. See the Memcached for more information. Both Memcached and Redis can be used for application caching. As a general rule, Memcached is simpler and thus more widely supported while Redis is more robust. Platform.sh recommends using Redis if possible but Memcached is fully supported if an application favors that cache service.” Supported versions Grid Dedicated 1.4 1.5 1.6 None available Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : {  cluster :  rjify4yjcwxaa-master-7rqtwti ,  host :  memcached.internal ,  hostname :  2fw7ykay5vo5cez7cgwccz5kqa.memcached.service._.eu-3.platformsh.site ,  ip :  169.254.235.192 ,  port : 11211,  rel :  memcached ,  scheme :  memcached ,  service :  memcached ,  type :  memcached:1.4  } Usage example In your .platform/services.yaml: cachemc:type:memcached:1.6 Now add a relationship in your .platform.app.yaml file: relationships:memcachedcache: cachemc:memcached  Note: You will need to use the memcached type when defining the service # .platform/services.yamlservice_name:type:memcached:version and the endpoint memcached when defining the relationship # .platform.app.yamlrelationships:relationship_name:“service_name:memcached” Your service_name and relationship_name are defined by you, but we recommend making them distinct from each other. If you are using PHP, configure the relationship and enable the PHP memcached extension in your .platform.app.yaml. (Note that the memcached extension requires igbinary and msgpack as well, but those will be enabled automatically.) runtime:extensions:- memcachedFor Python you will need to include a dependency for a Memcached library, either via your requirements.txt file or a global dependency. As a global dependency you would add the following to .platform.app.yaml: dependencies:python:python-memcached:\u0026#39;*\u0026#39;You can then use the service in a configuration file of your application with something like: Go Java Node.js PHP PHP package examples import (  fmt   github.com/bradfitz/gomemcache/memcache  psh  github.com/platformsh/config-reader-go/v2  gomemcache  github.com/platformsh/config-reader-go/v2/gomemcache  ) func UsageExampleMemcached() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // Get the credentials to connect to the Solr service. credentials, err := config.Credentials( memcached ) checkErr(err) // Retrieve formatted credentials for gomemcache. formatted, err := gomemcache.FormattedCredentials(credentials) checkErr(err) // Connect to Memcached. mc := memcache.New(formatted) // Set a value. key :=  Deploy_day  value :=  Friday  err = mc.Set(\u0026amp;memcache.Item{Key: key, Value: []byte(value)}) // Read it back. test, err := mc.Get(key) return fmt.Sprintf( Found value \u0026lt;strong\u0026gt;%s\u0026lt;/strong\u0026gt; for key \u0026lt;strong\u0026gt;%s\u0026lt;/strong\u0026gt;. , test.Value, key) } package sh.platform.languages.sample; import net.spy.memcached.MemcachedClient; import sh.platform.config.Config; import java.util.function.Supplier; import sh.platform.config.Memcached; public class MemcachedSample implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); // Get the credentials to connect to the Memcached service. Memcached memcached = config.getCredential( memcached , Memcached::new); final MemcachedClient client = memcached.get(); String key =  cloud ; String value =  platformsh ; // Set a value. client.set(key, 0, value); // Read it back. Object test = client.get(key); logger.append(String.format( Found value %s for key %s. , test, key)); return logger.toString(); } } const Memcached = require(\u0026#39;memcached\u0026#39;); const config = require( platformsh-config ).config(); const { promisify } = require(\u0026#39;util\u0026#39;); exports.usageExample = async function() { const credentials = config.credentials(\u0026#39;memcached\u0026#39;); let client = new Memcached(`${credentials.host}:${credentials.port}`); // The MemcacheD client is not Promise-aware, so make it so. const memcachedGet = promisify(client.get).bind(client); const memcachedSet = promisify(client.set).bind(client); let key = \u0026#39;Deploy-day\u0026#39;; let value = \u0026#39;Friday\u0026#39;; // Set a value. await memcachedSet(key, value, 10); // Read it back. let test = await memcachedGet(key); let output = `Found value \u0026lt;strong\u0026gt;${test}\u0026lt;/strong\u0026gt; for key \u0026lt;strong\u0026gt;${key}\u0026lt;/strong\u0026gt;.`; return output; }; \u0026lt;?php declare(strict_types=1); use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // Get the credentials to connect to the Memcached service. $credentials = $config-\u0026gt;credentials(\u0026#39;memcached\u0026#39;); try { // Connecting to Memcached server. $memcached = new Memcached(); $memcached-\u0026gt;addServer($credentials[\u0026#39;host\u0026#39;], $credentials[\u0026#39;port\u0026#39;]); $memcached-\u0026gt;setOption(Memcached::OPT_BINARY_PROTOCOL, true); $key =  Deploy day ; $value =  Friday ; // Set a value. $memcached-\u0026gt;set($key, $value); // Read it back. $test = $memcached-\u0026gt;get($key); printf(\u0026#39;Found value \u0026lt;strong\u0026gt;%s\u0026lt;/strong\u0026gt; for key \u0026lt;strong\u0026gt;%s\u0026lt;/strong\u0026gt;.\u0026#39;, $test, $key); } catch (Exception $e) { print $e-\u0026gt;getMessage(); } import pymemcache from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the Memcached service. credentials = config.credentials(\u0026#39;memcached\u0026#39;) try: # Try connecting to Memached server. memcached = pymemcache.Client((credentials[\u0026#39;host\u0026#39;], credentials[\u0026#39;port\u0026#39;])) memcached.set(\u0026#39;Memcached::OPT_BINARY_PROTOCOL\u0026#39;, True) key =  Deploy_day  value =  Friday  # Set a value. memcached.set(key, value) # Read it back. test = memcached.get(key) return \u0026#39;Found value \u0026lt;strong\u0026gt;{0}\u0026lt;/strong\u0026gt; for key \u0026lt;strong\u0026gt;{1}\u0026lt;/strong\u0026gt;.\u0026#39;.format(test.decode( utf-8 ), key) except Exception as e: return e Accessing Memcached directly To access the Memcached service directly you can simply use netcat as Memcached does not have a dedicated client tool. Assuming your Memcached relationship is named cache, the host name and port number obtained from PLATFORM_RELATIONSHIPS would be cache.internal and 11211. Open an SSH session and access the Memcached server as follows: netcat cache.internal 11211",
        "section": "Configure services",
        "subsections": " Supported versions Relationship Usage example Accessing Memcached directly  ",
        "image": "",
        "url": "/configuration/services/memcached.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "99d0f5a7f3e218cf2b284512684b4ede",
        "title": "Next steps",
        "description": "",
        "text": " In this guide you created a project using the management console and installed the Platform.sh CLI . Now you can explore some of the next steps for working with Platform.sh. Import your own code Templates are great, but configuring your own application to run on Platform.sh is the goal. Import your own code Use the CLI and a few configuration files to deploy your code on Platform.sh. Developing on Platform.sh Once an application has been migrated to Platform.sh, there’s plenty more features that will help improve your development life cycle. Local development Remotely connect to services and build your application locally during development. Development environments Activate development branches and test new features before merging into production. Additional Resources External Integrations Configure Platform.sh to mirror every push and pull request on GitHub, Gitlab, and Bitbucket. Going Live Set up your site for production, configure domains, and go live! Platform.sh Community Portal Check out how-tos, tutorials, and get help for your questions about Platform.sh. Platform.sh Blog Read news and how-to posts all about working with Platform.sh. Back",
        "section": "Getting started",
        "subsections": " Import your own code Developing on Platform.sh Additional Resources  ",
        "image": "",
        "url": "/gettingstarted/gettingstarted/template/next-steps.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "eccbc17c368c7aecee8ed162a98e45a8",
        "title": "Options",
        "description": "",
        "text": " Staging environments By default, the staging instance and production instance run on the same trio of virtual machines. That ensures identical configuration between them but can incur a performance penalty for production if the load generated during QA and UAT in staging is of any appreciable size. A dedicated single-node staging machine can be provisioned for your application with an identical software configuration to your production hardware, but reduced hardware specs. This gives the advantages of isolating the staging load from the production hardware as well as having an identical software configuration to perform UAT, but this option does not provide a bed for performance testing as the physical hardware configuration is not the same as production. Multiple applications Each application deployed to the Dedicated Cluster corresponds to a single Git repository in the Development Environment. Multiple .platform.app.yaml files are not supported. While it is possible to host multiple application code bases in separate subdirectories/subpaths of the application (such as /drupal, /api, /symfony, etc.) controlled by a single .platform.app.yaml, it is not recommended and requires additional configuration. One or more domains may be mapped to the application. Our experience has shown that hosting multiple applications on a common resource pool is often bad for all applications on the cluster. We therefore limit the number of applications that may be hosted on a single Dedicated Cluster. On a D6 instance, only one application is supported. On D12 and larger Dedicated plans multiple applications are supported at an extra cost. Each application would correspond to a different Development Environment and Git repository and cannot share data or files with other applications. This configuration is discouraged. Multiple-AZ The default configuration for Platform.sh Dedicated clusters is to launch into a single Availability Zone (AZ). This is for a few reasons: Because the members of your cluster communicate with each other via TCP to perform DB replication, cache lookup, and other associated tasks, the latency between data centers/AZs can become a significant performance liability. Having your entire cluster within one AZ ensures that the latency between cluster members is minimal, having a direct effect on perceived end-user performance. Network traffic between AZs is billed, whereas intra-AZ traffic is not. That leads to higher costs for this decreased performance. Some clients prefer the peace of mind of hosting across multiple AZs, but it should be noted that multiple-AZ configurations do not improve the contractual 99.99% uptime SLA, nor does our standard, single-AZ configuration decrease the 99.99% uptime SLA. We are responsible for meeting the 99.99% uptime SLA no matter what, so multiple-AZ deployments should only be considered in cases where it is truly appropriate. Multi-AZ deployments are available only on select AWS regions. Additional application servers For especially high-traffic sites we can also add additional application-only servers. These servers will contain just the application code; data storage services (SQL, Solr, Redis, etc.) are limited to the standard three. The cluster begins to look more like a standard N-Tier architecture at this point, with a horizontal line of web application servers in front of a 3 node (N\u0026#43;1) cluster of Galera database servers. Speak to your sales representative about the costs associated with adding additional application servers. This configuration requires a separate setup from the default so advanced planning is required. SFTP accounts In addition to SSH accounts, SFTP accounts can be created with a custom user/password that are restricted to certain directories. These directories must be one of the writeable mounts (or rather, there’s no point assigning them to the read-only code directory). There is no cost for this configuration, and it can be requested at any time via a support ticket. SSH public key based authentication is also supported on the SFTP account. Error handling On Platform.sh Professional, incoming requests are held at the edge router temporarily during a deploy. That allows a site to simply “respond slowly” rather than be offline during a deploy, provided the deploy time is short (a few seconds). On Platform.sh Dedicated, incoming requests are not held during deploy and receive a 503 error. As the Dedicated Cluster is almost always fronted by a CDN, the CDN will continue to serve cached pages during the few seconds of deploy, so for the vast majority of users there is no downtime or even slowdown. If a request does pass the CDN during a deploy that is not counted as downtime covered by our Service Level Agreement. By default, Platform.sh will serve generic Platform.sh-branded error pages for errors generated before a request reaches the application. (500 errors, some 400 errors, etc.) Alternatively you may provide a static error page for each desired error code via a ticket for us to configure with the CDN. This file may be any static HTML file but is limited to 64 KB in size. IP restrictions Platform.sh supports project-level IP restrictions (allow/deny) and HTTP Basic authentication. These may be configured through the Development Environment and will be automatically replicated from the production and staging branches to the production and staging environments, respectively. Note: Changing access control will trigger a new deploy of the current environment. However, the changes will not propagate to child environments until they are manually redeployed. Remote logging Platform.sh Dedicated supports sending logs to a remote logging service such as Loggly, Papertrail, or Logz.io using the rsyslog service. This is an optional feature and you can request that it be enabled via a support ticket. Once enabled and configured your application can direct log output to the system syslog facility and it will be replicated to the remote service you have configured. When contacting support to enable rsyslog, you will need: The name of the remote logging service you will be using. The message template format used by your logging service. The specific log files you want forwarded to your logging service. There is no cost for this functionality.",
        "section": "Platform.sh Dedicated cluster specifications",
        "subsections": " Staging environments Multiple applications Multiple-AZ Additional application servers SFTP accounts Error handling IP restrictions Remote logging  ",
        "image": "",
        "url": "/dedicated/architecture/options.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "0f5b183b69473bc449924aeb12dd1b19",
        "title": "PHP-FPM sizing",
        "description": "",
        "text": " Platform.sh uses a heuristic to automatically set the number of workers of the PHP-FPM runtime based on the memory available in the container. This heuristic is based on assumptions about the memory necessary on average to process a request. You can tweak those assumptions if your application will typically use considerably more or less memory. Note that this value is independent of the memory_limit set in php.ini, which is the maximum amount of memory a single PHP process can use before it is automatically terminated. These estimates are used only for determining the number of PHP-FPM workers to start. The heuristic The heuristic is based on three input parameters: The memory available for the container, which depends on the size of the container (S, M, L), The memory that an average request is expected to require, The memory that should be reserved for things that are not specific to a request (memory for nginx, the op-code cache, some OS page cache, etc.) The number of workers is calculated as: Defaults The default assumptions are: 45 MB for the average per-request memory 70 MB for the reserved memory These are deliberately conservative values that should allow most programs to run without modification. You can change them by using the runtime.sizing_hints.reserved_memory and runtime.sizing_hints.request_memory in your .platform.app.yaml. For example, if your application consumes on average 110 MB of memory for a request use: runtime:sizing_hints:request_memory:110The request_memory has a lower limit of 10 MB while reserved_memory has a lower limit of 70 MB. Values lower than those will be replaced with those minimums. You can check the maximum number of PHP-FPM workers by opening an SSH session and running following command (example for PHP 7.x): grep -e \u0026#39;^pm.max_children\u0026#39; /etc/php/*/fpm/php-fpm.conf pm.max_children = 2 Measuring PHP worker memory usage To see how much memory your PHP worker processes are using, you can open an SSH session and look at the PHP access log: less /var/log/php.access.log In the fifth column, you’ll see the peak memory usage that occurred while each request was handled. The peak usage will probably vary between requests, but in order to avoid the severe performance costs that come from swapping, your size hint should be somewhere between the average and worst case memory usages that you observe. A good way to determine an optimal request memory is with the following command: tail -n5000 /var/log/php.access.log | awk \u0026#39;{print $6}\u0026#39; | sort -n | uniq -c This will print out a table of how many requests used how much memory, in KB, for the last 5000 requests that reached PHP-FPM. (On an especially busy site you may need to increase that number). As an example, consider the following output: 1 4800 2048 948 4096 785 6144 584 8192 889 10240 492 12288 196 14336 68 16384 2 18432 1 22528 6 131072 This indicates that the majority of requests (4800) used 2048 KB of memory. In this case that’s likely application caching at work. Most requests used up to around 10 MB of memory, while a few used as much as 18 MB and a very very few (6 requests) peaked at 131 MB. (In this example those are probably cache clears.) A conservative approach would suggest an average request memory of 16 MB should be sufficient. A more aggressive stance would suggest 10 MB. The more aggressive approach would potentially allow for more concurrent requests at the risk of some requests needing to use swap memory, thus slowing them down. The web agency Pixelant has also published a log analyzer tool for Platform.sh that offers a better visualization of access logs to determine how much memory requests are using on average. It also offers additional insights into the operation of your site that can suggest places to further optimize your configuration and provide guidance on when it’s time to increase your plan size. (Please note that this tool is maintained by a 3rd party, not by Platform.sh.) Note: If you are running on PHP 5.x then don’t bother adjusting the worker memory usage until you upgrade to PHP 7.x. PHP 7 is vastly more memory efficient than PHP 5 and you will likely need less than half as much memory per process under PHP 7.",
        "section": "PHP",
        "subsections": " The heuristic Defaults Measuring PHP worker memory usage  ",
        "image": "",
        "url": "/languages/php/fpm.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "14e33caadcebae385e008c0ec8905e49",
        "title": "Region migration",
        "description": "",
        "text": " Platform.sh is available in a number of different Regions. Each region is a self-contained copy of Platform.sh in a single datacenter. When you first create a project you can specify which region it should be in. Platform.sh does not offer an automated way to migrate a project from one region to another after it is created. However, the process to do so manually is fairly straightforward and scriptable. Why migrate between regions? Different datacenters are located in different geographic areas, and you may want to keep your site physically close to the bulk of your user base for reduced latency. Only selected regions offer European Sovereign hosting. If you created a project in a non-Sovereign region you may need to migrate to a Sovereign region. Some regions are running older versions of the Platform.sh orchestration system that offers fewer features. In particular, the US-1 and EU-1 regions do not offer XL and 2XL plans, self-terminating builds in case of a build process that runs too long, or distributing environments across different grid hosts. If you are on one of those regions and desire those features you will need to migrate to the newer US-2 or EU-2 regions. Scripted migration process Although not directly supported by Platform.sh, an agency named Contextual Code has built a bash migration script to automate most common configurations. If your site is a typical single application with a single SQL database, the script should take care of most of the process for you. (If you have additional backend systems you may need to do some additional work manually, as documented below.) 0. Preparation Plan a timeframe in which to handle the migration. You will want to avoid developing any new code during that period (as your Git repository will change) and be prepared for a brief site outage when you migrate. You are essentially relaunching the site, just with the same host as previously. Plan accordingly. Set your DNS Time-to-Live as low as possible. 1. Create a new project Create a new Platform.sh Project in the desired region. You can initially create it as a Development project and change the plan size immediately before switching over or go ahead and use the desired size from the beginning. When the system asks if you want to use an existing template or provide your own code, select provide your own code. However, you do not need to push any code to it yet. Note the new project’s ID from the URL. 2. Download and invoke the script Download the Platform Migration tool from Contextual Code. The README file explains the step it uses in more detail. With a typical site it will carry you through the full process of transfering code, data, configuration, and the domain name to your new project. Note: You will still need to update your DNS record with your registrar to point to the new project when you are ready to go live. Also be aware that the script does not transfer external integrations — such as health notifications or 3rd party Git provider integrations — so you will need to re-enable those manually. 3. Remove the old project Once the new project is running and the DNS has fully propagated you can delete the old project. Manual migration process 0. Preparation Plan a timeframe in which to handle the migration. You will want to avoid developing any new code during that period (as your Git repository will change) and be prepared for a brief site outage when you migrate. You are essentially relaunching the site, just with the same host as previously. Plan accordingly. Set your DNS Time-to-Live as low as possible. 1. Create and populate a new project Create a new Platform.sh project in the desired region. You can initially create it as a Development project and change the plan size immediately before switching over or go ahead and use the desired size from the beginning. When the system asks if you want to use an existing template or provide your own code, select provide your own code. Make a Git clone of your existing project. Then add a Git remote to the new project, using the Git URL shown in the management console. Push the code for at least your master branch to the new project. (You can also transfer other branches if desired. That’s optional.) Alternatively, if you are using a 3rd party Git repository (GitHub, BitBucket, GitLab, etc.), you can add an integration to the new project just as you did the old one. It will automatically mirror your 3rd party repository exactly the same way as the old project and you won’t need to update it manually. Copy your existing user files on the old project to your computer using rsync. See the exporting page for details. Then use rsync to copy them to the same directory on the new project. See the migrating page for details. Export your database from the old project and import it into the new project. Again, see the exporting and migration pages, as well as the instructions for your specific database services. Re-enter any project or environment variables you’ve defined on your old project in your new project. Add any users to your new project that you want to continue to have access. If you have any 3rd party integrations active, especially the Health Notification checks, add them to the new project. 2. Maintain the mirror Most sites have generated data in Solr, Elasticsearch, or similar that needs to be regenerated. Take whatever steps are needed to reindex such systems. That may simply be allowing cron to run for a while, or your system may have a command to reindex everything faster. That will vary by your application. You can also periodically re-sync your data. For rsync the process should be quite fast as long as you maintain your local copy of it, as rsync will transfer only content that has changed. For the database it may take longer depending on the size of your data. Depending on your site’s size and your schedule, you can have the old and new project overlapping for only an hour or two or several weeks. That’s up to you. Be sure to verify that the new site is working as desired before continuing. 3. Launch the new site Once your new project is on the right production plan size you can cut over to it. Add your domain name(s) to your new project. If you have a custom SSL certificate you will need to add that at the same time. (Because the projects are in separate regions it’s safe to add the domain name to both at the same time, which reduces apparent downtime.) If possible, put your site into read-only mode or maintenance mode. Then do one final data sync (code and database) to ensure the new project starts with all fo the data from the old one. Once the domain is set, update your DNS provider’s records to point to the new site. Run platform environment:info edge_hostname -p \u0026lt;NEW_PROJECT_ID\u0026gt; to get the domain name to point the CNAME at. It may take some time for the DNS change and SSL change to propagate. Until it does, some browsers may not see the new site or may get an SSL mismatch error. In most cases that will resolve itself in 1-3 hours. 4. Remove the old project Once the new project is running and the DNS has fully propagated you can delete the old project.",
        "section": "Tutorials",
        "subsections": " Why migrate between regions? Scripted migration process  0. Preparation 1. Create a new project 2. Download and invoke the script 3. Remove the old project   Manual migration process  0. Preparation 1. Create and populate a new project 2. Maintain the mirror 3. Launch the new site 4. Remove the old project    ",
        "image": "",
        "url": "/tutorials/region-migration.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "aab5900a5b02608c3630b6f27a0692aa",
        "title": "Relationships",
        "description": "",
        "text": " The relationships block defines how services are mapped within your application. By default, your application may not talk to any other container within a project. To access another container you must define a relationship to it. Each relationship has an arbitrary name, although by convention the primary SQL database of an application (if any) is usually database. That is the name by which the relationship will be known in the PLATFORM_RELATIONSHIPS environment variable, which will include credentials for accessing the service. The relationship is specified in the form service_name:endpoint_name. The “service name” is the name of the service given in .platform/services.yaml, or the name of another application in the same project (that is, the name property of the .platform.app.yaml file for that application). The “endpoint” is the exposed functionality of the service to use. For most services the endpoint is the same as the service type. On a few services (i.e. MariaDB and Solr ) you can define additional explicit endpoints for multiple databases and cores in your services.yaml file, and you will need to match those endpoints in your relationships. See the Services documentation for a full list of currently supported service types and service endpoints. How do I get access to multiple services? In the following example, there is a single MySQL service named mysqldb offering two databases, a Redis cache service named rediscache, and an Elasticsearch service named searchserver. relationships:database:\u0026#39;mysqldb:db1\u0026#39;database2:\u0026#39;mysqldb:db2\u0026#39;cache:\u0026#39;rediscache:redis\u0026#39;search:\u0026#39;searchserver:elasticsearch\u0026#39;",
        "section": "Configure your application",
        "subsections": " How do I get access to multiple services?  ",
        "image": "",
        "url": "/configuration/app/relationships.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "974f0e315c9b3c74f5a0f411c8786a35",
        "title": "Troubleshooting",
        "description": "",
        "text": " Now your application has been taken live! As mentioned before, it may take a little time for the DNS to fully propagate, depending on the registrar. Otherwise, your domain should now point to the master production environment of your project. Additional information If through the following steps your project did not successfully configure to your domain, you can consult the Troubleshooting guide here: Going Live: Troubleshooting Consider using the Fastly CDN for increased performance and more control over caching: Going Live: Fastly Consider using Cloudflare’s TLS/SSL service to secure your site via HTTPS when using a CDN: Going Live: Cloudflare Back",
        "section": "Getting started",
        "subsections": "   Additional information    ",
        "image": "",
        "url": "/gettingstarted/next-steps/going-live/troubleshooting.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "c7ae3b87756f3ffdedf3bedc6693017b",
        "title": "Using Elasticsearch with Drupal 8.x",
        "description": "",
        "text": " Requirements Add the Drupal modules You will need to add the Search API and Elasticsearch Connector modules to your project. If you are using composer, the easiest way to add them is to simply run: $ composer require drupal/search_api drupal/elasticsearch_connector And then commit the changes to composer.json and composer.lock. Add an Elasticsearch service First you need to create an Elasticsearch service. In your .platform/services.yaml file, add or uncomment the following: elasticsearch:type:elasticsearch:6.5disk:2014The above definition defines a single Elasticsearch 6.5 server. Because Elasticsearch defines additional indexes dynamically there is no need to define custom endpoints. Expose the Elasticsearch service to your application In your .platform.app.yaml file, you now need to open a connection to the new Elasticsearch service. Under the relationships section, add or uncomment the following: relationships:elasticsearch:\u0026#39;elasticsearch:elasticsearch\u0026#39;Configuration Because Drupal defines connection information via the Configuration Management system, you will need to first define an Elasticsearch “Cluster” at admin/config/search/elasticsearch-connector. Note the “machine name” the server is given. Then, paste the following code snippet into your settings.platformsh.php file. Note: If you do not already have the Platform.sh Config Reader library installed and referenced at the top of the file, you will need to install it with composer require platformsh/config-reader and then add the following code before the block below: \u0026lt;?php $platformsh = new if (!$platformsh-\u0026gt;inRuntime()) { return; } Edit the value of $relationship_name if you are using a different relationship. Edit the value of $es_server_name to match the machine name of your cluster in Drupal. \u0026lt;?php // Update these values to the relationship name (from .platform.app.yaml) // and the machine name of the server from your Drupal configuration. $relationship_name = \u0026#39;elasticsearch\u0026#39;; $es_cluster_name = \u0026#39;YOUR_CLUSTER_HERE\u0026#39;; if ($platformsh-\u0026gt;hasRelationship($relationship_name)) { $platformsh-\u0026gt;registerFormatter(\u0026#39;drupal-elastic\u0026#39;, function($creds) { return sprintf(\u0026#39;http://%s:%s\u0026#39;, $creds[\u0026#39;host\u0026#39;], $creds[\u0026#39;port\u0026#39;]); }); // Set the connector configuration to the appropriate value, as defined by the formatter above. $config[\u0026#39;elasticsearch_connector.cluster.\u0026#39; . $es_cluster_name][\u0026#39;url\u0026#39;] = $platformsh-\u0026gt;formattedCredentials($relationship_name, \u0026#39;drupal-elastic\u0026#39;); } Commit that code and push. The specified cluster will now always point to the Elasticsearch service. Then configure Search API as normal.",
        "section": "Getting Started",
        "subsections": " Requirements  Add the Drupal modules Add an Elasticsearch service Expose the Elasticsearch service to your application   Configuration  ",
        "image": "",
        "url": "/frameworks/drupal8/elasticsearch.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "0b747495f2ea60c887be32512df5f482",
        "title": "Variables",
        "description": "",
        "text": " Platform.sh allows a high degree of control over both the build process and the runtime environment of a project. Part of that control comes in the form of variables that are set independently of the project’s code base but available either at build or runtime for your code to leverage. Platform.sh also exposes additional information to your application that way, including information like database credentials, the host or port it can use, and so forth. Type Definer Scope Inheritance Build Runtime Application Application Application n/a Yes Yes Project User Project n/a Yes Yes Environment User Environment Optional No Yes Platform.sh Pre-defined Environment n/a Some Yes All of those may be simple strings or base64-encoded JSON-serialized values. In case of name collisions, Platform.sh-provided values override user-provided environment variables, which override user-provided project-level variables, which override application-provided variables. (That is, lower items in the list above take precedence.) Types Application-provided variables Variables may be set in code , using the .platform.app.yaml file. These values of course will be the same across all environments and present in the Git repository, which makes them a poor fit for API keys and such. This capability is mainly to define values that an application expects via an environment variable that should be consistent across all environments. For example, the PHP Symfony framework has a SYMFONY_ENV property that users may wish to set to prod on all environments to ensure a consistent build, or it may be used to set PHP configuration values . Application-provided variables are available at both build time and runtime. Project variables Project variables are defined by the user and bound to a whole project. They are available both at build time (and therefore from a build hook) and at runtime, and are the same for all environments in the project. New project variables can be added using the CLI. For example, the following command creates a project-level variable “foo” with the value “bar”: platform variable:create --level project --name foo --value bar Project variables are a good place to store secret information that is needed at build time, such as credentials for a private 3rd party code repository. By default, project variables will be available at both build time and runtime. You can suppress one or the other with the --no-visible-build and --no-visible-runtime flags, such as if you want to hide certain credentials from runtime entirely. For example, the following (silly) example will define a project variable but hide it from both build and runtime: platform variable:create --level project --name foo --value bar --visible-build false --visible-runtime false Naturally in practice you’ll want to use only one or the other, or allow the variable to be visible in both cases. Project variables may also be marked --sensitive true. That flag will mark the variable to not be readable through the management console once it is set. That makes it somewhat more private as requests through the Platform.sh CLI will not be able to view the variable. However, it will still be readable from within the application container like any other variable. Environment variables Environment-level variables can also be set through the management console , or using the CLI. Environment variables are bound to a specific environment or branch. An environment will also inherit variables from its parent environment, unless it has a variable defined with the same name. That allows you to define your development variables only once, and use them on all the child environments. For instance, to create an environment variable “foo” with the value “bar” on the currently checked out environment/branch, run: $ platform variable:create --level environment --name foo --value bar That will set a variable on the currently active environment (that is, the branch you have checked out). To set a variable on a different environment include the -e switch to specify the environment name. There are two additional flags available on environment variables: --inheritable and --sensitive. Setting --inheritable false will cause the variable to not be inherited by child environments. That is useful for setting production-only values on the master branch, and allowing all other environments to use a project-level variable of the same name. Setting --sensitive true flag will mark the variable to not be readable through the management console once it is set. That makes it somewhat more private as requests through the Platform.sh CLI will not be able to view the variable. However, it will still be readable from within the application container like any other variable. For example, the following command will allow you to set a PayPal secret value on the master branch only; other environments will not inherit it and either get a project variable of the same name if it exists or no value at all. It will also not be readable through the API. $ platform variable:create --name paypal_id --inheritable false --sensitive true If you omit the variable --value from the command line as above, you will be prompted to enter the value interactively. Changing an environment variable will cause that environment to be redeployed so that it gets the new value. However, it will not redeploy any child environments. If you want those to get the new value you will need to redeploy them yourself. Environment variables are a good place to store values that apply only on Platform.sh and not on your local development environment. This includes API credentials for 3rd party services, mode settings if your application has a separate “Dev” and “Prod” runtime toggle, etc. Platform.sh-provided variables Platform.sh also provides a series of variables by default. These inform an application about its runtime configuration. The most important of these is relationship information, which tells the application how to connect to databases and other services defined in services.yaml. They are always prefixed with PLATFORM_* to differentiate them from user-provided values. The following variables are only available at build time, and may be used in a build hook: PLATFORM_OUTPUT_DIR: The output directory for compiled languages at build time. Will be equivalent to PLATFORM_APP_DIR in most cases. The following variables are available at both runtime and at build time, and may be used in a build hook: PLATFORM_APP_DIR: The absolute path to the application directory. PLATFORM_APPLICATION: A base64-encoded JSON object that describes the application. It maps the content of the .platform.app.yaml that you have in Git and it has a few subkeys. PLATFORM_APPLICATION_NAME: The name of the application, as configured in the .platform.app.yaml file. PLATFORM_PROJECT: The ID of the project. PLATFORM_TREE_ID: The ID of the tree the application was built from. It’s essentially the SHA hash of the tree in Git. If you need a unique ID for each build for whatever reason this is the value you should use. PLATFORM_VARIABLES: A base64-encoded JSON object which keys are variables names and values are variable values (see below). Note that the values available in this structure may vary between build and runtime depending on the variable type as described above. PLATFORM_PROJECT_ENTROPY: A random value created when the project is first created, which is then stable throughout the project’s life. This can be used for Drupal hash salt, Symfony secret, or other similar values in other frameworks. The following variables exist only at runtime. If used in a build hook they will evaluate to an empty string like any other unset variable: PLATFORM_BRANCH: The name of the Git branch. PLATFORM_DOCUMENT_ROOT: The absolute path to the web document root, if applicable. PLATFORM_ENVIRONMENT: The name of the environment generated by the name of the Git branch. PLATFORM_SMTP_HOST: The SMTP host that email messages should be sent through. This value will be empty if mail is disabled for the current environment. PLATFORM_RELATIONSHIPS: A base64-encoded JSON object whose keys are the relationship name and the values are arrays of relationship endpoint definitions. See the documentation for each Service for details on each service type’s schema. PLATFORM_ROUTES: A base64-encoded JSON object that describes the routes that you defined in the environment. It maps the content of the .platform/routes.yaml file. On a Dedicated instance, the following additional variables are available at runtime only: PLATFORM_MODE: Set to enterprise in an Dedicated environment, both production and staging. Note that an Enterprise support plan doesn’t always imply a Dedicated production, but Dedicated production always implies an Enterprise support plan. PLATFORM_CLUSTER: Set to the cluster ID. PLATFORM_PROJECT: Set to the document root. This is typically the same as your cluster name for the production environment, while staging will have _stg or similar appended. Since values can change over time, the best thing is to inspect the variable at runtime then use it to configure your application. For example: echo $PLATFORM_RELATIONSHIPS | base64 --decode | json_pp {  database : [ {  host :  database.internal ,  ip :  246.0.97.91 ,  password :   ,  path :  main ,  port : 3306,  query : {  is_master : true },  scheme :  mysql ,  username :  user  } ],  redis : [ {  host :  redis.internal ,  ip :  246.0.97.88 ,  port : 6379,  scheme :  redis  } ] } Accessing variables You can get a list of all variables defined on a given environment either via the management console or using the CLI: $ platform variables \u0026#43;---------\u0026#43;-------\u0026#43;-----------\u0026#43;------\u0026#43; | ID | Value | Inherited | JSON | \u0026#43;---------\u0026#43;-------\u0026#43;-----------\u0026#43;------\u0026#43; | env:FOO | bar | No | No | \u0026#43;---------\u0026#43;-------\u0026#43;-----------\u0026#43;------\u0026#43; At build time Only Project variables are available at build time. They will be listed together in a single JSON array and exposed in the $PLATFORM_VARIABLES Unix environment variable. echo $PLATFORM_VARIABLES | base64 --decode { my_var :  this is a value } They can also be accessed from within a non-shell script via the language’s standard way of accessing environment variables. For instance, in PHP you would use getenv(\u0026#39;PLATFORM_VARIABLES\u0026#39;). Remember that in some cases they may be base64 JSON strings and will need to be unpacked. To do so from the shell, for instance, you would do: echo $PLATFORM_VARIABLES | base64 --decode { myvar :  this is a value } See below for how to expose a project variable as its own Unix environment variable. At runtime In a running container, which includes the deploy hook, your Project variables, Environment variables, and Platform.sh-provided variables are all exposed as Unix environment variables and can be accessed by your application through your language’s standard way of accessing environment variables. Platform.sh-defined variables will be exposed directly with the names listed above. Project and environment variables will be merged together into a single JSON array and exposed in the $PLATFORM_VARIABLES environment variable. In case of a matching name, an environment variable will override a variable of the same name in a parent environment, and both will override a project variable. For example, suppose we have the following variables defined: $ platform variables -e master Variables on the project Example (abcdef123456), environment master: \u0026#43;----------------\u0026#43;-------------\u0026#43;--------\u0026#43; | Name | Level | Value | \u0026#43;----------------\u0026#43;-------------\u0026#43;--------\u0026#43; | system_name | project | Spiffy | | system_version | project | 1.5 | | api_key | environment | abc123 | \u0026#43;----------------\u0026#43;-------------\u0026#43;--------\u0026#43; And the following variables defined on the branch feature-x, a child environment (and branch of) master: $ platform variables -e master Variables on the project Example (abcdef123456), environment feature-x: \u0026#43;----------------\u0026#43;-------------\u0026#43;--------\u0026#43; | Name | Level | Value | \u0026#43;----------------\u0026#43;-------------\u0026#43;--------\u0026#43; | system_name | project | Spiffy | | system_version | project | 1.5 | | api_key | environment | def456 | | system_version | environment | 1.7 | | debug_mode | environment | 1 | \u0026#43;----------------\u0026#43;-------------\u0026#43;--------\u0026#43; In this case, on the master environment $PLATFORM_VARIABLES would look like this: echo $PLATFORM_VARIABLES | base64 --decode | json_pp {  system_name :  Spiffy ,  system_version :  1.5 ,  api_key :  abc123  } While the same command on the feature-x branch would produce: {  system_name :  Spiffy ,  system_version :  1.7 ,  api_key :  def456 ,  debug_mode :  1  } In your application Check the individual documentation pages for accessing environment variables for your given application language. Shell: the jq utility PHP: the getenv() function Node.js: the process.env object Python: the os.environ object Ruby: the ENV accessor Java: the java.lang.System accessor Shell PHP Python Node.js Ruby Java export DB_USER= $(echo  $PLATFORM_RELATIONSHIPS  | base64 --decode | jq -r \u0026#39;.database[0].username\u0026#39;)  \u0026lt;?php // A simple variable. $projectId = getenv(\u0026#39;PLATFORM_PROJECT\u0026#39;); // A JSON-encoded value. $variables = json_decode(base64_decode(getenv(\u0026#39;PLATFORM_VARIABLES\u0026#39;)), TRUE); import os import json import base64 // A simple variable. project_id = os.getenv(\u0026#39;PLATFORM_PROJECT\u0026#39;) // A JSON-encoded value. variables = json.loads(base64.b64decode(os.getenv(\u0026#39;PLATFORM_VARIABLES\u0026#39;)).decode(\u0026#39;utf-8\u0026#39;)) // Utility to assist in decoding a packed JSON variable. function read_base64_json(varName) { try { return JSON.parse(new Buffer(process.env[varName], \u0026#39;base64\u0026#39;).toString()); } catch (err) { throw new Error(`no ${varName}environment variable`); } }; // A simple variable. let projectId = process.env.PLATFORM_PROJECT; // A JSON-encoded value. let variables = read_base64_json(\u0026#39;PLATFORM_VARIABLES\u0026#39;); // A simple variable. project_id = ENV[ PLATFORM_PROJECT ] || nil // A JSON-encoded value. variables = JSON.parse(Base64.decode64(ENV[ PLATFORM_VARIABLES ])) import com.fasterxml.jackson.databind.ObjectMapper; import java.io.IOException; import java.util.Base64; import java.util.Map; import static java.lang.System.getenv; import static java.util.Base64.getDecoder; public class App { public static void main(String[] args) throws IOException { // A simple variable. final String project = getenv( PLATFORM_PROJECT ); // A JSON-encoded value. ObjectMapper mapper = new ObjectMapper(); final Map\u0026lt;String, Object\u0026gt; variables = mapper.readValue( String.valueOf(getDecoder().decode(getenv( PLATFORM_VARIABLES ))), Map.class); } } Variable prefixes Certain variable name prefixes have special meaning. A few of these are defined by Platform.sh and are built-in. Others are simply available as a convention for your own application code to follow. Top-level environment variables By default, project and environment variables are only added as part of the $PLATFORM_VARIABLES Unix environment variable. However, you can also expose a variable as its own Unix environment variable by giving it the prefix env:. For example, the variable env:foo will create a Unix environment variable called FOO. (Note the automatic upper-casing.) $ platform variable:create --name env:foo --value bar With PHP, you can then access that variable with getenv(\u0026#39;FOO\u0026#39;). PHP-specific variables Any variable with the prefix php: will also be added to the php.ini configuration of all PHP-based application containers. For example, an environment variable named php:display_errors with value On is equivalent to placing the following in php.ini: display_errors = On This feature is primarily useful to override debug configuration on development environments, such as enabling errors or configuring the XDebug extension. For applying a configuration setting to all environments, or to vary them between different PHP containers in the same project, specify the variables in the .platform.app.yaml file for your application. See the PHP configuration page for more information. Drupal-specific variables As a convention, our provided Drupal template code will automatically map variables to Drupal’s configuration system. The logic varies slightly depending on the Drupal version. On Drupal 7 , any variable that begins with drupal: will be mapped to the global $conf array, which overrides Drupal’s variable_get() system. For instance, to force a site name from the Platform.sh variables (say to set it “This is a Dev site”) you would set the drupal:site_name variable. On Drupal 8 , any variable that begins with drupal: will be mapped to the global $settings array. That is intended for very low-level configuration. Also on Drupal 8, any variable that begins with d8config: will be mapped to the global $config array, which is useful for overriding drupal’s exportable configuration system. The variable name will need to contain two colons, one for d8config: and one for the name of the configuration object to override. For example, a variable named d8config:system.site:name will override the name property of the system.site configuration object. As the above logic is defined in a file in your Git repository you are free to change it if desired. The same behavior can also be easily implemented for any other application or framework. Shell variables You can also provide a .environment file as part of your application, in your application root (as a sibling of your .platform.app.yaml file, or files in the case of a multi-app configuration). That file will be sourced as a bash script when the container starts and on all SSH logins. It can be used to set any environment variables directly, such as the PATH variable. For example, the following .environment file will allow any executable installed using Composer as part of a project to be run regardless of the current directory: export PATH=/app/vendor/bin:$PATH Note that the file is sourced after all other environment variables above are defined, so they will be available to the script. That also means the .environment script has the “last word” on environment variable values and can override anything it wants to. How can I have a script behave differently on a dedicated cluster than on development? The following sample shell script will output a different value on the Dedicated cluster than the Development environment. if [  $PLATFORM_MODE  =  enterprise  ] ; then echo  Hello from the Enterprise  else echo  We\u0026#39;re on Development  fi How can I have a script behave differently on Production and Staging? In most Enterprise configurations the production branch is named production (whereas it is always master on Platform.sh Professional). The following test therefore should work in almost all cases: if [  $PLATFORM_MODE  =  enterprise  ] ; then if [  $PLATFORM_BRANCH  =  production  ] ; then echo  This is live on production  else echo  This is on staging  fi else echo  We\u0026#39;re on Development  fi",
        "section": "Development",
        "subsections": " Types  Application-provided variables Project variables Environment variables Platform.sh-provided variables   Accessing variables  At build time At runtime In your application   Variable prefixes  Top-level environment variables PHP-specific variables Drupal-specific variables   Shell variables How can I have a script behave differently on a dedicated cluster than on development? How can I have a script behave differently on Production and Staging?  ",
        "image": "",
        "url": "/development/variables.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "6fc12b48f3dc6db6e2ac5ef4d56e3d80",
        "title": "Access",
        "description": "",
        "text": " The access key in .platform.app.yaml defines the user roles who can log in via SSH to the environments they have permission to access. The specified role is a minimum; anyone with an access level of this role or higher can access the container via SSH. Possible values are admin, contributor, and viewer. The default is contributor. How do I restrict SSH access only to project administrators? The following block in .platform.app.yaml will restrict SSH access to just those users with “admin” privileges on the project or on the specific deployed environment. access:ssh:admin",
        "section": "Configure your application",
        "subsections": " How do I restrict SSH access only to project administrators?  ",
        "image": "",
        "url": "/configuration/app/access.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "dd81c9a0bdf8da0d4509b1a424a06ee4",
        "title": "Accessing your site",
        "description": "",
        "text": " Once you have an environment running, there are many ways to access it to perform needed tasks. The most obvious of course is to view it in a web browser; the available URLs are shown in the Platform.sh management console and on the command line after every Git push. By design, the only way to deploy new code is to push to the corresponding branch. That ensures a consistent, repeatable, auditable application instance at all times. Visiting the site on the web The web URL(s) for the site are listed in the management console under “Access site”. They can also be found on the command line, using the Platform.sh CLI : platform url Generally there will be two URLs created per route in your routes.yaml file: One HTTPS and one HTTP route that just redirects to HTTPS. If you are using the {all} placeholder in your routes.yaml file then there will be more, depending on how many domains you have configured in your project. Accessing the application with SSH Most interactions with Platform.sh require SSH key authentication, and you will need to set up your SSH keys before working on a site. Once that’s done, you can easily access the command line on your application over SSH. To log in to the environment that corresponds to your current branch, simply type: platform ssh To log in to some other environment, use the -e flag to specify the environment. The application container is a fully working Linux environment using the bash shell. Most of the system consists of a read-only file system (either the underlying container image or your built application image), so you cannot edit code live, but otherwise the full system is available to read and peruse. Any file mounts you have declared in your .platform.app.yaml will be writable. Additionally, you will be logged in as the same user that the web server runs as; that means you needn’t worry about the common problem of editing a file from the command line and from your application resulting in inconsistent and broken file ownership and permissions. Uploading and downloading files The writable static files in an application - including uploads, temporary and private files - are stored in mounts . The Platform.sh CLI can list mounts inside an application: $ platform mounts Mounts in the app drupal (environment master): \u0026#43;-------------------------\u0026#43;----------------------\u0026#43; | Path | Definition | \u0026#43;-------------------------\u0026#43;----------------------\u0026#43; | web/sites/default/files | shared:files/files | | private | shared:files/private | | tmp | shared:files/tmp | \u0026#43;-------------------------\u0026#43;----------------------\u0026#43; The CLI also helps transferring files to and from a mount, using the mount:upload and mount:download commands. These commands use the rsync utility, which in turn uses SSH. For example, to download files from the ‘private’ mount: $ platform mount:download --mount private --target ./private This will add, replace, and delete files in the local directory \u0026#39;private\u0026#39;. Are you sure you want to continue? [Y/n] Downloading files from the remote mount /app/private to /Users/alice/Projects/foo/private receiving file list ... done sent 16 bytes received 3.73K bytes 2.50K bytes/sec total size is 1.77M speedup is 471.78 time: 0.91s The download completed successfully. Uploading files to a mount is similar: $ platform mount:upload --mount private --source ./private This will add, replace, and delete files in the remote mount \u0026#39;private\u0026#39;. Are you sure you want to continue? [Y/n] Uploading files from /Users/alice/Projects/foo/private to the remote mount /app/private building file list ... done sent 2.35K bytes received 20 bytes 1.58K bytes/sec total size is 1.77M speedup is 745.09 time: 0.72s The upload completed successfully. Using SSH clients Many applications and protocols run on top of SSH, including SFTP, scp, and rsync. To obtain the SSH connection details for the environment either copy them out of the Platform.sh management console (under the “Access site” dropdown) or run: platform ssh --pipe That will output the connection string for SSH, including the username and host for the current project and environment. It will look something like \u0026lt;project ID\u0026gt;-\u0026lt;environment ID\u0026gt;--app@ssh.us.platform.sh. The part before the @ is the username, the part after is the host. Enter both of those into your SSH/SFTP client. No password is necessary, but your client will need to have access to the SSH private key that corresponds to the public key on Platform.sh. SFTP SFTP is another way to upload and download files to and from a remote environment. There are many SFTP clients available for every operating system; use whichever one works for you. SCP SCP is a simple command-line utility to copy files to and from a remote environment. For example, this command: scp  $(platform ssh --pipe) :web/uploads/diagram.png . will copy the file named diagram.png in the web/uploads directory (relative to the application root) to the current local directory. Reversing the order of the parameters will copy files up to the Platform.sh environment. Consult the SCP documentation for other possible options. Rsync For copying files to and from a remote environment, rsync is the best tool available. It is a little more complicated to use than scp, but it can also be a lot more efficient, especially if you are simply updating files that are already partially copied. The Platform.sh CLI mount:upload and mount:download commands (described above) are helpful wrappers around rsync that make it a little easier to use. However, it is also possible to use rsync on its own, for example: rsync -az  $(platform ssh --pipe) :web/uploads/ ./uploads/ This command will copy all files in the web/uploads directory on the remote environment to the uploads directory locally. Note that rsync is very sensitive about trailing / characters, so that may change the meaning of a command. Consult the rsync documentation for more details. Also see our migrating and exporting guides for more examples using rsync.",
        "section": "Development",
        "subsections": " Visiting the site on the web Accessing the application with SSH Uploading and downloading files Using SSH clients  SFTP SCP Rsync    ",
        "image": "",
        "url": "/development/access-site.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "ed19d198473b5dec3ab9b92ca8dc136e",
        "title": "Backup and restore",
        "description": "",
        "text": " Backups vary by our offering and their retention is governed by our [data retention]({{\u0026lt; relref “/security/data-retention.md” \u0026gt;}}). This section details our Recovery Point Objective (RPO) and Recovery Time Objective (RTO) for our Platform.sh Professional and Platform.sh Enterprise offerings. Platform.sh Professional With Platform.sh Professional, we have enabled our clients to manage their own backup and restore functions. Please see the backup and restore page for details. RPO: User defined. The RPO is configured by our clients. RTO: Variable. Recovery time will depend upon the size of the data we are recovering Platform.sh Enterprise-Grid and Enterprise-Dedicated RPO: 6 hours. Platform.sh takes a backup of Platform.sh Enterprise environments every 6 hours. RTO: Variable. Recovery time will depend upon the size of the data we are recovering",
        "section": "Security and compliance",
        "subsections": " Platform.sh Professional Platform.sh Enterprise-Grid and Enterprise-Dedicated  ",
        "image": "",
        "url": "/security/backups.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "6c121dcf0b7b4eb3df9def89475e1528",
        "title": "Configure services",
        "description": "",
        "text": " In the previous step, you created a collection of empty configuration files that will allow the project to be deployed on Platform.sh. Now you will need to include information that will tell Platform.sh how you want your application to connect to its services in a .platform/services.yaml file. With the following project structure: . ├── .platform │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml └── \u0026lt; application code \u0026gt; An example .platform/services.yaml will look something like this: .platform/services.yaml # This file defines services you want available to your application.elasticsearch:type:elasticsearch:7.2disk:256size:Sinfluxdb:type:influxdb:1.7disk:256size:Skafka:type:kafka:2.2disk:512size:Smemcached:type:memcached:1.4size:Smongodb:type:mongodb:3.6disk:1024size:Smysql:type:mariadb:10.4disk:256size:Spostgresql:type:postgresql:11disk:256size:Srabbitmq:type:rabbitmq:3.7disk:256size:Sredis:type:redis:5.0size:Ssolr:type:solr:8.0disk:256size:Sconfiguration:cores:maincore:conf_dir:!archive solr-config endpoints:solr:core:maincore If your application does not use any services at this point then you can leave it blank, but it must exist in your repository to run on Platform.sh. If your application does use a database or other services, you can configure them with the following attributes: name: Provide a name for the service, so long as it is alphanumeric. If your application requires multiple services of the same type, make sure to give them different names so that your data from one service is never overwritten by another. type: This specifies the service type and its version using the format type:versionConsult the table below that lists all Platform.sh maintained services, along with their type and supported versions. The links will take you to each service’s dedicated page in the documentation. Service type Supported version Headless Chrome chrome-headless 73 Elasticsearch elasticsearch 6.5, 7.2 InfluxDB influxdb 1.2, 1.3, 1.7 Kafka kafka 2.1, 2.2, 2.3, 2.4 MariaDB mariadb 10.0, 10.1, 10.2, 10.3, 10.4 Memcached memcached 1.4, 1.5, 1.6 MongoDB mongodb 3.0, 3.2, 3.4, 3.6 Network Storage network-storage 1.0 Oracle MySQL oracle-mysql 5.7, 8.0 PostgreSQL postgresql 9.6, 10, 11, 12 RabbitMQ rabbitmq 3.5, 3.6, 3.7, 3.8 Redis redis 3.2, 4.0, 5.0 Solr solr 3.6, 4.1, 6.3, 6.6, 7.6, 7.7, 8.0, 8.4 Varnish varnish 5.6, 6.0 disk: The disk attribute configures the amount of persistent disk that will be allocated between all of your services. Projects by default are allocated 5 GB (5120 MB), and that space can be distributed across all of your services. Note that not all services require disk space. If you specify a disk attribute for a service that doesn’t use it, like Redis, you will receive an error when trying to push your changes. Note: Each language and framework may have additional attributes that you will need to include in .platform/services.yaml depending on the needs of your application. To find out what else you may need to include to configure your services, consult The Services documentation for Platform.sh The documentation goes into far more extensive detail of which attributes can also be included for service configuration, and should be used as your primary reference. Language-specific templates for Platform.sh Projects: Compare the .platform/services.yaml file from the simple template above to other templates when writing your own. Platform.sh provides managed services, and each service comes with considerable default configuration that you will not have to include yourself in .services.yaml. Next, you will next need to tell Platform.sh how to build and deploy your application using the .platform.app.yaml file. Back I\u0026#39;ve configured my services",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/service-configuration.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "400014aac1482ba72a62ecbb87b9c99e",
        "title": "MongoDB (Database service)",
        "description": "",
        "text": " Supported versions Grid Dedicated 3.0 3.2 3.4 3.6 None available Note: Downgrades of MongoDB are not supported. MongoDB will update its own datafiles to a new version automatically but cannot downgrade them. If you want to experiment with a later version without committing to it use a non-master environment. Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : {  cluster :  rjify4yjcwxaa-master-7rqtwti ,  host :  mongodb.internal ,  hostname :  ldh423mk2e7o6qto2syljqbg5u.mongodb.service._.eu-3.platformsh.site ,  ip :  169.254.117.167 ,  password :  main ,  path :  main ,  port : 27017,  query : {  is_master : true },  rel :  mongodb ,  scheme :  mongodb ,  service :  mongodb ,  type :  mongodb:3.6 ,  username :  main  } Usage example In your .platform/services.yaml: dbmongo:type:mongodb:3.6disk:512 The minimum disk size for MongoDB is 512 (MB). relationships:mongodatabase: dbmongo:mongodb  Note: You will need to use the mongodb type when defining the service # .platform/services.yamlservice_name:type:mongodb:versiondisk:256 and the endpoint mongodb when defining the relationship # .platform.app.yamlrelationships:relationship_name:“service_name:mongodb” Your service_name and relationship_name are defined by you, but we recommend making them distinct from each other. For PHP, in your .platform.app.yaml add: runtime:extensions:- mongodb(Before PHP 7, use mongo instead.) You can then use the service in a configuration file of your application with something like: Go Java Node.js PHP Python package examples import (  context   fmt  psh  github.com/platformsh/config-reader-go/v2  mongoPsh  github.com/platformsh/config-reader-go/v2/mongo   go.mongodb.org/mongo-driver/bson   go.mongodb.org/mongo-driver/mongo   go.mongodb.org/mongo-driver/mongo/options   time  ) func UsageExampleMongoDB() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // Get the credentials to connect to the Solr service. credentials, err := config.Credentials( mongodb ) checkErr(err) // Retrieve the formatted credentials for mongo-driver. formatted, err := mongoPsh.FormattedCredentials(credentials) checkErr(err) // Connect to MongoDB using the formatted credentials. ctx, _ := context.WithTimeout(context.Background(), 10*time.Second) client, err := mongo.Connect(ctx, options.Client().ApplyURI(formatted)) checkErr(err) // Create a new collection. collection := client.Database( main ).Collection( starwars ) // Clean up after ourselves. err = collection.Drop(context.Background()) checkErr(err) // Create an entry. res, err := collection.InsertOne(ctx, bson.M{ name :  Rey ,  occupation :  Jedi }) checkErr(err) id := res.InsertedID // Read it back. cursor, err := collection.Find(context.Background(), bson.M{ _id : id}) checkErr(err) var name string var occupation string for cursor.Next(context.Background()) { document := struct { Name string Occupation string }{} err := cursor.Decode(\u0026amp;document) checkErr(err) name = document.Name occupation = document.Occupation } return fmt.Sprintf( Found %s (%s) , name, occupation) } package sh.platform.languages.sample; import com.mongodb.MongoClient; import com.mongodb.client.MongoCollection; import com.mongodb.client.MongoDatabase; import org.bson.Document; import sh.platform.config.Config; import sh.platform.config.MongoDB; import java.util.function.Supplier; import static com.mongodb.client.model.Filters.eq; public class MongoDBSample implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); // The \u0026#39;database\u0026#39; relationship is generally the name of primary database of an application. // It could be anything, though, as in the case here here where it\u0026#39;s called  mongodb . MongoDB database = config.getCredential( mongodb , MongoDB::new); MongoClient mongoClient = database.get(); final MongoDatabase mongoDatabase = mongoClient.getDatabase(database.getDatabase()); MongoCollection\u0026lt;Document\u0026gt; collection = mongoDatabase.getCollection( scientist ); Document doc = new Document( name ,  Ada Lovelace ) .append( city ,  London ); collection.insertOne(doc); Document myDoc = collection.find(eq( _id , doc.get( _id ))).first(); logger.append(collection.deleteOne(eq( _id , doc.get( _id )))); return logger.toString(); } } const mongodb = require(\u0026#39;mongodb\u0026#39;); const config = require( platformsh-config ).config(); exports.usageExample = async function() { const credentials = config.credentials(\u0026#39;mongodb\u0026#39;); const MongoClient = mongodb.MongoClient; var client = await MongoClient.connect(config.formattedCredentials(\u0026#39;mongodb\u0026#39;, \u0026#39;mongodb\u0026#39;)); let db = client.db(credentials[ path ]); let collection = db.collection( startrek ); const documents = [ {\u0026#39;name\u0026#39;: \u0026#39;James Kirk\u0026#39;, \u0026#39;rank\u0026#39;: \u0026#39;Admiral\u0026#39;}, {\u0026#39;name\u0026#39;: \u0026#39;Jean-Luc Picard\u0026#39;, \u0026#39;rank\u0026#39;: \u0026#39;Captain\u0026#39;}, {\u0026#39;name\u0026#39;: \u0026#39;Benjamin Sisko\u0026#39;, \u0026#39;rank\u0026#39;: \u0026#39;Prophet\u0026#39;}, {\u0026#39;name\u0026#39;: \u0026#39;Katheryn Janeway\u0026#39;, \u0026#39;rank\u0026#39;: \u0026#39;Captain\u0026#39;}, ]; await collection.insert(documents, {w: 1}); let result = await collection.find({rank: Captain }).toArray(); let output = \u0026#39;\u0026#39;; output \u0026#43;= `\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;Rank\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt;`; Object.keys(result).forEach((key) =\u0026gt; { output \u0026#43;= }); output \u0026#43;= // Clean up after ourselves. collection.remove(); return output; }; \u0026lt;?php declare(strict_types=1); use use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // The \u0026#39;database\u0026#39; relationship is generally the name of primary database of an application. // It could be anything, though, as in the case here here where it\u0026#39;s called  mongodb . $credentials = $config-\u0026gt;credentials(\u0026#39;mongodb\u0026#39;); try { $server = sprintf(\u0026#39;%s://%s:%s@%s:%d/%s\u0026#39;, $credentials[\u0026#39;scheme\u0026#39;], $credentials[\u0026#39;username\u0026#39;], $credentials[\u0026#39;password\u0026#39;], $credentials[\u0026#39;host\u0026#39;], $credentials[\u0026#39;port\u0026#39;], $credentials[\u0026#39;path\u0026#39;] ); $client = new Client($server); $collection = $client-\u0026gt;main-\u0026gt;starwars; $result = $collection-\u0026gt;insertOne([ \u0026#39;name\u0026#39; =\u0026gt; \u0026#39;Rey\u0026#39;, \u0026#39;occupation\u0026#39; =\u0026gt; \u0026#39;Jedi\u0026#39;, ]); $id = $result-\u0026gt;getInsertedId(); $document = $collection-\u0026gt;findOne([ \u0026#39;_id\u0026#39; =\u0026gt; $id, ]); // Clean up after ourselves. $collection-\u0026gt;drop(); printf( Found %s (%s)\u0026lt;br $document-\u0026gt;name, $document-\u0026gt;occupation); } catch $e) { print $e-\u0026gt;getMessage(); } from pymongo import MongoClient from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # The \u0026#39;database\u0026#39; relationship is generally the name of primary SQL database of an application. # It could be anything, though, as in the case here here where it\u0026#39;s called  mongodb . credentials = config.credentials(\u0026#39;mongodb\u0026#39;) try: formatted = config.formatted_credentials(\u0026#39;mongodb\u0026#39;, \u0026#39;pymongo\u0026#39;) server = \u0026#39;{0}://{1}:{2}@{3}\u0026#39;.format( credentials[\u0026#39;scheme\u0026#39;], credentials[\u0026#39;username\u0026#39;], credentials[\u0026#39;password\u0026#39;], formatted ) client = MongoClient(server) collection = client.main.starwars post = {  name :  Rey ,  occupation :  Jedi  } post_id = collection.insert_one(post).inserted_id document = collection.find_one( { _id : post_id} ) # Clean up after ourselves. collection.drop() return \u0026#39;Found {0} ({1})\u0026lt;br /\u0026gt;\u0026#39;.format(document[\u0026#39;name\u0026#39;], document[\u0026#39;occupation\u0026#39;]) except Exception as e: return e Exporting data The most straightforward way to export data from a MongoDB database is to open an SSH tunnel to it and simply export the data directly using MongoDB’s tools. First, open an SSH tunnel with the Platform.sh CLI: platform tunnel:open That will open an SSH tunnel to all services on your current environment, and produce output something like the following: SSH tunnel opened on port 30000 to relationship: database SSH tunnel opened on port 30001 to relationship: redis The port may vary in your case. You will also need to obtain the user, password, and database name from the relationships array, as above. Then, simply connect to that port locally using mongodump (or your favorite MongoDB tools) to export all data in that server: mongodump --port 30000 -u main -p main --authenticationDatabase main --db main (If necessary, vary the -u, -p, --authenticationDatabase and --db flags.) As with any other shell command it can be piped to another command to compress the output or redirect it to a specific file. For further references please see the official mongodump documentation . Upgrading To upgrade to 3.6 from a version earlier than 3.4, you must successively upgrade major releases until you have upgraded to 3.4. For example, if you are running a 3.0 image, you must upgrade first to 3.2 and then upgrade to 3.4 before you can upgrade to 3.6. For more details on upgrading and how to handle potential application backward compatibility issues, please see Release Notes for MongoDB .",
        "section": "Configure services",
        "subsections": " Supported versions Relationship Usage example Exporting data Upgrading  ",
        "image": "",
        "url": "/configuration/services/mongodb.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "2f655ad81e670538712fff005b813864",
        "title": "Spring",
        "description": "",
        "text": " The Spring Framework provides a comprehensive programming and configuration model for modern Java-based enterprise applications - on any kind of deployment platform. Platform.sh is flexible, and allows you to use Spring Framework in several flavors such as Spring MVC and Spring Boot . Services The configuration reader library for Java is used in these examples, so be sure to check out the documentation for installation instructions and the latest version. MongoDB You can use Spring Data MongoDB to use MongoDB with your application by first determining the MongoDB client programmatically. import com.mongodb.MongoClient; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.mongodb.config.AbstractMongoConfiguration; import sh.platform.config.Config; import sh.platform.config.MongoDB; @Configuration public class MongoConfig extends AbstractMongoConfiguration { private Config config = new Config(); @Override @Bean public MongoClient mongoClient() { MongoDB mongoDB = config.getCredential( database , MongoDB::new); return mongoDB.get(); } @Override protected String getDatabaseName() { return config.getCredential( database , MongoDB::new).getDatabase(); } } Apache Solr You can use Spring Data Solr to use Solr with your application by first determining the Solr client programmatically. import org.apache.solr.client.solrj.impl.HttpSolrClient; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.solr.core.SolrTemplate; import sh.platform.config.Config; import sh.platform.config.Solr; @Configuration public class SolrConfig { @Bean public HttpSolrClient elasticsearchTemplate() { Config config = new Config(); final Solr credential = config.getCredential( solr , Solr::new); final HttpSolrClient httpSolrClient = credential.get(); String url = httpSolrClient.getBaseURL(); httpSolrClient.setBaseURL(url.substring(0, url.lastIndexOf(\u0026#39;/\u0026#39;))); return httpSolrClient; } @Bean public SolrTemplate solrTemplate(HttpSolrClient client) { return new SolrTemplate(client); } } Redis You can use Spring Data Redis to use Redis with your application by first determining the Redis client programmatically. import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.redis.connection.jedis.JedisConnectionFactory; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.data.redis.serializer.GenericToStringSerializer; @Configuration public class RedisConfig { @Bean JedisConnectionFactory jedisConnectionFactory() { Config config = new Config(); RedisSpring redis = config.getCredential( redis , RedisSpring::new); return redis.get(); } @Bean public RedisTemplate\u0026lt;String, Object\u0026gt; redisTemplate() { final RedisTemplate\u0026lt;String, Object\u0026gt; template = new RedisTemplate\u0026lt;String, Object\u0026gt;(); template.setConnectionFactory(jedisConnectionFactory()); template.setValueSerializer(new GenericToStringSerializer\u0026lt;Object\u0026gt;(Object.class)); return template; } } MySQL MySQL is an open-source relational database technology. Spring has robust integration with this technology: Spring Data JPA . The first step is to choose the database that you would like to use in your project. Define the driver for MySQL and the Java dependencies, then determine the DataSource client programmatically: import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import sh.platform.config.Config; import sh.platform.config.MySQL; import javax.sql.DataSource; @Configuration public class DataSourceConfig { @Bean(name =  dataSource ) public DataSource getDataSource() { Config config = new Config(); MySQL database = config.getCredential( database , MySQL::new); return database.get(); } } Note: You can use the same MySQL driver for MariaDB as well if you wish to do so. MariaDB MariaDB is an open-source relational database technology. Spring has robust integration with this technology: Spring Data JPA . The first step is to choose the database that you would like to use in your project. Define the driver for MariaDB and the Java dependencies, then determine the DataSource client programmatically: import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import sh.platform.config.Config; import sh.platform.config.MariaDB; import javax.sql.DataSource; @Configuration public class DataSourceConfig { @Bean(name =  dataSource ) public DataSource getDataSource() { Config config = new Config(); MariaDB database = config.getCredential( database , MariaDB::new); return database.get(); } } PostgreSQL PostgreSQL is an open-source relational database technology. Spring has robust integration with this technology: Spring Data JPA . The first step is to choose the database that you would like to use in your project. Define the driver for PostgreSQL and the Java dependencies, then determine the DataSource client programmatically: import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import sh.platform.config.Config; import sh.platform.config.PostgreSQL; import javax.sql.DataSource; @Configuration public class DataSourceConfig { @Bean(name =  dataSource ) public DataSource getDataSource() { Config config = new Config(); PostgreSQL database = config.getCredential( database , PostgreSQL::new); return database.get(); } } RabbitMQ You can use Spring JMS to use RabbitMQ with your application by first determining the RabbitMQ client programmatically. import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.jms.annotation.EnableJms; import org.springframework.jms.connection.CachingConnectionFactory; import org.springframework.jms.support.converter.MappingJackson2MessageConverter; import org.springframework.jms.support.converter.MessageConverter; import org.springframework.jms.support.converter.MessageType; import sh.platform.config.Config; import sh.platform.config.RabbitMQ; import javax.jms.ConnectionFactory; @Configuration @EnableJms public class JMSConfig { private ConnectionFactory getConnectionFactory() { Config config = new Config(); final RabbitMQ rabbitMQ = config.getCredential( rabbitmq , RabbitMQ::new); return rabbitMQ.get(); } @Bean public MessageConverter getMessageConverter() { MappingJackson2MessageConverter converter = new MappingJackson2MessageConverter(); converter.setTargetType(MessageType.TEXT); converter.setTypeIdPropertyName( _type ); return converter; } @Bean public CachingConnectionFactory getCachingConnectionFactory() { ConnectionFactory connectionFactory = getConnectionFactory(); return new CachingConnectionFactory(connectionFactory); } } Templates Spring Boot MySQL Spring Boot MongoDB",
        "section": "Featured frameworks",
        "subsections": " Services  MongoDB Apache Solr Redis MySQL MariaDB PostgreSQL RabbitMQ   Templates  ",
        "image": "",
        "url": "/frameworks/spring.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "bbf975afd8c0db4f2c472daa517d43e8",
        "title": "Upgrading to the Integrated UI",
        "description": "",
        "text": " Older Platform.sh Dedicated projects (created prior to October 2017) used a separate Git repository for Production and Staging. That also necessitated running most configuration changes through a ticket, and maintaining separate SSH credentials for each environment. These older projects can be upgraded to the new Integrated UI, which eliminates the extra Git repositories, many “must be a ticket” configuration changes, and makes the Production and Staging environments available in the UI. To add these environments to the Project Web Interface, review this entire document, complete a few preparatory steps, and submit a ticket. Your ticket is added to a queue for updating existing Dedicated projects. The process may take time to complete, so check your ticket for details, timing, and other important information. We recommend this upgrade for all users. New Features The new Project Web Interface provides the following features for the Pro plan Staging and Production environments: Add and manage user access to the environments Sync code between Staging and Production to Integration environments Merge code from Integration environment to Staging environment to Production environment Add and manage environment variables Manage build and deploy hooks with the .platform.app.yaml file Manage PHP versions and variables with the .platform.app.yaml file Manage cron jobs with the .platform.app.yaml file Configure environment settings Access the environments using SSH and URL View status, build logs, and deployment history You must still submit a support ticket to update and modify the following in the Staging and Production environments information: Redirects from routes.yaml file Managing PHP extensions Managing mounts You cannot perform the following: Branch from the Staging and Production environments Synchronize data from the Staging and Production environments Snapshot the Staging and Production environments Branching hierarchy Before converting your project, the branches include a repository for Development, Staging, and Production. Each repository has a master branch with deployment targets configured for Staging and Production. After converting your project, the hierarchical relationships appear in your Project Web Interface with two, main environment branches for Staging and Production: Before you upgrade When we add Staging and Production access to the Project Web Interface, we leverage the user accounts, branch user permissions, and environment variables from your Development master environment. To prepare, verify that your settings and environment variables are correct. Verify code matches across environments Verify user account access Prepare variables Verify code We strongly recommend working in your local development environment, deploying to Development, deploying to Staging, and, finally, deploying to Production. All code should match 100% across each of these environments. Before submitting a ticket, make sure you sync your code. This process creates a new branch of code for Staging and Production environments. If you have additional code, such as new extensions in your Production environment without following this workflow, then deployments from Integration or Staging overwrite your Production code. Verify user account access We recommend verifying your user account access and permissions set in the Integration environment. When adding Staging and Production to the Project Web Interface, the process includes all user accounts and settings. You can modify the settings and values for these environments after they are added. Log in to your Platform.sh account. From your project, click Master to view the environment information and settings. Click Configure environment. Click the Users tab to review the user accounts and permission configurations. Add, delete, or update users, if needed. Prepare variables When we convert your project to the new Project Web Interface, we add variables from Development environment to the Staging and Production environments. You can review, modify, and add variables through the current Project Web Interface prior to conversion. Log in to your Platform.sh account. From your project, click the master branch to view the environment information and settings. Click Configure environment. On the Variables tab, review the environment variables. To create a new variable, click Add Variable. To update an existing variable, click Edit next to the variable. For environment-specific variables, including sensitive data and values, you can add those variables after we update your Project Web Interface. Environment variables defined in .platform.app.yaml or a .environment file will continue to work. You can add and manage these variables via SSH and CLI commands directly into the Staging and Production environments. Enter a ticket for updating the Project Web Interface Enter a Support ticket with the suggested title “Connect Stg / Prod to Project’s UI”. In the ticket, request to have your project enabled with Staging and Production in the UI and confirm that you’ve taken the steps above to prepare your project. We will review the infrastructure and settings, create user and environment variables for Staging and Production environments, and update the ticket with results. Once started the process usually takes less than an hour. There should be no downtime on your production site, although you should not push any code to Git while the upgrade is in progress. When done, you can access review your project through the Project Web Interface. (Optional) Migrate environment variables After conversion, you can manually migrate specific environment variables for Staging and Production. Open a terminal and checkout a branch in your local environment. List all environment variables: platform variable:list Log in to your Platform.sh account. Click the Projects tab and the name of your project. Click the Staging or Production environment. On the Variables tab, review the environment variables. Enter the variable name and value. Select the Override checkbox if you want variables in the Project Web Interface to override local CLI or database values.",
        "section": "Platform.sh Dedicated",
        "subsections": " New Features  Branching hierarchy   Before you upgrade  Verify code Verify user account access Prepare variables   Enter a ticket for updating the Project Web Interface  (Optional) Migrate environment variables    ",
        "image": "",
        "url": "/dedicated/overview/upgrading.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "dad2fdf067ad82c29edf30f2421234ce",
        "title": "Using Redis with Drupal 7.x",
        "description": "",
        "text": " There are two options for using Redis with Drupal on Platform.sh, you can either use the PhpRedis extension or the Predis library. PhpRedis requires a PHP extension and should therefore be faster in most situations. Predis is written entirely in PHP and so would require no PHP extension to install locally, but at the cost of some performance. If you are unsure which to use, we recommend using PhpRedis. Requirements Add a Redis service First you need to create a Redis service. In your .platform/services.yaml file, add or uncomment the following: rediscache:type:redis:5.0That will create a service named rediscache, of type redis, specifically version 3.0. Expose the Redis service to your application In your .platform.app.yaml file, you now need to open a connection to the new Redis service. Under the relationships section, add the following: relationships:redis: rediscache:redis The key (left side) is the name that will be exposed to the application in the PLATFORM_RELATIONSHIPS variable. The right hand side is the name of the service you specified above (rediscache) and the endpoint (redis). If you named the service something different above, change rediscache to that. Add the Redis PHP extension Because the Redis extension for PHP has been known to have BC breaks at times, we do not bundle a specific verison by default. Instead, we provide a script to allow you to build your desired version in the build hook. See the PHP-Redis page for a simple-to-install script and instructions. (Skip this part if using Predis.) Add the Drupal module You will need to add the Redis module to your project. If you are using Drush Make, you can add these lines to your project.make file: projects[redis][version] = 3.15 To use the Predis library, also add it to your make file: libraries[predis][download][type] = get libraries[predis][download][url] = https://github.com/nrk/predis/archive/v1.0.3.tar.gz libraries[predis][directory_name] = predis libraries[predis][destination] = libraries Configuration To make use of the Redis cache you will need to set some Drupal variables. You can either do this in your settings.php file or by setting Platform Variables directly via the management console. In general, using the settings.php file is easier. Via settings.php To configure Drupal 7 to use our Redis server for caching, place the following at the end of settings.php, after the include directive for settings.local.php: \u0026lt;?php if (!empty($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;])) { $relationships = json_decode(base64_decode($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;]), TRUE); if (!empty($relationships[\u0026#39;redis\u0026#39;])) { $conf[\u0026#39;redis_client_host\u0026#39;] = $relationships[\u0026#39;redis\u0026#39;][0][\u0026#39;host\u0026#39;]; $conf[\u0026#39;redis_client_port\u0026#39;] = $relationships[\u0026#39;redis\u0026#39;][0][\u0026#39;port\u0026#39;]; $conf[\u0026#39;redis_client_interface\u0026#39;] = \u0026#39;PhpRedis\u0026#39;; $conf[\u0026#39;cache_backends\u0026#39;][] = \u0026#39;sites/all/modules/contrib/redis/redis.autoload.inc\u0026#39;; $conf[\u0026#39;cache_default_class\u0026#39;] = \u0026#39;Redis_Cache\u0026#39;; // The \u0026#39;cache_form\u0026#39; bin must be assigned to non-volatile storage. $conf[\u0026#39;cache_class_cache_form\u0026#39;] = \u0026#39;DrupalDatabaseCache\u0026#39;; // The \u0026#39;cache_field\u0026#39; bin must be transactional. $conf[\u0026#39;cache_class_cache_field\u0026#39;] = \u0026#39;DrupalDatabaseCache\u0026#39;; } } If using Predis, change the PhpRedis reference to Predis (case-sensitive). If your redis module is not installed in sites/all/modules/contrib, modify the cache_backends line accordingly. Via the management console Alternatively, add the following environment variables using the Platform.sh management console. Note, if you set a directory in the make file you will need to alter the variables to match. drupal:cache_backends [  sites/all/modules/contrib/redis/redis.autoload.inc  ] Note: Remember to tick the JSON Value box. Use the actual path to your Redis module in case it is in a different location. For example: sites/all/modules/redis. The location used above is the default when using Drush on Platform.sh. drupal:redis_client_host redis.internal drupal:cache_default_class Redis_Cache drupal:cache_class_cache_form DrupalDatabaseCache drupal:cache_class_cache_field DrupalDatabaseCache And finally, set the client interface to either PhpRedis or Predis. drupal:redis_client_interface PhpRedis Or Predis Verifying Redis is running Run this command in a SSH session in your environment redis-cli -h redis.internal info. You should run it before you push all this new code to your repository. This should give you a baseline of activity on your Redis installation. There should be very little memory allocated to the Redis cache. After you push this code, you should run the command and notice that allocated memory will start jumping. Note: If you use Domain Access and Redis, ensure that your Redis settings (particularly $conf[\u0026#39;cache_backends\u0026#39;]) are included before the Domain Access settings.inc file - see this Drupal.org issue for more information.",
        "section": "Getting Started",
        "subsections": " Requirements  Add a Redis service Expose the Redis service to your application Add the Redis PHP extension Add the Drupal module   Configuration  Via settings.php Via the management console Verifying Redis is running    ",
        "image": "",
        "url": "/frameworks/drupal7/redis.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "e58bb863ace9a398c828ebf5e6718f12",
        "title": "Using Solr with Drupal 8.x",
        "description": "",
        "text": " The Drupal Search API Solr module has a somewhat involved setup process, as it requires live access to the Solr server in order to generate the configuration files for it. The following procedure is therefore necessary to ensure each step is able to proceed. Search API Solr stores its configuration in the Drupal Configuration API. However, that system does not easily support environment-aware information. The setup process therefore depends on config-overrides in settings.platformsh.php, which may need to be modified slightly depending on your Solr configuration. Search API Solr requires Solr 6.6 or higher, and recommends Solr 8 or higher. Advanced Solr service configuration and implementation in other frameworks other than Drupal can be found at the Solr services page . Steps 0. Upgrade Symfony Event Dispatcher If you are running Drupal older than 9.0, a small workaround will be needed. The Solarium library used by Search API Solr requires the 4.3 version of the Symfony Event Dispatcher, whereas Drupal core ships with 3.4. The Search API Solr issue queue has a more detailed description of the problem. As noted there, the workaround for now is to run: composer require symfony/event-dispatcher: 4.3.4 as 3.4.35  in your project root and commit the resulting change to composer.json and composer.lock. That will cause Composer to install the 4.3 version of Event Dispatcher. Once this issue is resolved in core this step will no longer be necessary. 1. Add the Drupal modules You will need to add the Search API and Search API Solr modules to your project. If you are using composer, the easiest way to add them is to simply run: $ composer require drupal/search_api_solr And then commit the changes to composer.json and composer.lock. 2. Add a default Solr service Add the following to your .platform/services.yaml file. search:type:solr:8.0disk:1024configuration:cores:maincore:conf_dir:{}endpoints:main:core:maincoreThe above definition defines a single Solr 8.0 server. That server has 1 core defined: maincore, which will use a default configuration. (The default configuration is not suitable for production but will allow the module to connect to it.) It then defines one endpoint, main, which is connected to the maincore. 3. Expose the Solr service to your application In your .platform.app.yaml file, we now need to open a connection to the new Solr service. Under the relationships section, add the following: relationships:solrsearch:\u0026#39;search:main\u0026#39;That is, the application’s environment would include a solrsearch relationship that connects to the main endpoint, which is the maincore core. The key (left side) is the name that will be exposed to the application in the PLATFORM_RELATIONSHIPS variable . The right hand side is the name of the service specified above (search) and the endpoint (main). If you named the service or endpoint something than different above, change those values accordingly. 4. Add auto-configuration code to settings.platformsh.php The configuration can be managed from settings.platformsh.php by adding the following code snippet. It will override the environment-specific parts of the configuration object with the correct values to connect to the Platform.sh Solr instance. Note: If you do not already have the Platform.sh Config Reader library installed and referenced at the top of the file, you will need to install it with composer require platformsh/config-reader and then add the following code before the block below: \u0026lt;?php $platformsh = new if (!$platformsh-\u0026gt;inRuntime()) { return; } Edit the value of $relationship_name if you are using a different relationship. Edit the value of $solr_server_name if you want to configure a Solr server in Drupal other than the default server automatically created by Search API Solr module. \u0026lt;?php $platformsh-\u0026gt;registerFormatter(\u0026#39;drupal-solr\u0026#39;, function($solr) { // Default the solr core name to `collection1` for pre-Solr-6.x instances. return [ \u0026#39;core\u0026#39; =\u0026gt; substr($solr[\u0026#39;path\u0026#39;], 5) ? : \u0026#39;collection1\u0026#39;, \u0026#39;path\u0026#39; =\u0026gt; \u0026#39;\u0026#39;, \u0026#39;host\u0026#39; =\u0026gt; $solr[\u0026#39;host\u0026#39;], \u0026#39;port\u0026#39; =\u0026gt; $solr[\u0026#39;port\u0026#39;], ]; }); // Update these values to the relationship name (from .platform.app.yaml) // and the machine name of the server from your Drupal configuration. $relationship_name = \u0026#39;solrsearch\u0026#39;; $solr_server_name = \u0026#39;default_solr_server\u0026#39;; if ($platformsh-\u0026gt;hasRelationship($relationship_name)) { // Set the connector configuration to the appropriate value, as defined by the formatter above. $config[\u0026#39;search_api.server.\u0026#39; . $solr_server_name][\u0026#39;backend_config\u0026#39;][\u0026#39;connector_config\u0026#39;] = $platformsh-\u0026gt;formattedCredentials($relationship_name, \u0026#39;drupal-solr\u0026#39;); } If you are connecting to multiple Solr cores, repeat the second block above for each relationship/server, modifying the two variables accordingly. Commit all of the changes above and then push to deploy. 5. Enable the modules Once the site is deployed, go to the /admin/modules page and enable the “Search API Solr” module. Also enable the “Search API Solr Search Defaults” module in order to get a default server configuration. If you would rather create one yourself you may do so but then you must change the value of $solr_server_name in the code snippet in settings.platformsh.php. 6. Export and modify configuration In the Drupal admin area, go to /admin/config/search/search-api and select your server. (If you used the Search Defaults module, it will be named simply “Solr Server”). First verify that the module is able to connect to your Solr instance by ensuring that the “Server connection” reports “The Solr server could be reached.” You can now generate a config.zip file using the button at the top of the page. That will produce a Solr configuration that is customized for your current field configuration. Extract the file into the .platform directory of your site. It should unpack into a directory named solr_8.x_config or similar. Inside that directory, locate the solrcore.properties file. In that file, delete the entry for solr.install.dir. Its default value will not work and it is not required for Solr to operate. (The server already knows its installation directory.) Finally, move that directory to .platform/, and update the conf_dir to point to it. The services.yaml entry should now look approximately like this: search:type:solr:8.0disk:1024configuration:cores:maincore:conf_dir:!archive solr_8.x_config/ endpoints:main:core:maincoreAdd the new directory and updated services.yaml to Git, commit, and push. Note: If you change your Solr configuration in Drupal, say to change the Solr field configuration, you may need to regenerate your configuration. If so, repeat this entire step. 7. Verify that it worked Return to the Drupal UI for your server page. After deploying and reloading the page, the “Core Connection” field should now read “The Solr core could be accessed”.",
        "section": "Getting Started",
        "subsections": " Steps  0. Upgrade Symfony Event Dispatcher 1. Add the Drupal modules 2. Add a default Solr service 3. Expose the Solr service to your application 4. Add auto-configuration code to settings.platformsh.php 5. Enable the modules 6. Export and modify configuration 7. Verify that it worked    ",
        "image": "",
        "url": "/frameworks/drupal8/solr.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "47e22330347056aef010efaf1e9f3cf7",
        "title": "Using Xdebug",
        "description": "",
        "text": " Xdebug is a real-time debugger extension for PHP. While usually used for local development, it can also be helpful for debugging aberrant behavior on the server. It is available on Platform.sh Grid instances running PHP 7.2 and higher. As configured on Platform.sh, it avoids any runtime overhead for non-debug requests, even in production, and only allows SSH-tunneled connections to avoid any security issues. Setting up Xdebug Xdebug is not enabled the same way as other extensions, as it should not be active on most requests. Xdebug has a substantial impact on performance and should not be run in production. Instead, Xdebug can be enabled by adding the following configuration to the application’s .platform.app.yaml file: runtime:xdebug:idekey:PHPSTORMThe idekey value can be any arbitrary alphanumeric string, as long as it matches your IDE’s configuration. When that key is defined, Platform.sh will start a second PHP-FPM process on the container that is identically configured but also has Xdebug enabled. Only incoming requests that have an Xdebug cookie or query parameter set will be forwarded to the debug PHP-FPM process. All other requests will be directed to the normal PHP-FPM process and thus have no performance impact. Xdebug has numerous other configuration options available. They are all set as php.ini values, and can be configured the same way as any other php.ini setting . Consult the Xdebug documentation for a full list of available options, although in most cases the default configuration is sufficient. Using Xdebug Open a tunnel From your local checkout of your application, run platform environment:xdebug (or just platform xdebug) to open an SSH tunnel to the server. That SSH tunnel will allow your IDE and the server to communicate debug information securely. By default, Xdebug operates on port 9000. Generally, it is best to configure your IDE to use that port. If you wish to use an alternate port use the --port flag. To close the tunnel and terminate the debug connection, press Ctrl-C. Install an Xdebug helper While Xdebug can be triggered from the browser by adding a special query parameter, the preferred way is to use a browser plugin helper. One is available for Firefox and for Chrome . Their respective plugin pages document how to trigger them when needed. Using PHPStorm The configuration for Xdebug will be slightly different for each IDE. Platform.sh has no preference as to the IDE or editor you use, but we have provided configuration instructions for PHPStorm/IntelliJ due to its popularity in the PHP ecosystem. 1. Configure Xdebug In your PHPStorm Settings window, go to Languages \u0026amp; Frameworks \u0026gt; PHP \u0026gt; Debug. Ensure that the “Debug port” is set to the expected value (9000, or whatever you want to use in the --port flag) and that “Can accept external connections” is checked. Other settings are at your discretion. 2. Set DBGp Proxy In your PHPStorm Settings window, go to Languages \u0026amp; Frameworks \u0026gt; PHP \u0026gt; Debug \u0026gt; DBGp Proxy. Ensure that the “IDE key” field is set to the same value as the idekey in .platform.app.yaml. The exact value doesn’t matter as long as it matches. 3. Configure a server In your PHPStorm Settings window, go to Languages \u0026amp; Frameworks \u0026gt; PHP \u0026gt; Servers. Add a new server for your Platform.sh environment. The “Host” should be the hostname of the environment on Platform.sh you will be debugging. The “Port” should always be 443 and the “Debugger” set to Xdebug. Ensure that “Use path mappings” is checked, which will make available a tree of your project with a column to configure the remote path that it should map to. This page lets you define what remote paths on the server correspond to what path on your local machine. In the majority of cases you can just define the root of your application (either the repository root or the root of your PHP code base specifically in a multi-app setup) to map to app, as in the example below. Note: It may be easier to allow the debug process to connect once, allow it to fail, and then select the “Configure server mappings” error message. That will pre-populate most of the fields in this page and only require you to set the app root mapping. 4. Listen for connections Toggle on PHPStorm’s Xdebug listener. Either select Run \u0026gt; Start listening for PHP debug connections from the menu or click the icon in the toolbar. To disable PHPStorm’s listener, either select Run \u0026gt; Stop listening for PHP debug connections from the menu or toggle the icon in the toolbar. 5. Start debugging While in listen mode, start the platform xdebug tunnel. Use the Xdebug helper plugin for your browser to enable debugging. Set a breakpoint in your application, then load a page in your browser. The request should pause at the breakpoint and allow you to examine the running application.",
        "section": "PHP",
        "subsections": " Setting up Xdebug Using Xdebug  Open a tunnel Install an Xdebug helper   Using PHPStorm  1. Configure Xdebug 2. Set DBGp Proxy 3. Configure a server 4. Listen for connections 5. Start debugging    ",
        "image": "",
        "url": "/languages/php/xdebug.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "1cc3e9ece5a5478cb425f71eeffd3339",
        "title": "Configure application",
        "description": "",
        "text": " You will next need to include information that defines how you want your application to behave each time it is built and deployed on Platform.sh in a .platform.app.yaml file. With the following project structure: . ├── .platform │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml └── \u0026lt; application code \u0026gt; An example .platform.app.yaml looks like this: Go Node.js PHP Python # This file describes an application. You can have multiple applications# in the same project.## See https://docs.platform.sh/user_guide/reference/platform-app-yaml.html# The name of this app. Must be unique within a project.name:app# The runtime the application uses.type:golang:1.14# The hooks executed at various points in the lifecycle of the application.hooks:build:| # Modify this line if you want to build differently or use an alternate name for your executable.gobuild-obin/app# The relationships of the application with services or other applications.## The left-hand side is the name of the relationship as it will be exposed# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand# side is in the form `\u0026lt;service name\u0026gt;:\u0026lt;endpoint name\u0026gt;`.relationships:database: db:mysql # The configuration of app when it is exposed to the web.web:upstream:socket_family:tcpprotocol:httpcommands:# If you change the build output in the build hook above, update this line as well.start:./bin/applocations:/:# Route all requests to the Go app, unconditionally.# If you want some files served directly by the web server without hitting Go, see# https://docs.platform.sh/configuration/app/web.htmlallow:falsepassthru:true# The size of the persistent disk of the application (in MB).disk:1024 # This file describes an application. You can have multiple applications# in the same project.## See https://docs.platform.sh/user_guide/reference/platform-app-yaml.html# The name of this app. Must be unique within a project.name:app# The runtime the application uses.type:nodejs:10# The relationships of the application with services or other applications.## The left-hand side is the name of the relationship as it will be exposed# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand# side is in the form `\u0026lt;service name\u0026gt;:\u0026lt;endpoint name\u0026gt;`.relationships:database: db:mysql # The configuration of app when it is exposed to the web.web:commands:start: node index.js # The size of the persistent disk of the application (in MB).disk:512mounts:\u0026#39;run\u0026#39;:source:localsource_path:run # This file describes an application. You can have multiple applications# in the same project.# The name of this app. Must be unique within a project.name:app# The type of the application to build.type:php:7.3build:flavor:composer# The hooks that will be performed when the package is deployed.hooks:build:| set -edeploy:| set -e# The relationships of the application with services or other applications.# The left-hand side is the name of the relationship as it will be exposed# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand# side is in the form `\u0026lt;service name\u0026gt;:\u0026lt;endpoint name\u0026gt;`.#relationships:# database:  db:mysql # The size of the persistent disk of the application (in MB).disk:2048# The mounts that will be performed when the package is deployed.mounts:# Because this directory is in the webroot, files here will be web-accessible.\u0026#39;web/uploads\u0026#39;:source:localsource_path:uploads# Files in this directory will not be web-accessible.\u0026#39;private\u0026#39;:source:localsource_path:private# The configuration of app when it is exposed to the web.web:locations: / :# The public directory of the app, relative to its root.root: web # The front-controller script to send non-static requests to.passthru: /index.php  # This file describes an application. You can have multiple applications# in the same project.## See https://docs.platform.sh/user_guide/reference/platform-app-yaml.html# The name of this app. Must be unique within a project.name:app# The runtime the application uses.type: python:3.7 # The build-time dependencies of the app.dependencies:python3:pipenv: 2018.10.13 # The hooks executed at various points in the lifecycle of the application.hooks:build:| pipenv install --system --deploy# The size of the persistent disk of the application (in MB).disk:1024# The relationships of the application with services or other applications.## The left-hand side is the name of the relationship as it will be exposed# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand# side is in the form `\u0026lt;service name\u0026gt;:\u0026lt;endpoint name\u0026gt;`.relationships:database: db:mysql rediscache: cache:redis # The configuration of app when it is exposed to the web.web:commands:start:pythonserver.py The .platform.app.yaml file is extremely flexible, and can contain many lines with very fine-grained control over your application. At the very least, Platform.sh requires three principle attributes in this file to control your builds: name: The name of your application container does not have to be the same as your project name, and in most single application cases you can simply name it app. You should notice in the next step, when you configure how requests are handled in .platform/routes.yaml that name is reused there, and it is important that they are the same. Note: If you are trying to to deploy microservices, the only constraint is that each of these application names must be unique. type: The type attribute in .platform.app.yaml sets the container base image for the application, and sets the primary language. In general, type should have the form type:\u0026lt;runtime\u0026gt;:\u0026lt;version\u0026gt;Set version to one supported by Platform.sh, which you can find below as well as in the documentation for each language: Language runtime Supported version C#/.Net Core dotnet 2.0, 2.1, 2.2, 3.1 Elixir elixir 1.9 Go golang 1.11, 1.12, 1.13, 1.14 Java java 11, 12, 8, 13 Lisp lisp 1.5 Node.js nodejs 6, 8, 10, 12, 14 PHP php 7.2, 7.3, 7.4 Python python 2.7, 3.5, 3.6, 3.7, 3.8 Ruby ruby 2.3, 2.4, 2.5, 2.6, 2.7 disk: The disk attribute defines that amount of persistent storage you need to have available for your application, and requires a minimum value of 256 MB. There are a few additional keys in .platform.app.yaml you will likely need to use to fully configure your application, but are not required: relationships: Relationships define how services are mapped within your application. Without this block, an application cannot by default communicate with a service container. Provide a unique name for each relationship and associate it with a service. For example, if in the previous step you defined a MariaDB container in your .platform/services.yaml with db:type:mysql:10.4disk:256 You must define a relationship (i.e. database) in .platform.app.yaml to connect to it: relationships:database: db:mysql  Build and deploy tasks : There are a number of ways in which your Git repository is turned into a running application. In general, the build process will run the the build flavor, install dependencies, and then execute the build hook you provide. When the build process is completed, the deploy process will run the deploy hook. build: The build key defines what happens during the build process using the flavor property. This is a common inclusion for PHP and Node.js applications, so check the the documentation to see if your configuration requires this key. dependencies: This key makes it possible to install system-level dependencies as part of the build process. hooks: Hooks define custom scripts that you want to run at different points during the deployment process. build: The build hook is run after the build flavor if that is present. The file system is fully writable, but no services and only a subset of variables are available at this point. The full list of build time and runtime variables is available on the variables section of the public documentation. deploy: The deploy hook is run after the application container has been started, but before it has started accepting requests. Services are now available, but the file system will be read-only from this point forward. post-deploy: The post-deploy hook functions exactly the same as the deploy hook, but after the container is accepting connections. web: The web key configures the web server through a single web instance container running a single Nginx server process, behind which runs your application. commands: Defines the command to actually launch the application. The start key launches your application. In all languages except for PHP, web.commands.start should be treated as required. For PHP, you will instead need to define a script name in passthru, described below in locations. locations: Allows you to control how the application container responds to incoming requests at a very fine-grained level. The simplest possible locations configuration is one that simply passes all requests on to your application unconditionally: web:locations:\u0026#39;/\u0026#39;:passthru:trueThe above configuration forwards all requests to /* to the process started by web.commands.start. In the case of PHP containers, passthru must specify what PHP file to forward the request to, as well as the docroot under which the file lives. For example, web:locations:\u0026#39;/\u0026#39;:root:\u0026#39;web\u0026#39;passthru:\u0026#39;/app.php\u0026#39; mounts: Configuring mounts are not required, unless part of your application requires write-access. By default, Platform.sh provided a read-only filesystem for your projects so that you can be confident in the health and security of your application once it has deployed. If your application requires writable storage to function properly (i.e., saving files; mounts should not contain code) it can be defined like so: mounts:\u0026#39;web/uploads\u0026#39;:source:localsource_path:uploadsIn this case, the application will be able to write to a mount that is visible in the /app/web/uploads directory of the application container, and which has a local source at /mnt/uploads. Consult the mounts documentation for a more thorough discussion of how these attributes should be written. Note: Each language and framework may have additional attributes that you will need to include in .platform.app.yaml depending on the needs of your application. To find out what else you may need to include to configure your application, consult The Application documentation for Platform.sh: The documentation goes into far more extensive detail of which attributes can also be included for application configuration, and should be used as your primary reference. Language-specific templates for Platform.sh Projects: Compare the .platform.app.yaml file from the simple template above to other templates when writing your own. Now that you have configured your application, you will next need to handle HTTP requests to your application using the .platform/routes.yaml file. Back I\u0026#39;ve configured my application",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/app-configuration.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "cb7db858ee8667698ac553db1f4966c0",
        "title": "Encryption",
        "description": "",
        "text": " Data in Transit Data in transit between the World and Platform.sh is always encrypted as all of the sites and tools which Platform.sh supports and maintains require TLS or SSH to access. This includes the Platform.sh management console, Accounts site, Git repositories, Documentation, and Helpdesk. Data in transit between the World and customer applications is encrypted by default. Only SSH and HTTPS connections are generally accepted, with HTTP request redirected to HTTPS. Users may opt-out of that redirect and accept HTTP requests via routes.yaml configuration, although that is not recommended. By default HTTPS connections use an automatically generated Let’s Encrypt certificate or users may provide their own TLS certificate. Data in transit on Platform.sh controlled networks (eg. between the application and a database) may or may not be encrypted, but is nonetheless protected by private networking rules. Data at Rest All application data is encrypted at rest by default using encrypted ephemeral storage (typically using an AES-256 block cipher). Some Enterprise-Dedicated clusters do not have full encryption at rest. If you have specific audit requirements surrounding data at rest encryption please contact us .",
        "section": "Security and compliance",
        "subsections": " Data in Transit Data at Rest  ",
        "image": "",
        "url": "/security/encryption.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "6c064bc9632b648e9a498ab19bdb8aac",
        "title": "HTTP Headers",
        "description": "",
        "text": " Platform.sh adds a number of HTTP headers to both inbound and outbound messages. We do not, however, modify or block existing headers on either request or response. Request headers Platform.sh adds the following HTTP headers in the router to give the application information about the connection. These are stable and may be examined by the application as necessary. X-Forwarded-Proto: The protocol forwarded to the application, e.g. “http”, “https”. X-Client-IP: The remote IP address of the request. X-Client-SSL: Set “on” only if the client is using SSL connection, otherwise the header is not added. X-Original-Route: The route in .platform/routes.yaml which is used currently, e.g. https://{default}/. Response headers Platform.sh adds a number of response headers automatically to assist in debugging connections. These headers should be treated as a semi-private API. Do not code against them, but they may be inspected to help determine how Platform.sh handled the request to aid in debugging. X-Platform-Cache: Either HIT or MISS to indicate if the router in your cluster served the response from its own cache or if the request was passed through to the application. X-Platform-Cluster: The ID of the cluster that received the request. The cluster name is formed from the project ID and environment ID. X-Platform-Processor: The ID of the container that generated the response. The container ID is the cluster ID plus the container name. X-Platform-Router: The ID of the router that served the request. The router ID is the processor ID of the router container, specifically. Custom headers Apart from those listed above, your application is responsible for setting its own response headers. To add headers to static files, use the headers key in the application’s web locations configuration .",
        "section": "Development",
        "subsections": " Request headers Response headers Custom headers  ",
        "image": "",
        "url": "/development/headers.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "67a6e4084a2d7235abcb4663ad45f6a1",
        "title": "MariaDB/MySQL (Database service)",
        "description": "",
        "text": " Platform.sh supports both MariaDB and Oracle MySQL. While there are some differences at the application level for developers, they function nearly identically from an infrastructure point of view. See the MariaDB documentation or MySQL documentation for more information. Supported versions The service types mariadb and mysql both refer to MariaDB for compatibility reasons. The service type oracle-mysql refers to MySQL as released by Oracle, Inc. Other than the type, MySQL and MariaDB are otherwise identical and the rest of this page refers to both equally. mariadb mysql oracle-mysql 10.0 10.1 10.2 10.3 10.4 10.0 10.1 10.2 10.3 10.4 5.7 8.0 Only MariaDB is available on Dedicated environments, using Galera for replication: 10.0 Galera 10.1 Galera 10.2 Galera Note: Downgrades of MySQL or MariaDB are not supported. Both will update their own datafiles to a new version automatically but cannot downgrade them. If you want to experiment with a later version without committing to it use a non-master environment. Dedicated environments do not support any storage engine other than InnoDB. Tables created using the MyISAM storage engine on dedicated environments will not replicate between cluster nodes. Deprecated versions The following versions are available but are not receiving security updates from upstream, so their use is not recommended. They will be removed at some point in the future. mariadb mysql oracle-mysql 5.5 5.5 Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : {  cluster :  rjify4yjcwxaa-master-7rqtwti ,  fragment : null,  host :  mysql.internal ,  host_mapped : false,  hostname :  nha5q7m5ik526umqw6cwrcvpvi.mysql.service._.eu-3.platformsh.site ,  ip :  169.254.4.210 ,  password :   ,  path :  main ,  port : 3306,  public : false,  query : {  is_master : true },  rel :  mysql ,  scheme :  mysql ,  service :  mysql ,  type :  mariadb:10.4 ,  username :  user  } Usage example For MariaDB your .platform/services.yaml can use the mysql service type: db:type:mysql:10.4disk:256 or the mariadb service type. db:type:mariadb:10.4disk:256 Oracle-mysql uses the oracle-mysql service type: dbmysql:type:oracle-mysql:8.0disk:256 Note that the minimum disk size for mysql/oracle-mysql is 256MB. Despite these service type differences, MariaDB and Oracle MySQL both use the mysql endpoint in their configuration. Note: You will need to use either the mariadb, mysql, or oracle-mysql type when defining the service # .platform/services.yamlservice_name:type:mariadb:versiondisk:256 and the endpoint mysql when defining the relationship # .platform.app.yamlrelationships:relationship_name:“service_name:mysql” Your service_name and relationship_name are defined by you, but we recommend making them distinct from each other. Exception: This pattern will be the case unless you explictly set additional endpoints for multiple databases, as shown in the section below. You can then use the service in a configuration file of your application with something like: Go Java Node.js PHP Python package examples import (  database/sql   fmt  _  github.com/go-sql-driver/mysql  psh  github.com/platformsh/config-reader-go/v2  sqldsn  github.com/platformsh/config-reader-go/v2/sqldsn  ) func UsageExampleMySQL() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // The \u0026#39;database\u0026#39; relationship is generally the name of the primary SQL database of an application. // That\u0026#39;s not required, but much of our default automation code assumes it. credentials, err := config.Credentials( database ) checkErr(err) // Using the sqldsn formatted credentials package. formatted, err := sqldsn.FormattedCredentials(credentials) checkErr(err) db, err := sql.Open( mysql , formatted) checkErr(err) defer db.Close() // Force MySQL into modern mode. db.Exec( SET NAMES=utf8 ) sql_mode = \u0026#39;ANSI,STRICT_TRANS_TABLES,STRICT_ALL_TABLES, NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO, // Creating a table. sqlCreate := CREATE TABLE IF NOT EXISTS PeopleGo ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT _, err = db.Exec(sqlCreate) checkErr(err) // Insert data. sqlInsert := INSERT INTO PeopleGo (name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La _, err = db.Exec(sqlInsert) checkErr(err) table := \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; var id int var name string var city string rows, err := db.Query( SELECT * FROM PeopleGo ) if err != nil { panic(err) } else { for rows.Next() { err = rows.Scan(\u0026amp;id, \u0026amp;name, \u0026amp;city) checkErr(err) table \u0026#43;= name, city) } table \u0026#43;= } _, err = db.Exec( DROP TABLE PeopleGo; ) checkErr(err) return table } package sh.platform.languages.sample; import sh.platform.config.Config; import sh.platform.config.MySQL; import javax.sql.DataSource; import java.sql.Connection; import java.sql.ResultSet; import java.sql.SQLException; import java.sql.Statement; import java.util.function.Supplier; public class MySQLSample implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); // The \u0026#39;database\u0026#39; relationship is generally the name of primary SQL database of an application. // That\u0026#39;s not required, but much of our default automation code assumes it. MySQL database = config.getCredential( database , MySQL::new); DataSource dataSource = database.get(); // Connect to the database try (Connection connection = dataSource.getConnection()) { // Creating a table. String sql =  CREATE TABLE JAVA_PEOPLE (  \u0026#43;   id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY,  \u0026#43;  name VARCHAR(30) NOT NULL,  \u0026#43;  city VARCHAR(30) NOT NULL) ; final Statement statement = connection.createStatement(); statement.execute(sql); // Insert data. sql =  INSERT INTO JAVA_PEOPLE (name, city) VALUES  \u0026#43;  (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;),  \u0026#43;  (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;),  \u0026#43;  (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La Jolla\u0026#39;) ; statement.execute(sql); // Show table. sql =  SELECT * FROM JAVA_PEOPLE ; final ResultSet resultSet = statement.executeQuery(sql); while (resultSet.next()) { int id = resultSet.getInt( id ); String name = resultSet.getString( name ); String city = resultSet.getString( city ); logger.append(String.format( the JAVA_PEOPLE id %d the name %s and city %s , id, name, city)); } statement.execute( DROP TABLE JAVA_PEOPLE ); return logger.toString(); } catch (SQLException exp) { throw new RuntimeException( An error when execute MySQL , exp); } } } const mysql = require(\u0026#39;mysql2/promise\u0026#39;); const config = require( platformsh-config ).config(); exports.usageExample = async function() { const credentials = config.credentials(\u0026#39;database\u0026#39;); const connection = await mysql.createConnection({ host: credentials.host, port: credentials.port, user: credentials.username, password: credentials.password, database: credentials.path, }); let sql = \u0026#39;\u0026#39;; // Creating a table. sql = `CREATE TABLE IF NOT EXISTS People ( id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL )`; await connection.query(sql); // Insert data. sql = `INSERT INTO People (name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La Jolla\u0026#39;);`; await connection.query(sql); // Show table. sql = `SELECT * FROM People`; let [rows] = await connection.query(sql); let output = \u0026#39;\u0026#39;; if (rows.length \u0026gt; 0) { output \u0026#43;=`\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt;`; rows.forEach((row) =\u0026gt; { output \u0026#43;= }); output \u0026#43;= } // Drop table. sql = `DROP TABLE People`; await connection.query(sql); return output; }; \u0026lt;?php declare(strict_types=1); use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // The \u0026#39;database\u0026#39; relationship is generally the name of primary SQL database of an application. // That\u0026#39;s not required, but much of our default automation code assumes it. $credentials = $config-\u0026gt;credentials(\u0026#39;database\u0026#39;); try { // Connect to the database using PDO. If using some other abstraction layer you would // inject the values from $database into whatever your abstraction layer asks for. $dsn = sprintf(\u0026#39;mysql:host=%s;port=%d;dbname=%s\u0026#39;, $credentials[\u0026#39;host\u0026#39;], $credentials[\u0026#39;port\u0026#39;], $credentials[\u0026#39;path\u0026#39;]); $conn = new $credentials[\u0026#39;username\u0026#39;], $credentials[\u0026#39;password\u0026#39;], [ // Always use Exception error mode with PDO, as it\u0026#39;s more reliable. =\u0026gt; // So we don\u0026#39;t have to mess around with cursors and unbuffered queries by default. =\u0026gt; TRUE, // Make sure MySQL returns all matched rows on update queries including // rows that actually didn\u0026#39;t have to be updated because the values didn\u0026#39;t // change. This matches common behavior among other database systems. =\u0026gt; TRUE, ]); // Creating a table. $sql =  CREATE TABLE People ( id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL ) ; $conn-\u0026gt;query($sql); // Insert data. $sql =  INSERT INTO People (name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La Jolla\u0026#39;); ; $conn-\u0026gt;query($sql); // Show table. $sql =  SELECT * FROM People ; $result = $conn-\u0026gt;query($sql); if ($result) { print \u0026lt;\u0026lt;\u0026lt;TABLE\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt; TABLE; foreach ($result as $record) { $record-\u0026gt;name, $record-\u0026gt;city); } print } // Drop table $sql =  DROP TABLE People ; $conn-\u0026gt;query($sql); } catch $e) { print $e-\u0026gt;getMessage(); } import pymysql from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # The \u0026#39;database\u0026#39; relationship is generally the name of primary SQL database of an application. # That\u0026#39;s not required, but much of our default automation code assumes it.\u0026#39; credentials = config.credentials(\u0026#39;database\u0026#39;) try: # Connect to the database using PDO. If using some other abstraction layer you would inject the values # from `database` into whatever your abstraction layer asks for. conn = pymysql.connect(host=credentials[\u0026#39;host\u0026#39;], port=credentials[\u0026#39;port\u0026#39;], database=credentials[\u0026#39;path\u0026#39;], user=credentials[\u0026#39;username\u0026#39;], password=credentials[\u0026#39;password\u0026#39;]) sql = \u0026#39;\u0026#39;\u0026#39; CREATE TABLE People ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL ) \u0026#39;\u0026#39;\u0026#39; cur = conn.cursor() cur.execute(sql) sql = \u0026#39;\u0026#39;\u0026#39; INSERT INTO People (name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La Jolla\u0026#39;); \u0026#39;\u0026#39;\u0026#39; cur.execute(sql) # Show table. sql = \u0026#39;\u0026#39;\u0026#39;SELECT * FROM People\u0026#39;\u0026#39;\u0026#39; cur.execute(sql) result = cur.fetchall() table = \u0026#39;\u0026#39;\u0026#39;\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt;\u0026#39;\u0026#39;\u0026#39; if result: for record in result: table \u0026#43;= record[2]) table \u0026#43;= # Drop table sql = \u0026#39;\u0026#39;\u0026#39;DROP TABLE People\u0026#39;\u0026#39;\u0026#39; cur.execute(sql) # Close communication with the database cur.close() conn.close() return table except Exception as e: return e Note: MySQL schema names can not use system reserved namespace. (mysql, information_schema, etc) Multiple databases If you are using version 10.0 or later of this service it is possible to define multiple databases as well as multiple users with different permissions. To do so requires defining multiple endpoints. Under the configuration key of your service there are two additional keys: schemas: This is a YAML array listing the databases that should be created. If not specified, a single database named main will be created. endpoints: This is a nested YAML array defining different credentials. Each endpoint may have access to one or more schemas (databases), and may have different levels of permission on each. The valid permission levels are: ro: Using this endpoint only SELECT queries are allowed. rw: Using this endpoint SELECT queries as well INSERT/UPDATE/DELETE queries are allowed. admin: Using this endpoint all queries are allowed, including DDL queries (CREATE TABLE, DROP TABLE, etc.). Consider the following illustrative example: db:type:mariadb:10.4disk:2048configuration:schemas:- main- legacyendpoints:admin:default_schema:mainprivileges:main:adminlegacy:adminreporter:privileges:main:roimporter:default_schema:legacyprivileges:legacy:rwThis example creates a single MySQL/MariaDB service named mysqldb. That server will have two databases, main and legacy. There will be three endpoints created. The first, named admin, will have full access to both databases. The second, reporter, will have SELECT query access to the main DB but no access to legacy at all. The importer user will have SELECT/INSERT/UPDATE/DELETE access (but not DDL access) to the legacy database but no access to main. If a given endpoint has access to multiple databases you should also specify which will be listed by default in the relationships array. If one isn’t specified the path property of the relationship will be null. While that may be acceptable for an application that knows the name of the database to connect to, it would mean that automated tools such as the Platform CLI will not be able to access the database on that relationship. For that reason the default_schema property is always recommended. Once those endpoints are defined, you need to expose them to your application as a relationship. Continuing with our example, this would be a possible corresponding block from .platform.app.yaml: relationships:database: db:admin reports: db:reporter imports: db:importer This block defines three relationships, database, reports, and imports. They’ll be available in the PLATFORM_RELATIONSHIPS environment variable and all have the same structure documented above, but with different credentials. You can use those to connect to the appropriate database with the specified restrictions using whatever the SQL access tools are for your language and application. If no configuration block is specified at all, it is equivalent to the following default: configuration:schemas:- mainendpoints:mysql:default_schema:mainprivileges:main:adminIf either schemas or endpoints are defined, then no default will be applied and you must specify the full configuration. Adjusting database configuration For MariaDB 10.1 and later Oracle MySQL 8.0 and later, a select few configuration properties from the my.cnf file are available for adjustment. Packet and connection sizing This value defaults to 16 (in MB). Legal values are from 1 to 100. db:type:mariadb:10.4disk:2048configuration:properties:max_allowed_packet:64The above code will increase the maximum allowed packet size (the size of a query or response) to 64 MB. However, increasing the size of the maximum packet will also automatically decrease the max_connections value. The number of connections allowed will depend on the packet size and the memory available to the service. In most cases leaving this value at the default is recommended. Character encoding For services created prior to February 2020, the default character set and collation is latin1, which is the default in most MySQL/MariaDB. For services created after February 2020, the default character set is utf8mb4 and the default collation is utf8mb4_unicode_ci. Both values can be adjusted at the server level in services.yaml: db:type:mariadb:10.4disk:2048configuration:properties:default_charset:utf8mb4default_collation:utf8mb4_unicode_ciNote that the effect of this setting is to set the character set and collation of any tables created once those properties are set. Tables created prior to when those settings are changed will be unaffected by changes to the services.yaml configuration. However, you can change your own table’s character set and collation through ALTER TABLE commands. For example: # To change defaults when creating new tables: ALTER DATABASE main CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; # To change defaults when creating new columns: ALTER TABLE table_name CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; # To convert existing data: ALTER TABLE table_name CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; Consult the MySQL documentation for further details. Storage Engine We recommend using the InnoDB storage engine wherever possible. MyISAM is only properly supported in Grid environments. In dedicated cluster environments there is no replication of MyISAM tables. If MyISAM tables are inadvertently created or imported in a dedicated environment they can be converted to use the InnoDB storage engine using the following procedure: RENAME TABLE \u0026lt;existing\u0026gt; \u0026lt;old\u0026gt;; INSERT INTO \u0026lt;existing\u0026gt; SELECT * from \u0026lt;old\u0026gt;; Access your MariaDB service Assuming your MariaDB relationship is named database, the host name and port number obtained from PLATFORM_RELATIONSHIPS would be database.internal and 3306. Open an SSH session and run the MySQL command line client. mysql -h database.internal -P 3306 -u user main Outside the application container, you can use Platform CLI platform sql. Exporting data The easiest way to download all data in a MariaDB instance is with the Platform.sh CLI. If you have a single SQL database, the following command will export all data using the mysqldump command to a local file: platform db:dump If you have multiple SQL databases it will prompt you which one to export. You can also specify one by relationship name explicitly: platform db:dump --relationship database By default the file will be uncompressed. If you want to compress it, use the --gzip (-z) option: platform db:dump --gzip You can use the --stdout option to pipe the result to another command. For example, if you want to create a bzip2-compressed file, you can run: platform db:dump --stdout | bzip2 \u0026gt; dump.sql.bz2 Importing data The easiest way to load data into a database is to pipe an SQL dump through the platform sql command, like so: platform sql \u0026lt; my_database_backup.sql That will run the database backup against the SQL database on Platform.sh. That will work for any SQL file, so the usual caveats about importing an SQL dump apply (e.g., it’s best to run against an empty database). As with exporting, you can also specify a specific environment to use and a specific database relationship to use, if there are multiple. platform sql --relationship database -e master \u0026lt; my_database_backup.sql Note: Importing a database backup is a destructive operation. It will overwrite data already in your database. Taking a backup or a database export before doing so is strongly recommended. Troubleshooting MySQL lock wait timeout definer/invoker of view lack rights to use them MySQL server has gone away",
        "section": "Configure services",
        "subsections": " Supported versions  Deprecated versions   Relationship Usage example Multiple databases Adjusting database configuration  Packet and connection sizing   Character encoding Storage Engine Access your MariaDB service Exporting data Importing data Troubleshooting  ",
        "image": "",
        "url": "/configuration/services/mysql.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "312d100b69834d64014c0062f45f576e",
        "title": "SimpleSAML",
        "description": "",
        "text": " SimpleSAMLphp is a library for authenticating a PHP-based application against a SAML server, such as Shibboleth. Although Drupal has modules available to authenticate using SimpleSAML some additional setup is required. The following configuration assumes you are building Drupal 8 using Composer. If not, you will need to download the library manually and adjust some paths accordingly. Download the library and Drupal module The easiest way to download SimpleSAMLphp is via Composer. The following command will add both the Drupal module and the PHP library to your composer.json file. composer require simplesamlphp/simplesamlphp drupal/externalauth drupal/simplesamlphp_auth Once that’s run, commit both composer.json and composer.lock to your repository. Include SimpleSAML cookies in the cache key The SimpleSAML client uses additional cookies besides the Drupal session cookie that need to be allowed for the cache. To do so, modify your routes.yaml file for the route that points to your Drupal site and add two additional cookies to the cache.cookies line. It should end up looking approximately like this:  https://{default}/ :type:upstreamupstream: app:http cache:enabled:truecookies:[\u0026#39;/^SS?ESS/\u0026#39;,\u0026#39;/^Drupal.visitor/\u0026#39;,\u0026#39;SimpleSAMLSessionID\u0026#39;,\u0026#39;SimpleSAMLAuthToken\u0026#39;]Commit this change to the Git repository. Expose the SimpleSAML endpoint The SimpleSAML library’s www directory needs to be publicly accessible. That can be done by mapping it directly to a path in the Application configuration. Add the following block to the web.locations section of .platform.app.yaml: web:locations:\u0026#39;/simplesaml\u0026#39;:root:\u0026#39;vendor/simplesamlphp/simplesamlphp/www\u0026#39;allow:truescripts:trueindex:- index.phpThat will map all requests to example.com/simplesaml/ to the vendor/simplesamlphp/www directory, allowing static files there to be served, PHP scripts to execute, and defaulting to index.php. Create a configuration directory Your SimpleSAMLphp configuration will need to be outside of the vendor directory. The composer require will download a template configuration file to vendor/simplesamlphp/simplesamlphp/config. Rather than modifying that file in place (as it won’t be included in Git), copy the vendor/simplesamlphp/simplesamlphp/config directory to simplesamlphp/config (in your application root). It should contain two files, config.php and authsources.php. Additionally, create a simplesamlphp/metadata directory. This directory will hold your IdP definitions. Consult the SimpleSAMLphp documentation and see the examples in vendor/simplesamlphp/simplesamlphp/metadata-templates. Next, you need to tell SimpleSAMLphp where to find that directory using an environment variable. The simplest way to set that is to add the following block to your .platform.app.yaml file: variables:env:SIMPLESAMLPHP_CONFIG_DIR:/app/simplesamlphp/configCommit the whole simplesamplphp directory and .platform.app.yaml to Git. Configure SimpleSAML to use the database SimpleSAMLphp is able to store its data either on disk or in the Drupal database. Platform.sh strongly recommends using the database. Open the file simplesamlphp/config/config.php that you created earlier. It contains a number of configuration properties that you can adjust as needed. Some are best edited in-place and the file already includes ample documentation, specifically: auth.adminpassword technicalcontact_name technicalcontact_email Others are a little more involved. In the interest of simplicity we recommend simply pasting the following code snippet at the end of the file, as it will override the default values in the array. \u0026lt;?php // Set SimpleSAML to log using error_log(), which on Platform.sh will // be mapped to the /var/log/app.log file. $config[\u0026#39;logging.handler\u0026#39;] = \u0026#39;errorlog\u0026#39;; // Set SimpleSAML to use the metadata directory in Git, rather than // the empty one in the vendor directory. $config[\u0026#39;metadata.sources\u0026#39;] = [ [\u0026#39;type\u0026#39; =\u0026gt; \u0026#39;flatfile\u0026#39;, \u0026#39;directory\u0026#39; =\u0026gt; dirname(__DIR__) . \u0026#39;/metadata\u0026#39;], ]; // Setup the database connection for all parts of SimpleSAML. if (isset($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;])) { $relationships = json_decode(base64_decode($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;]), TRUE); foreach ($relationships[\u0026#39;database\u0026#39;] as $instance) { if (!empty($instance[\u0026#39;query\u0026#39;][\u0026#39;is_master\u0026#39;])) { $dsn = sprintf( %s:host=%s;dbname=%s , $instance[\u0026#39;scheme\u0026#39;], $instance[\u0026#39;host\u0026#39;], $instance[\u0026#39;path\u0026#39;] ); $config[\u0026#39;database.dsn\u0026#39;] = $dsn; $config[\u0026#39;database.username\u0026#39;] = $instance[\u0026#39;username\u0026#39;]; $config[\u0026#39;database.password\u0026#39;] = $instance[\u0026#39;password\u0026#39;]; $config[\u0026#39;store.type\u0026#39;] = \u0026#39;sql\u0026#39;; $config[\u0026#39;store.sql.dsn\u0026#39;] = $dsn; $config[\u0026#39;store.sql.username\u0026#39;] = $instance[\u0026#39;username\u0026#39;]; $config[\u0026#39;store.sql.password\u0026#39;] = $instance[\u0026#39;password\u0026#39;]; $config[\u0026#39;store.sql.prefix\u0026#39;] = \u0026#39;simplesaml\u0026#39;; } } } // Set the salt value from the Platform.sh entropy value, provided for this purpose. if (isset($_ENV[\u0026#39;PLATFORM_PROJECT_ENTROPY\u0026#39;])) { $config[\u0026#39;secretsalt\u0026#39;] = $_ENV[\u0026#39;PLATFORM_PROJECT_ENTROPY\u0026#39;]; } Generate SSL certificates (optional) Depending on your Identity Provider (IdP), you may need to generate an SSL/TLS certificate to connect to the Service Provider (SP). If so, you should generate the certificate locally following the instructions in the SimpleSAMLphp documentation . Whatever your resulting idP file is should be placed in the simplesamlphp/metadata directory. The certificate should be placed in the simplesamlphp/cert directory. (Create it if needed.) Then add the following line to your simplesamlphp/config/config.php file to tell the library where to find the certificate: $config[\u0026#39;certdir\u0026#39;] = dirname(__DIR__) . \u0026#39;/cert\u0026#39;; Deploy Commit all changes and deploy the site, then enable the simplesamlphp_auth module within Drupal (usually by enabling it locally and pushing a config change). Consult the module documentation for further information on how to configure the module itself. Note that you should not check the “Activate authentication via SimpleSAMLphp” checkbox in the module configuration until you have the rest of the configuration completed or you may be locked out of the site.",
        "section": "Getting Started",
        "subsections": " Download the library and Drupal module Include SimpleSAML cookies in the cache key Expose the SimpleSAML endpoint Create a configuration directory Configure SimpleSAML to use the database Generate SSL certificates (optional) Deploy  ",
        "image": "",
        "url": "/frameworks/drupal8/simplesaml.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "bf38b98c1aa49340988e4b6215b11ba5",
        "title": "Storage",
        "description": "",
        "text": " The built file system image that results from your build process is mounted read-only. That means it cannot be edited in production, even by accident. Many applications still require the ability to write and store files, however. For that, applications can specify one or more mount points, that is, directories that will be mounted from a writable network file system cluster. They may be mounted anywhere within the file system of your application. If the specified directory already exists the contents of it will be masked by the writable mount and inaccessible at runtime. Disk Your plan storage size specifies the maximum total space available to all applications and services. When deploying your project, the sum of all disk keys defined in .platform.app.yaml and .platform/services.yaml must be equal or less than the plan storage size. For example, if your plan storage size is 5 GB, you can assign: 2 GB to your application, 3 GB to your database 1 GB to your application, 4 GB to your database 1 GB to your application, 1 GB to your database, 3 GB to your Elasticsearch service etc. If you receive an error on git push mentioning the total disk space configured for the application and its services exceeds the plan storage size, you need to either increase the disk space reserved for your project on the project setup page or lower the storage assigned to each service and the application. The disk key is optional. If set, it defines the size of the persistent disk of the application (in MB). Its minimum value is 256 MB and a validation error will occur if you try to set it lower. Mounts The mounts key is an object whose keys are paths relative to the root of the application (that is, where the .platform.app.yaml file lives), and values are a 2-line mount definition. This section is optional: if your application doesn’t need writable local file storage, you can omit the mounts section and set disk to the minimum value of 256. Note that whether a mounted directory is web-accessible or not depends on the configuration of the web.locations block in .platform.app.yaml. Depending on the application’s needs, it’s possible to publish files on writable mounts, leave them private, or have rules for different paths and file types as desired. Basic mounts The following block defines a single writable directory, web/uploads: mounts:\u0026#39;web/uploads\u0026#39;:source:localsource_path:uploadsThe source specifies where the writable mount is. source_path specifies the subdirectory from within the source that the mount should point at. It is often easiest to have it match the name of the mount point itself but that is not required. local mounts The local source indicates that the mount point will point to a local directory on the application container. The source_path is then a subpath of that. That means they may overlap. local mounts are not shared between different application containers or workers. Be aware that the entire local space for a single app container is a common directory, and the directory is not wiped. That means if you create a mount point with a source_path of “uploads”, then write files there, then remove the mount point, the files will still exist on disk indefinitely until manually removed. Local mounts require that the disk key be set. If it is omitted there will be no storage space available at all. service mounts A service mount refers to a network-storage service, as defined in services.yaml. They function in essentially the same way as a local mount, with two important differences: The disk size of the service mount is controlled in services.yaml; it is separate from the value of the disk key in .platform.app.yaml. Multiple application containers may refer to the same service mount and share files. A service mount works like so: mounts:\u0026#39;web/uploads\u0026#39;:source:serviceservice:filessource_path:uploadsThis assumes that a network-storage service named files has already been defined. See the Network Storage page for more details and examples. Multi-instance disk mounts If you have multiple application instances defined (using both web and workers), each instance will have its own local disk mounts. That’s the case even if they are named the same, and even if there is only a single top-level mounts directive. In that case, every instance will have an identical configuration, but separate, independent file spaces. If you want to have multiple application instances share file storage, you will need to use a service mount. How do I set up both public and private file uploads? The following example sets up two file mounts. One is mounted at /private within the application container, the other at /web/uploads. The two file mounts together have a limit of 1024 MB of storage. disk:1024mounts:\u0026#39;private\u0026#39;:source:localsource_path:private\u0026#39;web/uploads\u0026#39;:source:localsource_path:uploadsThen in the web.locations block, you’d specify that the web/uploads path is accessible. For example, this fragment would specify the /web path as the docroot but provide a more locked-down access to the /web/uploads path. web:locations:\u0026#39;/\u0026#39;:# The public directory of the application relative to its root.root:\u0026#39;web\u0026#39;# The front-controller script which determines where to send# non-static requests.passthru:\u0026#39;/app.php\u0026#39;# Allow uploaded files to be served, but do not run scripts.\u0026#39;/web/uploads\u0026#39;:root:\u0026#39;web/uploads\u0026#39;expires:300sscripts:falseallow:trueSee the web locations documentation for more details. Why can’t I mount a hidden folder? Platform.sh ignores YAML keys that start with a dot. This causes a mount like .myhiddenfolder to be ignored. If you want to mount a hidden folder, you’ll have to prepend it with a /: mounts:\u0026#39;/.myhiddenfolder\u0026#39;:source:localsource_path:\u0026#39;myhiddenfolder\u0026#39;How do I setup overlapping mount paths? While not recommended it is possible to setup multiple mount points whose source paths overlap. Consider the following example: mounts:\u0026#39;private\u0026#39;:source:localsource_path:stuff\u0026#39;secret\u0026#39;:source:localsource_path:stuff/secretIn this configuration, there will be two mount points as seen from the application: ~/private and ~/secret. However, the secret mount will point to a directory that is also under the mount point for private. That is, the secret path and the private/secret path will be the exact same directory. Although this configuration won’t cause any technical issues, it may be quite confusing so is generally not recommended. Checking the size of mounts You can use standard commands such as df -ah to find the total disk usage of mounts (which are usually all on the same filesystem) and du -hs /path/to/dir to check the size of individual directories. The CLI provides a command that combines these checks: $ platform mount:size Checking disk usage for all mounts of the application \u0026#39;app\u0026#39;... \u0026#43;-------------------------\u0026#43;-----------\u0026#43;---------\u0026#43;-----------\u0026#43;-----------\u0026#43;----------\u0026#43; | Mount(s) | Size(s) | Disk | Used | Available | Capacity | \u0026#43;-------------------------\u0026#43;-----------\u0026#43;---------\u0026#43;-----------\u0026#43;-----------\u0026#43;----------\u0026#43; | private | 55.2 MiB | 1.9 GiB | 301.5 MiB | 1.6 GiB | 15.5% | | tmp | 34.1 MiB | | | | | | web/sites/default/files | 212.2 MiB | | | | | \u0026#43;-------------------------\u0026#43;-----------\u0026#43;---------\u0026#43;-----------\u0026#43;-----------\u0026#43;----------\u0026#43;",
        "section": "Configure your application",
        "subsections": " Disk Mounts Basic mounts  local mounts service mounts   Multi-instance disk mounts How do I set up both public and private file uploads? Why can\u0026rsquo;t I mount a hidden folder? How do I setup overlapping mount paths? Checking the size of mounts  ",
        "image": "",
        "url": "/configuration/app/storage.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "aebb6ca80c4742b242be1da5716a356f",
        "title": "Symfony - Getting started",
        "description": "",
        "text": " Prerequisites Composer Composer is a tool for dependency management in PHP. It allows you to declare the dependent libraries your project needs and it will install them in your project for you. Install Composer Configure your app The ideal .platform.app.yaml file will vary from project project, and you are free to customize yours as needed. It also varies somewhat from one Symfony version to the next as Symfony has evolved. See the appropriate repository below for your Symfony version. Symfony 3 Symfony 4",
        "section": "Featured frameworks",
        "subsections": " Prerequisites Configure your app  ",
        "image": "",
        "url": "/frameworks/symfony.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "796231296a641294f370d54bae5213bf",
        "title": "Using Memcached with Drupal 7.x",
        "description": "",
        "text": " Platform.sh recommends using Redis for caching with Drupal 7 over Memcached, as Redis offers better performance when dealing with larger values as Drupal tends to produce. However, Memcached is also available if desired and is fully supported. Requirements Add a Memcached service First you need to create a Memcached service. In your .platform/services.yaml file, add or uncomment the following: cacheservice:type:memcached:1.4That will create a service named cacheservice, of type memcached, specifically version 1.4. Expose the Memcached service to your application In your .platform.app.yaml file, we now need to open a connection to the new Memcached service. Under the relationships section, add the following: relationships:cache:\u0026#39;cacheservice:memcached\u0026#39;The key (left side) is the name that will be exposed to the application in the PLATFORM_RELATIONSHIPS variable . The right hand side is the name of the service we specified above (cacheservice) and the endpoint (memcached). If you named the service something different above, change cacheservice to that. Add the Memcached PHP extension You will need to enable the PHP Memcached extension. In your .platform.app.yaml file, add the following right after the type block: # Additional extensionsruntime:extensions:- memcachedAdd the Drupal module You will need to add the Memcache module to your project. If you are using a Drush Make file, add the following line to your project.make file: projects[memcache][version] = 1.6 Then commit the Note: You must commit and deploy your code before continuing, then enable the module. The memcache module must be enabled before it is configured in the settings.platformsh.php file. Configuration The Drupal Memcache module must be configured via settings.platformsh.php. Place the following at the end of settings.platformsh.php. Note the inline comments, as you may wish to customize it further. Also review the README.txt file that comes with the memcache module, as it has a more information on possible configuration options. For instance, you may want to consider using memcache for locking as well and configuring cache stampede protection. The example below is intended as a “most common case”. \u0026lt;?php if (!empty($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;]) \u0026amp;\u0026amp; extension_loaded(\u0026#39;memcached\u0026#39;)) { $relationships = json_decode(base64_decode($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;]), true); // If you named your memcached relationship something other than  cache , set that here. $relationship_name = \u0026#39;cache\u0026#39;; if (!empty($relationships[$relationship_name])) { // These lines tell Drupal to use memcached as a backend. // Comment out just these lines if you need to disable it for some reason and // fall back to the default database cache. $conf[\u0026#39;cache_backends\u0026#39;][] = \u0026#39;sites/all/modules/contrib/memcache/memcache.inc\u0026#39;; $conf[\u0026#39;cache_default_class\u0026#39;] = \u0026#39;MemCacheDrupal\u0026#39;; $conf[\u0026#39;cache_class_cache_form\u0026#39;] = \u0026#39;DrupalDatabaseCache\u0026#39;; // While we\u0026#39;re at it, use Memcache for locking, too. $conf[\u0026#39;lock_inc\u0026#39;] = \u0026#39;sites/all/modules/contrib/memcache/memcache-lock.inc\u0026#39;; foreach ($relationships[$relationship_name] as $endpoint) { $host = sprintf( %s:%d , $endpoint[\u0026#39;host\u0026#39;], $endpoint[\u0026#39;port\u0026#39;]); $conf[\u0026#39;memcache_servers\u0026#39;][$host] = \u0026#39;default\u0026#39;; } // If using a multisite configuration, adapt this line to include a site-unique // value. $conf[\u0026#39;memcache_key_prefix\u0026#39;] = $PLATFORM_ENVIRONMENT; } }",
        "section": "Getting Started",
        "subsections": " Requirements  Add a Memcached service Expose the Memcached service to your application Add the Memcached PHP extension Add the Drupal module   Configuration  ",
        "image": "",
        "url": "/frameworks/drupal7/memcached.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "9718131189e48b804813ec41091b3eda",
        "title": "Using Redis with PHP",
        "description": "",
        "text": " Redis is a popular structured key-value service, supported by Platform.sh. It’s frequently used for caching. For PHP, Redis support is provided through a PECL extension called PhpRedis . Unfortunately, the extension has been known to break its API between versions, even between minor versions. That makes it difficult for Platform.sh to bundle like other PHP extensions . Fortunately, the extension is small enough that it’s reasonable to compile as part of the build step and enable yourself. That makes it possible to install the specific version of the extension that your application code requires. All of the necessary tools to compile PHP extensions are included in our PHP containers, so cloning the source code and compiling it on each build is straightforward. That does entail a few minute additions to each build, however. Alternatively, we have written a shell script that leverages the build cache directory to only compile the extension once, and supports compiling any version of the extension. Using the Redis builder script Copy the following script into a file named install-redis.sh in your application root (as a sibling of your .platform.app.yaml file). run() { # Run the compilation process. cd $PLATFORM_CACHE_DIR || exit 1; if [ ! -f  ${PLATFORM_CACHE_DIR}/phpredis/modules/redis.so  ]; then ensure_source checkout_version  $1  compile_source fi copy_lib enable_lib } enable_lib() { # Tell PHP to enable the extension. echo  Enabling PhpRedis extension.  echo  extension=${PLATFORM_APP_DIR}/redis.so  \u0026gt;\u0026gt; $PLATFORM_APP_DIR/php.ini } copy_lib() { # Copy the compiled library to the application directory. echo  Installing PhpRedis extension.  cp $PLATFORM_CACHE_DIR/phpredis/modules/redis.so $PLATFORM_APP_DIR } checkout_version () { # Check out the specific Git tag that we want to build. git checkout  $1  } ensure_source() { # Ensure that the extension source code is available and up to date. if [ -d  phpredis  ]; then cd phpredis || exit 1; git fetch --all --prune else git clone https://github.com/phpredis/phpredis.git cd phpredis || exit 1; fi } compile_source() { # Compile the extension. phpize ./configure make } ensure_environment() { # If not running in a Platform.sh build environment, do nothing. if [ -z  ${PLATFORM_CACHE_DIR}  ]; then echo  Not running in a Platform.sh build environment. Aborting Redis installation.  exit 0; fi } ensure_arguments() { # If no version was specified, don\u0026#39;t try to guess. if [ -z $1 ]; then echo  No version of the PhpRedis extension specified. You must specify a tagged version on the command line.  exit 1; fi } ensure_environment ensure_arguments  $1  run  $1  Invoke that script from your build hook, specifying a version. Any tagged version of the library is acceptable: hooks:build:| set -ebashinstall-redis.sh5.1.1If you ever wish to change the version of PhpRedis you are using, update the build hook and clear the build cache: platform project:clear-build-cache. The new version will not be used until you clear the build cache. There is no need to declare the extension in the runtime block. That is only for pre-built extensions. What the script does Downloads the PhpRedis source code. Checks out the version specified in the build hook. Compiles the extension. Copies the resulting redis.so file to your application root. Adds a line to the php.ini file in your application root to enable the extension, creating the file if necessary. If the script does not find a $PLATFORM_CACHE_DIR directory defined, it exits silently. That means if you run the build hook locally it will have no effect.",
        "section": "PHP",
        "subsections": " Using the Redis builder script What the script does  ",
        "image": "",
        "url": "/languages/php/redis.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "1a474894f62a15152af0f6ba24ecd6ed",
        "title": "Apache Solr Search",
        "description": "",
        "text": " Using Solr with the module Apache Solr Search on Drupal 7.x This page is about configuring Solr with the module Apache Solr Search . If your project uses Search API then you should follow the instructions Search API . Requirements You will need the module Apache Solr Search If you are using a make file, you can add those lines to your project.make: projects[apachesolr][version] = 1.8 Configuration The Apache Solr Search module allows configuration to be overridden from settings.php. Just add the following to your settings.platformsh.php file: \u0026lt;?php if (isset($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;])) { $relationships = json_decode(base64_decode($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;]), TRUE); if (!empty($relationships[\u0026#39;solr\u0026#39;])) { // Override search API server settings fetched from default configuration. foreach ($relationships[\u0026#39;solr\u0026#39;] as $endpoint) { // If your Solr server\u0026#39;s machine name is not  solr , update the following line. $environment_machine_name = \u0026#39;solr\u0026#39;; $environment_url =  http://  . $endpoint[\u0026#39;host\u0026#39;] .  :  . $endpoint[\u0026#39;port\u0026#39;] .  /  . $endpoint[\u0026#39;path\u0026#39;]; $conf[\u0026#39;apachesolr_default_environment\u0026#39;] = $environment_machine_name; $conf[\u0026#39;apachesolr_environments\u0026#39;][$environment_machine_name][\u0026#39;url\u0026#39;] = $environment_url; } } } Note that the Solr server must already be defined in Drupal and ideally exported to a Feature. The most common machine name used is just solr, as above. If you used a different name adjust the code as appropriate. Relationships configuration If you did not name the relationship solr in your .platform.app.yaml file, adjust the name accordingly. Also, if you have multiple Solr cores defined the above foreach() loop will not work. Most likely you will want to name the relationships by the machine name of the Solr server they should map to and then map each one individually. The file .platform.app.yaml must have the Solr relationship enabled, such as this snippet: relationships: solr: \u0026#39;solrsearch:solr\u0026#39;",
        "section": "Getting Started",
        "subsections": " Requirements Configuration Relationships configuration  ",
        "image": "",
        "url": "/frameworks/drupal7/apachesolr-module.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "83e480245f5fa48ef5d8938983abdc11",
        "title": "Configure routes",
        "description": "",
        "text": " The final configuration file you will need to modify in your repository is the .platform/routes.yaml file, which describes how an incoming HTTP request is going to be processed by Platform.sh. . ├── .platform │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml └── \u0026lt; application code \u0026gt; A minimal configuration .platform/routes.yaml for all languages will look very similar: # The routes of the project.## Each route describes how an incoming URL is going# to be processed by Platform.sh. https://{default}/ :type:upstreamupstream: app:http  https://www.{default}/ :type:redirectto: https://{default}/  Configuring the routes can be done using either an absolute URL or a URL template as shown in the examples above that have the form http://www.{default}, where {default} will be substituted by either your configured domain or those automatically generated by Platform.sh. If you set up a domain of example.com, the route configuration http://www.{default} will be resolved to http://www.example.com/. Your production (master) environment’s routes will be configured according to these rules, but so will each development environment that you activate. Each route can then be configured with the following properties: type: upstream: serves the application. Takes the form upstream: \u0026lt;application name\u0026gt;:http, using the application name set in your .platform.app.yaml. redirect: configures redirects from http://{default} to your application. cache: controls caching behavior of the route . redirects: controls redirect rules associated with the route. Note: Each language and framework may have additional attributes that you will need to include in .platform/routes.yaml depending on the needs of your application. To find out what else you may need to include to configure your routes, consult The Routes documentation for Platform.sh The documentation goes into far more extensive detail of which attributes can also be included for route configuration, and should be used as your primary reference. Language-specific templates for Platform.sh Projects: Compare the .platform/routes.yaml file from the simple template above to other templates when writing your own. In the next step, you will be able to see how Platform.sh leverages environment variables to make connecting your application to its services simple. Back I\u0026#39;ve configured my routes",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/routes-configuration.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "ed15401e7bb55e05a30363df77a8541c",
        "title": "Network Storage",
        "description": "",
        "text": " Platform.sh supports internal “storage as a service” to provide a file store that can be shared between different application containers. The network storage service enables a new kind of mount that refers to a shared service rather than to a local directory. Any application can use both local and/or service mounts, or neither. Supported versions Grid Dedicated 1.0 None available (This is a reference to a version of our network storage implementation, not to a version of a 3rd party application.) Supported regions The Network storage service is available on all regions except: eu.platform.sh us.platform.sh If you are on one of those and require the service we suggest you migrate your project to one of the newer regions (such as eu-2, us-2, ca, au, fr-1 or de-2). Define the service First, declare a new service in the services.yaml file like so: files:type:network-storage:1.0disk:256 This example creates a service named files that is of type network-storage, and gives it 256 MB of storage total. Declare the mount Second, add the following entry to your mounts list: mounts:\u0026#39;my/files\u0026#39;:source:serviceservice:filessource_path:files This block will declare a writeable mount on the application container at the path my/files, which will be provided by the files service defined above. The source_path specifies the path within the network service that the mount points to. It is often easiest to have it match the name of the mount point itself but that is not required. Note that you do not need to add a relationship to point to the files service. That is handled automatically by the system. The application container can now read from and write to the my/files path just as if it were a local writeable mount. Note: There is a small performance hit for using a network mount over a local mount. In most cases it should not be noticeable. However, high-volume sequential file creation (that is, creating a large number of small files in rapid succession) may see a more significant performance hit. If that is something your application does regularly then a local mount will be more effective. Multi-application usage If your project contains more than one application (that is, multiple directories with their own .platform.app.yaml files), they can all use the same network mounts if desired. If the source_path is the same in both .platform.app.yaml files then the files will be shared between both applications, even if the mount location is different. It is also possible to have one application mount a source_path that is a subdirectory of another application’s mount. For example: app1: mounts:\u0026#39;my/files\u0026#39;:source:serviceservice:filessource_path:files app2: mounts:\u0026#39;process\u0026#39;:source:serviceservice:filessource_path:uploads/incoming\u0026#39;done\u0026#39;:source:serviceservice:filessource_path:uploads/doneIn this example, app1 will have access to the entire uploads directory by writing to web/uploads. app2, by contrast, will have two mounts that it can write to: process and done. The process mount will refer to the same directory as the web/uploads/incoming directory does on app1, and the done mount will refer to the same directory as the web/uploads/done directory on app1. Worker instances When defining a Worker instance it is important to keep in mind what mount behavior is desired. Unless the mounts block is defined within the web and workers sections separately, a top level mounts block will apply to both instances. However, local mounts will be a separate storage area for each instance while service mounts will refer to the same file system. For example: name:apptype:php:7.2disk:1024mounts:\u0026#39;network_dir\u0026#39;:source:serviceservice:filessource_path:our_stuff\u0026#39;local_dir\u0026#39;:source:localsource_path:my_stuffweb:locations: / :root: public passthru: /index.php workers:queue:commands:start:| php worker.phpIn this case, both the web instance and the queue worker will have two mount points: network_dir and local_dir. The local_dir mount on each will be independent and not connected to each other at all, and they will each take 1024 MB of space. The network_dir mount on each will point to the same network storage space on the files service. They will both be able to read and write to it simultaneously. The amount of space it has available will depend on the disk key specified in services.yaml. How do I give my workers access to my main application’s files? The most common use case for network-storage is to allow a CMS-driven site to use a worker that has access to the same file mounts as the web-serving application. For that case, all that is needed is to set the necessary file mounts as service mounts. For example, the following .platform.app.yaml file (fragment) will keep Drupal files directories shared between web and worker instances while keeping the Drush backup directory web-only (as it has no need to be shared). (This assumes a service named files has already been defined in services.yaml.) name:\u0026#39;app\u0026#39;type:\u0026#39;php:7.2\u0026#39;relationships:database:\u0026#39;db:mysql\u0026#39;hooks:# ...web:locations:\u0026#39;/\u0026#39;:# ...disk:1024mounts:# The public and private files directories are# network mounts shared by web and workers.\u0026#39;web/sites/default/files\u0026#39;:source:serviceservice:filessource_path:files\u0026#39;private\u0026#39;:source:serviceservice:filessource_path:private# The backup, temp, and cache directories for# Drupal\u0026#39;s CLI tools don\u0026#39;t need to be shared.# It wouldn\u0026#39;t hurt anything to make them network# shares, however.\u0026#39;/.drush\u0026#39;:source:localsource_path:drush\u0026#39;tmp\u0026#39;:source:localsource_path:tmp\u0026#39;drush-backups\u0026#39;:source:localsource_path:drush-backups\u0026#39;/.console\u0026#39;:source:localsource_path:console# Crons run on the web container, so they have the# same mounts as the web container.crons:drupal:spec:\u0026#39;*/20 * * * *\u0026#39;cmd:\u0026#39;cd web ; drush core-cron\u0026#39;# The worker defined here will also have the same 6 mounts;# 2 of them will be shared with the web container,# the other 4 will be local to the worker.workers:queue:commands:start:| cd web \u0026amp;\u0026amp; drush queue-run myqueueHow can I migrate a local storage to a network storage? There is no automated way of transferring data from one storage type to another. However, the process is fundamentally “just” moving files around on disk, so it is reasonably straightforward. Suppose you have this mount configuration: mounts:web/uploads:source:localsource_path:uploadsAnd want to move that to a network storage mount. The following approximate steps will do so with a minimum of service interruption. Add a new network-storage service, named files, that has at least enough space for your existing files with some buffer. You may need to increase your plan’s disk size to accommodate it. Add a new mount to the network storage service on a non-public directory: mounts:new-uploads:source:serviceservice:filessource_path:uploads(Remember the source_path can be the same since they’re on different storage services.) Deploy these changes. Then use rsync to copy all files from the local mount to the network mount. (Be careful of the trailing /.) rsync -avz web/uploads/* new-uploads/ Reverse the mounts. That is, point the web/uploads directory to the network mount instead: mounts:web/uploads:source:serviceservice:filessource_path:uploadsold-uploads:source:localsource_path:uploadsCommit and push that. Test to make sure the network files are accessible. Cleanup. First, run another rsync just to make sure any files uploaded during the transition are not lost. (Note the command is different here.) rsync -avz old-uploads/* web/uploads/ Once you’re confident all the files are accounted for, delete the entire contents of old-uploads. If you do not, the files will remain on disk but inaccessible, just eating up disk space needlessly. Once that’s done you can remove the old-uploads mount and push again to finish the process. You are also free to reduce the disk size in the .platform.app.yaml file if desired, but make sure to leave enough for any remaining local mounts. Why do I get an invalid service type error with network storage? The network-storage service is only available on our newer regions. If you are running on the older us or eu regions and try to create a network-storage service you will receive this error. To make use of network-storage you will need to migrate to the newer us-2 or eu-2 regions. See our tutorial on how to migrate regions for more information.",
        "section": "Configure services",
        "subsections": " Supported versions Supported regions Define the service Declare the mount Multi-application usage Worker instances How do I give my workers access to my main application\u0026rsquo;s files? How can I migrate a local storage to a network storage? Why do I get an invalid service type error with network storage?  ",
        "image": "",
        "url": "/configuration/services/network-storage.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "ddbc4f599821eaf5253c34ccf4987b27",
        "title": "Protective block",
        "description": "",
        "text": " The Platform.sh service has a protective blocking feature that, under certain circumstances, restricts access to web sites with security vulnerabilities. We use this partial blocking method to prevent exploitation of known security vulnerabilities. The protective block is meant for high impact, low complexity attacks. The Platform.sh security block Outdated software often contains known vulnerabilities that can be exploited from the Internet. Sites that can be exploited are protected by Platform.sh. The system partially blocks access to these sites. How the protective block works Platform.sh maintains a database of signatures of known security vulnerabilities in open-source software that are commonly deployed on our infrastructure. The security check only analyze known vulnerabilities in open-source projects like Drupal, Symfony or WordPress. It cannot examine customizations written by Platform.sh customers. We analyze the code of your application: When you push new code to Git Regularly when new vulnerabilities are added to our database If a vulnerability deemed as critical is detected in your application, Platform.sh is going to reject the Git push. We run two types of blocks: For production websites, we run a “partial block” that allows the site to stay mostly online. Depending on the nature of the vulnerability, parts of a request, such as a query string, cookies or any additional headers, may be removed from GET requests. All other requests may be blocked entirely - this could apply to logging in, form submission or product checkout. For development websites, we run complete blocks, and the error message gives you detailed information about the vulnerability. Unblocking is automated upon resolution of the security risk. The block is removed soon after a customer applies a security upgrade and removes the vulnerability. Opting out of the protective block The protective block is there to protect you against known vulnerabilities in the software you deploy on Platform.sh . If nonetheless you want to opt out of the protective block, you simply need to specify it in your .platform.app.yaml like this: preflight:enabled:falseYou can also explicitly opt-out of some specific check like this: preflight:enabled:trueignore_rules:[ drupal:SA-CORE-2014-005 ]",
        "section": "Security and compliance",
        "subsections": " The Platform.sh security block How the protective block works Opting out of the protective block  ",
        "image": "",
        "url": "/security/protective-block.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "f686f84c271008c87faf3fe54b6cb243",
        "title": "Sending E-mail",
        "description": "",
        "text": " By default only the master environment can send emails. For the non-master environments, you can configure outgoing emails via the [management console]({{\u0026lt; relref “/administration/web/configure-environment.md#settings” \u0026gt;}}). Emails from Platform.sh are sent via a SendGrid-based SMTP proxy. Each Platform.sh project is provisioned as a SendGrid sub-account. These SendGrid sub-accounts are capped at 12k emails per month. You can use /usr/sbin/sendmail on your application container to send emails with the assigned SendGrid sub-account. Alternatively, you can use the PLATFORM_SMTP_HOST environment variable to use in your SMTP configuration.” We do not guarantee the deliverability of emails, and we do not support white-labeling them. Our SMTP proxy is intended as a zero-configuration, best effort service. If needed, you can instead use your own SMTP server or email delivery service provider. In that case, please bear in mind that TCP port 25 is blocked for security reasons; use TCP port 465 or 587 instead. Note: You may follow the SPF setup guidelines on SendGrid to improve email deliverability with our SMTP proxy. However, as we do not support white-labeling of emails, DKIM is not provided for our standard email handling (meaning that DMARC can’t be set up either). Thus, for maximum deliverability own mail host must be engaged. Enabling/disabling email Email support can be enabled/disabled per-environment. By default, it is enabled on the master environment and disabled elsewhere. That can be toggled in through the management console or via the command line, like so: platform environment:info enable_smtp true platform environment:info enable_smtp false When SMTP support is enabled the environment variable PLATFORM_SMTP_HOST will be populated with the address of the SMTP host that should be used. When SMTP support is disabled that environment variable will be empty. Note: Changing the SMTP status will not take effect immediately. You will need to issue a new build, not just a new deploy, for the changes to take effect. Sending email in PHP When you send email, you can simply use the built-in mail() function in PHP. The PHP runtime is configured to send email automatically via the assigned SendGrid sub-account. Note that the From header is required; email will not send if that header is missing. Beware of the potential security problems when using the mail() function, which arise when using user-supplied input in the fifth ($additional_parameters) argument. See the PHP mail() documentation for more information. SwiftMailer In Symfony, if you use the default SwiftMailer service, we recommend the following settings in your app/config/parameters.yaml: parameters:mailer_transport:smtpmailer_host: %env(PLATFORM_SMTP_HOST)% mailer_user:nullmailer_password:nullIf you are using a file spool facility, you will probably need to setup a read/write mount for it in .platform.app.yaml, for example: mounts:\u0026#39;app/spool\u0026#39;:source:localsource_path:spoolSending email in Java JavaMail is a Java API used to send and receive email via SMTP, POP3 and IMAP. JavaMail is built into the Jakarta EE platform, but also provides an optional package for use in Java SE. Jakarta Mail defines a platform-independent and protocol-independent framework to build mail and messaging applications. Below the sample code that uses Jakarta Mail : import sh.platform.config.Config; import javax.mail.Message; import javax.mail.MessagingException; import javax.mail.Session; import javax.mail.Transport; import javax.mail.internet.InternetAddress; import javax.mail.internet.MimeMessage; import java.util.Properties; import java.util.logging.Level; import java.util.logging.Logger; public class JavaEmailSender { private static final Logger LOGGER = Logger.getLogger(JavaEmailSender.class.getName()); public void send() { Config config = new Config(); String to =   ;//change accordingly String from =   ;//change accordingly String host = config.getSmtpHost(); //or IP address //Get the session object Properties properties = System.getProperties(); properties.setProperty( mail.smtp.host , host); Session session = Session.getDefaultInstance(properties); //compose the message try { MimeMessage message = new MimeMessage(session); message.setFrom(new InternetAddress(from)); message.addRecipient(Message.RecipientType.TO, new InternetAddress(to)); message.setSubject( Ping ); message.setText( Hello, this is example of sending email  ); // Send message Transport.send(message); System.out.println( message sent successfully.... ); } catch (MessagingException exp) { exp.printStackTrace(); LOGGER.log(Level.SEVERE,  there is an error to send an message , exp); } } } There is plenty of additional documentation about using JavaMail, like this one, that shows how to send email with HTML formatting and attachments . References Platform.sh Email documentation https://mkyong.com/java/java-how-to-send-email/ JavaMail API Ports Port 465 and 587 should be used to send email to your own external email server. Port 25 should be used to send through PLATFORM_SMTP_HOST. (this is the default in most mailers). We proxy your emails through our own smtp host, and encrypt them over port 465 before sending them through to the outside world.",
        "section": "Development",
        "subsections": " Enabling/disabling email Sending email in PHP  SwiftMailer   Sending email in Java  References   Ports  ",
        "image": "",
        "url": "/development/email.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "9a8da44c1d094537af4601d1bbfcfb12",
        "title": "TYPO3 - Getting started",
        "description": "",
        "text": " The supported way to run TYPO3 on Platform.sh is through Composer . If you do not already have it installed then you should do so before proceeding. Avoiding deadlock A default TYPO3 installation has a risk of deadlocks when run on low-worker PHP-FPM configurations. Specifically, TYPO3 handles 403 and 404 error pages by issuing a full HTTP request back to itself with no timeout, which can lead to process starvation. There are two required steps to avoid this problem: Add the line $GLOBALS['TYPO3_CONF_VARS']['HTTP']['timeout'] = 3; to your typo3config/AdditionalConfiguration.php file, which will set an HTTP timeout of 3 seconds instead of the default several minutes. (You may select a different number, but keep it under 10 seconds.) Install and enable the Local Page Error Handler plugin for TYPO3. The easiest way to do so is through composer: composer require pixelant/pxa-lpeh that will provide a non-loopback way to handle error pages which avoids this race condition entirely. Both of these steps are already set in the Platform.sh TYPO3 template , although you will need to enable the plugin yourself post-install. Configure your app The ideal .platform.app.yaml file will vary from project to project, and you are free to customize yours as needed. A recommended baseline TYPO3 configuration is listed below, and can also be found in our TYPO3 template project . # This file describes an application. You can have multiple applications# in the same project.# The name of this app. Must be unique within a project.name:app# The type of the application to build.type:php:7.4build:flavor:composer# The relationships of the application with services or other applications.# The left-hand side is the name of the relationship as it will be exposed# to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand# side is in the form `\u003cservice name\u003e:\u003cendpoint name\u003e`.relationships:database:'db:mysql'rediscache:'cache:redis'# The configuration of app when it is exposed to the web.web:locations:'/':# The public directory of the app, relative to its root.root:'public'passthru:'/index.php'index:- 'index.php'allow:falserules:# Allow access to common static The size of the persistent disk of the application (in MB).disk:2048# The mounts that will be performed when the package is deployed.mounts: public/typo3temp :source:localsource_path: typo3temp  public/fileadmin :source:localsource_path: fileadmin  var :source:localsource_path: var # The hooks that will be performed when the package is deployed.hooks:build:| set -e# Install the PhpRedis extensionbashinstall-redis.sh5.1.1phpvendor/bin/typo3cmsinstall:setup--install-steps-config=src/SetupConfiguration.yaml--no-interaction--skip-extension-setupphpvendor/bin/typo3cmsinstall:generatepackagestates# Enable the install tool for 60mins after deploymenttouchpublic/typo3conf/ENABLE_INSTALL_TOOL# Keep the checked-in LocalConfiguration available, but make the actual file writable later-on# by creating a symlink which will be accesible belowif[-fpublic/typo3conf/LocalConfiguration.php];thenmvpublic/typo3conf/LocalConfiguration.phppublic/typo3conf/LocalConfiguration.FromSource.phpln-sf../../var/LocalConfiguration.phppublic/typo3conf/LocalConfiguration.phpfi;# Clean up the FIRST_INSTALL file, that was createdif[-fpublic/FIRST_INSTALL];thenrmpublic/FIRST_INSTALLfi;deploy:| set -eif[!-fvar/platformsh.installed];then# copy the created LocalConfiguration into the writable locationcppublic/typo3conf/LocalConfiguration.FromSource.phpvar/LocalConfiguration.php# This creates the initial admin user with a default password.# *CHANGE THIS VALUE IMMEDIATELY AFTER on * * * * cmd: vendor/bin/typo3 scheduler:run ",
        "section": "Featured frameworks",
        "subsections": " Avoiding deadlock Configure your app  ",
        "image": "",
        "url": "/frameworks/typo3.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "d87b913352af9d10b8d6569a95f1aaa7",
        "title": "Using Lando for local Drupal development",
        "description": "",
        "text": " Lando is a local development platform that works well with Platform.sh. Once installed locally it is a simple matter to create an approximate equivalent of your Platform.sh environment for development. If using Drupal 8 there is a drupal8 recipe available that is a good starting point for your site. Setting up the Lando environment # Download your project. platform get \u0026lt;projectId\u0026gt; cd \u0026lt;projectId\u0026gt; # Create a basic Drupal 8 Lando config file. lando init --recipe drupal8 # Commit the Lando config file to your repository. git add .lando.yml git commit -m  Add Lando configuration  You can now customize the configuration file as needed. In addition to the general recommendations for all Lando-with-Platform.sh sites the following additions are recommended for Drupal 8: # Name the application the same as in your .platform.app.yaml.name:apprecipe:drupal8config:# Enable XDebug for local development.xdebug:true# If you are providing a custom php.ini configuration for Platform.sh, specifying# the same file here will allow the one file to drive both environments.conf:php:php.iniOnce the configuration file is set, you should start Lando running: lando start # To download dependencies. lando composer install Setting up Drupal for Lando The Platform.sh environment variables are not available in Lando, but Lando provides its own alternates. To connect Drupal to the database add the following to your web/sites/default/settings.local.php file: \u0026lt;?php $databases[\u0026#39;default\u0026#39;][\u0026#39;default\u0026#39;] = [ \u0026#39;driver\u0026#39; =\u0026gt; \u0026#39;mysql\u0026#39;, \u0026#39;database\u0026#39; =\u0026gt; getenv(\u0026#39;DB_NAME\u0026#39;), \u0026#39;username\u0026#39; =\u0026gt; getenv(\u0026#39;DB_USER\u0026#39;), \u0026#39;password\u0026#39; =\u0026gt; getenv(\u0026#39;DB_PASSWORD\u0026#39;), \u0026#39;host\u0026#39; =\u0026gt; getenv(\u0026#39;DB_HOST\u0026#39;), \u0026#39;port\u0026#39; =\u0026gt; getenv(\u0026#39;DB_PORT\u0026#39;), ]; If you need to add additional services (Redis cache, Solr, Elasticsearch, etc.) see the Lando documentation . Place any additional configuration necessary for those in the settings.local.php file as well. Downloading data from Platform.sh to Lando Assuming you’re using a standard Drupal 8 configuration, the following commands will fully synchronize the SQL database and uploaded files to your local system. First, make sure the git branch you have checked out is the environment you want to synchronize from. Then run the following from the repository root: # Download a database backup and import it into Lando. platform db:dump --gzip -f database.sql.gz lando db-import database.sql.gz # Download all user files locally, skipping those already found locally. rsync -az `platform ssh --pipe`:/app/web/sites/default/files/ ./web/sites/default/files/ rsync -az `platform ssh --pipe`:/app/private/ ./private/ If you have customized the file mounts in your .platform.app.yaml file then update the rsync commands accordingly.",
        "section": "Getting Started",
        "subsections": " Setting up the Lando environment Setting up Drupal for Lando Downloading data from Platform.sh to Lando  ",
        "image": "",
        "url": "/frameworks/drupal8/lando.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "1bff9011c2335f06ca9790010536d649",
        "title": "Variables",
        "description": "",
        "text": " Platform.sh provides a number of ways to set variables , either globally or specific to a single environment. For values that should be consistent between different environments (because they’re configuring the application or runtime itself, generally) the easiest way to control them is to set them in the .platform.app.yaml file. Only prefixed variables may be set from the .platform.app.yaml file. Some prefixes have specific meaning while others are only significant to a particular application. Nested variables will be automatically converted into a nested array or list structure as appropriate to the language. For example, the following section in .platform.app.yaml will set a single variable named env:AUTHOR to the value Juan. variables:env:AUTHOR:\u0026#39;Juan\u0026#39;That will have the exact same runtime effect as setting a project variable via the CLI as follows, except it will be versioned along with the code: $ platform variable:create env:AUTHOR --level project --value Juan The variable name may itself have punctuation in it. For example, to set a Drupal 8 configuration override (assuming you’re using the recommended settings.platformsh.php file) you can do the following: variables:d8config: system.site:name : \u0026#39;My site rocks\u0026#39;This will create a Platform.sh variable, that is, an item in the $PLATFORM_VARIABLES environment variable, named d8config:system.site:name with value “My site rocks”. Complex values The value for a variable may be more than just a string; it may also be a nested structure. If the variable is in the env namespace, it will be mapped to a Unix environment variable as a JSON string. If not, it will be included in the PLATFORM_VARIABLES environment variable. For example, the following variable definitions: variables:env:BASIC: a string INGREDIENTS:- \u0026#39;peanut butter\u0026#39;- \u0026#39;jelly\u0026#39;QUANTITIES: milk :  1 liter  cookies :  1 kg stuff:STEPS:[\u0026#39;un\u0026#39;,\u0026#39;deux\u0026#39;,\u0026#39;trois\u0026#39;]COLORS:red:\u0026#39;#FF0000\u0026#39;green:\u0026#39;#00FF00\u0026#39;blue:\u0026#39;#0000FF\u0026#39;Would appear this way in various languages: PHP Python \u0026lt;?php var_dump($_ENV[\u0026#39;BASIC\u0026#39;]); // string(8)  a string  var_dump($_ENV[\u0026#39;INGREDIENTS\u0026#39;]); // string(26)  [ peanut butter ,  jelly ]  var_dump($_ENV[\u0026#39;QUANTITIES\u0026#39;]); // string(38)  { milk :  1 liter ,  cookies :  1 kg }  $variables = json_decode(base64_decode($_ENV[\u0026#39;PLATFORM_VARIABLES\u0026#39;]), TRUE); print_r($variables[\u0026#39;stuff:STEPS\u0026#39;]); /* array(3) { [0]=\u0026gt; string(2)  un  [1]=\u0026gt; string(4)  deux  [2]=\u0026gt; string(5)  trois  } */ print_r($variables[\u0026#39;stuff:COLORS\u0026#39;]); /* array(3) { [ red ]=\u0026gt; string(7)  #FF0000  [ green ]=\u0026gt; string(7)  #00FF00  [ blue ]=\u0026gt; string(7)  #0000FF  } */ import os import json import base64 print os.getenv(\u0026#39;BASIC\u0026#39;) // a string print os.getenv(\u0026#39;INGREDIENTS\u0026#39;) // [ peanut butter ,  jelly ] print os.getenv(\u0026#39;QUANTITIES\u0026#39;) // { milk :  1 liter ,  cookies :  1 kg } variables = json.loads(base64.b64decode(os.getenv(\u0026#39;PLATFORM_VARIABLES\u0026#39;)).decode(\u0026#39;utf-8\u0026#39;)) print variables[\u0026#39;stuff:STEPS\u0026#39;] // [u\u0026#39;un\u0026#39;, u\u0026#39;deux\u0026#39;, u\u0026#39;trois\u0026#39;] print variables[\u0026#39;stuff:COLORS\u0026#39;] // {u\u0026#39;blue\u0026#39;: u\u0026#39;#0000FF\u0026#39;, u\u0026#39;green\u0026#39;: u\u0026#39;#00FF00\u0026#39;, u\u0026#39;red\u0026#39;: u\u0026#39;#FF0000\u0026#39;}",
        "section": "Configure your application",
        "subsections": " Complex values  ",
        "image": "",
        "url": "/configuration/app/variables.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "af1d5514d9f13903877a1105ec5ace41",
        "title": "Accessing logs",
        "description": "",
        "text": " Logs for various tasks on an application container are available in the /var/log directory. They can be accessed on the normal shell after logging in with platform ssh. Alternatively, they may also be accessed remotely using the platform logcommand. The CLI lets you specify which log file to access (the name of the file below minus the .log extension, as well as view the entire file in a pager, only the most recent lines, and so forth. Run platform log --help for complete documentation. A number of different log files are available depending on the application container in use. Although the files in /var/log are writable, they should not be written to directly. Only write to it via standard logging mechanisms, such as your application’s logging facility. If your application has its own logging mechanism that should be written to a dedicated logs mount in your application. All log files are trimmed to 100 MB automatically. But if you need to have complete logs, you can set up cron which will upload them to third-party storage. Contextual Code made a simple and well-described example how to achieve it. access.log This is the raw access log for the nginx instance running on the application container. That is, it does not include any requests that return a redirect or cache hit from the router . app.log Any log messages generated by the application will be sent to this file. That includes language errors such as PHP Errors, Warnings, and Notices, as well as uncaught exceptions. cron.log The cron log contains the output of all recent cron executions. If there is no cron hook specified in the container configuration then this file will be absent. It also will not exist until the first time cron has run. deploy.log The deploy log contains the output of the most recent run of the deploy hook for the container. If there is no deploy hook then this file will be absent. nginx/error.log nginx startup log messages will be recorded in this file. It is rarely needed except when debugging possible nginx configuration errors. This file is not currently available using the platform log command. error.log nginx-level errors that occur once nginx has fully started will be recorded here. This will include HTTP 500 errors for missing directories, file types that are excluded based on the .platform.app.yaml file, etc. php.access.log On a PHP container, the php.access.log contains a record of all requests to the PHP service. post_deploy.log The post_deploy log contains the output of the most recent run of the post_deploy hook for the container. If there is no post_deploy hook then this file will be absent.",
        "section": "Development",
        "subsections": " access.log app.log cron.log deploy.log nginx/error.log error.log php.access.log post_deploy.log  ",
        "image": "",
        "url": "/development/logs.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "f434ce14b76096738eaa23d09a26ab65",
        "title": "Build and deploy",
        "description": "",
        "text": " The .platform.app.yaml file provides a number of ways to control how an application gets turned from a directory in Git into a running application. There are three blocks that control different parts of the process: the build flavor, dependencies, and hooks. The build process will run the build flavor, then install dependencies, then run the user-provided build hook. The deploy process will run the deploy hook. Build The build defines what happens when building the application. Its only property is flavor, which specifies a default set of build tasks to run. Flavors are language-specific. PHP (composer by default) composer will run composer --no-ansi --no-interaction install --no-progress --prefer-dist --optimize-autoloader if a composer.json file is detected. drupal will run drush make automatically in one of a few different ways. See the Drupal 7 documentation for more details. We recommend only using this build mode for Drupal 7. Node.js (default by default) default will run npm prune --userconfig .npmrc \u0026amp;\u0026amp; npm install --userconfig .npmrc if a package.json file is detected. Note that this also allows you to provide a custom .npmrc file in the root of your application (as a sibling of the .platform.app.yaml file.) In all languages you can also specify a flavor of none (which is the default for any language other than PHP and Node.js); as the name suggests it will take no action at all. That is useful when you want complete control over your build steps, such as to run a custom Composer command or use an alternate Node.js package manager. build:flavor:composerBuild dependencies It is also possible to install additional system-level dependencies as part of the build process. These can be installed before the build hook runs using the native package manager for several web-focused languages. Platform.sh supports pulling any dependencies for the following languages: PHP (via Composer ) Python 2 and 3 (via Pip ) Ruby (via Bundler ) Node.js (via NPM ) Java (via Apache Maven , Gradle , or Apache Ant ) Python dependencies Applications can have both Python 2 and Python 3 dependencies, using the version of each that is packaged with the most recent Debian distribution. The format of Python dependencies complies with PEP 394 . That is, specifying a dependency in a python or python2 block will use pip2 and Python 2, while specifying a dependency in a python3 block will use pip3 and Python 3. We suggest that you specify your dependencies with the specific version of Python you wish to use (i.e. either python2 or python3), rather than with the generic python declaration, to ensure your application will function normally in the future if Debian’s default version of Python changes. Specifying dependencies Build dependencies are independent of the eventual dependencies of your application and are available in the PATH during the build process and in the runtime environment of your application. Note that in many cases a given package can be installed either as a global dependency or as part of your application’s own dependencies. In such cases it’s up to you which one to use. You can specify those dependencies as shown below: dependencies:php:# Specify one Composer package per line.drush/drush:\u0026#39;8.0.0\u0026#39;python:# Specify one Python 2 package per line.behave:\u0026#39;*\u0026#39;python2:# Specify one Python 2 package per line.requests:\u0026#39;*\u0026#39;python3:# Specify one Python 3 package per line.numpy:\u0026#39;*\u0026#39;ruby:# Specify one Bundler package per line.sass:\u0026#39;3.4.7\u0026#39;nodejs:# Specify one NPM package per line.grunt-cli:\u0026#39;~0.1.13\u0026#39;Note that the package name format for each language is defined by the package manager used; similarly, the version constraint string will be interpreted by the package manager. Consult the appropriate package manager’s documentation for the supported formats. Hooks Platform.sh supports three “hooks”, or points in the deployment of a new version of an application that you can inject a custom script into. Each runs at a different stage of the process. Each hook is executed as a single script, so they will be considered failed only if the final command in them fails. To cause them to fail on the first failed command, add set -e to the beginning of the hook. If a build hook fails for any reason then the build is aborted and the deploy will not happen. The “home” directory for each hook is the application root. If your scripts need to be run from the doc root of your application, you will need to cd to it first; e.g.: cd web. hooks:build:| set -ecdwebcpsome_file.phpsome_other_file.phpdeploy:| update_schema.shpost_deploy:| set -eimport_new_content.shclear_cache.shThe | character tells YAML that the lines that follow should be interpreted literally as a newline-containing string rather than as multiple lines of YAML properties. Hooks are executed using the dash shell, not the bash shell used by normal SSH logins. In most cases that makes no difference but may impact some more involved scripts. Build hook The build hook is run after the build flavor (if any). At this point no services (such as a database) are available nor any persistent file mounts, as the application has not yet been deployed. Environment variables that exist only at runtime such as PLATFORM_BRANCH, PLATFORM_DOCUMENT_ROOT etc. are not available during this phase. The full list of build time and runtime variables is available on the variables page . There are three writeable directories at this time: $PLATFORM_APP_DIR - This is where your code is checked out, and is the working directory when the build hook starts. The contents of this directory after the build hook is what will be “the application” that gets deployed. (This directory is always /app, but it’s better to use the variable or rely on the working directory than to hard code that.) Most of the time, this is the only directory you use. $PLATFORM_CACHE_DIR - This directory persists between builds, but is NOT deployed as part of your application. It’s a good place for temporary build artifacts, such as downloaded .tar.gz or .zip files, that can be reused between builds. Note that it is shared by all builds on all branches, so if using the cache directory make sure your build code accounts for that. /tmp - The temp directory is also useful for writing files that are not needed in the final application, but it will be wiped between each build. There are no constraints on what can be downloaded during your build hook except for the amount of disk available at that time. Independent of the mounted disk size you have allocated for deployment, build environments (the application plus the cache directory) and therefore application images are limited to 4 GB during the build phase. If you exceed this limit you will receive a No space left on device error. It is possible to increase this limit in certain situations, but it will be necessary to open a support ticket in order to do so. Consult the Troubleshooting guide for more information on this topic. Deploy hook The deploy hook is run after the application container has been started, but before it has started accepting requests. You can access other services at this stage (MySQL, Solr, Redis, etc.). The disk where the application lives is read-only at this point. Note that the deploy hook will only run on a web instance, not on a worker instance. Be aware: The deploy hook blocks the site accepting new requests. If your deploy hook is only a few seconds then incoming requests in that time are paused and will continue when the hook completes, effectively appearing as the site just took a few extra seconds to respond. If it takes too long, however, requests cannot be held and will appear as dropped connections. Only run tasks in your deploy hook that have to be run exclusively, such as database schema updates or some types of cache clear. A post-deploy task that can safely run concurrently with new incoming requests should be run as a post_deploy hook instead. After a Git push, in addition to the log shown in the activity log, you can see the results of the deploy hook in the /var/log/deploy.log file when logged in to the environment via SSH. It contains the log of the execution of the deployment hook. For example: [2014-07-03 10:03:51.100476] Launching hook \u0026#39;cd public ; drush -y updatedb\u0026#39;. My_custom_profile 7001 Update 7001: Enable the Platform module. Do you wish to run all pending updates? (y/n): y Performed update: my_custom_profile_update_7001 \u0026#39;all\u0026#39; cache was cleared. Finished performing updates. Post-Deploy hook The post_deploy hook functions exactly the same as the deploy hook, but after the container is accepting connections. That is, it will run concurrently with normal incoming traffic. That makes it well suited to any updates that do not require exclusive database access. What is “safe” to run in a post_deploy hook vs. in a deploy hook will vary by the application. Often times content imports, some types of cache warmups, and other such tasks are good candidates for a post_deploy hook. The post_deploy hook logs to its own file in addition to the activity log, /var/log/post-deploy.log. How do I compile Sass files as part of a build? As a good example of combining dependencies and hooks, you can compile your SASS files using Grunt. Let’s assume that your application has Sass source files (Sass being a Ruby tool) in the web/styles directory. That directory also contains a package.json file for npm and Gruntfile.js for Grunt (a Node.js tool). The following blocks will download a specific version of Sass and Grunt pre-build, then during the build step will use them to install any Grunt dependencies and then run the grunt command. This assumes that your Grunt command includes the Sass compile command. dependencies:ruby:sass:\u0026#39;3.4.7\u0026#39;nodejs:grunt-cli:\u0026#39;~0.1.13\u0026#39;hooks:build:| cd web/stylesnpminstallgruntHow can I run certain commands only on certain environments? The deploy and post_deploy hooks have access to all of the same environment variables as the application does normally, which makes it possible to vary those hooks based on the environment. A common example is to enable certain modules only in non-production environments. Because the hook is simply a shell script we have full access to all shell scripting capabilities, such as if/then directives. The following Drupal example checks the $PLATFORM_BRANCH variable to see if we’re in a production environment (the master branch) or not. If so, it forces the devel module to be disabled. If not, it forces the devel module to be enabled, and also uses the drush Drupal command line tool to strip user-specific information from the database. hooks:deploy:| if [  $PLATFORM_BRANCH  = master ]; then# Use Drush to disable the Devel module on the Master environment.drushdisdevel-yelse# Use Drush to enable the Devel module on other environments.drushendevel-y# Sanitize your database and get rid of sensitive information from Master environment.drush-ysql-sanitize--sanitize-email=user_%uid@example.com--sanitize-password=custompasswordfidrush-yupdatedb",
        "section": "Configure your application",
        "subsections": " Build  PHP (composer by default) Node.js (default by default)   Build dependencies  Python dependencies Specifying dependencies   Hooks  Build hook Deploy hook Post-Deploy hook   How do I compile Sass files as part of a build? How can I run certain commands only on certain environments?  ",
        "image": "",
        "url": "/configuration/app/build.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "c98c87467c55692189a11832c170de1d",
        "title": "Compliance guidance",
        "description": "",
        "text": " Platform.sh has many PCI and SOC 2 certified customers using our services. Some requirements are the responsibility of the host and others are the responsibility of the application developer. Basic compliance questions can be handled by our support team via a ticket. For more advanced questions or walk-through of a full audit please contact your Platform.sh Account Manager. Overview Platform.sh provides a Platform as a Service (PaaS) solution which our customers may use for applications requiring PCI compliance. Note: Cardholder processing activity is discouraged. Please use a third party processor. Security \u0026amp; Compensating Controls For a list of security measures, please see our Security page . Please take note that customer environments are deployed in a read-only instance, segregated with GRE and IPSEC tunnels, which often permits compensating controls to be claimed for several PCI requirements. Because customers can use our PaaS in a variety of ways, the best approach with auditors is to focus is on “What do I, the customer, control/configure and how is it managed in a compliant manner?” Responsibility Platform.sh and customers have shared responsibility for ensuring an up to date and secure system. Compliance is ultimately the responsibility of the customer, however. Platform.sh is responsible for: Physical and Environmental controls - We use third party hosting and thus these requirements are passed through to those providers (e.g. AWS). Patch Management - Platform.sh is responsible for patching and fixing underlying system software, management software, and environment images. Configuration Management - Platform.sh maintains the configuration of its infrastructure and devices. Awareness and Training - Platform.sh trains its own employees in secure software development and management. Capacity Management - Platform.sh is responsible for capacity management of the infrastructure, such as server allocation and bandwidth management. Access Control - Platform.sh is responsible for providing access control mechanisms to customers and for vetting all Platform.sh personnel access. Backups - Platform.sh is responsible for backing up the infrastructure and management components of the system. On Platform.sh Dedicated Enterprise (only), Platform.sh will also backup application code and databases on behalf of customers. Customers are responsible for: Patch Management - Customers are responsible for maintaining and patching application code uploaded to Platform.sh, either written by them or by a third party. Configuration Management - Customers are responsible for the secure configuration of their application, including Platform.sh configuration and routes managed through YAML files. Awareness and Training - Customers are responsible for training their own employees and users in secure software practices. Capacity Management - Customers are responsible for ensuring their application containers have sufficient resources for their selected tasks. Access Control - Customers are responsible for effectively leveraging available access control mechanisms, including proper access control settings, secrets management, ssh keys management, and the use of two-factor authentication. Backups - On Platform.sh Professional customers are responsible for all application and database backups. The Platform.sh PCI Responsibility Matrix (Excel) provides guidance on shared responsibilities to achieve PCI DSS compliance using PCI DSS 3.2 as a reference. This document was prepared by Platform.sh for informational purposes only. It is provided as a courtesy to facilitate customers’ consideration and review of the Platform.sh PCI Responsibility Summary, but the spreadsheet does not replace and is separate from the Platform.sh PCI AOC (available upon request). Customers may use the spreadsheet in conjunction with the Platform.sh PCI AOC solely to facilitate understanding elements of the report. The spreadsheet does not create any warranties, representations, contractual commitments, conditions, or assurances from Platform.sh, its affiliates, vendors, or licensors. By opening the linked document you accept and agree to these Terms of Use. If you do not wish to adhere to these Terms of Use, do not open, download, save, or otherwise access the linked document.",
        "section": "Security and compliance",
        "subsections": " Overview Security \u0026amp; Compensating Controls Responsibility  ",
        "image": "",
        "url": "/security/compliance-guidance.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "9be9559b8490787614597b1a7e0c9060",
        "title": "Connect to services",
        "description": "",
        "text": " At this point you have configured your application’s services as well as its build and deploy process in your .platform/services.yaml and .platform.app.yaml files. As an example, in your .platform.app.yaml file you may have defined a relationship called database: relationships:database: mysqldb:mysql which was configured in .platform/services.yaml with mysqldb:type:mysql:10.4disk:1024 Note: If your application does not require access to services and you left .platform/services.yaml blank, feel free to proceed to the next step. In order to connect to this service and use it in your application, Platform.sh exposes its credentials in the application container within a base64-encoded JSON PLATFORM_RELATIONSHIPS environment variable. To access this variable you can install a Platform.sh configuration reader library Go Node.js Java (Gradle) Java (Maven) PHP Python go mod edit -require=github.com/platformsh/config-reader-go/v2 npm install platformsh-config --save compile group: \u0026#39;sh.platform\u0026#39;, name: \u0026#39;config\u0026#39;, version: 2.2.0\u0026#39; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;sh.platform\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;config\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; composer install platformsh/config-reader pip install platformshconfig and access the credentials of database PHP Python Node.js Java Go \u0026lt;?php use $config = new Config(); $credentials = $config-\u0026gt;credentials(\u0026#39;database\u0026#39;); from platformshconfig import Config config = Config() credentials = config.credentials(\u0026#39;database\u0026#39;) const config = require( platformsh-config ).config(); const credentials = config.credentials(\u0026#39;database\u0026#39;); import Config; Config config = new Config(); Credential cred = config.getCredential(\u0026#39;database\u0026#39;) import psh  github.com/platformsh/config-reader-go/v2  config, err := psh.NewRuntimeConfig() // Handle err credentials, err := config.Credentials( database ) // Handle err or read and decode the environment variable directly. Node.js PHP Python relationships = JSON.parse(new Buffer(process.env[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;], \u0026#39;base64\u0026#39;).toString()); credentials = relationships[\u0026#39;database\u0026#39;]; \u0026lt;?php $relationships = json_decode(base64_decode(getenv(\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;)), TRUE); $credentials = $relationships[\u0026#39;database\u0026#39;]; import os, json, base64 relationships = json.loads(base64.b64decode(os.environ[ PLATFORM_RELATIONSHIPS ])) credentials = relationships[\u0026#39;database\u0026#39;] In either case, credentials can now be used to connect to database: {  username :  user ,  scheme :  mysql ,  service :  mysql ,  fragment : null,  ip :  169.254.197.253 ,  hostname :  czwb2d7zzunu67lh77infwkm6i.mysql.service._.eu-3.platformsh.site ,  public : false,  cluster :  rjify4yjcwxaa-master-7rqtwti ,  host :  mysql.internal ,  rel :  mysql ,  query : {  is_master : true },  path :  main ,  password :   ,  type :  mysql:10.2 ,  port : 3306 } You can find out more information about Platform.sh Config Reader libraries on GitHub: PHP Config Reader Python Config Reader Node.js Config Reader Java Config Reader Go Config Reader You can also find examples of how to connect to each of Platform.sh managed services in multiple languages in the Services Documentation . Project configured, services connected - time to commit the changes and push your repository onto your project. Back I\u0026#39;ve connected to my services",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/connect-services.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "453b61a907646f08dfbdb9fe1509879b",
        "title": "Multiple Drupal sites in a single Project",
        "description": "",
        "text": " Platform.sh supports running multiple applications multiple applications in the same project, and these can be two or more Drupal sites. But they would be separate Drupal instances: they will have their assets separate and live their lives apart, and it would be much better for them not to share the same database (though they could). Drupal “Multisite” and Platform.sh Platform.sh actively discourages running Drupal in “multisite” mode. Doing so eliminates many of the advantages Platform.sh offers, such as isolation and safe testing. Additionally, because of the dynamic nature of the domain names that are created for the different environments, the multisite configuration would be complex and fragile. We recommend running separate projects for separate Drupal sites, or using one of the various “single instance” options available such as Domain Access , Organic Groups , or Workbench Access . The only reason to use Drupal Multisite would be to manage a series of nearly-identical sites with separate databases. For that case we have built a template repository that uses a unified lookup key for a subdomain, database name, and file paths. Note that it will likely require modification for your specific setup and some configurations may require a different approach. In particular, this example: Defines two MySQL databases. Uses a modified settings.platformsh.php that accepts a key variable from settings.php to specify which database and file system paths to use. Extracts the the sites directory to use from the domain.",
        "section": "Getting Started",
        "subsections": " Drupal \u0026ldquo;Multisite\u0026rdquo; and Platform.sh  ",
        "image": "",
        "url": "/frameworks/drupal8/multi-site.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "7ecc1f3cb2b5e091e2684a865be23584",
        "title": "PostgreSQL (Database service)",
        "description": "",
        "text": " PostgreSQL is a high-performance, standards-compliant relational SQL database. See the PostgreSQL documentation for more information. Supported versions Grid Dedicated 9.6 10 11 12 None available Note: Upgrading to PostgreSQL 12 using the postgis extension is not currently supported. Attempting to upgrade with this extension enabled will result in a failed deployment that will require support intervention to fix. See the Upgrading to PostgreSQL 12 with postgis section below for more details. Deprecated versions The following versions are available but are not receiving security updates from upstream, so their use is not recommended. They will be removed at some point in the future. Grid Dedicated 9.3 None available Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : {  cluster :  rjify4yjcwxaa-master-7rqtwti ,  host :  postgresql.internal ,  hostname :  2tcu2y75zg2ub6wzufaz2nxlcm.postgresql.service._.eu-3.platformsh.site ,  ip :  169.254.58.227 ,  password :  main ,  path :  main ,  port : 5432,  query : {  is_master : true },  rel :  postgresql ,  scheme :  pgsql ,  service :  postgresql ,  type :  postgresql:11 ,  username :  main  } Usage example In your .platform/services.yaml add: dbpostgres:type:postgresql:12disk:256 Add a relationship to the service in your .platform.app.yaml: relationships:postgresdatabase: dbpostgres:postgresql  Note: You will need to use the postgresql type when defining the service # .platform/services.yamlservice_name:type:postgresql:versiondisk:256 and the endpoint postgresql when defining the relationship # .platform.app.yamlrelationships:relationship_name:“service_name:postgresql” Your service_name and relationship_name are defined by you, but we recommend making them distinct from each other. For PHP, in your .platform.app.yaml add: runtime:extensions:- pdo_pgsqlYou can then use the service in a configuration file of your application with something like: Go Java Node.js PHP Python package examples import (  database/sql   fmt  _  github.com/lib/pq  psh  github.com/platformsh/config-reader-go/v2  libpq  github.com/platformsh/config-reader-go/v2/libpq  ) func UsageExamplePostgreSQL() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // The \u0026#39;database\u0026#39; relationship is generally the name of the primary SQL database of an application. // It could be anything, though, as in the case here where it\u0026#39;s called  postgresql . credentials, err := config.Credentials( postgresql ) checkErr(err) // Retrieve the formatted credentials. formatted, err := libpq.FormattedCredentials(credentials) checkErr(err) // Connect. db, err := sql.Open( postgres , formatted) checkErr(err) defer db.Close() // Creating a table. sqlCreate := CREATE TABLE IF NOT EXISTS PeopleGo ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT _, err = db.Exec(sqlCreate) checkErr(err) // Insert data. sqlInsert := INSERT INTO PeopleGo(name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La _, err = db.Exec(sqlInsert) checkErr(err) table := \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; var id int var name string var city string // Read it back. rows, err := db.Query( SELECT * FROM PeopleGo ) if err != nil { panic(err) } else { for rows.Next() { err = rows.Scan(\u0026amp;id, \u0026amp;name, \u0026amp;city) checkErr(err) table \u0026#43;= name, city) } table \u0026#43;= } _, err = db.Exec( DROP TABLE PeopleGo; ) checkErr(err) return table } package sh.platform.languages.sample; import sh.platform.config.Config; import sh.platform.config.MySQL; import sh.platform.config.PostgreSQL; import javax.sql.DataSource; import java.sql.Connection; import java.sql.ResultSet; import java.sql.SQLException; import java.sql.Statement; import java.util.function.Supplier; public class PostgreSQLSample implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); // The \u0026#39;database\u0026#39; relationship is generally the name of primary SQL database of an application. // It could be anything, though, as in the case here here where it\u0026#39;s called  postgresql . PostgreSQL database = config.getCredential( postgresql , PostgreSQL::new); DataSource dataSource = database.get(); // Connect to the database try (Connection connection = dataSource.getConnection()) { // Creating a table. String sql =  CREATE TABLE JAVA_FRAMEWORKS (  \u0026#43;   id SERIAL PRIMARY KEY,  \u0026#43;  name VARCHAR(30) NOT NULL) ; final Statement statement = connection.createStatement(); statement.execute(sql); // Insert data. sql =  INSERT INTO JAVA_FRAMEWORKS (name) VALUES  \u0026#43;  (\u0026#39;Spring\u0026#39;),  \u0026#43;  (\u0026#39;Jakarta EE\u0026#39;),  \u0026#43;  (\u0026#39;Eclipse JNoSQL\u0026#39;) ; statement.execute(sql); // Show table. sql =  SELECT * FROM JAVA_FRAMEWORKS ; final ResultSet resultSet = statement.executeQuery(sql); while (resultSet.next()) { int id = resultSet.getInt( id ); String name = resultSet.getString( name ); logger.append(String.format( the JAVA_FRAMEWORKS id %d the name %s  , id, name)); } statement.execute( DROP TABLE JAVA_FRAMEWORKS ); return logger.toString(); } catch (SQLException exp) { throw new RuntimeException( An error when execute PostgreSQL , exp); } } } const pg = require(\u0026#39;pg\u0026#39;); const config = require( platformsh-config ).config(); exports.usageExample = async function() { const credentials = config.credentials(\u0026#39;postgresql\u0026#39;); const client = new pg.Client({ host: credentials.host, port: credentials.port, user: credentials.username, password: credentials.password, database: credentials.path, }); client.connect(); let sql = \u0026#39;\u0026#39;; // Creating a table. sql = `CREATE TABLE IF NOT EXISTS People ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL )`; await client.query(sql); // Insert data. sql = `INSERT INTO People (name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La Jolla\u0026#39;);`; await client.query(sql); // Show table. sql = `SELECT * FROM People`; let result = await client.query(sql); let output = \u0026#39;\u0026#39;; if (result.rows.length \u0026gt; 0) { output \u0026#43;=`\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt;`; result.rows.forEach((row) =\u0026gt; { output \u0026#43;= }); output \u0026#43;= } // Drop table. sql = `DROP TABLE People`; await client.query(sql); return output; }; \u0026lt;?php declare(strict_types=1); use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // The \u0026#39;database\u0026#39; relationship is generally the name of primary SQL database of an application. // It could be anything, though, as in the case here here where it\u0026#39;s called  postgresql . $credentials = $config-\u0026gt;credentials(\u0026#39;postgresql\u0026#39;); try { // Connect to the database using PDO. If using some other abstraction layer you would // inject the values from $database into whatever your abstraction layer asks for. $dsn = sprintf(\u0026#39;pgsql:host=%s;port=%d;dbname=%s\u0026#39;, $credentials[\u0026#39;host\u0026#39;], $credentials[\u0026#39;port\u0026#39;], $credentials[\u0026#39;path\u0026#39;]); $conn = new $credentials[\u0026#39;username\u0026#39;], $credentials[\u0026#39;password\u0026#39;], [ // Always use Exception error mode with PDO, as it\u0026#39;s more reliable. =\u0026gt; // So we don\u0026#39;t have to mess around with cursors and unbuffered queries by default. ]); $conn-\u0026gt;query( DROP TABLE IF EXISTS People ); // Creating a table. $sql =  CREATE TABLE IF NOT EXISTS People ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL ) ; $conn-\u0026gt;query($sql); // Insert data. $sql =  INSERT INTO People (name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La Jolla\u0026#39;); ; $conn-\u0026gt;query($sql); // Show table. $sql =  SELECT * FROM People ; $result = $conn-\u0026gt;query($sql); if ($result) { print \u0026lt;\u0026lt;\u0026lt;TABLE\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt; TABLE; foreach ($result as $record) { $record-\u0026gt;name, $record-\u0026gt;city); } print } // Drop table. $sql =  DROP TABLE People ; $conn-\u0026gt;query($sql); } catch $e) { print $e-\u0026gt;getMessage(); } import psycopg2 from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # The \u0026#39;database\u0026#39; relationship is generally the name of primary SQL database of an application. # That\u0026#39;s not required, but much of our default automation code assumes it.\u0026#39; database = config.credentials(\u0026#39;postgresql\u0026#39;) try: # Connect to the database. conn_params = { \u0026#39;host\u0026#39;: database[\u0026#39;host\u0026#39;], \u0026#39;port\u0026#39;: database[\u0026#39;port\u0026#39;], \u0026#39;dbname\u0026#39;: database[\u0026#39;path\u0026#39;], \u0026#39;user\u0026#39;: database[\u0026#39;username\u0026#39;], \u0026#39;password\u0026#39;: database[\u0026#39;password\u0026#39;] } conn = psycopg2.connect(**conn_params) # Open a cursor to perform database operations. cur = conn.cursor() cur.execute( DROP TABLE IF EXISTS People ) # Creating a table. sql = \u0026#39;\u0026#39;\u0026#39; CREATE TABLE IF NOT EXISTS People ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL ) \u0026#39;\u0026#39;\u0026#39; cur.execute(sql) # Insert data. sql = \u0026#39;\u0026#39;\u0026#39; INSERT INTO People (name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La Jolla\u0026#39;); \u0026#39;\u0026#39;\u0026#39; cur.execute(sql) # Show table. sql = \u0026#39;\u0026#39;\u0026#39;SELECT * FROM People\u0026#39;\u0026#39;\u0026#39; cur.execute(sql) result = cur.fetchall() table = \u0026#39;\u0026#39;\u0026#39;\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt;\u0026#39;\u0026#39;\u0026#39; if result: for record in result: table \u0026#43;= record[2]) table \u0026#43;= # Drop table sql =  DROP TABLE People  cur.execute(sql) # Close communication with the database cur.close() conn.close() return table except Exception as e: return e Exporting data The easiest way to download all data in a PostgreSQL instance is with the Platform CLI. If you have a single SQL database, the following command will export all data using the pg_dump command to a local file: platform db:dump If you have multiple SQL databases it will prompt you which one to export. You can also specify one by relationship name explicitly: platform db:dump --relationship database By default the file will be uncompressed. If you want to compress it, use the --gzip (-z) option: platform db:dump --gzip You can use the --stdout option to pipe the result to another command. For example, if you want to create a bzip2-compressed file, you can run: platform db:dump --stdout | bzip2 \u0026gt; dump.sql.bz2 Importing data The easiest way to load data into a database is to pipe an SQL dump through the platform sql command, like so: platform sql \u0026lt; my_database_backup.sql That will run the database backup against the SQL database on Platform.sh. That will work for any SQL file, so the usual caveats about importing an SQL dump apply (e.g., it’s best to run against an empty database). As with exporting, you can also specify a specific environment to use and a specific database relationship to use, if there are multiple. platform sql --relationship database -e master \u0026lt; my_database_backup.sql Note: Importing a database backup is a destructive operation. It will overwrite data already in your database. Taking a backup or a database export before doing so is strongly recommended. Extensions Platform.sh supports a number of PostgreSQL extensions. To enable them, list them under the configuration.extensions key in your services.yaml file, like so: db:type:postgresql:12disk:1025configuration:extensions:- pg_trgm- hstoreIn this case you will have pg_trgm installed, providing functions to determine the similarity of text based on trigram matching, and hstore providing a key-value store. Available extensions The following is the extensive list of supported extensions. Note that you cannot currently add custom extensions not listed here. address_standardizer - Used to parse an address into constituent elements. Generally used to support geocoding address normalization step. address_standardizer_data_us - Address Standardizer US dataset example adminpack - administrative functions for PostgreSQL autoinc - functions for autoincrementing fields bloom - bloom access method - signature file based index (requires 9.6 or higher) btree_gin - support for indexing common datatypes in GIN btree_gist - support for indexing common datatypes in GiST chkpass - data type for auto-encrypted passwords citext - data type for case-insensitive character strings cube - data type for multidimensional cubes dblink - connect to other PostgreSQL databases from within a database dict_int - text search dictionary template for integers dict_xsyn - text search dictionary template for extended synonym processing earthdistance - calculate great-circle distances on the surface of the Earth file_fdw - foreign-data wrapper for flat file access fuzzystrmatch - determine similarities and distance between strings hstore - data type for storing sets of (key, value) pairs insert_username - functions for tracking who changed a table intagg - integer aggregator and enumerator (obsolete) intarray - functions, operators, and index support for 1-D arrays of integers isn - data types for international product numbering standards lo - Large Object maintenance ltree - data type for hierarchical tree-like structures moddatetime - functions for tracking last modification time pageinspect - inspect the contents of database pages at a low level pg_buffercache - examine the shared buffer cache pg_freespacemap - examine the free space map (FSM) pg_prewarm - prewarm relation data (requires 9.6 or higher) pg_stat_statements - track execution statistics of all SQL statements executed pg_trgm - text similarity measurement and index searching based on trigrams pg_visibility - examine the visibility map (VM) and page-level visibility info (requires 9.6 or higher) pgcrypto - cryptographic functions pgrouting - pgRouting Extension (requires 9.6 or higher) pgrowlocks - show row-level locking information pgstattuple - show tuple-level statistics plpgsql - PL/pgSQL procedural language postgis - PostGIS geometry, geography, and raster spatial types and functions postgis_sfcgal - PostGIS SFCGAL functions postgis_tiger_geocoder - PostGIS tiger geocoder and reverse geocoder postgis_topology - PostGIS topology spatial types and functions postgres_fdw - foreign-data wrapper for remote PostgreSQL servers refint - functions for implementing referential integrity (obsolete) seg - data type for representing line segments or floating-point intervals sslinfo - information about SSL certificates tablefunc - functions that manipulate whole tables, including crosstab tcn - Triggered change notifications timetravel - functions for implementing time travel tsearch2 - compatibility package for pre-8.3 text search functions (obsolete, only available for 9.6 and 9.3) tsm_system_rows - TABLESAMPLE method which accepts number of rows as a limit (requires 9.6 or higher) tsm_system_time - TABLESAMPLE method which accepts time in milliseconds as a limit (requires 9.6 or higher) unaccent - text search dictionary that removes accents uuid-ossp - generate universally unique identifiers (UUIDs) xml2 - XPath querying and XSLT Note: Upgrading to PostgreSQL 12 using the postgis extension is not currently supported. Attempting to upgrade with this extension enabled will result in a failed deployment that will require support intervention to fix. See the Upgrading to PostgreSQL 12 with postgis section below for more details. Notes Could not find driver If you see this error: Fatal error: Uncaught exception \u0026#39;PDOException\u0026#39; with message \u0026#39;could not find driver\u0026#39;, this means you are missing the pdo_pgsql PHP extension. You simply need to enable it in your .platform.app.yaml (see above). Upgrading PostgreSQL 10 and later include an upgrade utility that can convert databases from previous versions to version 10 or 11. If you upgrade your service from a previous version of PostgreSQL to version 10 or above (by modifying the services.yaml file) the upgrader will run automatically. The upgrader does not work to upgrade to PostgreSQL 9 versions, so upgrades from PostgreSQL 9.3 to 9.6 are not supported. Upgrade straight to version 11 instead. Warning: Make sure you first test your migration on a separate branch. Warning: Be sure to take a backup of your master environment before you merge this change. Downgrading is not supported. If you want, for whatever reason, to downgrade you should dump to SQL, remove the service, recreate the service, and import your dump. Upgrading to PostgreSQL 12 with the postgis extension Upgrading to PostgreSQL 12 using the postgis extension is not currently supported. Attempting to upgrade with this extension enabled will result in a failed deployment that will require support intervention to fix. If you need to upgrade, you should follow the same steps recommended for performing downgrades: dump the database, remove the service, recreate the service with PostgreSQL 12, and then import the dump to that service.",
        "section": "Configure services",
        "subsections": " Supported versions  Deprecated versions   Relationship Usage example Exporting data Importing data Extensions  Available extensions   Notes  Could not find driver   Upgrading  Upgrading to PostgreSQL 12 with the postgis extension    ",
        "image": "",
        "url": "/configuration/services/postgresql.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "e59885d50d221542f252c65644027fdf",
        "title": "Using Solr with the module Search API on Drupal 7.x",
        "description": "",
        "text": " This page is about configuring Solr with the module Search API . If your project uses Apache Solr Search then you should follow the instructions Apache Solr Search . Requirements You will need to add the Search API and Search API Solr modules to your project. The Search API Override module is strongly recommended in order to allow the Solr configuration to be populated from settings.php. If you are using a make file, you can add those lines to your project.make: projects[entity][version] = 1.8 projects[search_api][version] = 1.20 projects[search_api_solr][version] = 1.11 projects[search_api_override][version] = 1.0-rc1 Configuration The Search API module includes recommended configuration files to use with Drupal. See the Solr configuration page for details of how to configure your Solr server to use the Drupal configuration files. Note that the Drupal 7 version of Search API Solr does not include configuration files for Solr 6. The Drupal 8 version of the module does, however, and should work acceptably. It can also be customized as desired. The Search API Override module (listed above) allows Search API configuration to be overridden from settings.php. Once it has been enabled, add the following to your settings.platformsh.php file: \u0026lt;?php if (isset($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;])) { $relationships = json_decode(base64_decode($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;]), TRUE); if (!empty($relationships[\u0026#39;solr\u0026#39;])) { // Override search API server settings fetched from default configuration. $conf[\u0026#39;search_api_override_mode\u0026#39;] = \u0026#39;load\u0026#39;; foreach ($relationships[\u0026#39;solr\u0026#39;] as $endpoint) { $conf[\u0026#39;search_api_override_servers\u0026#39;] = array( \u0026#39;MACHINE_NAME_OF_SOLR_SERVER\u0026#39; =\u0026gt; array( \u0026#39;options\u0026#39; =\u0026gt; array( \u0026#39;host\u0026#39; =\u0026gt; $endpoint[\u0026#39;host\u0026#39;], \u0026#39;port\u0026#39; =\u0026gt; $endpoint[\u0026#39;port\u0026#39;], \u0026#39;path\u0026#39; =\u0026gt; \u0026#39;/\u0026#39; . $endpoint[\u0026#39;path\u0026#39;], \u0026#39;http_method\u0026#39; =\u0026gt; \u0026#39;POST\u0026#39;, ), ), ); } } } Replace MACHINE_NAME_OF_SOLR_SERVER with the Drupal machine name of the server you want to override. The solr server must already be defined in Drupal and ideally exported to a Feature. Relationships configuration If you did not name the relationship solr in your .platform.app.yaml file, adjust the name accordingly. Also, if you have multiple Solr cores defined the above foreach() loop will not work. Most likely you will want to name the relationships by the machine name of the Solr server they should map to and then map each one individually. The file .platform.app.yaml must have the Solr relationship enabled, such as this snippet: relationships: solr: \u0026#39;solrsearch:solr\u0026#39;",
        "section": "Getting Started",
        "subsections": " Requirements Configuration Relationships configuration  ",
        "image": "",
        "url": "/frameworks/drupal7/search-api-module.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "faab687d8873a9ac8a02d1d2a9d26f0a",
        "title": "Wordpress",
        "description": "",
        "text": " The recommended way to deploy WordPress on Platform.sh is using Composer. The most popular and supported way to do so is with the John Bloch script. Platform.sh strongly recommends starting new WordPress projects from our WordPress Template , which is built using Composer and includes the WP-CLI by default. It also includes modifications to the configuration files necessary to connect to a database on Platform.sh automatically. Plugin compatibility Platform.sh does not explicitly block any WordPress plugins. However, some plugins are known to require write access to their own file system as part of their setup process. That is not possible on Platform.sh. The file system where code lives is read-only for security reasons and cannot be written to from the application, only during the build hook . In some cases that can be worked around by copying a file as part of the build hook process. In other cases the plugin is simply incompatible with Platform.sh. If you find a plugin that tries to write to its own directory we recommend filing an issue with that plugin, as such behavior should be viewed as a security bug.",
        "section": "Featured frameworks",
        "subsections": " Plugin compatibility  ",
        "image": "",
        "url": "/frameworks/wordpress.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "cfba97e470c8fcf1b79b618ed376c603",
        "title": "Build, Deploy, Done!",
        "description": "",
        "text": " With your configuration files complete, all that’s left is to commit the changes and push to Platform.sh. Commit and push Run the commands git add . git commit -m  Add config files.  git push -u platform master Platform.sh will detect the presence of your configuration files and use them to build the application. Verify When the build is completed, you can verify the deployment by typing the command platform url This will return a list of your routes. Pick the primary route 0 and click Enter, which will open your application in a browser window. Alternatively, you can also log back into the management console in your new project. Select the Master environment in the Environments list and click the link below the Overview box on the left side of the page. That’s it! Using the Platform.sh CLI and a few properly configured files, pushing your application to run on Platform.sh takes only a few minutes. Now that your code is on Platform.sh, check out some of the Next Steps to get started developing. Back I\u0026#39;ve deployed my application",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/push-project.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "19ddeafc0f577a4cd1f561813af16b90",
        "title": "Drupal Frequently Asked Questions (FAQ)",
        "description": "",
        "text": " How can I import configuration on production? If you don’t want to do so manually, include the following lines in your deploy hook in .platformsh.app.yaml: drush-yupdatedbdrush-yconfig-importThat will automatically run update.php and import configuration on every new deploy. The above configuration is included by default if you used our Drupal 8 example repository or created a project through the management console. I’m getting a PDO Exception ‘MySQL server has gone away’ Normally, this means there is a problem with the MySQL server container and you may need to increase the storage available to MySQL to resolve the issue. Ballooning MySQL storage can be caused by a number of items: A large number of watchdog entries being captured. Fix the errors being generated or disable database logging. Cron should run at regular intervals to ensure cache tables get cleared out. Why do I get “MySQL cannot connect to the database server”? If you are having a problem connecting to the database server, you will need force a re-deployment of the database container. To do so, you can edit the service definition to add or remove a small amount of storage and then push. Can I use the name of the session cookie for caching? For Drupal sites, the name of the session cookie is based on a hash of the domain name. This means that it will actually be consistent for a specific website and can safely be used as a fixed value.",
        "section": "Getting Started",
        "subsections": " How can I import configuration on production? I\u0026rsquo;m getting a PDO Exception \u0026lsquo;MySQL server has gone away\u0026rsquo; Why do I get \u0026ldquo;MySQL cannot connect to the database server\u0026rdquo;? Can I use the name of the session cookie for caching?  ",
        "image": "",
        "url": "/frameworks/drupal8/faq.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "cc7908825606a195061bc05118b5bf6a",
        "title": "RabbitMQ (Message queue service)",
        "description": "",
        "text": " RabbitMQ is an open source message broker software (sometimes called message-oriented middleware) that implements the Advanced Message Queuing Protocol (AMQP). See the RabbitMQ documentation for more information.” Supported versions Grid Dedicated 3.5 3.6 3.7 3.8 None available Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : {  cluster :  rjify4yjcwxaa-master-7rqtwti ,  host :  rabbitmq.internal ,  hostname :  cefpddpigx4xs4alwihkji65fe.rabbitmq.service._.eu-3.platformsh.site ,  ip :  169.254.178.95 ,  password :  guest ,  port : 5672,  rel :  rabbitmq ,  scheme :  amqp ,  service :  rabbitmq ,  type :  rabbitmq:3.7 ,  username :  guest  } Usage example In your .platform/services.yaml: queuerabbit:type:rabbitmq:3.8disk:256 In your .platform.app.yaml: relationships:rabbitmqqueue: queuerabbit:rabbitmq  Note: You will need to use the rabbitmq type when defining the service # .platform/services.yamlservice_name:type:rabbitmq:versiondisk:256 and the endpoint rabbitmq when defining the relationship # .platform.app.yamlrelationships:relationship_name:“service_name:rabbitmq” Your service_name and relationship_name are defined by you, but we recommend making them distinct from each other. You can then use the service in a configuration file of your application with something like: Go Java PHP Python package examples import (  fmt  psh  github.com/platformsh/config-reader-go/v2  amqpPsh  github.com/platformsh/config-reader-go/v2/amqp   github.com/streadway/amqp   sync  ) func UsageExampleRabbitMQ() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // Get the credentials to connect to RabbitMQ. credentials, err := config.Credentials( rabbitmq ) checkErr(err) // Use the amqp formatted credentials package. formatted, err := amqpPsh.FormattedCredentials(credentials) checkErr(err) // Connect to the RabbitMQ server. connection, err := amqp.Dial(formatted) checkErr(err) defer connection.Close() // Make a channel. channel, err := connection.Channel() checkErr(err) defer channel.Close() // Create a queue. q, err := channel.QueueDeclare(  deploy_days , // name false, // durable false, // delete when unused false, // exclusive false, // no-wait nil, // arguments ) body :=  Friday  msg := fmt.Sprintf( Deploying on %s , body) // Publish a message. err = channel.Publish(   , // exchange q.Name, // routing key false, // mandatory false, // immediate amqp.Publishing{ ContentType:  text/plain , Body: []byte(msg), }) checkErr(err) outputMSG := fmt.Sprintf( [x] Sent \u0026#39;%s\u0026#39; \u0026lt;br\u0026gt; , body) // Consume the message. msgs, err := channel.Consume( q.Name, // queue   , // consumer true, // auto-ack false, // exclusive false, // no-local false, // no-wait nil, // args ) checkErr(err) var received string var wg sync.WaitGroup wg.Add(1) go func() { for d := range msgs { received = fmt.Sprintf( [x] Received message: \u0026#39;%s\u0026#39; \u0026lt;br\u0026gt; , d.Body) wg.Done() } }() wg.Wait() outputMSG \u0026#43;= received return outputMSG } package sh.platform.languages.sample; import sh.platform.config.Config; import sh.platform.config.RabbitMQ; import javax.jms.Connection; import javax.jms.ConnectionFactory; import javax.jms.MessageConsumer; import javax.jms.MessageProducer; import javax.jms.Queue; import javax.jms.Session; import javax.jms.TextMessage; import java.util.function.Supplier; public class RabbitMQSample implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); try { // Get the credentials to connect to the RabbitMQ service. final RabbitMQ credential = config.getCredential( rabbitmq , RabbitMQ::new); final ConnectionFactory connectionFactory = credential.get(); // Connect to the RabbitMQ server. final Connection connection = connectionFactory.createConnection(); connection.start(); final Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); Queue queue = session.createQueue( cloud ); MessageConsumer consumer = session.createConsumer(queue); // Sending a message into the queue. TextMessage textMessage = session.createTextMessage( Platform.sh ); textMessage.setJMSReplyTo(queue); MessageProducer producer = session.createProducer(queue); producer.send(textMessage); // Receive the message. TextMessage replyMsg = (TextMessage) consumer.receive(100); logger.append( Message:  ).append(replyMsg.getText()); // close connections. producer.close(); consumer.close(); session.close(); connection.close(); return logger.toString(); } catch (Exception exp) { throw new RuntimeException( An error when execute RabbitMQ , exp); } } } \u0026lt;?php declare(strict_types=1); use use use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // Get the credentials to connect to the RabbitMQ service. $credentials = $config-\u0026gt;credentials(\u0026#39;rabbitmq\u0026#39;); try { $queueName = \u0026#39;deploy_days\u0026#39;; // Connect to the RabbitMQ server. $connection = new AMQPStreamConnection($credentials[\u0026#39;host\u0026#39;], $credentials[\u0026#39;port\u0026#39;], $credentials[\u0026#39;username\u0026#39;], $credentials[\u0026#39;password\u0026#39;]); $channel = $connection-\u0026gt;channel(); $channel-\u0026gt;queue_declare($queueName, false, false, false, false); $msg = new AMQPMessage(\u0026#39;Friday\u0026#39;); $channel-\u0026gt;basic_publish($msg, \u0026#39;\u0026#39;, \u0026#39;hello\u0026#39;); echo  [x] Sent // In a real application you\u0026#39;t put the following in a separate script in a loop. $callback = function ($msg) { printf( [x] Deploying on %s\u0026lt;br $msg-\u0026gt;body); }; $channel-\u0026gt;basic_consume($queueName, \u0026#39;\u0026#39;, false, true, false, false, $callback); // This blocks on waiting for an item from the queue, so comment it out in this demo script. //$channel-\u0026gt;wait(); $channel-\u0026gt;close(); $connection-\u0026gt;close(); } catch (Exception $e) { print $e-\u0026gt;getMessage(); } import pika from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the RabbitMQ service. credentials = config.credentials(\u0026#39;rabbitmq\u0026#39;) try: # Connect to the RabbitMQ server creds = pika.PlainCredentials(credentials[\u0026#39;username\u0026#39;], credentials[\u0026#39;password\u0026#39;]) parameters = pika.ConnectionParameters(credentials[\u0026#39;host\u0026#39;], credentials[\u0026#39;port\u0026#39;], credentials=creds) connection = pika.BlockingConnection(parameters) channel = connection.channel() # Check to make sure that the recipient queue exists channel.queue_declare(queue=\u0026#39;deploy_days\u0026#39;) # Try sending a message over the channel channel.basic_publish(exchange=\u0026#39;\u0026#39;, routing_key=\u0026#39;deploy_days\u0026#39;, body=\u0026#39;Friday!\u0026#39;) # Receive the message def callback(ch, method, properties, body): print(  [x] Received {} .format(body)) # Tell RabbitMQ that this particular function should receive messages from our \u0026#39;hello\u0026#39; queue channel.basic_consume(\u0026#39;deploy_days\u0026#39;, callback, auto_ack=False) # This blocks on waiting for an item from the queue, so comment it out in this demo script. # print(\u0026#39; [*] Waiting for messages. To exit press CTRL\u0026#43;C\u0026#39;) # channel.start_consuming() connection.close() return   [x] Sent \u0026#39;Friday!\u0026#39;\u0026lt;br/\u0026gt;  except Exception as e: return e (The specific way to inject configuration into your application will vary. Consult your application or framework’s documentation.) Connecting to RabbitMQ From your local development environment For debugging purposes, it’s sometimes useful to be able to directly connect to a service instance. You can do this using SSH tunneling. To open a tunnel, log into your application container like usual, but with an extra flag to enable local port forwarding: ssh -L 5672:rabbitmqqueue.internal:5672 \u0026lt;projectid\u0026gt;-\u0026lt;branch_ID\u0026gt;@ssh.eu.platform.sh Within that SSH session, use the following command to pretty-print your relationships. This lets you see which username and password to use, and you can double check that the remote service’s port is 5672. php -r \u0026#39;print_r(json_decode(base64_decode($_ENV[ PLATFORM_RELATIONSHIPS ])));\u0026#39; If your service is running on a different port, you can re-open your SSH session with the correct port by modifying your -L flag: -L 5672:rabbitmqqueue.internal:\u0026lt;remote port\u0026gt;. Finally, while the session is open, you can launch a RabbitMQ client of your choice from your local workstation, configured to connect to localhost:5672 using the username and password you found in the relationship variable. Access the management plugin (Web UI) In case you want to access the browser-based UI, you have to use an SSH tunnel. To open a tunnel, log into your application container like usual, but with an extra flag to enable local port forwarding: ssh -L 15672:rabbitmqqueue.internal:15672 \u0026lt;projectid\u0026gt;-\u0026lt;branch_ID\u0026gt;@ssh.eu.platform.sh After you successfully established a connection, you should be able to open http://localhost:15672 in your browser. You’ll find the credentials like mentioned above. From the application container The application container currently doesn’t include any useful utilities to connect to RabbitMQ with. However, you can install your own by adding a client as a dependency in your .platform.app.yaml file. For example, you can use amqp-utils by adding this: dependencies:ruby:amqp-utils: 0.5.1 Then, when you SSH into your container, you can simply type any amqp- command available to manage your queues. Configuration Virtual hosts You can configure additional virtual hosts to a RabbitMQ service, which can be useful for separating resources, such as exchanges, queues, and bindings, to their own namespace. In your .platform/services.yaml file define the names of the virtual hosts under the configuration.vhosts attribute: rabbitmq:type:rabbitmq:3.8disk:512configuration:vhosts:- foo- bar",
        "section": "Configure services",
        "subsections": " Supported versions Relationship Usage example Connecting to RabbitMQ  From your local development environment Access the management plugin (Web UI) From the application container   Configuration  Virtual hosts    ",
        "image": "",
        "url": "/configuration/services/rabbitmq.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "6443a42b289ac42d9412648e0a5068e5",
        "title": "SimpleSAML",
        "description": "",
        "text": " SimpleSAMLphp is a library for authenticating a PHP-based application against a SAML server, such as Shibboleth. Although Drupal has modules available to authenticate using SimpleSAML some additional setup is required. The following setup assumes you’re using the drupal build flavor and building your site with Drush Make. If not, you may need to adjust some paths in the configuration but the basics are the same. Download the library First, download the 3rd party SimpleSAMLphp library . When you unpack the tar.gz file it will contain a directory named simplesamplephp-???, where the ??? is the version number of the library. Place that directory at the root of your application, as a sibling of your .platform.app.yaml file, named simplesamplephp. (The directory name doesn’t really matter but removing the version number means that it won’t change in future updates.) The drupal build flavor will move that directory to the public/sites/default/ directory during build. The rest of the configuration is based on that behavior. Include SimpleSAML cookies in the cache key The SimpleSAML client uses additional cookies besides the Drupal session cookie that need to be allowed for the cache. To do so, modify your routes.yaml file for the route that points to your Drupal site and add two additional cookies to the cache.cookies line. It should end up looking approximately like this:  https://{default}/ :type:upstreamupstream: app:http cache:enabled:truecookies:[\u0026#39;/^SS?ESS/\u0026#39;,\u0026#39;/^Drupal.visitor/\u0026#39;,\u0026#39;SimpleSAMLSessionID\u0026#39;,\u0026#39;SimpleSAMLAuthToken\u0026#39;]Commit this change to the Git repository. Expose the SimpleSAML endpoint The SimpleSAML library’s www directory needs to be publicly accessible. That can be done by mapping it directly to a path in the Application configuration. Add the following block to the web.locations section of .platform.app.yaml: web:locations:\u0026#39;/simplesaml\u0026#39;:root:\u0026#39;public/sites/default/simplesamlphp/www\u0026#39;allow:truescripts:trueindex:- index.phpThat will map all requests to example.com/simplesaml/ to the simplesamlphp/www directory, allowing static files there to be served, PHP scripts to execute, and defaulting to index.php. Install the simpleSAMLphp Authentication module You will need to install the simpleSAMLphp Authentication module. If using Drush Make then the easiest way to do so is simply to add the following line to your project.make file: projects[simplesamlphp_auth][version] = 2.0-alpha2 (Adjust the version to whatever is current.) Much of the module configuration will depend on your Identity Provider (IdP). However, the module also need to know the location of your simplesamlphp_auth module. The easiest way to set it is to include the following at the end of your settings.platformsh.php file: \u0026lt;?php // Set the path for the SimpleSAMLphp library dynamically. $conf[\u0026#39;simplesamlphp_auth_installdir\u0026#39;] = __DIR__ . \u0026#39;/simplesamlphp\u0026#39;; Deploy the site and enable the simplesamlphp_auth module. Consult the module documentation for further information on how to configure the module itself. Note that you should not check the “Activate authentication via SimpleSAMLphp” checkbox in the module configuration until you have the rest of the configuration completed or you may be locked out of the site. Configure SimpleSAML to use the database SimpleSAMLphp is able to store its data either on disk or in the Drupal database. Platform.sh strongly recommends using the database. Open the file simplesamlphp/config/config.php. It contains a number of configuration properties that you can adjust as needed. Some are best edited in-place and the file already includes ample documentation, specifically: auth.adminpassword technicalcontact_name technicalcontact_email Others are a little more involved. In the interest of simplicity we recommend simply pasting the following code snippet at the end of the file, as it will override the default values in the array. \u0026lt;?php // Set SimpleSAML to log using error_log(), which on Platform.sh will // be mapped to the /var/log/app.log file. $config[\u0026#39;logging.handler\u0026#39;] = \u0026#39;errorlog\u0026#39;; // Set SimpleSAML to use the metadata directory in Git, rather than // the empty one in the vendor directory. $config[\u0026#39;metadata.sources\u0026#39;] = [ [\u0026#39;type\u0026#39; =\u0026gt; \u0026#39;flatfile\u0026#39;, \u0026#39;directory\u0026#39; =\u0026gt; dirname(__DIR__) . \u0026#39;/metadata\u0026#39;], ]; // Setup the database connection for all parts of SimpleSAML. if (isset($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;])) { $relationships = json_decode(base64_decode($_ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;]), TRUE); foreach ($relationships[\u0026#39;database\u0026#39;] as $instance) { if (!empty($instance[\u0026#39;query\u0026#39;][\u0026#39;is_master\u0026#39;])) { $dsn = sprintf( %s:host=%s;dbname=%s , $instance[\u0026#39;scheme\u0026#39;], $instance[\u0026#39;host\u0026#39;], $instance[\u0026#39;path\u0026#39;] ); $config[\u0026#39;database.dsn\u0026#39;] = $dsn; $config[\u0026#39;database.username\u0026#39;] = $instance[\u0026#39;username\u0026#39;]; $config[\u0026#39;database.password\u0026#39;] = $instance[\u0026#39;password\u0026#39;]; $config[\u0026#39;store.type\u0026#39;] = \u0026#39;sql\u0026#39;; $config[\u0026#39;store.sql.dsn\u0026#39;] = $dsn; $config[\u0026#39;store.sql.username\u0026#39;] = $instance[\u0026#39;username\u0026#39;]; $config[\u0026#39;store.sql.password\u0026#39;] = $instance[\u0026#39;password\u0026#39;]; $config[\u0026#39;store.sql.prefix\u0026#39;] = \u0026#39;simplesaml\u0026#39;; } } } // Set the salt value from the Platform.sh entropy value, provided for this purpose. if (isset($_ENV[\u0026#39;PLATFORM_PROJECT_ENTROPY\u0026#39;])) { $config[\u0026#39;secretsalt\u0026#39;] = $_ENV[\u0026#39;PLATFORM_PROJECT_ENTROPY\u0026#39;]; } Generate SSL certs (optional) You may need to generate an SSL/TLS certificate, depending on your Identity Provider (IdP). If so, you should generate the certificate locally following the instructions in the SimpleSAMLphp documentation . Your resulting IdP file should be placed in the simplesamlphp/metadata directory. The certificate should be placed in the simplesamlphp/cert directory. (Create it if needed.) Then add the following line to your simplesamlphp/config/config.php file to tell the library where to find the certificate: \u0026lt;?php $config[\u0026#39;certdir\u0026#39;] = dirname(__DIR__) . \u0026#39;/cert\u0026#39;; Deploy Commit all changes and deploy the site, then enable the simplesamlphp_auth module within Drupal. Consult the module documentation for further information on how to configure the module itself. Note that you should not check the “Activate authentication via SimpleSAMLphp” checkbox in the module configuration until you have the rest of the configuration completed or you may be locked out of the site. Recovering from a locked site If SimpleSAML is misconfigured it is possible to find yourself locked out of the site, as it will try to authenticate against a SimpleSAML server, fail, and then disallow other logins. If that happens, the easiest way to recover it is to disable the SimpleSAML login. That can be done with the following command: platform ssh  cd public \u0026amp;\u0026amp; drush vset simplesamlphp_auth_activate 0  Alternatively you could log into the server and run the drush command there yourself. If that doesn’t work it is likely that the configuration is “pinned” using Features or via settings.php. Instead disable the module entirely, then remove the “pin” and re-enable it. platform ssh  cd public \u0026amp;\u0026amp; drush pm-disable simplesamlphp_auth -y ",
        "section": "Getting Started",
        "subsections": " Download the library Include SimpleSAML cookies in the cache key Expose the SimpleSAML endpoint Install the simpleSAMLphp Authentication module Configure SimpleSAML to use the database Generate SSL certs (optional) Deploy Recovering from a locked site  ",
        "image": "",
        "url": "/frameworks/drupal7/simplesaml.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "ec3c067f73cfba8ac0cf97bc59ee5d90",
        "title": "Use a private Git repository",
        "description": "",
        "text": " Pull code from a private Git repository Let’s say you’re building a module (or theme, library…) which is stored in a private Git repository that you have access to, and you want to use it on your project. Platform.sh allows you to include code dependencies that are stored in external private Git repositories (e.g. from a Drupal .make file, a PHP composer.json file). To grant Platform.sh access to your private Git repository, you need to add the project public SSH key to the deploy keys of your Git repository. You can copy your project’s public key by going to the Settings tab on the management console and then clicking the Deploy Key tab on the left hand side. If your private repository is on GitHub, go to the target repository’s settings page. Go to Deploy Keys and click Add deploy key. Paste the public SSH key in and submit. By default, on github, deploy keys are read only, so you don’t need to worry about the system pushing code to the private repository. If you’re using Drupal for example, you can now use your private module by adding it to your make file: ; Add private repository from GitHub projects[module_private][type] = module projects[module_private][subdir] =  contrib  projects[module_private][download][type] = git projects[module_private][download][branch] = dev projects[module_private][download][url] =  git@github.com:guguss/module_private.git  Note: In the make file use the \u0026lt;user\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;path\u0026gt;.git format, or ssh://\u0026lt;user\u0026gt;@\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;path\u0026gt;.git if using a non-standard port. Using multiple private Git repositories More complex projects may have many repositories that they want to include, but GitHub only allows you to associate a deploy key with a single repository. If your project needs to access multiple repositories, you can choose to attach an SSH key to an automated user account. Since this account won’t be used by a human, it’s called a machine user. You can then add the machine account as collaborator or add the machine user to a team with access to the repositories it needs to manipulate. More information about this is available on GitHub .",
        "section": "Development",
        "subsections": " Pull code from a private Git repository Using multiple private Git repositories  ",
        "image": "",
        "url": "/development/private-repository.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "c722a57676dea0d653a458d18a4f43c3",
        "title": "Vulnerability Scanning and Penetration Testing",
        "description": "",
        "text": " Platform.sh understands the need for application owners to ensure the integrity, and standards compliance, of their applications. Because there could be adverse impacts to other clients which would violate our terms of service, we only permit certain types of tests. Approved Activities Vulnerability scanning of your web application. You are free to perform this as often as required without approval from Platform.sh. Web application penetration tests that do not result in high network load. You are free to perform this as often as required without approval from Platform.sh. Approved Activities by Prior Arrangement For Platform.sh Enterprise-Dedicated customers we do permit infrastructure penetration testing (but not load testing) by prior arrangement. This requires special advanced preparation. You must submit a support ticket request a minimum of three (3) weeks in advance for us to coordinate this on your behalf. Prohibited Activities Vulnerability scanning of web applications which you do not own. Denial of Service tests and any other type of testing which results in heavy network load. Social engineering tests of Platform.sh services including falsely representing yourself as a Platform.sh employee. Infrastructure penetration tests for non-Dedicated-Enterprise customers. This includes SSH and database testing. Rate Limits Please limit scans to a maximum of 20 Mbps and 50 requests per second in order to prevent triggering denial of service bans. Troubleshooting If your vulnerability scanning suggests there may be an issue with Platform.sh’s service, please ensure your container is updated and retest. If the problem remains, please contact support .",
        "section": "Security and compliance",
        "subsections": " Approved Activities Approved Activities by Prior Arrangement Prohibited Activities Rate Limits Troubleshooting  ",
        "image": "",
        "url": "/security/pen-test.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "1f80afed50618a7f5be17adaaf31af33",
        "title": "Web",
        "description": "",
        "text": " The web key defines a single web instance container running a single web server process (currently Nginx), behind which runs your application. The web key configures the web server, including what requests should be served directly (such as static files) and which should be passed to your application. The server is extremely flexible, which means that some configurations will be more involved than others. Additionally, defaults may vary somewhat between different language base images (specified by the type key of .platform.app.yaml). The first section on this page explains the various options the file supports. If you prefer, the later sections include various example configurations to demonstrate common patterns and configurations. You can also examine the .platform.app.yaml files of the provided project templates for various common Free Software applications. See the various language pages for an index of available examples. The web key defines how the application is exposed to the web (in HTTP). Here we tell the web application how to serve content, including static files, front-controller scripts, index files, index scripts, and so on. We support any directory structure, so the static files can be in a subdirectory and the index.php file can be further down. Commands The commands key defines the command to launch the application. For now there is only a single command, start, but more will be added in the future. The start key specifies the command to use to launch your application. That could be running a uwsgi command for a Python application or a unicorn command for a Ruby application, or simply running your compiled Go application. If the command specified by the start key terminates it will be restarted automatically. web:commands:start:\u0026#39;uwsgi --ini conf/server.ini\u0026#39; Note: Never “background” a start process using \u0026amp;. That will be interpreted as the command terminating and the supervisor process will start a second copy, creating an infinite loop until the container crashes. Just run it as normal and allow the Platform.sh supervisor to manage it. On PHP containers this value is optional and will default to starting PHP-FPM (i.e. /usr/sbin/php-fpm7.0 on PHP7 and /usr/sbin/php5-fpm on PHP5). On all other containers it should be treated as required. It can also be set explicitly on a PHP container in order to run a dedicated process such as React PHP or Amp . Upstream upstream specifies how the front server will connect to your application (the process started by commands.start above). It has two keys: socket_family: Default: tcp. Describes whether your application will listen on a Unix socket (unix) or a TCP socket (tcp). protocol: Specifies whether your application is going to receive incoming requests over HTTP (http) or FastCGI (fastcgi). The default varies depending on which application runtime you’re using. Other values will be supported in the future. On a PHP container with FPM there is almost never a reason to set the upstream explicitly, as the defaults are already configured properly for PHP-FPM. On all other containers the default is tcp and http. web:upstream:socket_family:tcpprotocol:httpThe above configuration (which is the default on non-PHP containers) will forward connections to the process started by commands.start as a raw HTTP request to a TCP port, as though the process were listening to the incoming request directly. Socket family If the socket_family is set to tcp, then your application should listen on the port specified by the PORT environment variable. (In practice it is almost always 8888, but checking the variable is preferred.) If the socket_family is set to unix, then your application should open the unix socket file specified by the SOCKET environment variable. If your application isn’t listening at the same place that the runtime is sending requests, you’ll see 502 Bad Gateway errors when you try to connect to your web site. Locations The locations block is the most powerful, and potentially most involved, section of the .platform.app.yaml file. It allows you to control how the application container responds to incoming requests at a very fine-grained level. Common patterns also vary between language containers due to the way PHP-FPM handles incoming requests. Each entry of the locations block is an absolute URI path (with leading /) and its value includes the configuration directives for how the web server should handle matching requests. That is, if your domain is example.com then \u0026#39;/\u0026#39; means “requests for example.com/”, while \u0026#39;/admin\u0026#39; means “requests for example.com/admin”. If multiple blocks could match an incoming request then the most-specific will apply. web:locations:\u0026#39;/\u0026#39;:# Rules for all requests that don\u0026#39;t otherwise match....\u0026#39;/sites/default/files\u0026#39;:# Rules for any requests that begin with /sites/default/files....The simplest possible locations configuration is one that simply passes all requests on to your application unconditionally: web:locations:\u0026#39;/\u0026#39;:passthru:trueThat is, all requests to /* should be forwarded to the process started by web.commands.start above. Note that for PHP containers the passthru key must specify what PHP file the request should be forwarded to, and must also specify a docroot under which the file lives. For example: web:locations:\u0026#39;/\u0026#39;:root:\u0026#39;web\u0026#39;passthru:\u0026#39;/app.php\u0026#39;This block will serve requests to / from the web directory in the application, and if a file doesn’t exist on disk then the request will be forwarded to the /app.php script. A full list of the possible subkeys for locations is below. root: The folder from which to serve static assets for this location relative to the application root. The application root is the directory in which the .platform.app.yaml file is located. Typical values for this property include public or web. Setting it to \u0026#39;\u0026#39; is not recommended, and its behavior may vary depending on the type of application. Absolute paths are not supported. passthru: Whether to forward disallowed and missing resources from this location to the application and can be true, false or an absolute URI path (with leading /). The default value is false. For non-PHP applications it will generally be just true or false. In a PHP application this will typically be the front controller such as /index.php or /app.php. This entry works similar to mod_rewrite under Apache. Note: If the value of passthru does not begin with the same value as the location key it is under, the passthru may evaluate to another entry. That may be useful when you want different cache settings for different paths, for instance, but want missing files in all of them to map back to the same front controller. See the example block below. index: The files to consider when serving a request for a directory: an array of file names or null. (typically [\u0026#39;index.html\u0026#39;]). Note that in order for this to work, access to the static files named must be allowed by the allow or rules keys for this location. expires: How long to allow static assets from this location to be cached (this enables the Cache-Control and Expires headers) and can be a time or -1 for no caching (default). Times can be suffixed with “ms” (milliseconds), “s” (seconds), “m” (minutes), “h” (hours), “d” (days), “w” (weeks), “M” (months, 30d) or “y” (years, 365d). scripts: Whether to allow loading scripts in that location (true or false). This directive is only meaningful on PHP. allow: Whether to allow serving files which don’t match a rule (true or false, default: true). headers: Any additional headers to apply to static assets. This section is a mapping of header names to header values. Responses from the application aren’t affected, to avoid overlap with the application’s own ability to include custom headers in the response. rules: Specific overrides for a specific location. The key is a PCRE (regular expression) that is matched against the full request path. request_buffering: Most application servers do not support chunked requests (e.g. fpm, uwsgi), so Platform.sh enables request_buffering by default to handle them. That default configuration would look like this if it was present in .platform.app.yaml: web:locations:\u0026#39;/\u0026#39;:passthru:truerequest_buffering:enabled:truemax_request_size:250mIf the application server can already efficiently handle chunked requests, the request_buffering subkey can be modified to disable it entirely (enabled: false). Additionally, applications that frequently deal with uploads greater than 250MB in size can update the max_request_size key to the application’s needs. Note that modifications to request_buffering will need to be specified at each location where it is desired. Rules The rules block warrants its own discussion as it allows overriding most other keys according to a regular expression. The key of each item under the rules block is a regular expression matching paths more specifically than the locations block entries. If an incoming request matches the rule, then its handling will be overridden by the properties under the rule. Note that it will override the entire rule in the case of a compound rule like headers. (See example below.) For example, the following file will serve dynamic requests from index.php in the public directory and disallow requests for static files anywhere. Then it sets a rule to explicitly allow common image file formats, and sets a cache lifetime for them of 5 minutes. web:locations:\u0026#39;/\u0026#39;:root:\u0026#39;public\u0026#39;passthru:\u0026#39;/index.php\u0026#39;allow:falserules:# Allow common image files you can imagine the locations and rules blocks can be used to create highly involved and powerful configurations, but obeys Parker’s Law. (With great power comes great responsibility.) The examples below demonstrate various common configurations and recommended defaults. How do I setup a basic PHP application with front-controller? The following web block is a reasonable starting point for a custom PHP application. It sets the directory public as the docroot, and any missing files will get mapped to the /index.php file. mp4 files are forbidden entirely. Image files from the images URL (which will be served from the /public/images directory) will have an expiration time set, but non-image files will be disallowed. web:locations:\u0026#39;/\u0026#39;:root:\u0026#39;public\u0026#39;passthru:\u0026#39;/index.php\u0026#39;index:- index.php# No caching for static files.# (Dynamic pages use whatever cache headers are generated by the program.)expires:-1scripts:trueallow:truerules:# Disallow .mp4 files Set a 5 min expiration time for static files here; a missing URL# will passthru to the \u0026#39;/\u0026#39; location above and hit the application# front-controller.\u0026#39;/images\u0026#39;:expires:300passthru:trueallow:falserules:# Only allow static image files from the images can I serve a static-only site? Although most websites today have some dynamic component, static site generators are a valid way to build a site. This documentation is built using a tool called Hugo, and served by Platform.sh as a static site. You can see the entire repository on GitHub. The .platform.app.yaml file it uses is listed below. Note in particular the web.commands.start directive. There needs to be some background process so it’s set to the sleep shell command, which will simply block forever (or some really long time, as computers don’t know about forever) and restart if needed. The file also runs the Hugo build process, and then specifies the files that are allowed to serve. # .platform.app.yaml# The name of this application, which must be unique within a project.name:\u0026#39;docs\u0026#39;# The type key specifies the language and version for your application.type:\u0026#39;nodejs:12\u0026#39;# The hooks that will be triggered when the package is deployed.hooks:# Build hooks can modify the application files on disk but not access any services like databases.build:!includetype:stringpath:build.shdeploy:| cp data/templates.yaml public/scripts/xss/dist/config/templates.yamlcppublic/index.jsonpublic/scripts/xss/dist/config/index.json# The configuration of the application when it is exposed to the web.web:commands:# Run a no-op process that uses no CPU resources, since this is a static site.start:sleepinfinitylocations:\u0026#39;/\u0026#39;:# The public directory of the application relative to its How can I control the headers sent with my files? There are many use cases for setting custom headers on static content, such as custom content type headers, limiting cross-origin usage, etc. Consider the following example: web:locations: / :root: public passthru: /index.php index:- the headers directive sets the X-Frame-Options header to SAMEORIGIN for all static files. That directive is then overriden by the two rules blocks. For *.mp4 files, two custom headers will be sent: X-Frame-Options and Content-Type. The repeated X-Frame-Options is necessary as the headers directive in the rule overrides the parent, rather than extending it. Therefore, the rule for *.mp3 files will add only an X-Specialness header, and no X-Frame-Options header. This example also demonstrates an effective way to set custom Content-Type headers for unusual file types using rules. Note that the headers directive applies only to static content. Headers for responses generated by your application are unaffected. If custom headers for certain file types or frame control are needed, set them from within the application. How can I rewrite an incoming request without a redirect? Rules blocks support regular expression capture groups that can be referenced in a passthru command. For example, the following configuration will result in requests to /project/123 being seen by the application as a request to /index.php?projectid=123 without causing an HTTP redirect. Note that query parameters present in the request are unaffected and will, unconditionally, appear in the request as seen by the application. web:locations:\u0026#39;/\u0026#39;:root:\u0026#39;public\u0026#39;passthru:\u0026#39;/index.php\u0026#39;index:- index.phpscripts:trueallow:truerules:\u0026#39;^/project/(?\u0026lt;projectid\u0026gt;.*)$\u0026#39;:passthru:\u0026#39;/index.php?projectid=$projectid\u0026#39;How can I serve directories at different paths than in my application? Although it’s common for the directories on disk to be served directly by the web server, that’s not actually a requirement. If desired it is quite possible to create a web URL structure that does not map 1:1 to the structure on disk. Consider the following example. The git repository is structured like so: .platform/ services.yaml routes.yaml .platform.app.yaml application/ conf/ server.ini application.py gitbook-src/ old-docs/ The application directory contains a Python application. The gitbook-src directory contains a GitBook project that is the public documentation for the application. The old-docs directory contains a static HTML backup of legacy documentation for an older version of the application that is still needed. Assume that the GitBook source is compiled by the build process into the _book directory, as in the example above. The following web block will: Start your Python application using uwsgi. Route all requests to ‘/’ to the Python application unconditionally, unless one of the following two rules apply. Route requests to the /docs path to the _book directory, which contains our generated documentation, with a short cache lifetime. Route requests to the /docs/legacy path to the old-docs directory, which contains plain old HTML, with a very long cache lifetime since those files should never change. web:commands:start:\u0026#39;uwsgi --ini application/conf/server.ini\u0026#39;locations:\u0026#39;/\u0026#39;:passthru:true\u0026#39;/docs\u0026#39;:root:\u0026#39;_book\u0026#39;index:-  index.html expires:300sscripts:falseallow:true\u0026#39;/docs/legacy\u0026#39;:root:\u0026#39;old-docs\u0026#39;index:-  index.html expires:4wscripts:falseallow:trueEven though the URL structure doesn’t match the directory names or hierarchy on disk, that’s no issue. It also means the application can safely coexist with static files as if it were a single site hierarchy without the need to mix the static pages in with your Python code.",
        "section": "Configure your application",
        "subsections": " Commands Upstream  Socket family   Locations  Rules   How do I setup a basic PHP application with front-controller? How can I serve a static-only site? How can I control the headers sent with my files? How can I rewrite an incoming request without a redirect? How can I serve directories at different paths than in my application?  ",
        "image": "",
        "url": "/configuration/app/web.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "0691da9cff24fc173e25a67f96a292f2",
        "title": "Multiple Drupal  sites in a single Project",
        "description": "",
        "text": " Platform.sh supports running multiple applications in the same project and these can be two or more Drupal site. But, they would be separate Drupal instances , they will have their assets separate and live their lives apart and it would be much better for them not to share the same database (though they could). Note, that the same Drupal instance can also use multiple databases (just add multiple instances to services.yaml and use db_select) you will need to override settings.php as described here and add the other databases you could then use db_select to switch between those. Old Style “Mutlisite” and Platform.sh Platform.sh actively discourages running Drupal in “multisite” mode. Doing so eliminates many of the advantages Platform.sh offers, such as isolation, safe testing, and so forth. Additionally, because of the dynamic nature of the domain names that are created for the different environments the multisite configuration would likely be complex and fragile. We recommend running separate projects for separate Drupal sites, or using one of the various “single instance” options available such as Domain Access , Organic Groups , or Workbench Access . Using Domain Access Of course Platform.sh supports the Domain Access module, as it supports anything Drupal. If the multiple sites are part of the same project this makes sense. Because of the dynamic nature of routes in Platform.sh you will need to implement some logic (here you would replace MYMODULE with a convenient name of your own and include it in your custom modules for your Drupal installation). \u0026lt;?php /** * Implements hook_domain_default_domains(). */ function MYMODULE_domain_default_domains() { $domains = array(); $domains[\u0026#39;wipe-domain-tables\u0026#39;] = \u0026#39;wipe-domain-tables\u0026#39;; $routes = (array) json_decode(base64_decode(getenv(\u0026#39;PLATFORM_ROUTES\u0026#39;))); if (!empty($routes) \u0026amp;\u0026amp; is_array($routes)) { $weight = -1; foreach ($routes as $url =\u0026gt; $route) { if ( $route-\u0026gt;upstream == \u0026#39;drupal\u0026#39; \u0026amp;\u0026amp; $url, $matches) \u0026amp;\u0026amp; $route-\u0026gt;original_url, $matches2) ) { $scheme = $matches[1]; $domain_name = $matches[2]; $machine_name = $matches2[1]; $domains[$machine_name] = array( \u0026#39;subdomain\u0026#39; =\u0026gt; $domain_name, \u0026#39;sitename\u0026#39; =\u0026gt; MYMODULE_get_sitename($machine_name), \u0026#39;scheme\u0026#39; =\u0026gt; $scheme, \u0026#39;valid\u0026#39; =\u0026gt; 1, \u0026#39;weight\u0026#39; =\u0026gt; $weight\u0026#43;\u0026#43;, \u0026#39;is_default\u0026#39; =\u0026gt; ($machine_name == \u0026#39;www\u0026#39; ? 1 : 0), \u0026#39;machine_name\u0026#39; =\u0026gt; $machine_name, ); } } } return $domains; }",
        "section": "Getting Started",
        "subsections": " Using Domain Access  ",
        "image": "",
        "url": "/frameworks/drupal7/multi-site.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "5b08bd32e99bce47ad55d0af03caa38b",
        "title": "Next steps",
        "description": "",
        "text": " In this guide you created a project using the CLI and configured your project to run on Platform.sh using a few simple configuration files. Don’t stop now! There are far more features that make Platform.sh profoundly helpful to developers that you have left to explore. Developing on Platform.sh Once an application has been migrated to Platform.sh, there’s plenty more features that will help improve your development life cycle. Local development Remotely connect to services and build your application locally during development. Development environments Activate development branches and test new features before merging into production. Additional Resources External Integrations Configure Platform.sh to mirror every push and pull request on GitHub, Gitlab, and Bitbucket. Going Live Set up your site for production, configure domains, and go live! Platform.sh Community Portal Check out how-tos, tutorials, and get help for your questions about Platform.sh. Platform.sh Blog Read news and how-to posts all about working with Platform.sh. Back",
        "section": "Getting started",
        "subsections": "   Developing on Platform.sh Additional Resources    ",
        "image": "",
        "url": "/gettingstarted/gettingstarted/own-code/next-steps.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "64b63a3d122539ad6dad284ea666ba7c",
        "title": "Redis (Object cache)",
        "description": "",
        "text": " Redis is a high-performance in-memory object store, well-suited for application level caching. See the Redis documentation for more information. Platform.sh supports two different Redis configurations: One persistent (useful for key-value application data) and one ephemeral (in-memory only, useful for application caching). Aside from that distinction they are identical. Supported versions Grid Dedicated 3.2 4.0 5.0 5.0 Deprecated versions The following versions are available but are not receiving security updates from upstream, so their use is not recommended. They will be removed at some point in the future. Grid Dedicated 2.8 3.0 3.2 Note: Versions 3.0 and higher support up to 64 different databases per instance of the service, but Redis 2.8 is configured to support only a single database. Ephemeral Redis The redis service type is configured to serve as a LRU cache; its storage is not persistent. It is not suitable for use except as a disposable cache. To add an Ephemeral Redis service, specify it in your .platform/services.yaml file like so: cacheredis:type:redis:5.0 Data in an Ephemeral Redis instance is stored only in memory, and thus requires no disk space. When the service hits its memory limit it will automatically evict old cache items according to the configured eviction rule to make room for new ones. Persistent Redis The redis-persistent service type is configured for persistent storage. That makes it a good choice for fast application-level key-value storage. To add a Persistent Redis service, specify it in your .platform/services.yaml file like so: data:type:redis-persistent:5.0disk:256 The disk key is required for redis-persistent to tell Platform.sh how much disk space to reserve for Redis’ persistent data. Note: Switching a service from Persistent to Ephemeral configuration is not supported at this time. To switch between modes, use a different service with a different name. Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : {  cluster :  rjify4yjcwxaa-master-7rqtwti ,  fragment : null,  host :  redis.internal ,  host_mapped : false,  hostname :  ftfs74bvoizcu4ua5eisskh7re.redis.service._.eu-3.platformsh.site ,  ip :  169.254.0.86 ,  password : null,  path : null,  port : 6379,  public : false,  query : [],  rel :  redis ,  scheme :  redis ,  service :  redis ,  type :  redis:5.0 ,  username : null } The format is identical regardless of whether it’s a persistent or ephemeral service. Usage example In your .platform/services.yaml: cacheredis:type:redis:5.0 If you are using PHP, configure a relationship and enable the PHP redis extension in your .platform.app.yaml. runtime:extensions:- redisrelationships:rediscache: cache:redis You can then use the service in a configuration file of your application with something like: Java Node.js PHP Python package sh.platform.languages.sample; import redis.clients.jedis.Jedis; import redis.clients.jedis.JedisPool; import sh.platform.config.Config; import sh.platform.config.Redis; import java.util.Set; import java.util.function.Supplier; public class RedisSample implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); // The \u0026#39;database\u0026#39; relationship is generally the name of primary database of an application. // It could be anything, though, as in the case here here where it\u0026#39;s called  redis . Redis database = config.getCredential( redis , Redis::new); JedisPool dataSource = database.get(); // Get a Redis Client final Jedis jedis = dataSource.getResource(); // Set a values jedis.sadd( cities ,  Salvador ); jedis.sadd( cities ,  London ); jedis.sadd( cities ,  São Paulo ); // Read it back. Set\u0026lt;String\u0026gt; cities = jedis.smembers( cities ); logger.append( cities:   \u0026#43; cities); jedis.del( cities ); return logger.toString(); } } const redis = require(\u0026#39;redis\u0026#39;); const config = require( platformsh-config ).config(); const { promisify } = require(\u0026#39;util\u0026#39;); exports.usageExample = async function() { const credentials = config.credentials(\u0026#39;redis\u0026#39;); var client = redis.createClient(credentials.port, credentials.host); // The Redis client is not Promise-aware, so make it so. const redisGet = promisify(client.get).bind(client); const redisSet = promisify(client.set).bind(client); let key = \u0026#39;Deploy day\u0026#39;; let value = \u0026#39;Friday\u0026#39;; // Set a value. await redisSet(key, value); // Read it back. let test = await redisGet(key); let output = `Found value \u0026lt;strong\u0026gt;${test}\u0026lt;/strong\u0026gt; for key \u0026lt;strong\u0026gt;${key}\u0026lt;/strong\u0026gt;.`; return output; }; \u0026lt;?php declare(strict_types=1); use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // Get the credentials to connect to the Redis service. $credentials = $config-\u0026gt;credentials(\u0026#39;redis\u0026#39;); try { // Connecting to Redis server. $redis = new Redis(); $redis-\u0026gt;connect($credentials[\u0026#39;host\u0026#39;], $credentials[\u0026#39;port\u0026#39;]); $key =  Deploy day ; $value =  Friday ; // Set a value. $redis-\u0026gt;set($key, $value); // Read it back. $test = $redis-\u0026gt;get($key); printf(\u0026#39;Found value \u0026lt;strong\u0026gt;%s\u0026lt;/strong\u0026gt; for key \u0026lt;strong\u0026gt;%s\u0026lt;/strong\u0026gt;.\u0026#39;, $test, $key); } catch (Exception $e) { print $e-\u0026gt;getMessage(); } from redis import Redis from platformshconfig import Config def usage_example(): # Create a new config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the Redis service. credentials = config.credentials(\u0026#39;redis\u0026#39;) try: redis = Redis(credentials[\u0026#39;host\u0026#39;], credentials[\u0026#39;port\u0026#39;]) key =  Deploy day  value =  Friday  # Set a value redis.set(key, value) # Read it back test = redis.get(key) return \u0026#39;Found value \u0026lt;strong\u0026gt;{0}\u0026lt;/strong\u0026gt; for key \u0026lt;strong\u0026gt;{1}\u0026lt;/strong\u0026gt;.\u0026#39;.format(test.decode( utf-8 ), key) except Exception as e: return e Multiple databases Redis 3.0 and above are configured to support up to 64 databases. Redis does not support distinct users for different databases so the same relationship connection gives access to all databases. To use a particular database, use the Redis select command through your API library. For instance, in PHP you could write: $redis-\u0026gt;select(0); // switch to DB 0 $redis-\u0026gt;set(\u0026#39;x\u0026#39;, \u0026#39;42\u0026#39;); // write 42 to x $redis-\u0026gt;move(\u0026#39;x\u0026#39;, 1); // move to DB 1 $redis-\u0026gt;select(1); // switch to DB 1 $redis-\u0026gt;get(\u0026#39;x\u0026#39;); // will return 42 Consult the documentation for your connection library and Redis itself for further details. Eviction policy On the Ephemeral redis service it is also possible to select the key eviction policy. That will control how Redis behaves when it runs out of memory for cached items and needs to clear old items to make room. cache:type:redis:5.0configuration:maxmemory_policy:allkeys-lruThe default value if not specified is allkeys-lru, which will simply remove the oldest cache item first. Legal values are: noeviction allkeys-lru volatile-lru allkeys-random volatile-random volatile-ttl See the Redis documentation for a description of each option. Using redis-cli to access your Redis service Assuming a Redis relationship named applicationcache defined in .platform.app.yaml relationships:rediscache: cacheredis:redis  and services.yaml cacheredis:type:redis:5.0 The host name and port number obtained from PLATFORM_RELATIONSHIPS would be applicationcache.internal and 6379. Open an SSH session and access the Redis server using the redis-cli tool as follows: redis-cli -h applicationcache.internal -p 6379 Using Redis as handler for native PHP sessions Using the same configuration but with your Redis relationship named sessionstorage: .platform/services.yaml cacheredis:type:redis:5.0 .platform.app.yaml relationships:sessionstorage: cache:redis variables:php:session.save_handler:redissession.save_path: tcp://sessionstorage.internal:6379 ",
        "section": "Configure services",
        "subsections": " Supported versions  Deprecated versions   Ephemeral Redis Persistent Redis Relationship Usage example Multiple databases Eviction policy Using redis-cli to access your Redis service  Using Redis as handler for native PHP sessions    ",
        "image": "",
        "url": "/configuration/services/redis.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "1550b99deff1aae06d4fec82b5986ac1",
        "title": "Update all the things",
        "description": "",
        "text": " The Platform.sh Rule: Update Early, Update Often Platform.sh periodically updates its container images for the latest security updates from upstream providers. (PHP versions, Ruby versions, MariaDB versions, etc.). These do not always happen immediately but when a security vulnerability is identified and released it tends to be fairly soon after. However, these updates are not automatically propagated to individual projects as that would involve potential customer downtime. Instead, the latest available version of every requested container is loaded on each deploy to a given environment. After a deploy you are always guaranteed to be running the latest Platform.sh-provided version of a container. If you have regular redeploys scheduled for Let’s Encrypt SSL certificates then that will also ensure your container versions are up to date at the same time. For that reason we recommend all customers setup the appropriate cron task to redeploy every two weeks or so.",
        "section": "Security and compliance",
        "subsections": "",
        "image": "",
        "url": "/security/updates.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "5d80ca831390a154bc72c43f1efaae35",
        "title": "Using Git submodules",
        "description": "",
        "text": " Clone submodules during deployment Platform.sh allows you to use submodules in your Git repository. They are usually listed in a .gitmodules file at the root of your Git repository. When you push via Git, Platform.sh will try to clone them automatically. Here is an example of a .gitmodules file: [submodule  app/Oro ] path = src/Oro url = https://github.com/orocrm/platform.git [submodule  src/OroPackages/src/Oro/Bundle/EntitySerializedFieldsBundle ] path = src/OroPackages/src/Oro/Bundle/EntitySerializedFieldsBundle url = https://github.com/orocrm/OroEntitySerializedFieldsBundle.git [submodule  src/OroB2B ] path = src/OroB2B url = https://github.com/orocommerce/orocommerce.git When you run git push, you can see the output of the log: Validating submodules. Updated submodule git://github.com/orocommerce/orocommerce: 4 references updated. Updated submodule git://github.com/orocrm/platform: 229 references updated. Updated submodule git://github.com/orocrm/OroEntitySerializedFieldsBundle: 11 references updated. Error when validating submodules If you see the following error: Validating submodules. Found unresolvable links, updating submodules. E: Error validating submodules in tree: - /src/Oro: Exception: commit 03567c6 not found. This might be due to the following errors fetching submodules: - git@github.com:orocommerce/orocommerce.git: HangupException: The remote server unexpectedly closed the connection. Since the Platform.sh Git server cannot connect to Github via SSH without being granted an SSH key to do so, you should not be using an SSH URL: git@github.com:..., but you should use an HTTPS URL instead: https://github.com/.... Use of private git repositories When using Git submodules that are hosted on private repositories, using the https protocol will fail with errors like: GitProtocolError: unexpected http resp 401 for https://bitbucket.org/myusername/mymodule.git/info/refs?service=git-upload-pack To fix this, you need to: Change your .gitmodules file from the HTTPS syntax to the SSH syntax, e.g. from: [submodule  support/mymodule ] path = support/mymodule url = https://bitbucket.org/myusername/mymodule.git to: [submodule  support/mymodule ] path = support/mymodule url=git@bitbucket.org:myusername/mymodule.git Add the SSH public key in the Platform.sh project settings “Deploy Key” tab in the Web UI as per the Private Repository documentation page, which will allow our Git service to pull the module from the remote git service. This assumes you have configured the remote git repository to allow this by generating a private/public key pair. For example, see the Bitbucket documentation .",
        "section": "Development",
        "subsections": " Clone submodules during deployment Error when validating submodules Use of private git repositories  ",
        "image": "",
        "url": "/development/submodules.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "e73050e3b22ec50821dd721acbd10923",
        "title": "Workers",
        "description": "",
        "text": " Every application may also define zero or more worker instances. A worker instance runs as its own container independently of the web instance and has no Nginx instance running. The router service cannot direct public requests to it, either, so running your own web server on a worker (using Node.js or Go) is not useful. A worker instance is the exact same code and compilation output as a web instance. The container image is built only once, and then deployed multiple times if needed. That is, the build hook and dependencies may not vary from one instance to another. What may vary is how the container is then configured and how resources are allocated. Worker instances are well suited for background tasks such as queue workers, updating indexes, or for running periodic reporting tasks that are too long to make sense as a cron job. (Although those should often be broken up into queue tasks.) A basic, common worker configuration could look like this: workers:queue:size:Scommands:start:| php worker.phpThat defines a single worker named queue, which will be a “small” container, and wil run the command php worker.php on startup. If worker.php ever exits it will be automatically restarted. Any number of workers may be defined with their own distinct name, subject to available resources on your plan. For resource allocation reasons, using workers in your project requires a Medium plan or larger. Accessing the Worker Container Like with any other application container Platform.sh allows you to connect to the worker instance through SSH to inspect logs and interact with it. Using the Platform CLI you would use the --worker switch, like so: platform ssh --worker=queue To output the SSH command you can use: platform ssh --worker=queue --pipe You will see the url is the name of the worker added to the user name after the application name part of the SSH url preceded by a double dash (--). For example given a project with id 3seb7f2j6ogbm you would connect to its master environment for an app called app with a url such as: ssh 3seb7f2j6ogbm-master-7rqtwti--app@ssh.us-2.platform.sh To connect to a worker called queue (as in the example above) you would use an SSH url that would look as follows: ssh 3seb7f2j6ogbm-master-7rqtwti--app--queue@ssh.us-2.platform.sh Workers vs Cron Both worker instances and cron tasks address similar use cases: They both address out-of-band work that an application needs to do but that should not or cannot be done as part of a normal web request. They do so in different ways, however, and so are fit for different use cases. A Cron job is well suited for tasks when: They need to happen on a fixed schedule, not continually. The task itself is not especially long, as a running cron job will block a new deployment. Or it is long, but can be easily divided into many small queued tasks. A delay between when a task is registered and when it actually happens is acceptable. A dedicated worker instance is a better fit if: Tasks should happen “now”, but not block a web request. Tasks are large enough that they risk blocking a deploy, even if they are subdivided. The task in question is a continually running process rather than a stream of discrete units of work. The appropriateness of one approach over the other also varies by language; single-threaded languages would benefit more from either cron or workers than a language with native multi-threading, for instance. If a given task seems like it would run equally well as a worker or as a cron, cron will generally be more efficient as it does not require its own container. Commands The commands key defines the command to launch the worker application. For now there is only a single command, start, but more will be added in the future. The commands.start property is required. The start key specifies the command to use to launch your worker application. It may be any valid shell command, although most often it will run a command in your application in the language of your application. If the command specified by the start key terminates it will be restarted automatically. Note that deploy and post_deploy hooks , as well as cron commands , will run only on the web container, not on workers. Inheritance Any top-level definitions for size , relationships , access , disk and mount , and variables will be inherited by every worker, unless overridden explicitly. That means, for example, that the following two .platform.app.yaml definitions produce identical workers. name:apptype:python:3.5disk:256mounts:test:source:localsource_path:testrelationships:database:\u0026#39;mysqldb:mysql\u0026#39;workers:queue:commands:start:| python queue-worker.pymail:commands:start:| python mail-worker.pyname:apptype:python:3.5workers:queue:commands:start:| python queue-worker.pydisk:256mounts:test:source:localsource_path:testrelationships:database:\u0026#39;mysqldb:mysql\u0026#39;mail:commands:start:| python mail-worker.pydisk:256mounts:test:source:localsource_path:testrelationships:database:\u0026#39;mysqldb:mysql\u0026#39;In both cases, there will be two worker instances named queue and mail. Both will have access to a MySQL/MariaDB service defined in services.yaml named mysqldb through the database relationship. Both will also have their own separate, independent local disk mount at /app/test with 256 MB of allowed space. Customizing a worker The most common properties to set in a worker to override the top-level settings are size and variables. size lets you allocate fewer resources to a container that will be running only a single background process (unlike the web site which will be handling many requests at once), while variables lets you instruct the application to run differently as a worker than as a web site. For example, consider this .platform.app.yaml: name:apptype: python:3.7 disk:2048hooks:build:| pip install -r requirements.txtpipinstall-e.pipinstallgunicornrelationships:database:\u0026#39;mysqldb:mysql\u0026#39;messages:\u0026#39;rabbitqueue:rabbitmq\u0026#39;variables:env:type:\u0026#39;none\u0026#39;web:commands:start: gunicorn -b $PORT project.wsgi:application variables:env:type:\u0026#39;web\u0026#39;mounts:uploads:source:localsource_path:uploadslocations: / :root:  passthru:trueallow:false /static :root: static/ allow:trueworkers:queue:size:\u0026#39;M\u0026#39;commands:start:| python queue-worker.pyvariables:env:type:\u0026#39;worker\u0026#39;disk:512mounts:scratch:source:localsource_path:scratchmail:size:\u0026#39;S\u0026#39;commands:start:| python mail-worker.pyvariables:env:type:\u0026#39;worker\u0026#39;disk:256mounts:{}relationships:emails:\u0026#39;rabbitqueue:rabbitmq\u0026#39;There’s a lot going on here, but it’s all reasonably straightforward. This configuration will take a single Python 3.7 code base from your repository, download all dependencies in requirements.txt, and the install Gunicorn. That artifact (your code plus the downloaded dependencies) will be deployed as three separate container instances, all running Python 3.7. The web instance will start a gunicorn process to serve a web application. It will run the gunicorn process to serve web requests, defined by the project/wsgi.py file which contains an application definition. It will have an environment variable named TYPE with value web. It will have a writable mount at /app/uploads with a maximum space of 2048 MB. It will have access to both a MySQL database and a RabbitMQ server, both of which are defined in services.yaml. Platform.sh will automatically allocate resources to it as available on the plan, once all fixed-size containers are allocated. The queue instance will be a worker that is not web-accessible. It will run the queue-worker.py script, and restart it automatically if it ever terminates. It will have an environment variable named TYPE with value worker. It will have a writable mount at /app/scratch with a maximum space of 512 MB. It will have access to both a MySQL database and a RabbitMQ server, both of which are defined in services.yaml (because it doesn’t specify otherwise). It will have “Medium” levels of CPU and RAM allocated to it, always. The mail instance will be a worker that is not web-accessible. It will run the mail-worker.py script, and restart it automatically if it ever terminates. It will have an environment variable named TYPE with value worker. It will have no writable file mounts at all. It will have access only to the RabbitMQ server, through a different relationship name than on the web instance. It will have no access to MySQL whatsoever. It will have “Small” levels of CPU and RAM allocated to it, always. This way, the web instance has a large upload space, the queue instance has a small amount of scratch space for temporary files, and the mail instance has no persistent writable disk space at all as it doesn’t need it. The mail instance also does not need any access to the SQL database so for security reasons it has none. The workers have known fixed sizes, while web can scale to as large as the plan allows. Each instance can also check the TYPE environment variable to detect how it’s running and, if appropriate, vary its behavior accordingly.",
        "section": "Configure your application",
        "subsections": " Accessing the Worker Container Workers vs Cron Commands Inheritance Customizing a worker  ",
        "image": "",
        "url": "/configuration/app/workers.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "553e2358ea3742bbf22cc783b7621238",
        "title": "Cron jobs",
        "description": "",
        "text": " Cron jobs allow you to run scheduled tasks at specified times or intervals. The crons section of .platform.app.yaml describes these tasks and the schedule when they are triggered. Each item in the list is a unique name identifying a separate cron job. Crons are started right after build phase. It has a few subkeys listed below: spec: The cron specification . For example: */19 * * * * to run every 19 minutes. cmd: The command that is executed, for example cd public ; drush core-cron The minimum interval between cron runs is 5 minutes, even if specified as less. Additionally, a variable delay is added to each cron job in each project in order to prevent host overloading should every project try to run their nightly tasks at the same time. Your crons will not run exactly at the time that you specify, but will be delayed by 0-300 seconds. A single application container may have any number of cron tasks configured, but only one may be running at a time. That is, if a cron task fires while another cron task is still running it will pause and then continue normally once the first has completed. Cron runs are executed using the dash shell, not the bash shell used by normal SSH logins. In most cases that makes no difference but may impact some more involved cron scripts. If an application defines both a web instance and a worker instance, cron tasks will be run only on the web instance. Note: Cron log output is captured in the at /var/log/cron.log. See the Log page for more information on logging. How do I setup Cron for a typical Drupal site? The following example runs Drupal’s normal cron hook every 19 minutes, using Drush. It also sets up a second cron task to run Drupal’s queue runner on the aggregator_feeds queue every 7 minutes. crons:# Run Drupal\u0026#39;s cron tasks every 19 minutes.drupal:spec:\u0026#39;*/19 * * * *\u0026#39;cmd:\u0026#39;cd web ; drush core-cron\u0026#39;# But also run pending queue tasks every 7 minutes.# Use an odd number to avoid running at the same time as the `drupal` cron.drush-queue:spec:\u0026#39;*/7 * * * *\u0026#39;cmd:\u0026#39;cd web ; drush queue-run aggregator_feeds\u0026#39;",
        "section": "Configure your application",
        "subsections": " How do I setup Cron for a typical Drupal site?  ",
        "image": "",
        "url": "/configuration/app/cron.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "406dc89b82bd7f55dd6ea9a56a731831",
        "title": "Drupal Frequently Asked Questions (FAQ)",
        "description": "",
        "text": " How should I name my make files? In order for Platform to automatically detect your make file, you need to call it project.make. You can also have a specific make file for Drupal core called project-core.make When I push changes to a make file, does Platform.sh run the update? After a push, Platform.sh will rebuild your environment and download all the modules that are in your make file. If an update function (hook_update) needs to run, you’ll have to manually trigger it by going to /update.php or use the deployment hooks to automatically run the updates. How can I provide a robots.txt file in production? If using the drupal build mode with a Drush make file, place your robots.txt file in your application root, as a sibling of .platform.app.yaml. It will get moved to the appropriate location automatically by the build process. For all other cases just include the file in your web root normally. On non-production environments Platform.sh automatically blocks web crawlers using the X-Robots-Tag header . You can disable that per-environment if needed. I’m getting a PDO Exception ‘MySQL server has gone away’ Normally, this means there is a problem with the MySQL server container and you may need to increase the storage available to MySQL to resolve the issue. Ballooning MySQL storage can be caused by a number of items: A large number of watchdog entries being captured. Fix the errors being generated or disable database logging. Cron should run at regular intervals to ensure cache tables get cleared out. If you’re using Drupal Commerce Core \u0026lt; 1.10, you may have an extremely large cache_form table . Upgrade to Commerce Core 1.10 to resolve. Why do I get “MySQL cannot connect to the database server”? If you are having a problem connecting to the database server, you will need force a re-deployment of the database container. To do so, you can edit the service definition to add or remove a small amount of storage and then push. Can I use the name of the session cookie for caching? For Drupal sites, the name of the session cookie is based on a hash of the domain name. This means that it will actually be consistent for a specific website and can safely be used as a fixed value. How can I rebuild the site registry? During the migration process, one or more modules may have changed location. This could result in a WSOD (white screen of death), any number of errors (fatal or otherwise), or just a plain broken site. To remedy this situation, the registry will need to be rebuilt . To rebuild the Drupal registry on your Platform.sh instance, you will need to do the following: First, SSH into your web container. $ ssh [SSH-URL] Second, execute the following commands to download, tweak, and run the registry rebuild. $ drush dl registry_rebuild-7.x-2.3 --destination=/app/tmp $ sed -i \u0026#39;s/, define_drupal_root()/, /app/tmp/registry_rebuild/registry_rebuild.php $ cd /app/public $ php ../tmp/registry_rebuild/registry_rebuild.php Can I use Backup \u0026amp; Migrate? The Backup \u0026amp; Migrate module is a Drupal module that provides automated scheduled dumps of a Drupal site’s content. It does so in the form of an SQL dump and/or tar.gz archived copy of your site’s file directory, which can then be optionally uploaded to a remote storage service. In general B\u0026amp;M is not necessary when running on Platform.sh. Platform.sh’s Backup functionality offers a faster, more robust and easier to restore backup, and for exporting data using the Platform.sh CLI is just as effective. If, however, you find it necessary to still run B\u0026amp;M be aware that its resource requirements can be quite high. B\u0026amp;M requires a great deal of memory in order to create a backup, over and above Drupal’s memory requirements. It is possible for B\u0026amp;M to create a backup in the system’s temp folder, then PHP runs out of memory before it can complete sending the backup to a 3rd party or cleaning up the temp file. In the latter case, a full temp disk can result in other, seemingly unrelated issues such as an inability to upload files. If you find B\u0026amp;M failing or the temp directory filling up mysteriously, try increasing the PHP memory limit to account for B\u0026amp;M’s needs. For example, add the following to .platform.app.yaml: variables:php:memory_limit:512MIf that is still insufficient, your site may simply be too large to work effectively with B\u0026amp;M. We recommend setting up automated scheduled backups instead.",
        "section": "Getting Started",
        "subsections": " How should I name my make files? When I push changes to a make file, does Platform.sh run the update? How can I provide a robots.txt file in production? I\u0026rsquo;m getting a PDO Exception \u0026lsquo;MySQL server has gone away\u0026rsquo; Why do I get \u0026ldquo;MySQL cannot connect to the database server\u0026rdquo;? Can I use the name of the session cookie for caching? How can I rebuild the site registry? Can I use Backup \u0026amp; Migrate?  ",
        "image": "",
        "url": "/frameworks/drupal7/faq.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "5df2bf6b6b667b9475acc639cf1a4233",
        "title": "Solr (Search service)",
        "description": "",
        "text": " Apache Solr is a scalable and fault-tolerant search index. Solr search with generic schemas provided, and a custom schema is also supported. See the Solr documentation for more information.” Supported versions Grid Dedicated 3.6 4.1 6.3 6.6 7.6 7.7 8.0 8.4 4.10 6.3 6.6 8.0 Deprecated versions The following versions are available but are not receiving security updates from upstream, so their use is not recommended. They will be removed at some point in the future. Grid Dedicated None available Relationship The format exposed in the $PLATFORM_RELATIONSHIPS environment variable : {  cluster :  rjify4yjcwxaa-master-7rqtwti ,  host :  solr.internal ,  hostname :  7iug3likvszuk2vnf4y3dafara.solr.service._.eu-3.platformsh.site ,  ip :  169.254.202.136 ,  path :  solr/maincore ,  port : 8080,  rel :  solr ,  scheme :  solr ,  service :  solr ,  type :  solr:8.0  } Usage example In your .platform/services.yaml: searchsolr:type:solr:8.4disk:256 In your .platform.app.yaml: relationships:solrsearch: searchsolr:solr  Note: You will need to use the solr type when defining the service # .platform/services.yamlservice_name:type:solr:versiondisk:256 and the endpoint solr when defining the relationship # .platform.app.yamlrelationships:relationship_name:“service_name:solr” Your service_name and relationship_name are defined by you, but we recommend making them distinct from each other. Exception: This pattern will be the case unless you explictly set additional endpoints for multiple cores, as shown in the section below. You can then use the service in a configuration file of your application with something like: Go Java Node.js PHP Python package examples import (  fmt  psh  github.com/platformsh/config-reader-go/v2  gosolr  github.com/platformsh/config-reader-go/v2/gosolr  solr  github.com/rtt/Go-Solr  ) func UsageExampleSolr() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // Get the credentials to connect to the Solr service. credentials, err := config.Credentials( solr ) checkErr(err) // Retrieve Solr formatted credentials. formatted, err := gosolr.FormattedCredentials(credentials) checkErr(err) // Connect to Solr using the formatted credentials. connection := \u0026amp;solr.Connection{URL: formatted} // Add a document and commit the operation. docAdd := map[string]interface{}{  add : []interface{}{ map[string]interface{}{ id : 123,  name :  Valentina Tereshkova }, }, } respAdd, err := connection.Update(docAdd, true) checkErr(err) // Select the document. q := \u0026amp;solr.Query{ Params: solr.URLParamMap{  q : []string{ id:123 }, }, } resSelect, err := connection.CustomSelect(q,  query ) checkErr(err) // Delete the document and commit the operation. docDelete := map[string]interface{}{  delete : map[string]interface{}{  id : 123, }, } resDel, err := connection.Update(docDelete, true) checkErr(err) message := one document - %s\u0026lt;br\u0026gt; Selecting document (1 expected): %d\u0026lt;br\u0026gt; Deleting document - %s\u0026lt;br\u0026gt; respAdd, resSelect.Results.NumFound, resDel) return message } package sh.platform.languages.sample; import org.apache.solr.client.solrj.SolrQuery; import org.apache.solr.client.solrj.SolrServerException; import org.apache.solr.client.solrj.impl.HttpSolrClient; import org.apache.solr.client.solrj.impl.XMLResponseParser; import org.apache.solr.client.solrj.response.QueryResponse; import org.apache.solr.client.solrj.response.UpdateResponse; import org.apache.solr.common.SolrDocumentList; import org.apache.solr.common.SolrInputDocument; import sh.platform.config.Config; import sh.platform.config.Solr; import java.io.IOException; import java.util.function.Supplier; public class SolrSample implements Supplier\u0026lt;String\u0026gt; { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); Solr solr = config.getCredential( solr , Solr::new); try { final HttpSolrClient solrClient = solr.get(); solrClient.setParser(new XMLResponseParser()); // Add a document SolrInputDocument document = new SolrInputDocument(); final String id =  123456 ; document.addField( id , id); document.addField( name ,  Ada Lovelace ); document.addField( city ,  London ); solrClient.add(document); final UpdateResponse response = solrClient.commit(); logger.append( Adding one document. Status (0 is success):  ) SolrQuery query = new SolrQuery(); query.set( q ,  city:London ); QueryResponse queryResponse = solrClient.query(query); SolrDocumentList results = queryResponse.getResults(); logger.append(String.format( Selecting documents (1 expected): %d results.getNumFound())); // Delete one document solrClient.deleteById(id); logger.append(String.format( Deleting one document. Status (0 is success): %s solrClient.commit().getStatus())); } catch (SolrServerException | IOException exp) { throw new RuntimeException( An error when execute Solr  , exp); } return logger.toString(); } } const solr = require(\u0026#39;solr-node\u0026#39;); const config = require( platformsh-config ).config(); exports.usageExample = async function() { let client = new solr(config.formattedCredentials(\u0026#39;solr\u0026#39;, \u0026#39;solr-node\u0026#39;)); let output = \u0026#39;\u0026#39;; // Add a document. let addResult = await client.update({ id: 123, name: \u0026#39;Valentina Tereshkova\u0026#39;, }); output \u0026#43;=  Adding one document. Status (0 is success):   \u0026#43; addResult.responseHeader.status \u0026#43;  \u0026lt;br // Flush writes so that we can query against them. await client.softCommit(); // Select one document: let strQuery = client.query().q(); let writeResult = await client.search(strQuery); output \u0026#43;=  Selecting documents (1 expected):   \u0026#43; writeResult.response.numFound \u0026#43;  \u0026lt;br // Delete one document. let deleteResult = await client.delete({id: 123}); output \u0026#43;=  Deleting one document. Status (0 is success):   \u0026#43; deleteResult.responseHeader.status \u0026#43;  \u0026lt;br return output; }; \u0026lt;?php declare(strict_types=1); use use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // Get the credentials to connect to the Solr service. $credentials = $config-\u0026gt;credentials(\u0026#39;solr\u0026#39;); try { $config = [ \u0026#39;endpoint\u0026#39; =\u0026gt; [ \u0026#39;localhost\u0026#39; =\u0026gt; [ \u0026#39;host\u0026#39; =\u0026gt; $credentials[\u0026#39;host\u0026#39;], \u0026#39;port\u0026#39; =\u0026gt; $credentials[\u0026#39;port\u0026#39;], \u0026#39;path\u0026#39; =\u0026gt;  /  . $credentials[\u0026#39;path\u0026#39;], ] ] ]; $client = new Client($config); // Add a document $update = $client-\u0026gt;createUpdate(); $doc1 = $update-\u0026gt;createDocument(); $doc1-\u0026gt;id = 123; $doc1-\u0026gt;name = \u0026#39;Valentina Tereshkova\u0026#39;; $update-\u0026gt;addDocuments(array($doc1)); $update-\u0026gt;addCommit(); $result = $client-\u0026gt;update($update); print  Adding one document. Status (0 is success):   .$result-\u0026gt;getStatus().  \u0026lt;br // Select one document $query = $client-\u0026gt;createQuery($client::QUERY_SELECT); $resultset = $client-\u0026gt;execute($query); print  Selecting documents (1 expected):   .$resultset-\u0026gt;getNumFound() .  \u0026lt;br // Delete one document $update = $client-\u0026gt;createUpdate(); $update-\u0026gt;addDeleteById(123); $update-\u0026gt;addCommit(); $result = $client-\u0026gt;update($update); print  Deleting one document. Status (0 is success):   .$result-\u0026gt;getStatus().  \u0026lt;br } catch (Exception $e) { print $e-\u0026gt;getMessage(); } import pysolr from xml.etree import ElementTree as et from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the Solr service. credentials = config.credentials(\u0026#39;solr\u0026#39;) try: formatted_url = config.formatted_credentials(\u0026#39;solr\u0026#39;, \u0026#39;pysolr\u0026#39;) # Create a new Solr Client using config variables client = pysolr.Solr(formatted_url) # Add a document message = \u0026#39;\u0026#39; doc_1 = {  id : 123,  name :  Valentina Tereshkova  } result0 = client.add([doc_1]) client.commit() message \u0026#43;= \u0026#39;Adding one document. Status (0 is success): {0} \u0026lt;br /\u0026gt;\u0026#39;.format(et.fromstring(result0)[0][0].text) # Select one document query = client.search(\u0026#39;*:*\u0026#39;) message \u0026#43;= documents (1 expected): {0} \u0026lt;br /\u0026gt;\u0026#39;.format(str(query.hits)) # Delete one document result1 = client.delete(doc_1[\u0026#39;id\u0026#39;]) client.commit() message \u0026#43;= one document. Status (0 is success): {0}\u0026#39;.format(et.fromstring(result1)[0][0].text) return message except Exception as e: return e Solr 4 For Solr 4, Platform.sh supports only a single core per server called collection1. You must provide your own Solr configuration via a core_config key in your .platform/services.yaml: search:type:solr:4.10disk:1024configuration:core_config:!archive \u0026lt;directory\u0026gt; The directory parameter points to a directory in the Git repository, in or below the .platform/ folder. This directory needs to contain everything that Solr needs to start a core. At the minimum, solrconfig.xml and schema.xml. For example, place them in .platform/solr/conf/ such that the schema.xml file is located at .platform/solr/conf/schema.xml. You can then reference that path like this - search:type:solr:4.10disk:1024configuration:core_config:!archive solr/conf/ Solr 6 and later For Solr 6 and later Platform.sh supports multiple cores via different endpoints. Cores and endpoints are defined separately, with endpoints referencing cores. Each core may have its own configuration or share a configuration. It is best illustrated with an example. search:type:solr:8.4disk:1024configuration:cores:mainindex:conf_dir:!archive core1-conf extraindex:conf_dir:!archive core2-conf endpoints:main:core:mainindexextra:core:extraindexThe above definition defines a single Solr 8.0 server. That server has 2 cores defined: mainindex — the configuration for which is in the .platform/core1-conf directory — and extraindex — the configuration for which is in the .platform/core2-conf directory. It then defines two endpoints: main is connected to the mainindex core while extra is connected to the extraindex core. Two endpoints may be connected to the same core but at this time there would be no reason to do so. Additional options may be defined in the future. Each endpoint is then available in the relationships definition in .platform.app.yaml. For example, to allow an application to talk to both of the cores defined above its .platform.app.yaml file should contain the following: relationships:solrsearch1:\u0026#39;search:main\u0026#39;solrsearch2:\u0026#39;search:extra\u0026#39;That is, the application’s environment would include a solr1 relationship that connects to the main endpoint, which is the mainindex core, and a solr2 relationship that connects to the extra endpoint, which is the extraindex core. The relationships array would then look something like the following: {  solr1 : [ {  path :  solr/mainindex ,  host :  248.0.65.197 ,  scheme :  solr ,  port : 8080 } ],  solr2 : [ {  path :  solr/extraindex ,  host :  248.0.65.197 ,  scheme :  solr ,  port : 8080 } ] } Configsets For even more customizability, it’s also possible to define Solr configsets. For example, the following snippet would define one configset, which would be used by all cores. Specific details can then be overriden by individual cores using core_properties, which is equivalent to the Solr core.properties file. search:type:solr:8.4disk:1024configuration:configsets:mainconfig:!archive configsets/solr8 cores:english_index:core_properties:| configSet=mainconfigschema=english/schema.xmlarabic_index:core_properties:| configSet=mainconfigschema=arabic/schema.xmlendpoints:english:core:english_indexarabic:core:arabic_indexIn this example, the directory .platform/configsets/solr8 contains the configuration definition for multiple cores. There are then two cores created: english_index uses the defined configset, but specifically the .platform/configsets/solr6/english/schema.xml file, while arabic_index is identical except for using the .platform/configsets/solr6/arabic/schema.xml file. Each of those cores is then exposed as its own endpoint. Note that not all core.properties features make sense to specify in the core_properties. Some keys, such as name and dataDir, are not supported, and may result in a solrconfig that fails to work as intended, or at all. Default configuration If no configuration is specified, the default configuration is equivalent to: search:type:solr:8.4configuration:cores:collection1:conf_dir:\u0026#39;{}\u0026#39;# This will pick up the default Drupal 8 configurationendpoints:solr:core:collection1The default configuration is based on an older version of the Drupal 8 Search API Solr module that is no longer in use. While it may work for generic cases defining your own custom configuration, core, and endpoint is strongly recommended. Limitations The recommended maximum size for configuration directories (zipped) is 2MB. These need to be monitored to ensure they don’t grow beyond that. If the zipped configuration directories grow beyond this, performance will decline and deploys will become longer. The directory archives will be compressed and string encoded. You could use this bash pipeline echo $(($(tar czf - . | base64 | wc -c )/(1024*1024))) Megabytes inside the directory to get an idea of the archive size. The configuration directory is a collection of configuration data, like a data dictionary, e.g. small collections of key/value sets. The best way to keep the size small is to restrict the directory context to plain configurations. Including binary data like plugin .jar files will inflate the archive size, and is not recommended. Accessing the Solr server administrative interface Because Solr uses HTTP for both its API and admin interface it’s possible to access the admin interface over an SSH tunnel. platform tunnel:open That will open an SSH tunnel to all services on the current environment, and give an output similar to: SSH tunnel opened on port 30000 to relationship: solr SSH tunnel opened on port 30001 to relationship: database Logs are written to: /home/myuser/.platformsh/tunnels.log List tunnels with: platform tunnels View tunnel details with: platform tunnel:info Close tunnels with: platform tunnel:close In this example, you can now open http://localhost:30000/solr/ in a browser to access the Solr admin interface. Note that you cannot create indexes or users this way, but you can browse the existing indexes and manipulate the stored data. Note: Platform.sh Dedicated users can use ssh -L 8888:localhost:8983 \u0026lt;user\u0026gt;@\u0026lt;cluster-name\u0026gt;.ent.platform.sh to open a tunnel instead, after which the Solr server administrative interface will be available at http://localhost:8888/solr/. Upgrading The Solr data format sometimes changes between versions in incompatible ways. Solr does not include a data upgrade mechanism as it is expected that all indexes can be regenerated from stable data if needed. To upgrade (or downgrade) Solr you will need to use a new service from scratch. There are two ways of doing that. Destructive In your services.yaml file, change the version of your Solr service and its name. Then update the name in the .platform.app.yaml relationships block. When you push that to Platform.sh, the old service will be deleted and a new one with the name name created, with no data. You can then have your application reindex data as appropriate. This approach is simple but has the downside of temporarily having an empty Solr instance, which your application may or may not handle gracefully, and needing to rebuild your index afterward. Depending on the size of your data that could take a while. Transitional For a transitional approach you will temporarily have two Solr services. Add a second Solr service with the new version a new name and give it a new relationship in .platform.app.yaml. You can optionally run in that configuration for a while to allow your application to populate indexes in the new service as well. Once you’re ready to cut over, remove the old Solr service and relationship. You may optionally have the new Solr service use the old relationship name if that’s easier for your application to handle. Your application is now using the new Solr service. This approach has the benefit of never being without a working Solr instance. On the downside, it requires two running Solr servers temporarily, each of which will consume resources and need adequate disk space. Depending on the size of your data that may be a lot of disk space.",
        "section": "Configure services",
        "subsections": " Supported versions  Deprecated versions   Relationship Usage example Solr 4 Solr 6 and later  Configsets Default configuration Limitations   Accessing the Solr server administrative interface Upgrading  Destructive Transitional    ",
        "image": "",
        "url": "/configuration/services/solr.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "dde2996e92ca2fe8488627346a50a929",
        "title": "Using SSH keys",
        "description": "",
        "text": " One of the ways Platform.sh keeps things secure is by using SSH behind the scenes. Users can interact with their environment through a command shell, or push changes to the environment’s Git repository, and both of these features rely on SSH. You can manage SSH keys through the CLI (see below), or through the SSH keys tab under Account Settings. Find your Public-Private keypair If you use Linux, you probably already have keys. The private key is usually in a file named ~/.ssh/id_rsa and the public key in ~/.ssh/id_rsa.pub, Searching for a public key file: Open up a command prompt. Run the following commands: $ cd ~/.ssh $ ls -a id_rsa id_rsa.pub known_hosts authorized_keys If you find a file named id_rsa.pub, you can use it with Platform.sh. If you don’t find an existing key, see the steps to create a new one in the next section . Create a New Public-Private Keypair Note: If you already have a SSH keypair, you can skip this step. Create a public-private keypair: $ ssh-keygen -t rsa -C  your_email_address@example.com  ssh-keygen generates the key pair and will ask you where you want to save the file: Generating public/private rsa key pair. Enter file in which to save the key (/Users/your_username/.ssh/id_rsa): The default location is fine in most cases. Now it’s time to create a passphrase. A good, strong passphrase is highly recommended, to make your key less useful if it falls into the wrong hands. Enter passphrase (empty for no passphrase): [Type a passphrase] Enter same passphrase again: [Type passphrase again] That’s it. Keys generated! Here are the results: Your identification has been saved in /Users/your_username/.ssh/id_rsa. Your public key has been saved in /Users/your_username/.ssh/id_rsa.pub. The key fingerprint is: 55:c5:d7:a9:1f:dc:7a:67:31:70:fd:87:5a:a6:d0:69 your_email_address@example.com Note: Make note of the location of your public key, you’re going to need that in the next section. Add the SSH key to your Platform account First off, you’ll need to copy your public key to the clipboard. Head over to your user account page on the Platform.sh Accounts page and navigate to the Account Settings tab. In the left side-bar, select SSH keys. Click the Add a public key button. Paste the key that you copied earlier into the ‘Key’ text box. You can also add a title if you like, otherwise it will be auto-generated. Click ‘Save’. That’s it! You’re all set. Now you’ll be able to use Git and command shells with any Platform.sh environment that your user account is authorized to work with. Forwarding keys by default It may be helpful to set your SSH client to always forward keys to Platform.sh servers, which can simplify other SSH or Rsync commands. To do so, include a block in your local ~/.ssh/config file like so: Host *.us.platform.sh ForwardAgent yes Host *.eu.platform.sh ForwardAgent yes Include one Host entry for each Platform.sh region you want to connect to, such as us-2 or eu-4. (You can include other configuration as desired.) SSH to your Web Server In the management console header, click on the environment tab and select the environment that you want to SSH into. Then click the SSH dropdown button towards the top right. $ ssh wk5fqz6qoo123-master@ssh.eu.platform.sh ___ _ _ __ | _ |__ _| |_ / _|___ _ _ _ __ | _/ / _` | _| _/ _ \u0026#39;_| \u0026#39; |_| |_|_|_| Welcome to Platform. This is environment master of project wk5fqz6qoo123. web@wk5fqz6qoo123-master--php:~$ Troubleshoot SSH While trying to log in via SSH, this can happen: $ ssh [SSH-URL] Permission denied (publickey). Don’t panic! It’s an issue which can happen for the following reasons: Your environment is inactive You haven’t redeployed (i.e. git push) your environment since adding the new public key You didn’t upload your public key to your user profile Your SSH private key has not been added into your ssh-agent Your SSH key files have incorrect permissions Check your public key Make sure your public key has been uploaded to your user account. Check your ssh-agent Check that your key is properly added to your SSH agent. This is an authentication agent that manages your private key. Check your SSH agent. Run the command ssh-add -l in your terminal: $ ssh-add -l 2048 12:b0:13:83:7f:56:18:9b:78:ca:54:90:a7:ff:12:69 /Users/nick/.ssh/id_rsa (RSA) Check that file name on the right (.ssh/id_rsa in the example above). Does it match your private key file? If you don’t see your private key file, add your private key: $ ssh-add path-to-your-key Try again. Specify your identity file If your identity (SSH key) associated with Platform.sh is not in a default file name (as may be explained in your SSH software manual, for example) you may have to append a specification like the one below so that the SSH software finds the correct key. Host platform.sh IdentityFile ~/.ssh/id_platformsh Be aware that, above, platform.sh stands for a hostname. Each different hostname you connect to Platform.sh at may have to be specified in the host line, separated by spaces. Still having trouble? If you followed all the steps above, you may also notice an error message similar to below while attempting to SSH to platform.sh: Hello Your Name, you successfully connected, but you do not have access to service \u0026#39;xxxxxxxxxxxxxx-master\u0026#39;: check permissions. Received disconnect from 54.210.49.244: 14: No more auth methods available This usually means a deployment has not been committed yet. When a new key is added, it only becomes immediately active for use with Git. For use with SSH, it will not be activated until a deployment is made. An easy way to force this is to create and push an empty commit: $ git commit --allow-empty -m \u0026#39;force redeploy\u0026#39; $ git push origin master If all else fails, generate some SSH debug information If your private key and public key both look OK but you don’t have any luck logging in, print debugging information. These lines often give clues about what is going wrong. Run the SSH command with the -v option, like this: $ ssh -v [SSH-URL] OpenSSH_6.7.8, OpenSSL 1.2.3 1 Sep 2014 debug1: Connecting to ssh.eu.platform.sh [54.32.10.98] port 22. debug1: Connection established. debug1: identity file /Users/nick/.ssh/id_rsa type 1 ...(30 more lines of this light reading)... debug1: Offering RSA public key: /Users/nick/.ssh/id_rsa debug1: Authentications that can continue: publickey debug1: No more authentication methods to try. Permission denied (publickey). or $ GIT_SSH_COMMAND= git -v  git clone [REPO-URL] You can use this information to make one last check of the private key file. If you’re still stuck, don’t hesitate to submit a support ticket, we’ll help you solve your problem.",
        "section": "Development",
        "subsections": " Find your Public-Private keypair Create a New Public-Private Keypair Add the SSH key to your Platform account  Forwarding keys by default   SSH to your Web Server Troubleshoot SSH  Check your public key Check your ssh-agent Specify your identity file Still having trouble? If all else fails, generate some SSH debug information    ",
        "image": "",
        "url": "/development/ssh.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "c6b2b8e5ca4b9e80f09785cf4b8df380",
        "title": "[Beta] Source operations",
        "description": "",
        "text": " An application can define a number of operations that apply to its source code and that can be automated. Note: Source Operations are currently in Beta. While the syntax is not expected to change, some behavior might in the future. A basic, common source operation could be to automatically update Composer dependencies like this: source:operations:update:command:| set -ecomposerupdategitaddcomposer.lockgitcommit-m Update Composer dependencies. The update key is the name of the operation. It is arbitrary, and multiple source operations can be defined. (You may wish to include more robust error handling than this example.) The environment resource gets a new source-operation action which can be triggered by the CLI: platform source-operation:run update The source-operation:run command takes the command name to run. Additional variables can be added to inject into the environment of the source operation. They will be interpreted the same way as any other variable set through the UI or CLI, which means you need an env: prefix to expose them as a Unix environment variable. They can then be referenced by the source operation like any other variable. platform source-operation:run update --variable env:FOO=bar --variable env:BAZ=beep When this operation is triggered: A clean Git checkout of the current environment HEAD commit is created; this checkout doesn’t have any remotes, has all the tags defined in the project, but only has the current environment branch. Sequentially, for each application that has defined this operation, the operation command is launched in the container image of the application. The environment will have all of the variables available during the build phase, optionally overridden by the variables specified in the operation payload. At the end of the process, if any commits were created, the new commits are pushed to the repository and the normal build process of the environment is triggered. Note that these operations run in an isolated container: it is not part of the runtime cluster of the environment, and doesn’t require the environment to be running. Also be aware that if multiple applications in a single project both result in a new commit, that will appear as two distinct commits in the Git history but only a single new build/deploy cycle will occur. Source Operations with an external Git integration Git integration can be configured to send commits made to the Platform.sh Git remote, to the upstream repository instead. This means that if a source operation did generate a new commit, the commit will be pushed to the upstream repository. Note: Currently, this configuration requires the enable_codesource_integration_push setting to be turned on by a Platform.sh staff and is only available to selected Beta customers. Source Operations can only be triggered on environment created by a branch, and not to environment created by a Pull Request on the external upstream (GitHub, Bitbucket, Gitlab). Automated Source Operations using cron You can use cron to automatically run your source operations. Note: Automated source operations using cron requires to get an API token and install the CLI in your application container. Once the CLI is installed in your application container and an API token has been configured, you can add a cron task to run your source operations once a day. We do not recommend triggering source operations on your master production environment, but rather on a dedicated environment which you can use for testing before deployment. The example below synchronizes the update-dependencies environment with its parent before running the update source operation: crons:update:# Run the \u0026#39;update\u0026#39; source operation every day at midnight.spec:\u0026#39;0 0 * * *\u0026#39;cmd:| set -eif[ $PLATFORM_BRANCH =update-dependencies];thenplatformenvironment:synccodedata--no-wait--yesplatformsource-operation:runupdate--no-wait--yesfi",
        "section": "Configure your application",
        "subsections": " Source Operations with an external Git integration Automated Source Operations using cron  ",
        "image": "",
        "url": "/configuration/app/source-operations.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "1e15d230f09f7bd37dc0348cd5a202ef",
        "title": "Multiple applications",
        "description": "",
        "text": " Platform.sh supports building multiple applications per project (for example RESTful web services with a front-end, or a main website and a blog). For resource allocation reasons, however, that is not supported on Standard plan. Note: This page only applies to Grid projects. Contact your sales representative for advanced Dedicated environment configurations. Project structure There are multiple ways to structure such a project, depending on the way your source code is organized and what your goal is. All of these approaches may be used within a single project simultaneously, although it is often easier to maintain if you settle on just one approach for a given project. Discrete code bases If your project consists of a discrete code base for each application, the most straightforward approach is to put both code bases into a single project repository in separate directories. Each will have its own .platform.app.yaml file, which will define how that particular application gets built, using the code in that directory. For example, if you have a Drupal back end with an AngularJS front end you could organize the repository like this: .git/ .platform/ drupal/ .platform.app.yaml ... angular/ .platform.app.yaml Each .platform.app.yaml file will define a single application container, and build code in that directory. The .platform directory is outside of all of them and still defines additional services you require, as well as routes. Note that disk paths in the .platform.app.yaml file are relative to the directory where that file lives by default. This is the recommended approach for most configurations. Explicit source.root As an alternative, you may specify a source.root key in a .platform.app.yaml file to override the “application root is where the file is” logic. The .platform.app.yaml file may then live anywhere in the repository but use code from another directory. Two separate .platform.app.yaml files may refer to the same directory if desired. For example: # .platform.app.yamlsource:root:restapp.platform/ main/ .platform.app.yaml .platform.app.yaml restapp/ # Your code here In this case, the .platform.app.yaml file in main does not specify a source.root, and so will be built from the code in main. The top-level .platform.app.yaml includes the YAML fragment above. It will get built using the code in restapp, as if it were in that directory. Note that disk parameters in the .platform.app.yaml file will be relative to the source.root directory if specified. The source.root path is relative to the repository root. The primary use case for this configuration is if the source code is pulled in as a Git submodule or downloaded during the build phase. applications.yaml It is possible to define an application in a .platform/applications.yaml file in addition to discrete .platform.app.yaml files. The syntax is nearly identical, but the source.root key is required. The applications.yaml file is then a YAML array of application definitions. For example, the following .platform/applications.yaml file defines three applications: # .platform/applications.yaml- name:apitype:golang:1.14source:root:apiapphooks:build:| go build -o bin/appweb:upstream:socket_family:tcpprotocol:httpcommands:start:./bin/applocations:/:allow:falsepassthru:true- name:maintype: php:7.4 source:root:mainappweb:locations: / :root: web passthru: /index.php - name:admintype: php:7.4 size:Ssource:root:mainappweb:locations: / :root: web passthru: /admin.php In this example, the apiapp directory will get built as a Go application while the mainapp directory will get built as two separate PHP applications, even though none of those directories has a .platform.app.yaml file. The two PHP applications will use the same source code, but have different front controllers for the admin and main applications. The admin instance will also be fixed at an S size container, while main will scale freely. The primary use case for this configuration is defining multiple applications with different configuration off of the same source code, or when the source code is downloaded during the build phase. Submodules Platform.sh supports Git submodules, so each application can be in a separate repository. However, there is currently a notable limitation: the .platform.app.yaml files must be in the top-level repository. That means the project must be structured like this: .git/ .platform/ routes.yaml services.yaml app1/ .platform.app.yaml app1-submodule/ index.php app2/ .platform.app.yaml app2-submodule/ index.php This puts your applications’ files at a different path relative to your .platform.app.yaml files. The recommended way to handle that is to specify a source.root key in the .platform.app.yaml file and have it reference the submodule directory. Multi-app Routes Every application, however it is defined, must have a unique name property. The routes.yaml file may then refer to that application by name as an upstream for whatever route is appropriate. For example, assuming this configuration from above: .git/ .platform/ drupal/ .platform.app.yaml ... angular/ .platform.app.yaml The .platform/routes.yaml file can be structured like this:  https://backend.{default}/ :type:upstreamupstream: drupal:http  https://{default}/ :type:upstreamupstream: angular:http (This assumes your Drupal application is named drupal and your Angular front-end is named angular.) Assuming a domain name of example.com, that will result in: https://backend.example.com/ being served by the Drupal instance. https://example.com/ being served by the AngularJS instance. There is no requirement that an application be web-accessible. If it is not specified in routes.yaml then it will not be web-accessible at all. However, if you are building a non-routable application off of the same code base as another application, you should probably consider defining it as a worker instead. The net result is the same but it is much easier to manage. Relationships In a multi-app configuration, applications by default cannot access each other. However, they may declare a relationships block entry that references another application rather than a service. In that case the endpoint is http. However, be aware that circular relationships are not supported. That is, application A cannot have a relationship to application B if application B also has a relationship to application A. Such circular relationships are usually a sign that the applications should be coordinating through a shared data store, like a database, RabbitMQ server , or similar.",
        "section": "Configure your application",
        "subsections": " Project structure  Discrete code bases Explicit source.root applications.yaml   Submodules Multi-app Routes Relationships  ",
        "image": "",
        "url": "/configuration/app/multi-app.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "c42490bfed602e1e255a9e23a03a5ba5",
        "title": "Transferring data to and from a Dedicated cluster",
        "description": "",
        "text": " Backing up staging and production files Platform.sh automatically creates a backup of the staging and production instances on a Dedicated cluster every six hours. However, those are only useful for a full restore of the environment and can only be done by the Platform.sh team. At times you’ll want to make a manual backup yourself. To create a manual ad-hoc backup of all files on the staging or production environment, use the standard rsync command. rsync -avzP \u0026lt;USERNAME\u0026gt;@\u0026lt;CLUSTER_NAME\u0026gt;.ent.platform.sh:pub/static/ pub/static/ That will copy all files from the pub/static directory on the production instance to the pub/static directory, relative to your local directory where you’re running that command. Backing up the staging and production database To backup your database to your local system you’ll need to get the database credentials to use. First, login to the cluster and run the following command: echo $PLATFORM_RELATIONSHIPS | base64 --decode | json_pp Which should give a JSON output containing something like this:  database  : [ {  path  :  main ,  service  :  mysqldb ,  rel  :  mysql ,  host  :  database.internal ,  ip  :  246.0.80.64 ,  scheme  :  mysql ,  cluster  :  jyu7wavyy6n6q-master-7rqtwti ,  username  :  user ,  password  :   ,  query  : {  is_master  : true },  port  : 3306 } ] The part you want is the user, password, and “path”, which means the DB name. Ignore the rest. Now, run the following command on your local computer: ssh \u0026lt;USERNAME\u0026gt;@\u0026lt;CLUSTER_NAME\u0026gt;.ent.platform.sh \u0026#39;mysqldump --single-transaction -u \u0026lt;user\u0026gt; -p\u0026lt;pass\u0026gt; -h localhost \u0026lt;dbname\u0026gt; | gzip\u0026#39; \u0026gt; database.gz That will run a mysqldump command on the server, compress it using gzip, and stream the output to a file named database.gz on your local computer. (If you’d prefer, bzip2 and xz are also available.) Synchronizing files from dev to staging/production To transfer data into either the staging or production environments, you can either download it from your Platform.sh Development environment to your local system first or transfer it directly between environments using SSH based tools (e.g. SCP, Rsync). First, set up SSH forwarding by default for Platform.sh domains. Then run platform ssh with the master branch checked out to connect to the master dev environment. Files are the easier data to transfer, and can be done with rsync. rsync -avzP pub/static/ \u0026lt;USERNAME\u0026gt;@\u0026lt;CLUSTER_NAME\u0026gt;.ent.platform.sh:pub/static/ Replace pub/static with the path to your files on system, such as web/sites/default/files/. Note that rsync is very picky about trailing / characters. Consult the rsync documentation for more that can be done with that command. Synchronizing the database from development to staging/production The database can be copied directly from the development environment to staging or production, but doing so requires noting the appropriate credentials first on both systems. First, login to the production environment over SSH: ssh \u0026lt;USERNAME\u0026gt;@\u0026lt;CLUSTER_NAME\u0026gt;.ent.platform.sh Once there, you can look up database credentials by running: echo $PLATFORM_RELATIONSHIPS | base64 --decode | json_pp Which should give a JSON output containing something like this: {  database  : [ {  password  :  abc123 ,  username  :  projectname ,  path  :  projectname ,  port  :  3306 ,  scheme  :  mysql ,  host  :  127.0.0.1 ,  query  : {  is_master  : true,  compression  : true } } ] } The part we want is the host, user, password, and the “path”, which is the database name. Ignore the rest. Now, in a separate terminal login to the development instance using platform ssh. Run the same echo command as above to get the credentials for the database on the development instance. (The JSON will be slightly different but again we’re only interested in the user, password, host, and “path”/database name). With the credentials from both databases we can construct a command that will export data from the dev server and write it directly to the Dedicated cluster’s server. mysqldump -u \u0026lt;dev_user\u0026gt; -p\u0026lt;dev_password\u0026gt; -h \u0026lt;dev_host\u0026gt; \u0026lt;dev_dbname\u0026gt; --single-transaction | ssh -C \u0026lt;USERNAME\u0026gt;@\u0026lt;CLUSTER_NAME\u0026gt;.ent.platform.sh \u0026#39;mysql -u \u0026lt;prod_user\u0026gt; -p\u0026lt;prod_password\u0026gt; -h \u0026lt;prod_host\u0026gt; \u0026lt;prod_dbname\u0026gt;\u0026#39; That will dump all data from the database as a stream of queries that will get run on the production database without ever having to create an intermediary file. The -C on the SSH command tells SSH to compress the connection to save time. (Be aware, this is a destructive operation that overwrites data. Backup first.)",
        "section": "Development",
        "subsections": " Backing up staging and production files Backing up the staging and production database Synchronizing files from dev to staging/production Synchronizing the database from development to staging/production  ",
        "image": "",
        "url": "/development/transfer-dedicated.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "3cbb1b5f9439dafe374ca7374a5992aa",
        "title": "Varnish",
        "description": "",
        "text": " Varnish is a popular HTTP proxy server, often used for caching. It is usually not needed on Platform.sh, as each project’s router provides an HTTP cache already and most more advanced use cases will use a CDN instead, both of which render Varnish redundant. However, it is possible to configure a Varnish instance as part of an application if Varnish-specific functionality is needed. Supported versions Grid Dedicated 5.6 6.0 None available How it works All incoming requests still go through the environment’s router first. When using Varnish, a Varnish service sits between the router and the application server or servers. web -\u0026gt; router -\u0026gt; varnish -\u0026gt; application -\u0026gt; application2 Configuration Add a Varnish service Add the following to your .platform/services.yaml file: proxy:type:varnish:6.0relationships:application:\u0026#39;app:http\u0026#39;configuration:vcl:!includetype:stringpath:config.vcl In the relationships block, define a relationship (application) to the application container (app) using the http endpoint. That allows Varnish to talk to the application container. The configuration block is required, and must reference a VCL file (here config.vcl). The file name is relative to the .platform directory. Create a VCL template file The VCL file you provide has three specific requirements over and above the VCL syntax itself. You MUST NOT define a vcl_init() function. Platform.sh will auto-generate that function based on the relationships you define. In particular, it will define a “backend” for each relationship defined in services.yaml, named the same as the relationship. You MUST NOT include the preamble at the beginning of the file, specifying the VCL version. That will be auto-generated as well. You CAN add imports, but not std and directors. You MUST specify the backend to use in vcl_recv(). If you have a single app container/relationship/backend, it’s just a single line. If you want to split requests to different relationships/backends based on some rule then the logic for doing so should be incorporated into the vcl_recv() function. The absolute bare minimum VCL file is: sub vcl_recv { set req.backend_hint = application.backend(); } Where application is the name of the relationship defined in services.yaml. (If the relationship was named differently, use that name instead.) If you have multiple applications fronted by the same Varnish instance then you will need to include logic to determine to which application a request is forwarded. For example: varnish:type:varnish:6.0relationships:blog:\u0026#39;blog:http\u0026#39;main:\u0026#39;app:http\u0026#39;configuration:vcl:!includetype:stringpath:config.vcl# config.vcl sub vcl_recv { if (req.url ~  ^/blog/ ) { set req.backend_hint = blog.backend(); } else { set req.backend_hint = main.backend(); } } This configuration will direct all requests to a URL beginning with a /blog/ path to the application on the relationship blog, and all other requests to the application on the relationship main. Besides that, the VCL file, including the vcl_recv() function, can be arbitrarily complex to suit the needs of the project. That includes additional include directives if appropriate. See the Varnish documentation for more details on the functionality offered by Varnish. Note: A misconfigured VCL file can result in incorrect, often mysterious and confusing behavior. Platform.sh does not provide support for VCL configuration options beyond the basic connection logic documented here. Route incoming requests to Varnish To enable Varnish now, edit the .platform/routes.yaml file to point to the Varnish service you just created. You also need to disable the router cache as it is now entirely redundant with Varnish. For example:  https://{default}/ :type:upstreamupstream: varnish:http cache:enabled:false That will map all incoming requests to the Varnish service rather than the application. Varnish will then, based on the VCL file, forward requests to the application as appropriate. Modules Platform.sh supports a number of optional modules you can include in your VCLs, namely: cookie header saintmode softpurge tcp var vsthrottle xkey To use in your VCL, add an import such as: import xkey; Circular relationships At this time Platform.sh does not support circular relationships between services or applications. That means you cannot add a relationship in your .platform.app.yaml that points to the Varnish service. If you do so then one of the relationships will be skipped and the connection will not work. This limitation may be lifted in the future. Stats endpoint The Varnish service also offers an http\u0026#43;stats endpoint, which provides access to some Varnish analysis and debugging tools. To access it, from a dedicated app container add the following to .platform.app.yaml: relationships:varnishstats: proxy:http\u0026#43;stats  You can then access the varnishstats relationship over HTTP at the following paths to get diagnostic information: /: returns the error if generating the VCL failed with an error /config: returns the generated VCL /stats: returns the output of varnishstat /logs: returns a streaming response of varnishlog Note that because of the circular relationship issue noted above this cannot be done on the application that Varnish is forwarding to. It will need to be run on a separate application container.",
        "section": "Configure services",
        "subsections": " Supported versions How it works Configuration  Add a Varnish service Create a VCL template file Route incoming requests to Varnish   Modules Circular relationships Stats endpoint  ",
        "image": "",
        "url": "/configuration/services/varnish.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "2e3b4ae352691be6251ee1a7231ca714",
        "title": "Public IP addresses",
        "description": "",
        "text": " Platform.sh regions reach the outside through a limited number of IP addresses. Use the inbound IP addresses if you have a corporate firewall which blocks outgoing SSH connections. In that case, simply add our IP addresses for inbound traffic below to your allow list. Note: These IP addresses are stable, but not guaranteed to never change. Prior to any future change, all affected customers will receive ample warning. Europe West (eu.platform.sh) Outbound IPs: 54.72.94.105 54.76.137.67 54.76.137.94 Inbound IPs (gw.eu.platform.sh): 54.76.137.79 54.76.137.151 54.76.136.188 West 2 (eu-2.platform.sh) Outbound IPs: 52.208.123.9 52.214.63.84 52.30.200.164 Inbound IPs (gw.eu-2.platformsh.site): 34.248.104.12 34.241.191.143 52.210.208.94 West 4 (eu-4.platform.sh) Outbound IPs: 18.200.158.188 18.200.157.200 18.200.184.206 Inbound IPs (gw.eu-4.platformsh.site): 52.215.88.119 52.208.179.40 18.200.179.139 Germany 2 (de-2.platform.sh) (Data Location Guarantee) Outbound IPs: 35.246.248.138 35.246.184.45 35.242.229.239 Inbound IP (gw.de-2.platformsh.site): 35.246.248.138 35.246.184.45 35.242.229.239 France 1 (fr-1.platform.sh) Outbound IPs: 90.84.47.148 90.84.46.222 90.84.46.40 Inbound IPs (gw.fr-1.platformsh.site): 90.84.47.148 90.84.46.222 90.84.46.40 France 2 (ovh-fr-2.platform.sh) Outbound IPs: 51.178.62.146 51.178.61.63 51.178.56.77 Inbound IPs (gw.ovh-fr-2.platformsh.site): 51.178.62.146 51.178.61.63 51.178.56.77 United Kingdom 1 (uk-1.platform.sh) Outbound IPs: 35.242.142.110 35.189.126.202 35.242.183.249 Inbound IPs (gw.uk-1.platformsh.site): 35.242.142.110 35.189.126.202 35.242.183.249 United States East (us.platform.sh) Outbound IPs: 54.88.149.31 54.209.114.37 54.210.53.51 Inbound IPs (gw.us.platform.sh): 54.210.49.244 54.210.55.162 54.88.225.116 East 2 (us-2.platform.sh) Outbound IPs: 34.238.64.193 52.4.246.137 54.157.66.30 Inbound IPs (gw.us-2.platformsh.site): 34.226.46.235 34.238.11.122 54.89.106.200 Canada Outbound IPs: 35.182.24.224 52.60.213.255 35.182.220.113 Inbound IPs: 35.182.174.169 35.182.59.77 52.60.219.22 Australia (au.platform.sh) Outbound IPs: 13.55.135.0 13.54.121.225 13.55.215.151 Inbound IPs (gw.au.platformsh.site): 13.54.88.239 13.55.140.143 13.54.222.56",
        "section": "Development",
        "subsections": " Europe  West (eu.platform.sh) West 2 (eu-2.platform.sh) West 4 (eu-4.platform.sh) Germany 2 (de-2.platform.sh) (Data Location Guarantee) France 1 (fr-1.platform.sh) France 2 (ovh-fr-2.platform.sh) United Kingdom 1 (uk-1.platform.sh)   United States  East (us.platform.sh) East 2 (us-2.platform.sh)   Canada Australia (au.platform.sh)  ",
        "image": "",
        "url": "/development/public-ips.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "cdc8658cf343ae74328b018adda933dc",
        "title": "Upgrading",
        "description": "",
        "text": " Changes in version 2019.05 The !archive tag in YAML has been un-deprecated, and is now favored over the !include option. !include is still available for other include types (yaml, binary, and string). Changes in version 2017.11 (2017-11-09) The !archive tag in YAML files is now deprecated in favor of the more generic !include . For example, the following services.yaml snippet: mysearch:type:solr:6.3disk:1024configuration:core_config:!archive myconfdir Can now be written as: mysearch:type:solr:6.3disk:1024configuration:core_config:!includetype:archivepath: myconfdir  The syntax for the mounts key in .platform.app.yaml has changed. Rather than a parsed string, the value of each mount is a multi-key definition . That is, the following example: mounts: tmp :  shared:files/tmp  logs :  shared:files/logs Can now be written as: mounts:tmp:source:localsource_path:tmplogs:source:localsource_path:logs Changes in version 2016.6 (2016-11-18) Application containers now include the latest LTS version of Node.js, 6.9.1. The previously included version was 4.6.1. Composer was briefly called with --no-dev, but as of 2016-11-21 this change has been reverted, because of the unintended effect it had on projects using the Symfony framework. Changes in version 2016.5 As of October 2016, the default behaviour of the expires key, which controls client-side caching of static files, has changed. Previously, if the key was unset, the Expires and Cache-Control HTTP headers were left unset in the response, which meant that client side caching behaviour was left undefined. To ensure consistent behaviour that doesn’t depend on which browser the client is using, the new default behaviour is to set these headers to values that disable client-side caching. This change only affects static files served directly by the web server. Responses served from passthru URLs continue to use whatever caching headers were set by the application.. To enable caching on your static files, make sure you include an expires key in your web configuration , as shown below: web:locations: / :root: public passthru: /index.php index:- in version 2016.4 As of July 2016, we no longer create default configuration files if one is not provided. The defaults we used to provide were tailored specifically for Drupal 7, which is now a legacy-support version with the release of Drupal 8 and not especially useful for non-Drupal or non-PHP sites. They also defaulted to software versions that are no longer current and recommended. Instead, you must provide your own .platform.app.yaml, .platform/routes.yaml, and .platform/services.yaml files. Additionally, a version for a language or service should always be specified as well. That allows you to control when you upgrade from one version to another without relying on a network default. The previous default files, for reference, are: .platform.app.yaml name:phptype: php:5.4 build:flavor: drupal access:ssh:contributorrelationships:database: mysql:mysql solr: solr:solr redis: redis:redis web:document_root: / passthru: /index.php disk:2048mounts: public/sites/default/files :  shared:files/files  tmp :  shared:files/tmp  private :  shared:files/private crons:drupal:spec: */20 * * * * cmd: cd public ; drush core-cron .platform/routes.yaml  http://{default}/ :type:upstreamupstream: php:http cache:enabled:truessi:enabled:false http://www.{default}/ :type:redirectto: http://{default}/ .platform/services.yaml mysql:type:mysql:5.5disk:2048redis:type:redis:2.8solr:type:solr:3.6disk:1024Changes in version 2016.3 As we are aiming to always provide you more control and flexibility on how to deploy your applications, the .platform.app.yaml format has been greatly improved. It is now way more flexible, and also much more explicit to describe what you want to do. The web key is now a set of locations where you can define very precisely the behavior of each URL prefix. Note, we no longer move your application from “/” to “public/” automatically if the new format is adopted. If you are using Drupal, move all of your Drupal files into “public/” in the Git repository. Old format: web:document_root: / passthru: /index.php index_files:-  index.php expires:300whitelist:- format: web:locations: / :root: public passthru: /index.php index:- compatibility Of course, we alway keep backward compatibility with the previous configuration format. Here is what happens if you don’t upgrade your configuration: # The following parameters are automatically moved as a  /  block in the#  locations  object, and are invalid if there is a valid  locations  block.document_root: /public # Converted to [locations][/][root]passthru: /index.php # Converted to [locations][/][passthru]index_files:- index.php# Converted to [locations][/][index]whitelist:[]# Converted to [locations][/][rules]blacklist:[]# Converted to [locations][/][rules]expires:3d# Converted to [locations][/][expires]Changes in version 2015.7 The .platform.app.yaml configuration file now allows for a much clearer syntax, which you can (and should) start using now. The old format had a single string to identify the ‘toolstack’ you use: toolstack: php:drupal The new syntax allows to separate the concerns of what language you are running and the build process that is going to happen on deployment: type:phpbuild:flavor:drupalCurrently we only support php in the ‘type’ key. Current supported build flavors are drupal, composer and symfony. Changes in version 2014.9 This version introduces changes in the configuration files format. Most of the old configuration format is still supported, but customers are invited to move to the new format. For an example upgrade path, see the Drupal 7.x branch of the Platformsh-examples repository on GitHub. Configuration items for PHP that previously was part of .platform/services.yaml are now moved into .platform.app.yaml, which gains the following top-level items: name: should be  php  relationships, access and disk: should be the same as the relationships key of PHP in .platform/services.yaml Note that there is now a sane default for access (SSH access to PHP is granted to all users that have role “collaborator” and above on the environment) so most customers can now just omit this key in .platform.app.yaml. In addition, version 1.7.0 now has consistency checks for configuration files and will reject git push operations that contain configuration files that are invalid. In this case, just fix the issues as they are reported, commit and push again.",
        "section": "Configure your application",
        "subsections": " Changes in version 2019.05 Changes in version 2017.11 (2017-11-09) Changes in version 2016.6 (2016-11-18) Changes in version 2016.5 Changes in version 2016.4  .platform.app.yaml .platform/routes.yaml .platform/services.yaml   Changes in version 2016.3  Backward compatibility   Changes in version 2015.7 Changes in version 2014.9  ",
        "image": "",
        "url": "/configuration/app/upgrading.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "e719452c6e42346a004592fc5e2ce067",
        "title": "[Beta] Outbound firewall",
        "description": "",
        "text": " In some situations, compliance regulations may require you to limit outbound traffic from your application. The firewall property allows you to do so. This setting has no impact on inbound requests to your application. For that, use the environment access control settings in the Management Console. Note: The outbound firewall is currently in Beta. While the syntax is not expected to change, some behavior might in the future. Syntax The firewall property defines one or more allowed entries for outbound requests. Its basic syntax is as follows: firewall:outbound:- protocol:tcpips:[ 1.1.1.1/32 ]ports:[443]- protocol:tcpips:[ 1.2.3.4/32 ]ports:[443]The above example allows two outbound rules over TCP. All other outbound requests will be blocked and will time out eventually (usually after 30 seconds). If no rules are specified, the default firewall configuration is equivalent to: firewall:outbound:- protocol:tcpips:[ 0.0.0.0/0 ]That is, all outbound TCP traffic is allowed on all ports (aside from port 25, which is always blocked without exception). In the majority of cases the default is sufficient for most applications. Options Each firewall rule has three configuration values. At least one of ips or ports is required, but both may also be specified. protocol The default and only legal value for the protocol is tcp. Outbound UDP ports are not allowed. As a result this property can be omitted in virtually every circumstance. ips This property is an array of IP addresses in CIDR notation . CIDR allows you to specify a range of IP addresses in a compact format, using a bitmask. Most commonly the bitmask is 8, 16, or 32 but that is not required. For example, 1.2.3.4/8 will match any IP address whose first 8 bits match 1.2.3.4, which corresponds to the first segment. Therefore it will allow 1.*.*.*. In comparison, 1.2.3.4/24 will allow 1.2.3.*. A mask of 32 will match only the IP address specified, so to allow a single specific IP you must write 1.2.3.4/32. IP Address Guide has a useful CIDR format calculator. If no ports property is specified, requests to any port on the specified IP addresses are permitted. ports This property is an array of ports in the range 1 to 65535 that are allowed. For example, [80, 443] will only allow requests to the specified IPs on ports 80 and 443 (typically HTTP and HTTPS, respectively). Requests to any other port will be blocked. If not specified, requests to a given IP may be to any port. If no ips property is specified, requests to any IP address are permitted on the specified port(s). Multiple rules It is possible to define an arbitrary number of allowed firewall rules, as in the example above. If multiple rules are specified, a given outbound request will be allowed if it matches ANY of the defined rules. That means that, for this configuration: firewall:outbound:- ips:[ 1.2.3.4/32 ]ports:[443]- ports:[80]Requests to port 80 on any IP will be allowed, and requests to 1.2.3.4 on either port 80 or 443 will be allowed, even though the first rule only lists port 443. Usage considerations Be aware that many services your application may wish to connect to will be using a domain name that is not on a fixed IP address, or is load-balanced between multiple IP addresses. You will need to contact the administrator of that service in order to determine the correct IP addresses to allow. Also be aware that many services are behind a Content Delivery Network (CDN). For most CDNs, routing is done via domain name, not IP address, so thousands of domain names may share the same public IP addresses at the CDN. If you allow the IP address of a CDN, you will in most cases be allowing many or all of the other customers hosted behind that CDN. That has security implications and limits the usefulness of this configuration option.",
        "section": "Configure your application",
        "subsections": " Syntax Options  protocol ips ports   Multiple rules Usage considerations  ",
        "image": "",
        "url": "/configuration/app/firewall.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "0dc6292b6223331a466ea810b8f50743",
        "title": "Frequently Asked Questions (FAQ)",
        "description": "",
        "text": " What is the difference between a Platform, a Project and an Environment? Platform or Platform.sh is the infrastructure which is running all your projects. A project is the site that you’re working on. Each project corresponds to one Git repository. A project may contain multiple applications that will run in their own isolated containers. Each branch of a project may be deployed in its own environment. An environment is a standalone copy of your site, complete with code, data, and running services. The master branch is the production environment, while any other branch can be setup as an otherwise identical testing environment. How can I cancel my subscription? If you want to delete your project and cancel your subscription, simply go to your user profile and click on “Edit plan” on the project you want to delete. Then you can click on the link: “delete your Platform.sh plan”. This will delete your project and stop invoicing for this project. If you have multiple projects, your subscription will continue until you don’t have any projects left. Does branching an environment duplicate services? Yes! Branching an environment creates an exact copy (snapshot) of the parent environment, containing the files, the database, and code. Each environment runs independently of every other, so if you have four active environments then you have four copies of your application, four copies of your database, four copies of your files, etc. Do you have a local writable file-system? Yes! Platform.sh supports non-ephemeral storage. When you configure your application you can tell it what directories you want to be read/write. (These are called mounts .) These will be mounted on a distributed file system (which is transparent for you). When you backup your environment they will be backed up as well. When you create a new staging environment, these mounts will be cloned with the rest of your data. What happens if I push a local branch to my project? If you push a local branch that you created with Git, you create what is called an inactive environment, that is, an environment that is not deployed. This means there won’t be any services attached to this branch. You are able to convert an inactive environment into an active environment and vice versa back from the environment configuration page or using the CLI with platform environment:activate. How does Master (the live site) scale? The master environment gets a pool of resources based on your plan size, which is then split up between the applications and services you have defined. (For example, PHP 40%, MySQL 30%, Redis 10%, Solr 20%, etc). Increasing your plan size will increase the pool of CPU and RAM that gets split between each container. All containers on development plans are “small” containers. See the sizing configuration page for more details. What exactly am I SSHing into? The platform ssh command allows you to log into your application container (where your PHP app or Node app or Java app is running). It is a fully running Linux environment, but almost all of the disk will be read-only, with the exception of mounts you have defined. Can I edit a quick fix on a Platform environment without triggering a rebuild? No. Changes to the code can only be made through deploying new Git commits. That ensures that “hot patches” don’t get lost in the net update, that all changes are auditable, and that if a security break-in happens the attacker still cannot modify your application code. What do I see when I push code? When you git push new code, Platform.sh rebuilds and redeploys the application. What shows on the command line is the output of your build process (composer, pip, bundler, etc. plus your own build hook) followed by the deploy process. It ends with a description of what was just deployed and the URLs that are now active. To supress the output, run platform push -W. The -W means --no-wait, and will disconnect the connection once the commits are pushed so that you can continue to use your local terminal. The exact same output is also available in the Web Management Console. What Linux distribution is Platform.sh using? Platform.sh is built on Debian. If I choose the Development plan, can I use that plan for production? No. The Development plan provides all the tools to build your website. You can create as many development profiles as you wish for yourself and for your team. However, it does not allow for full production-level resources on the master branch and does not allow you to configure a custom domain name. Once your project is complete and ready for production, you can choose another plan to go live. These plans are available on the pricing page . I am getting weird errors when I push (something with paramiko..) Please validate the syntax of your YAML file. Don’t use tabs. If all fails, contact support. Which geographic zones does Platform.sh cover? Platform.sh works with multiple cloud infrastructure providers, including Amazon Web Services, Microsoft Azure, and Orange. We offer public regions in several parts of the world: USA (East Coast), Canada (East Coast), Europe (Dublin), Europe (Germany), and Australia (Sydney). Dedicated projects can deploy production to any public AWS or Azure region. Why did you choose the .sh extension for your domain? ‘sh’ is the short version of shell. According to Wikipedia™, in computing, a shell is a user interface for access to an operating system’s services. Generally, operating system shells use either a command-line interface (CLI) or graphical user interface (GUI). This is exactly what Platform.sh is about: Giving developers tools to build, test, deploy, and run great websites! “.sh” is also the TLD for Saint Helena that looks like a lovely island, and whose motto is: “Loyal and Unshakeable” which we also strive to be. IDE Specific Tips MAMP pro: In order for MAMP to work well with the symlinks created by the Platform.sh CLI , add the following to the section under Hosts \u0026gt; Advanced called “Customized virtual host general settings.” For more details visit MAMP Pro documentation page . \u0026lt;Directory\u0026gt; Options FollowSymLinks AllowOverride All \u0026lt;/Directory\u0026gt; Note: When you specify your document root, MAMP will follow the symlink and substitute the actual build folder path. This means that when you rebuild your project locally, you will need to repoint the docroot to the symlink again so that it will refresh the build path. Do you support two-factor authentication? Yes, and encourage its use. To do so please go to your Account Settings on our Account site . Then click on the left tab called Security which will propose you to enable TFA Application.",
        "section": "Development",
        "subsections": " What is the difference between a Platform, a Project and an Environment? How can I cancel my subscription? Does branching an environment duplicate services? Do you have a local writable file-system? What happens if I push a local branch to my project? How does Master (the live site) scale? What exactly am I SSHing into? Can I edit a quick fix on a Platform environment without triggering a rebuild? What do I see when I push code? What Linux distribution is Platform.sh using? If I choose the Development plan, can I use that plan for production? I am getting weird errors when I push (something with paramiko..) Which geographic zones does Platform.sh cover? Why did you choose the .sh extension for your domain? IDE Specific Tips Do you support two-factor authentication?  ",
        "image": "",
        "url": "/development/faq.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "a8a30a7691b7a83ed28fdbaaa271fdfc",
        "title": "Troubleshooting",
        "description": "",
        "text": " Force a redeploy There are times where you might want to trigger a redeployment of your application. That can be done with the following command: platform redeploy Do not trigger a redeploy if there are builds in a “Pending” state, as these will block deployment. Wait for those builds to complete. Clear the build cache In rare circumstances the build cache, used to speed up the build process, may become corrupted. That may happen if, for example, code is being downloaded from a 3rd party language service like Packagist or NPM while that service is experiencing issues. To flush the build cache entirely run the following command: platform project:clear-build-cache That will wipe the build cache for the current project entirely. Naturally the next build for each environment will likely be longer as the cache rebuilds. HTTP responses 502 Bad Gateway or 503 Service Unavailable These errors indicate your application (or application runner, like PHP-FPM) is crashing or unavailable. Typical causes include: Your .platform.app.yaml configuration has an error and the process is not starting or requests are not able to be forwarded to it correctly. Check your web.commands.start entry or that your passthru configuration is correct. The amount of traffic coming to your site exceeds the processing power of your application. Certain code path(s) in your application are too slow and timing out. A PHP process is crashing because of a segmentation fault (see below). A PHP process is killed by the kernel out-of-memory killer (see below). Error provisioning the new certificate One reason Let’s Encrypt certificates may fail to provision on your environments has to do with the 64 character limit Let’s Encrypt places on URLs. If the names of your branches are too long, the Platform.sh generated environment URL will go over this limit, and the certificate will be rejected. See Let’s Encrypt limits and branch names for a more detailed breakdown of this issue. Total disk usage exceeds project maximum One of the billable parameters in your project’s settings is Storage. This global storage pool is allocated among the various services and application containers in your project via the disk parameter. The sum of all disk parameters in your project’s YAML config files must be less than or equal to the global project storage number. Error: Resources exceeding plan limit; disk: 8192.00MB \u0026gt; 5120.00MB; try removing a service, or add more storage to your plan This means that you have allocated, for example, disk: 4096 in a MySQL service in services.yaml and also disk: 4096 in the .platform.app.yaml for your application, while only having the minimum default of 5GB storage for your project as a whole. The solution is either to lower the disk parameters to within the limits of 5GB of storage, or raise the global storage parameter on your project’s settings to at least 10GB. Because storage is a billable component of your project, only the project’s owner can make this change. Low disk space When you receive a low-disk space notification for your application container: Check your application’s disk space Run platform ssh within your project folder to login to the container’s shell. Then use the df command to check the available writable space for your application. df -h -x tmpfs -x squashfs | grep -v /run/shared This command will show the writable mounts on the system, similar to: Filesystem Size Used Avail Use% Mounted on /dev/mapper/platform-syd7waxqy4n5q--master--7rqtwti----app 2.0G 37M 1.9G 2% /mnt /dev/mapper/platform-tmp--syd7waxqy4n5q--master--7rqtwti----app 3.9G 42M 3.8G 2% /tmp The first line shows the storage device that is shared by all of your persistent disk mounts . All defined mounts use a common storage pool. In this example, the application container has allocated 2 GB of the total disk space. Of those 2GB, 2% (37 MB) is used by all defined mounts. The second line is the operating system temporary directory, which is always the same size. While you can write to the /tmp directory files there are not guaranteed to persist and may be deleted on deploy. Increase the disk space available The sum of all disk keys defined in your project’s .platform.app.yaml and .platform/services.yaml files must be equal or less than the available storage in your plan. Buy extra storage for your project Each project comes with 5GB of Disk Storage available to each environment. To increase the disk space available for your project, click on “Edit Plan” to increase your storage in bulks of 5GB. See Extra Storage for more information. Increase your application and services disk space Once you have enough storage available, you can increase the disk space allocated for your application and services using disk keys in your .platform.app.yaml and .platform/services.yaml. Check the following resources for more details: Application’s disk space Services’ disk space Check your database disk space For a MariaDB database, the command platform db:size will give approximate disk usage as reported by MariaDB. However, be aware that due to the way MySQL/MariaDB store and pack data this number is not always accurate, and may be off by as much as 10 percentage points. \u0026#43;--------------\u0026#43;--------\u0026#43; | Property | Value | \u0026#43;--------------\u0026#43;--------\u0026#43; | max | 2048MB | | used | 189MB | | percent_used | 9% | \u0026#43;--------------\u0026#43;--------\u0026#43; For the most reliable disk usage warnings, we strongly recommend all customers enable Health notifications on all projects. That will provide you with a push-notification through your choice of channel when the available disk space on any service drops too low. No space left on device During the build hook, you may run into the following error depending on the size of your application: W: [Errno 28] No space left on device: ... The cause of this issue has to do with the amount of disk provided to the build container before it is deployed. Application images are restricted to 4 GB during build, no matter how much writable disk has been set aside for the deployed application. Some build tools (yarn/npm) store cache for different versions of their modules. This can cause the build cache to grow over time beyond the maximum of 4GB. Try clearing the build cache and redeploying. In most cases, this will resolve the issue. If for some reason your application requires more than 4 GB during build, you can open a support ticket to have this limit increased. The most disk space available during build still caps off at 8 GB in these cases. MySQL lock wait timeout If you receive MySQL error messages like this: SQLSTATE[HY000]: General error: 1205 Lock wait timeout exceeded; This means a process running in your application acquired a lock from MySQL for a long period of time. That is typically caused by one of the following: There are multiple places acquiring locks in different order. For example, code path 1 first locks record A and then locks record B. Code path 2, in contrast, first locks record B and then locks record A. There is a long running background process executed by your application that holds the lock until it ends. If you’re using MariaDB 10\u0026#43; , you can use the SQL query SHOW FULL PROCESSLIST to list DB queries waiting for locks. Find output like the following, and start debugging. \u0026lt; skipped \u0026gt; Command: Query Time: ... State: Waiting for table metadata lock Info: SELECT ... \u0026lt; skipped \u0026gt; To find active background processes, run ps aufx on your application container. Also, please make sure that locks are acquired in a pre-defined order and released as soon as possible. MySQL: definer/invoker of view lack rights to use them There is a single MySQL user, so you can not use “DEFINER” Access Control mechanism for Stored Programs and Views. When creating a VIEW, you may need to explicitly set the SECURITY parameter to INVOKER: CREATE OR REPLACE SQL SECURITY INVOKER VIEW `view_name` AS SELECT MySQL server has gone away Disk space issues Errors such as “PDO Exception ‘MySQL server has gone away’” are usually simply the result of exhausting your existing diskspace. Be sure you have sufficient space allocated to the service in .platform/services.yaml . The current disk usage can be checked using the CLI command platform db:size. Because of issues with the way InnoDB reports its size, this can out by up to 20%. As table space can grow rapidly, it is usually advisable to make your database mount size twice the size reported by the db:size command. You are encouraged to add a low-disk warning notification to proactively warn of low disk space before it becomes an issue. Worker timeout Another possible cause of “MySQL server has gone away” errors is a server timeout. MySQL has a built-in timeout for idle connections, which defaults to 10 minutes. Most typical web connections end long before that is ever approached, but it’s possible that a long-running worker may idle and not need the database for longer than the timeout. In that case the same “server has gone away” message may appear. If that’s the case, the best way to handle it is to wrap your connection logic in code that detects a “server has gone away” exception and tries to re-establish the connection. Alternatively, if your worker is idle for too long it can self-terminate. Platform.sh will automatically restart the worker process, and the new process can establish its own new database connection. Packet size limitations Another cause of the “MySQL server has gone away” errors can be the size of the database packets. If that is the case, the logs may show warnings like “Error while sending QUERY packet” before the error. One way to resolve the issue is to use the max_allowed_packet parameter described above . ERROR: permission denied to create database The provided user does not have permission to create databases. The database is created for you and can be found in the path field of the $PLATFORM_RELATIONSHIPS environment variable. “Read-only file system” error Everything will be read-only, except the writable mounts you declare. Writable mounts are there for your data: for file uploads, logs and temporary files. Not for your code. In order to change code on Platform.sh you have to go through Git. This is what gives you all of the benefits of having repeatable deployments, consistent backups, traceability, and the magically fast creation of new staging/dev environments. In Platform.sh, you cannot just “hack production”. It is a constraint, but it is a good constraint. During the build phase of your application, the main filesystem is writable. So you can do whatever you want (e.g. compile code or generate anything you need). But during and after the deploy phase , the main filesystem will be read-only. RootNotFoundException from the CLI If you check out a project via Git directly and not using the platform get command, you may end up with the CLI unable to determine what project it’s in. If you run a CLI command from within the project directory you’ve checked out but get an error like this: [RootNotFoundException] Project root not found. This can only be run from inside a project directory. Then the CLI hasn’t been able to determine the project to use. To fix that, run: platform project:set-remote \u0026lt;project_id\u0026gt; where \u0026lt;project_id\u0026gt; is the random-character ID of the project. That can be found by running platform projects from the command line to list all accessible projects. Alternatively, it can be found in the management console after the platform get command shown or in the URL of the management console or project domain. “File not found” in Drupal If you see a bare “File not found” error when accessing your Drupal site with a browser, this means that you’ve pushed your code as a vanilla project but no index.php has been found. Make sure your repository contains an index.php file in the web location root , or that your Drush make files are properly named. PHP-specific error messages server reached max_children You may see a line like the following in the /var/log/app.log file: WARNING: [pool web] server reached max_children setting (2), consider raising it That indicates that the server is receiving more concurrent requests than it has PHP processes allocated, which means some requests will have to wait until another finishes. In this example there are 2 PHP processes that can run concurrently. Platform.sh sets the number of workers based on the available memory of your container and the estimated average memory size of each process. There are two ways to increase the number of workers: Adjust the worker sizing hints for your project. Upgrade your subscription on Platform.sh to get more computing resources. To do so, log into your account and edit the project. Execution timeout If your PHP application is not able to handle the amount of traffic or it is slow, you should see log lines from /var/log/app.log like any of the below: WARNING: [pool web] child 120, script \u0026#39;/app/public/index.php\u0026#39; (request:  GET /index.php ) execution timed out (358.009855 sec), terminating That means your PHP process is running longer than allowed. You can adjust the max_execution_time value in php.ini, but there is still a 5 minute hard cap on any web request that cannot be adjusted. The most common cause of a timeout is either an infinite loop (which is a bug that you should fix) or the work itself requires a long time to complete. For the latter case, you should consider putting the task into a background job. The following command will identify the 20 slowest requests in the last hour, which can provide an indication of what code paths to investigate. grep $(date \u0026#43;%Y-%m-%dT%H --date=\u0026#39;-1 hours\u0026#39;) /var/log/php.access.log | sort -k 4 -r -n | head -20 If you see that the processing time of certain requests is slow (e.g. taking more than 1000ms), you may wish to consider using a profiler like Blackfire to debug the performance issue. Otherwise, you may check if the following options are applicable: Find the most visited pages and see if they can be cached and/or put behind a CDN. You may refer to how caching works . Upgrade your subscription on Platform.sh to get more computing resources. To do so, log into your account and edit the project subscription. PHP process crashed If your PHP process crashed with a segmentation fault, you should see log lines in /var/log/app.log like below: WARNING: [pool web] child 112 exited on signal 11 (SIGSEGV) after 7.405936 seconds from start This is complicated, either a PHP extension is hitting a segmentation fault or your PHP application code is crashing. You should review recent changes in your application and try to find the cause of it, probably with the help of XDebug. PHP process is killed If your PHP process is killed by the kernel, you should see log lines in /var/log/app.log like this: WARNING: [pool web] child 429 exited on signal 9 (SIGKILL) after 50.938617 seconds from start That means the memory usage of your container exceeds the limit allowed on your plan so the kernel kills the offending process. You should try the following: Check if the memory usage of your application is expected and try to optimize it. Use sizing hints to reduce the amount of PHP workers which reduces the memory footprint. Upgrade your subscription on Platform.sh to get more computing resources. To do so, log into your account and edit the project. Stuck build or deployment If you see a build or deployment running longer than expected, that may be one of the following cases: The build is blocked by a process in your build hook. The deployment is blocked by a long running process in your deploy hook. The deployment is blocked by a long running cron job in the environment. The deployment is blocked by a long running cron job in the parent environment. To determine if your environment is being stuck in the build or the deployment, you can look at the build log available in the management console. If you see a line similar to the following: Re-deploying environment w6ikvtghgyuty-drupal8-b3dsina. It means the build has completed successfully and the system is trying to deploy. If that line never appears then it means the build is stuck. For a blocked build (when you don’t find the Re-deployment environment ... line), create a support ticket to have the build killed. In most regions the build will self-terminate after one hour. In older regions (US and EU) the build will need to be killed by our support team. When a deployment is blocked, you should try the following: Use SSH to connect to your environment. Find any long-running cron jobs or deploy hooks on the environment by running ps afx. Once you have identified the long running process on the environment, kill it with kill \u0026lt;PID\u0026gt;. PID stands for the process id shown by ps afx. If you’re performing “Sync” or “Activate” on an environment and the process is stuck, use SSH to connect to the parent environment and identify any long running cron jobs with ps afx. Kill the job(s) if you see any. Slow or failing build or deployment Builds that take long time or fail is a common problem. Most of the time it’s related to an application issue and they can be hard to troubleshoot without guidance. Here are a few tips that can help you solve the issues you are experiencing. Check for errors in the logs Invisible errors during the build and deploy phase can cause increased wait times, failed builds and other problems. Investigating each log and fixing errors is essential. Related documentation: Accessing logs Build and deploy hooks Hooks are frequently the cause of long build time. If they run into problem they can cause the build to fail or hang indefinitely. The build hook can be tested in your local environment. Because the deployed environment on Platform.sh is read-only the build hooks cannot be rerun there. Deploy hooks can be tested either locally or by logging into the application over SSH and running them there. They should execute safely but be aware that depending on what your scripts are doing they may have an adverse impact on the running application (e.g., flushing all caches). Furthermore, you can test your hooks with these Linux commands to help figure out any problems: time $cmd # Print execution time strace -T $cmd # Print a system call report Related documentation: Build and deploy hooks Cron jobs Containers cannot be shutdown while long-running tasks are active. That means long-running cron jobs will block a container from being shut down to make way for a new deploy. For that reason, make sure your custom cron jobs execution times are low and that they are running properly. Be aware that cron jobs may invoke other services in unexpected ways, which can increase execution time. note Drupal’s drush core-cron run installed module’s cron task. Those can be, for example; evicting invalid cache, updating database records, regenerating assets. Be sure to frequently benchmark the drush core-cron command in all your environments, as it is a common source of performance issues. Related documentation: Cron and scheduled tasks",
        "section": "Development",
        "subsections": " Force a redeploy Clear the build cache HTTP responses 502 Bad Gateway or 503 Service Unavailable Error provisioning the new certificate Total disk usage exceeds project maximum Low disk space  Check your application\u0026rsquo;s disk space Increase the disk space available Check your database disk space   No space left on device MySQL lock wait timeout MySQL: definer/invoker of view lack rights to use them MySQL server has gone away  Disk space issues Worker timeout Packet size limitations   ERROR: permission denied to create database \u0026ldquo;Read-only file system\u0026rdquo; error RootNotFoundException from the CLI \u0026ldquo;File not found\u0026rdquo; in Drupal PHP-specific error messages  server reached max_children Execution timeout PHP process crashed PHP process is killed   Stuck build or deployment Slow or failing build or deployment  Check for errors in the logs Build and deploy hooks Cron jobs    ",
        "image": "",
        "url": "/development/troubleshoot.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "eb0a25ed94126292f8adc0b75980807c",
        "title": "Bitbucket",
        "description": "",
        "text": " The Bitbucket integration allows you to manage your Platform.sh environments directly from your Bitbucket repository. Set up an OAuth consumer You can integrate your Bitbucket repositories with Platform.sh by creating an OAuth consumer for your Workspace. Go to your Bitbucket Workspace and click “Settings”. Under “APPS AND FEATURES” click “OAuth Consumers”. Click the “Add consumer” button. Fill out the information for the consumer. In order for the integration to work correctly, it’s required that you include: Name: Give the consumer a recognizable name, like Platform.sh consumer or Platform.sh integration. Callback URL: The URL users will be redirected to after access authorization. It is sufficient to set this value to http://localhost. Set as a private consumer: At the bottom of the “Details” section, select the “This is a private consumer” checkbox. Permissions: Sets the integration permissions for Platform.sh. These permissions will create the webhooks that will enable Platform.sh to mirror actions from the Bitbucket repository. Account - Email, Read Repositories - Read, Write Pull requests - Read Webhooks - Read and write After you have completed the form, Save the consumer. After you have saved, you will see your consumer listed in the “OAuth consumers” section. If you open that item, it will expose two variables that you will need to complete the integration using the Platform.sh CLI: Key and Secret. Local Install the Platform.sh CLI if you have not already done so. Retrieve a PROJECT_ID for an existing project with platform project:list or create a new project with platform project:create. Then run the integration command: platform integration:add --type=bitbucket --project \u0026lt;PLATFORMSH_PROJECT_ID\u0026gt; --key \u0026lt;CONSUMER_KEY\u0026gt; --secret \u0026lt;CONSUMER_SECRET\u0026gt; --repository \u0026lt;USER\u0026gt;/\u0026lt;REPOSITORY\u0026gt; where PLATFORMSH_PROJECT_ID is the project ID for your Platform.sh project. CONSUMER_KEY is the Key variable of the consumer you created. CONSUMER_SECRET is the Secret variable of the consumer you created. USER/REPOSITORY is the location of the repository. Validate the integration In both cases, you can verify that your integration is functioning properly using the CLI command platform integration:validate Optional parameters By default several parameters will be set for the Bitbucket integration. They can be changed using the platform integration:update command. --fetch-branches: Track and deploy branches (true by default) --prune-branches: Delete branches that do not exist in the remote Bitbucket repository (true by default) --build-pull-requests: Track and deploy pull-requests (true by default) --build-pull-requests-post-merge: false to have Platform.sh build the branch specified in a PR. true to build the result of merging the PR. (false by default) --pull-requests-clone-parent-data: Set to false to disable cloning of parent environment data when creating a PR environment, so each PR environment starts with no data. (true by default) For more information see: platform help integration:update Note: The –prune-branches option depends on –fetch-branches being enabled. If –fetch-branches is disabled, –prune-branches will automatically be set to false, even if specifically set to true.",
        "section": "Source Integrations",
        "subsections": " Set up an OAuth consumer  Local   Validate the integration Optional parameters  ",
        "image": "",
        "url": "/integrations/source/bitbucket.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "a062392cca085e24cdadc3a20cd04b8f",
        "title": "Blackfire",
        "description": "",
        "text": " Platform.sh supports Blackfire.io. Blackfire is a PHP profiler and automated performance testing tool that can be used in the development Integration, Staging, and Production environments. It grants details information on your PHP code’s resources consumption across Wall-Time, CPU, I/O, Memory, Network Calls, HTTP requests and SQL queries. In addition, it can profile your code automatically and notify you whenever your code does not comply with best practices for PHP, Symfony, Drupal, eZPlatform, Typo3 \u0026amp; Magento code performance management. For a high level overview and demo of Blackfire, check out the full video tutorial . Version Check the latest versions of the probe and CLI tool on Blackfire’s documentation . On a Grid plan 1. Get your credentials Sign up for the free 15 days Premium trial at Blackfire.io and install the Blackfire Companion web browser extension ( Chrome or Firefox ). Note: Blackfire also offers a perpetually-free edition but it is for local development only and will not run on Platform.sh. Go to your Dashboard and create a new environment under the Environments tab . You will need to store the server credentials for further configuration. You can find them any time under the “Settings” tab of your environment in Blackfire. 2. Enable the Blackfire extension Configure the extension in your .platform.app.yaml as follows: runtime:extensions:- blackfirePush the changes to your Platform environment to enable Blackfire as follows: git add .platform.app.yaml git commit -m  Enable Blackfire.  git push 3. Configure your server credentials Blackfire enables to have a fine grained configuration of server credentials across branches and environments on Platform.sh. Configuring global server credentials Configuring server credentials on your master branch will enable you to make sure you can profile any other branch: platform variable:create -e master env:BLACKFIRE_SERVER_ID --value \u0026lt;insert your Server ID\u0026gt; platform variable:create -e master env:BLACKFIRE_SERVER_TOKEN --value \u0026lt;insert your Server Token\u0026gt; Configuring server credentials per branch A recommendation is to have a Blackfire environment for production, another one for staging, and another one for development/integration. That can be mapped in Platform.sh to one Blackfire environment for the production branch, one for the staging branch, and one for all feature branches. platform variable:create -e=\u0026lt;insert your branch name\u0026gt; env:BLACKFIRE_SERVER_ID \u0026lt;insert your Server ID\u0026gt; platform variable:create -e=\u0026lt;insert your branch name\u0026gt; env:BLACKFIRE_SERVER_TOKEN \u0026lt;insert your Server Token\u0026gt; 4. Confirm it’s running Login via SSH to your container and confirm that Blackfire is running as follows: php --ri blackfire blackfire blackfire =\u0026gt; enabled blackfire =\u0026gt; 1.16.1 Timing measurement =\u0026gt; gtod Num of CPU =\u0026gt; 8 ... On a Dedicated cluster Sign up for the free 15 days Premium trial at blackfire.io and install the Blackfire Companion web browser extension ( Chrome or Firefox ). Then open a support ticket with the Backfire server ID and token. The client ID and token is optional. Our support team will install it for you. Note, Blackfire integration works only on profiling your cluster via the URL to the origin. Do not profile your site going through the CDN. Profiling web requests Access your site via your browser and click Profile in the Blackfire Companion. That’s it! Your site will be profiled and you should get all the results in your Blackfire account. Profiling CLI commands To profile your PHP CLI scripts, use the following command line: blackfire --config /etc/platform/$USER/blackfire.ini \u0026lt;command\u0026gt; Going further with Blackfire Blackfire also enables to: collaborate with the rest of your team write performance tests automate profiling with periodic builds integrate further with Platform.sh by enabling to automate profiling as each code commit integrate with New Relic for combined benefits of monitoring and profiling integrate with GitHub, Bitbucket and GitLab to show the results of Blackfire builds at the commit status level Check Blackfire’s documentation for more information. Note: Those features may require a Premium or an Enterprise subscription. We offer attractive bundles of Platform.sh and Blackfire.io subscriptions. Please contact our sales department to discuss how we can help you. Troubleshooting Bypassing Reverse Proxy, Cache, and Content Delivery Networks (CDN) If you are using one of those, you will need them to let Blackfire access your servers. More information on how to configure a bypass . HTTP Cache configuration If you are using the HTTP cache with cookies , please update in your .platform.app.yaml the cookies that are allowed to go through the cache. You need to allow the __blackfire cookie name. Something like: cache:enabled:truecookies:[ /SESS.*/ , __blackfire ]Reaching out to the Blackfire support If the above didn’t help, collect the following and send it to the Blackfire support : The output of platform ssh -- php -d display_startup_errors=on --ri blackfire command The Blackfire logs Getting the Blackfire logs Please execute the following in the environment where you’re facing the issue: platform variable:create php:blackfire.log_file --value /tmp/blackfire.log platform variable:create php:blackfire.log_level --value 4 start a profile/build again You will get the logs with platform ssh -- cat /tmp/blackfire.log \u0026gt; blackfire.log. Disabling the Blackfire logs Once you are done, please disable logging with: platform variable:delete php:blackfire.log_file platform variable:delete php:blackfire.log_level",
        "section": "Profiling",
        "subsections": " Version On a Grid plan  1. Get your credentials 2. Enable the Blackfire extension 3. Configure your server credentials 4. Confirm it\u0026rsquo;s running   On a Dedicated cluster Profiling web requests Profiling CLI commands Going further with Blackfire Troubleshooting  Bypassing Reverse Proxy, Cache, and Content Delivery Networks (CDN) HTTP Cache configuration   Reaching out to the Blackfire support  Getting the Blackfire logs Disabling the Blackfire logs    ",
        "image": "",
        "url": "/integrations/profiling/blackfire.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "e08fe797431a40542d8f4bfdb7631551",
        "title": "C#/.NET Core",
        "description": "",
        "text": " Platform.sh supports deploying .NET applications by allowing developers to define a build process and pass its variables to the .NET Core build environment. Supported versions Grid Dedicated 2.0 2.1 2.2 3.1 None available To specify a .NET Core container, use the type property in your .platform.app.yaml. type:\u0026#39;dotnet:3.1\u0026#39; Building the application For simple applications, using the dotnet publish default framework-dependent deployment method is sufficient for building applications in .NET containers: hooks:build:| set -xedotnetpublish--output $PLATFORM_OUTPUT_DIR -p:UseRazorBuildServer=false-p:UseSharedCompilation=falsewhere PLATFORM_OUTPUT_DIR is the output directory for compiled languages available at build time. Typically .NET Core builds will start a collection of build servers, which are helpful for repeated builds. On Platform.sh, however, if this process is not disabled, the build process will not finish until the idle timeout is reached. As a result, it is recommended to include -p toggles that disable the Razor compiler for dynamic cshtml pages (UseRazorBuildServer) and the .NET msbuild compiler (UseSharedCompilation). If making multiple builds is desired for your application, make sure to call dotnet build-server shutdown at the end of your build hook. Running the application .NET Core applications should be started using the web.commands.start directive in .platform.app.yaml. This ensures that the command starts at the right moment and stops gracefully when a re-deployment needs to be executed. Also, should the program terminate for any reason, it will be automatically restarted. Note that the start command must run in the foreground. Incoming requests are passed to the application using either a TCP (default) or UNIX socket. The application must use the appropriate environment variable to determine the URI to listen on. In case of a TCP socket ( recommended ), the application must listen on http://127.0.0.1, using the PORT environment variable. There will be an Nginx server sitting in front of your application. Serving static content via Nginx is recommended, as this allows easy control of headers (including cache headers) and also has marginal performance benefits. Note that HTTPS is also terminated at the Ngnix proxy, so the app.UseHttpsRedirection(); line in Startup.cs should be removed. To force HTTPS-only, please refer to the routes documentation . The following example configures an environment to serve the static content folders commonly found in ASP.NET MVC templates using Nginx, while routing other traffic to the .NET application. web:locations: / :root: wwwroot allow:truepassthru:truerules:# Serve these common asset types with customs cache WebApplication1.dll You can also route all requests to the application unconditionally: web:locations: / :allow:falsepassthru:truecommands:start: dotnet WebApplication1.dll Project templates Platform.sh offers project templates for .NET Core applications using the structure described above. They can be used as a starting point or reference for building your own website or web application. ASP.NET Core ASP.NET Core This template builds the ASP.NET Core framework for Platform.sh. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. ASP.NET Core is an open-source and cross-platform .NET framework for building modern cloud-based web applications. Services: .NET 2.2 MariaDB 10.2 Redis 5.0 View the repository on GitHub.",
        "section": "Languages",
        "subsections": " Supported versions Building the application Running the application Project templates  ",
        "image": "",
        "url": "/languages/dotnet.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "6952a1a08417ce5b7038705e04998b98",
        "title": "Elixir",
        "description": "",
        "text": " Platform.sh supports building and deploying applications written in Elixir. There is no default flavor for the build phase, but you can define it explicitly in your build hook. Platform.sh Elixir images support both committed dependencies and download-on-demand. The underlying Erlang version is 22.0.7. Supported versions Grid Dedicated 1.9 None available To specify an Elixir container, use the type property in your .platform.app.yaml. type:\u0026#39;elixir:1.9\u0026#39; Platform.sh variables Platform.sh exposes relationships and other configuration as environment variables . Most notably, it allows a program to determine at runtime what HTTP port it should listen on and what the credentials are to access other services . To get the PORT environment variable (the port on which your web application is supposed to listen) you would: String.to_integer(System.get_env( PORT ) ||  8888 ) Some of the environment variables are in JSON format and are base64 encoded. You would need to import a JSON parsing library such as Jason or Poison to read those. (There is an example for doing this to decode the PLATFORM_RELATIONSHIPS environment variable in the section below .) Tip: Remember config/prod.exs is evaluated at build time and will not have access to runtime configuration. Use config/releases.exs to configure your runtime environment. Building and running the application If you are using Hex to manage your dependencies, it will be necessary to specify a set of environment variables in your .platform.app.yaml file that define the MIX_ENV and SECRET_KEY_BASE, which can be set to the Platform.sh-provided PLATFORM_PROJECT_ENTROPY environment variable: variables:env:SECRET_KEY_BASE:$PLATFORM_PROJECT_ENTROPYMIX_ENV:\u0026#39;prod\u0026#39;Include in your build hook the steps to retrieve a local Hex and rebar, and then run mix do deps.get, deps.compile, compile on your application to build a binary. hooks:build:| mix local.hex --forcemixlocal.rebar--forcemixdodeps.get--onlyprod,deps.compile,compile Note: The above build hook will work for most cases, and assumes that your mix.exs file is located at the root of your application. Assuming mix.exs is present at the root of your repository and your build hook matches the above, you can then start it from the web.commands.start directive. Note: The start command must run in the foreground, so you should set the --no-halt flag when calling mix run. The following basic .platform.app.yaml file is sufficient to run most Elixir applications. name:apptype:elixir:1.9variables:env:MIX_ENV:\u0026#39;prod\u0026#39;SECRET_KEY_BASE:$PLATFORM_PROJECT_ENTROPYhooks:build:| mix local.hex --forcemixlocal.rebar--forcemixdodeps.get--onlyprod,deps.compile,compileweb:commands:start:mixrun--no-haltlocations:/:allow:falsepassthru:trueNote that there will still be an Nginx proxy server sitting in front of your application. If desired, certain paths may be served directly by Nginx without hitting your application (for static files, primarily) or you may route all requests to the Elixir application unconditionally, as in the example above. Dependencies The recommended way to handle Elixir dependencies on Platform.sh is using Hex. You can commit a mix.exs file in your repository and the system will download the dependencies in your deps section using the build hook above. defp deps do [ {:platformshconfig,  ~\u0026gt; 0.1.0 } ] end Accessing Services The simplest possible way to go around this is to use the Platform.sh Config Reader library from hex. The libraray source is also available on GitHub . If you are building a Phoenix app for example, it would suffice to add a database to .platform/services.yaml and a relationship in .platform.app.yaml. Put the lib in your deps and, assuming you renamed the proc.secrets.exs to releases.exs per the Phoenix guide , change: System.get_env( DATABASE_URL ) to Platformsh.Config.ecto_dsn_formatter( database ) See Platform.sh Config Reader Documentation for the full API. Accessing Services Manually The services configuration is available in the environment variable PLATFORM_RELATIONSHIPS. Given a relationship defined in .platform.app.yaml: relationships:postgresdatabase: dbpostgres:postgresql  Assuming you have in mix.exs the Poison library to parse JSON: defp deps do [ {:poison,  ~\u0026gt; 3.0 } ] end And assuming you use ecto you could put in config/config.exs: relationships = Poison.decode!(Base.decode64!(System.get_env( PLATFORM_RELATIONSHIPS ))) [postgresql_config | _tail] = relationships[ postgresdatabase ] config :my_app, Repo, database: postgresql_config[ path ], username: postgresql_config[ username ], password: postgresql_config[ password ], hostname: postgresql_config[ host ] and setup Ecto during the deploy hook: deploy:| mix do ecto.setupProject templates Platform.sh offers a number of project templates using the structure described above. It can be used as a starting point or reference for building your own website or web application. Templates in development.",
        "section": "Languages",
        "subsections": " Supported versions Platform.sh variables Building and running the application Dependencies Accessing Services  Accessing Services Manually   Project templates  ",
        "image": "",
        "url": "/languages/elixir.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "0d409dfd98746e9ff89dfba775c49ce1",
        "title": "Example: Slack",
        "description": "",
        "text": " The following example activity script will post a message to a Slack channel every time it is triggered. To use it, paste it as-is into a .js file and then add it as a new integration. Be sure to specify which events it should trigger on using the --events switch, and if desired which --environments you want. Second, create a new Slack webhook through your Slack administrative interface. See the Slack documentation for how to do so. At the end you will be given a URL that points to https://hooks.slack.com/.... Third, add that URL to your project as a variable named SLACK_URL. Now, any activities that meet the events/environment criteria you specified will get reported to Slack. Once you have it working, you’re free to modify the code below as desired. See the Slack messaging documentation for how to format more complex messages. /** * Sends a color-coded formatted message to Slack. * * You must first configure a Platform.sh variable named  SLACK_URL . * That is the group and channel to which the message will be sent. * * To control what events it will run on, use the --events switch in * the Platform.sh CLI. * * @param {string} title * The title of the message block to send. * @param {string} message * The message body to send. */ function sendSlackMessage(title, message) { console.log((new Date).getDay()); if ((new Date).getDay() === 5) { message \u0026#43;= a Friday! :calendar: ; } var color = activity.result === \u0026#39;success\u0026#39; ? \u0026#39;#66c000\u0026#39; : \u0026#39;#ff0000\u0026#39;; var body = { \u0026#39;attachments\u0026#39;: [{  title : title,  text : message,  color : color, }], }; var url = variables()[\u0026#39;SLACK_URL\u0026#39;]; if (!url) { throw new Error(\u0026#39;You must define a SLACK_URL project variable.\u0026#39;); } var resp = fetch(url,{ method: \u0026#39;POST\u0026#39;, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, }, body: JSON.stringify(body), }); if (!resp.ok) { console.log( Sending slack message failed:   \u0026#43; resp.body.text()); } } function variables() { var vars = {}; activity.payload.deployment.variables.forEach(function(variable) { vars[variable.name] = variable.value; }); return vars; } sendSlackMessage(activity.text, activity.log); Common properties you may want to send to Slack (in the last line of the script) include: activity.text: A brief, one-line statement of what happened. activity.log: The complete build and deploy log output, as it would be seen in the Management Console log screen.",
        "section": "Activity scripts",
        "subsections": "",
        "image": "",
        "url": "/integrations/activity/slack.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "c81498fc4d42dce84431608e41d29e1c",
        "title": "External integrations",
        "description": "",
        "text": " While you can host your application repository entirely on Platform.sh, it’s likely that you will want to integrate your deployments with your pre-existing service. Platform.sh can be easily integrated with external services such as GitHub, Gitlab, or Bitbucket. Choose your current service, and this guide will take you through the steps to mirror your repository on Platform.sh and have environments created automatically for your pull requests and branches. GitHub Bitbucket GitLab These steps assume that you have already: Signed up for a free trial account with Platform.sh. If you have not completed these steps by now, click the links and do so before you begin.",
        "section": "Getting started",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/next-steps/integrations.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "14846ee4b8bc7a6521a3ec9e63e7da51",
        "title": "GitHub",
        "description": "",
        "text": " The GitHub integration allows you to manage your Platform.sh environments directly from your GitHub repository. Features supported: Create a new environment when creating a branch or opening a pull request on GitHub. Rebuild the environment when pushing new code to GitHub. Delete the environment when merging a pull request. Setup 1. Generate a token To integrate your Platform.sh project with an existing GitHub repository, you first need to generate a token on your GitHub user profile. Simply go to your Settings, then select Developer settings and click Personal access tokens. Here you can Generate a new token . Give it a description and then ensure the token has the following scopes: To integrate with public repositories: public_repo To integrate with your own private repositories: repo To integrate with your organization’s private repositories: repo and read:org Copy the token and make a note of it (temporarily). Note that for the integration to work, your GitHub user needs to have permission to push code to the repository. 2. Enable the integration Note that only the project owner can manage integrations. Open a terminal window (you need to have the Platform.sh CLI installed). Enable the GitHub integration as follows: platform integration:add --type=github --project=PLATFORMSH_PROJECT_ID --token=GITHUB-USER-TOKEN --repository=USER/REPOSITORY where PLATFORMSH_PROJECT_ID is the project ID for your Platform.sh project GITHUB-USER-TOKEN is the token you generated in step 1 USER is your github user name REPOSITORY is the name of the repository in github (not the git address) Note that if your repository belongs to an organization, use --repository=ORGANIZATION/REPOSITORY. e.g. platform integration:add --type=github --project=abcde12345 --token=xxx --repository=platformsh/platformsh-docs Optional parameters: --fetch-branches: Track and deploy branches (true by default) --prune-branches: Delete branches that do not exist in the remote GitHub repository (true by default) --build-pull-requests: Track and deploy pull-requests (true by default) --build-pull-requests-post-merge: false to have Platform.sh build the branch specified in a PR. true to build the result of merging the PR. (false by default) --pull-requests-clone-parent-data: Set to false to disable cloning of parent environment data when creating a PR environment, so each PR environment starts with no data. (true by default) --base-url: Only set if using GitHub Enterprise, hosted on your own server. If so, set this to the base URL of your private server (the part before the user and repository name). The CLI will create the necessary webhook for you when there’s correct permission set in the given token. Note that the --prune-branches option depends on --fetch-branches being enabled. If --fetch-branches is disabled, --prune-branches will automatically be set to false, even if specifically set to true. 3. Add the webhook If you see the message Failed to read or write webhooks, you will need to add a webhook manually: Copy the hook URL shown in the message. Go to your GitHub repository and click Settings, select the Webhooks and Services tab, and click Add webhook. Paste the hook URL, choose application/json for the content type, choose “Send me everything” for the events you want to receive, and click Add webhook. You can now start pushing code, creating new branches or opening pull requests directly on your GitHub repository. Note that if you have created your account using the GitHub oAuth Login then in order to use the Platform CLI, you will need to setup a password . 4. Validate the integration You can then verify that your integration is functioning properly using the CLI command platform integration:validate Types of environments Environments based on GitHub pull requests will have the correct ‘parent’ environment on Platform.sh; they will be activated automatically with a copy of the parent’s data. However, environments based on (non-pull-request) branches cannot have parents; they will inherit directly from master and start inactive by default.",
        "section": "Source Integrations",
        "subsections": " Setup  1. Generate a token 2. Enable the integration 3. Add the webhook 4. Validate the integration   Types of environments  ",
        "image": "",
        "url": "/integrations/source/github.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "42f7310904b3bbc0ce94c8b6ae063d0f",
        "title": "GitLab",
        "description": "",
        "text": " The GitLab integration allows you to manage your Platform.sh environments directly from your GitLab repository. Features supported: Create a new environment when creating a branch or opening a pull request on GitLab. Rebuild the environment when pushing new code to GitLab. Delete the environment when merging a pull request. Setup 1. Generate a token To integrate your Platform.sh project with an existing GitLab repository, you first need to generate a token on your GitLab user profile. Simply go to your Settings page on GitLab and click Access Tokens. Fill the Name field for example with “Platform.sh Integration” and optionally set an expiration time. Give it a description and then ensure the token has the following scopes: api - Access your API read_user - Read user information read_repository - Read repositories Copy the token and make a note of it (temporarily). Note that for the integration to work, your GitLab user needs to have permission to push code to the repository. 2. Enable the integration Note that only project owner or project admin can manage the integrations. Open a terminal window (you need to have the Platform.sh CLI installed). Enable the GitLab integration as follows: platform integration:add --type=gitlab --token=GITLAB-ACCESS-TOKEN --base-url=https://THE-URL-OF-YOUR-GITLAB --server-project=MY-NAMESPACE/MY-PROJECTNAME --project=PLATFORMSH_PROJECT_ID where PLATFORMSH_PROJECT_ID is the project ID for your Platform.sh project GITLAB-ACCESS-TOKEN is the token you generated in step 1 --base-url is used as the base to call the Gitlab API; you should point it to https://gitlab.com if your project is hosted on Gitlab, or the URL for your own Gitlab instance otherwise. It should not include your namespace and project name. MY-NAMESPACE/MY-PROJECTNAME describes the namespace of your GitLab project, not including the base url. For example, if your repository is located at https://gitlab.com/sandbox/my_application, the integration command would be platform integration:add --type=gitlab --token=GITLAB-ACCESS-TOKEN --base-url=https://gitlab.com --server-project=sandbox/my_application --project=PLATFORMSH_PROJECT_ID Optional parameters: --build-merge-requests: Track and deploy merge-requests (true by default) --merge-requests-clone-parent-data : should merge requests clone the data from the parent environment (true by default) --fetch-branches: Track and deploy branches (true by default) --prune-branches: Delete branches that do not exist in the remote GitLab repository (true by default) --base-url: Only set if using self-hosted GitLab on your own server. If so, set this to the base URL of your private server (the part before the user and repository name). Note that the --prune-branches option depends on --fetch-branches being enabled. If --fetch-branches is disabled, --prune-branches will automatically be set to false, even if specifically set to true. 3. Add the webhook The previous command, if successful should output the configuration of the integration. The last element would look like: | hook_url | https://{region}.platform.sh/api/projects/{projectid}/integrations/{hook_id}/hook | The CLI will create the necessary webhook using the above URL for you when there’s correct permission set in the given token. If you see the message Failed to read or write webhooks, you will need to add a webhook manually: Copy the hook URL shown in the message. Go to your GitLab repository and click Settings \u0026gt; Integrations. Paste the hook URL. In the Triggers section choose Push events, Tag push events and Merge Request events. Click on Add webhook. You can now start pushing code, creating new branches or opening merge requests directly on your GitLab repository. You will see environments get automatically created and updated on the Platform.sh side. 4. Validate the integration You can then verify that your integration is functioning properly using the CLI command platform integration:validate Types of environments Environments based on GitLab merge requests will have the correct ‘parent’ environment on Platform.sh; they will be activated automatically with a copy of the parent’s data (unless you have set the option merge-requests-clone-parent-data to false). However, environments based on (non-merge-request) branches cannot have parents; they will inherit directly from master and start inactive by default.",
        "section": "Source Integrations",
        "subsections": " Setup  1. Generate a token 2. Enable the integration 3. Add the webhook 4. Validate the integration   Types of environments  ",
        "image": "",
        "url": "/integrations/source/gitlab.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "19c5bc510de6c4ad7ec751f70cdfdaf6",
        "title": "Go",
        "description": "",
        "text": " Platform.sh supports building and deploying applications written in Go using Go modules. They are compiled during the Build hook phase, and support both committed dependencies and download-on-demand. Supported versions Grid Dedicated 1.11 1.12 1.13 1.14 None available To specify a Go container, use the type property in your .platform.app.yaml. type:\u0026#39;golang:1.14\u0026#39; Deprecated versions The following container versions are also available. However, due to their lack of Go module support and the difficulties in supporting the GOPATH during the Platform.sh build they are not recommended. 1.10 1.8 1.9 Go modules The recommended way to handle Go dependencies on Platform.sh is using Go module support in Go 1.11 and later. That allows the build process to use go build directly without any extra steps, and you can specify an output executable file of your choice. (See the examples below.) Platform.sh variables Platform.sh exposes relationships and other configuration as environment variables . To make them easier to access you should use the provided Config Reader library . Most notably, it allows a program to determine at runtime what HTTP port it should listen on and what the credentials are to access other services . package main import ( _  github.com/go-sql-driver/mysql  psh  github.com/platformsh/gohelper   net/http  ) func main() { p, err := psh.NewPlatformInfo() if err != nil { panic( Not in a Platform.sh Environment. ) } http.HandleFunc( /bar , func(w http.ResponseWriter, r *http.Request) { // ... }) http.ListenAndServe( : \u0026#43;p.Port, nil) } Building and running the application Assuming your go.mod and go.sum files are present in your repository, the application may be built with a simple go build command that will produce a working executable. You can then start it from the web.commands.start directive. Note that the start command must run in the foreground. Should the program terminate for any reason it will be automatically restarted. The following basic .platform.app.yaml file is sufficient to run most Go applications. name:apptype:golang:1.14hooks:build:| # Modify this line if you want to build differently or use an alternate name for your executable.gobuild-obin/appweb:upstream:socket_family:tcpprotocol:httpcommands:# If you change the build output in the build hook above, update this line as well.start:./bin/applocations:/:# Route all requests to the Go app, unconditionally.# If you want some files served directly by the web server without hitting Go, see# https://docs.platform.sh/configuration/app/web.htmlallow:falsepassthru:truedisk:1024Note that there will still be an Nginx proxy server sitting in front of your application. If desired, certain paths may be served directly by Nginx without hitting your application (for static files, primarily) or you may route all requests to the Go application unconditionally, as in the example above. Accessing services To access various services with Go, see the following examples. The individual service pages have more information on configuring each service. Memcached MongoDB MySQL PostgreSQL RabbitMQ Solr package examples import (  fmt   github.com/bradfitz/gomemcache/memcache  psh  github.com/platformsh/config-reader-go/v2  gomemcache  github.com/platformsh/config-reader-go/v2/gomemcache  ) func UsageExampleMemcached() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // Get the credentials to connect to the Solr service. credentials, err := config.Credentials( memcached ) checkErr(err) // Retrieve formatted credentials for gomemcache. formatted, err := gomemcache.FormattedCredentials(credentials) checkErr(err) // Connect to Memcached. mc := memcache.New(formatted) // Set a value. key :=  Deploy_day  value :=  Friday  err = mc.Set(\u0026amp;memcache.Item{Key: key, Value: []byte(value)}) // Read it back. test, err := mc.Get(key) return fmt.Sprintf( Found value \u0026lt;strong\u0026gt;%s\u0026lt;/strong\u0026gt; for key \u0026lt;strong\u0026gt;%s\u0026lt;/strong\u0026gt;. , test.Value, key) } package examples import (  context   fmt  psh  github.com/platformsh/config-reader-go/v2  mongoPsh  github.com/platformsh/config-reader-go/v2/mongo   go.mongodb.org/mongo-driver/bson   go.mongodb.org/mongo-driver/mongo   go.mongodb.org/mongo-driver/mongo/options   time  ) func UsageExampleMongoDB() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // Get the credentials to connect to the Solr service. credentials, err := config.Credentials( mongodb ) checkErr(err) // Retrieve the formatted credentials for mongo-driver. formatted, err := mongoPsh.FormattedCredentials(credentials) checkErr(err) // Connect to MongoDB using the formatted credentials. ctx, _ := context.WithTimeout(context.Background(), 10*time.Second) client, err := mongo.Connect(ctx, options.Client().ApplyURI(formatted)) checkErr(err) // Create a new collection. collection := client.Database( main ).Collection( starwars ) // Clean up after ourselves. err = collection.Drop(context.Background()) checkErr(err) // Create an entry. res, err := collection.InsertOne(ctx, bson.M{ name :  Rey ,  occupation :  Jedi }) checkErr(err) id := res.InsertedID // Read it back. cursor, err := collection.Find(context.Background(), bson.M{ _id : id}) checkErr(err) var name string var occupation string for cursor.Next(context.Background()) { document := struct { Name string Occupation string }{} err := cursor.Decode(\u0026amp;document) checkErr(err) name = document.Name occupation = document.Occupation } return fmt.Sprintf( Found %s (%s) , name, occupation) } package examples import (  database/sql   fmt  _  github.com/go-sql-driver/mysql  psh  github.com/platformsh/config-reader-go/v2  sqldsn  github.com/platformsh/config-reader-go/v2/sqldsn  ) func UsageExampleMySQL() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // The \u0026#39;database\u0026#39; relationship is generally the name of the primary SQL database of an application. // That\u0026#39;s not required, but much of our default automation code assumes it. credentials, err := config.Credentials( database ) checkErr(err) // Using the sqldsn formatted credentials package. formatted, err := sqldsn.FormattedCredentials(credentials) checkErr(err) db, err := sql.Open( mysql , formatted) checkErr(err) defer db.Close() // Force MySQL into modern mode. db.Exec( SET NAMES=utf8 ) sql_mode = \u0026#39;ANSI,STRICT_TRANS_TABLES,STRICT_ALL_TABLES, NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO, // Creating a table. sqlCreate := CREATE TABLE IF NOT EXISTS PeopleGo ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT _, err = db.Exec(sqlCreate) checkErr(err) // Insert data. sqlInsert := INSERT INTO PeopleGo (name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La _, err = db.Exec(sqlInsert) checkErr(err) table := \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; var id int var name string var city string rows, err := db.Query( SELECT * FROM PeopleGo ) if err != nil { panic(err) } else { for rows.Next() { err = rows.Scan(\u0026amp;id, \u0026amp;name, \u0026amp;city) checkErr(err) table \u0026#43;= name, city) } table \u0026#43;= } _, err = db.Exec( DROP TABLE PeopleGo; ) checkErr(err) return table } package examples import (  database/sql   fmt  _  github.com/lib/pq  psh  github.com/platformsh/config-reader-go/v2  libpq  github.com/platformsh/config-reader-go/v2/libpq  ) func UsageExamplePostgreSQL() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // The \u0026#39;database\u0026#39; relationship is generally the name of the primary SQL database of an application. // It could be anything, though, as in the case here where it\u0026#39;s called  postgresql . credentials, err := config.Credentials( postgresql ) checkErr(err) // Retrieve the formatted credentials. formatted, err := libpq.FormattedCredentials(credentials) checkErr(err) // Connect. db, err := sql.Open( postgres , formatted) checkErr(err) defer db.Close() // Creating a table. sqlCreate := CREATE TABLE IF NOT EXISTS PeopleGo ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT _, err = db.Exec(sqlCreate) checkErr(err) // Insert data. sqlInsert := INSERT INTO PeopleGo(name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La _, err = db.Exec(sqlInsert) checkErr(err) table := \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; var id int var name string var city string // Read it back. rows, err := db.Query( SELECT * FROM PeopleGo ) if err != nil { panic(err) } else { for rows.Next() { err = rows.Scan(\u0026amp;id, \u0026amp;name, \u0026amp;city) checkErr(err) table \u0026#43;= name, city) } table \u0026#43;= } _, err = db.Exec( DROP TABLE PeopleGo; ) checkErr(err) return table } package examples import (  fmt  psh  github.com/platformsh/config-reader-go/v2  amqpPsh  github.com/platformsh/config-reader-go/v2/amqp   github.com/streadway/amqp   sync  ) func UsageExampleRabbitMQ() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // Get the credentials to connect to RabbitMQ. credentials, err := config.Credentials( rabbitmq ) checkErr(err) // Use the amqp formatted credentials package. formatted, err := amqpPsh.FormattedCredentials(credentials) checkErr(err) // Connect to the RabbitMQ server. connection, err := amqp.Dial(formatted) checkErr(err) defer connection.Close() // Make a channel. channel, err := connection.Channel() checkErr(err) defer channel.Close() // Create a queue. q, err := channel.QueueDeclare(  deploy_days , // name false, // durable false, // delete when unused false, // exclusive false, // no-wait nil, // arguments ) body :=  Friday  msg := fmt.Sprintf( Deploying on %s , body) // Publish a message. err = channel.Publish(   , // exchange q.Name, // routing key false, // mandatory false, // immediate amqp.Publishing{ ContentType:  text/plain , Body: []byte(msg), }) checkErr(err) outputMSG := fmt.Sprintf( [x] Sent \u0026#39;%s\u0026#39; \u0026lt;br\u0026gt; , body) // Consume the message. msgs, err := channel.Consume( q.Name, // queue   , // consumer true, // auto-ack false, // exclusive false, // no-local false, // no-wait nil, // args ) checkErr(err) var received string var wg sync.WaitGroup wg.Add(1) go func() { for d := range msgs { received = fmt.Sprintf( [x] Received message: \u0026#39;%s\u0026#39; \u0026lt;br\u0026gt; , d.Body) wg.Done() } }() wg.Wait() outputMSG \u0026#43;= received return outputMSG } package examples import (  fmt  psh  github.com/platformsh/config-reader-go/v2  gosolr  github.com/platformsh/config-reader-go/v2/gosolr  solr  github.com/rtt/Go-Solr  ) func UsageExampleSolr() string { // Create a NewRuntimeConfig object to ease reading the Platform.sh environment variables. // You can alternatively use os.Getenv() yourself. config, err := psh.NewRuntimeConfig() checkErr(err) // Get the credentials to connect to the Solr service. credentials, err := config.Credentials( solr ) checkErr(err) // Retrieve Solr formatted credentials. formatted, err := gosolr.FormattedCredentials(credentials) checkErr(err) // Connect to Solr using the formatted credentials. connection := \u0026amp;solr.Connection{URL: formatted} // Add a document and commit the operation. docAdd := map[string]interface{}{  add : []interface{}{ map[string]interface{}{ id : 123,  name :  Valentina Tereshkova }, }, } respAdd, err := connection.Update(docAdd, true) checkErr(err) // Select the document. q := \u0026amp;solr.Query{ Params: solr.URLParamMap{  q : []string{ id:123 }, }, } resSelect, err := connection.CustomSelect(q,  query ) checkErr(err) // Delete the document and commit the operation. docDelete := map[string]interface{}{  delete : map[string]interface{}{  id : 123, }, } resDel, err := connection.Update(docDelete, true) checkErr(err) message := one document - %s\u0026lt;br\u0026gt; Selecting document (1 expected): %d\u0026lt;br\u0026gt; Deleting document - %s\u0026lt;br\u0026gt; respAdd, resSelect.Results.NumFound, resDel) return message } Project templates Platform.sh offers a project templates for Go applications using the structure described above. It can be used as a starting point or reference for building your own website or web application. Basic Go Basic Go This template provides the most basic configuration for running a custom Go project. Go is a statically typed, compiled language with an emphasis on easy concurrency and network services. Services: Go 1.14 MariaDB 10.2 View the repository on GitHub. Beego Beego This template builds the Beego framework for Platform.sh. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. Beego is a popular web framework written in Go. Services: Go 1.14 MariaDB 10.2 View the repository on GitHub. Echo Echo This template builds the Echo framework for Platform.sh. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. Echo is a lightweight, minimalist web framework written in Go. Services: Go 1.14 MariaDB 10.2 View the repository on GitHub. Gin Gin This template builds the Gin framework for Platform.sh. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. Gin is a lightweight web framework written in Go that emphasizes performance. Services: Go 1.14 MariaDB 10.2 View the repository on GitHub. Hugo Hugo This template provides a basic Hugo skeleton. All files are generated at build time, so at runtime only static files need to be served. Hugo is a static site generator written in Go, using Go\u0026#39;s native template packages for formatting. Services: Go 1.14 View the repository on GitHub. Mattermost Mattermost This template builds Mattermost on Platform.sh, configuring the deployment through user-defined environment variables. Mattermost is an open-source messaging framework written in Go and React. Services: Go 1.14 PostgreSQL 12 Elasticsearch 7.2 View the repository on GitHub.",
        "section": "Languages",
        "subsections": " Supported versions Deprecated versions Go modules Platform.sh variables Building and running the application Accessing services Project templates  ",
        "image": "",
        "url": "/languages/go.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "fac8ed7c2573b3f346514947e35e542e",
        "title": "Going live",
        "description": "",
        "text": " You’ve set up a project on Platform.sh by either pushing your code directly or by setting up an integration to an external repository. Now it’s time to take your site live. This guide will take you through the process configuring your project for production, setting up a domain, and configuring DNS so that your users can reach the application the way you want them to. Take your site live!",
        "section": "Introduction",
        "subsections": "",
        "image": "",
        "url": "/gettingstarted/next-steps/going-live.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "8d005add4555dd120e963ec5a2445bb4",
        "title": "HipChat",
        "description": "",
        "text": " The HipChat integration allows you to send notifications about your Platform.sh activity directly to HipChat. Setup 1. Find the HipChat ROOM-ID. In the HipChat web administrative UI, go to Admin \u0026gt; Rooms and click on the room to link notifications. Note down the “APP ID” listed in the Room Details on the Room’s ‘Summary’ page (you can also find the ID from the URL). 2. Generate a room-specific HIPCHAT-TOKEN. Click on the Room’s ‘Tokens’ page in the sidebar. In the Create New Token section specify ‘PlatformSH’ as the token’s label and click “Create” button. Note down the Token value. 3. Create the HipChat webhook with Platform CLI. platform integration:add --type=hipchat --room=ROOM-ID --token=HIPCHAT-TOKEN There are a number of optional parameters as well who’s default values include: --events=* (All Events) --environments=* (All Environments) --excluded-environments= (Empty) --states=complete (Complete state only) You’re given a chance to customize these parameters in an interactive shell prompt, or you may override the defaults on the command line: --states=pending,in_progress,complete (All states) Validate the integration You can then verify that your integration is functioning properly using the CLI command platform integration:validate",
        "section": "Activity scripts",
        "subsections": " Setup  1. Find the HipChat ROOM-ID. 2. Generate a room-specific HIPCHAT-TOKEN. 3. Create the HipChat webhook with Platform CLI.   Validate the integration  ",
        "image": "",
        "url": "/integrations/activity/hipchat.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "3976528693a0108357f4928017600865",
        "title": "Introduction",
        "description": "",
        "text": "",
        "section": "Platform.sh",
        "subsections": " Git Driven Infrastructure  Infrastructure as code Full stack management    ",
        "image": "",
        "url": "/",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "0377a167a2e4769e2bf0380ef510e9bf",
        "title": "Java",
        "description": "",
        "text": " Java is a general-purpose programming language, and one of the most popular in the world today. Platform.sh supports Java runtimes that can be used with build management tools such as Gradle, Maven, and Ant. Supported versions OpenJDK versions: Grid Dedicated 8 11 12 13 None available To specify a Java container, use the type property in your .platform.app.yaml. type:'java:13' Support libraries While it is possible to read the environment directly from your application, it is generally easier and more robust to use the platformsh/config-reader which handles decoding of service credential information for you. Support build automation Platform.sh supports the most common project management tools in the Java ecosystem, including: Gradle Maven Ant Accessing services To access various services with Java, see the following examples. The individual service pages have more information on configuring each service. Elasticsearch Kafka Memcached MongoDB MySQL PostgreSQL RabbitMQ Redis Solr package sh.platform.languages.sample; import org.elasticsearch.action.admin.indices.refresh.RefreshRequest; import org.elasticsearch.action.admin.indices.refresh.RefreshResponse; import org.elasticsearch.action.delete.DeleteRequest; import org.elasticsearch.action.index.IndexRequest; import org.elasticsearch.action.search.SearchRequest; import org.elasticsearch.action.search.SearchResponse; import org.elasticsearch.client.RequestOptions; import org.elasticsearch.client.RestHighLevelClient; import org.elasticsearch.index.query.QueryBuilders; import org.elasticsearch.search.SearchHit; import org.elasticsearch.search.builder.SearchSourceBuilder; import sh.platform.config.Config; import sh.platform.config.Elasticsearch; import java.io.IOException; import java.util.Arrays; import java.util.HashMap; import java.util.List; import java.util.Map; import java.util.function.Supplier; import static java.util.concurrent.ThreadLocalRandom.current; public class ElasticsearchSample implements Supplier\u003cString\u003e { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); Elasticsearch elasticsearch = config.getCredential( elasticsearch , Elasticsearch::new); // Create an Elasticsearch client object. RestHighLevelClient client = elasticsearch.get(); try { String index =  animals ; String type =  mammals ; // Index a few document. final List\u003cString\u003e animals = Arrays.asList( dog ,  cat ,  monkey ,  horse ); for (String animal : animals) { Map\u003cString, Object\u003e jsonMap = new HashMap\u003c\u003e(); jsonMap.put( name , animal); jsonMap.put( age , current().nextInt(1, 10)); jsonMap.put( is_cute , current().nextBoolean()); IndexRequest indexRequest = new IndexRequest(index, type) .id(animal).source(jsonMap); client.index(indexRequest, RequestOptions.DEFAULT); } RefreshRequest refresh = new RefreshRequest(index); // Force just-added items to be indexed RefreshResponse refreshResponse = client.indices().refresh(refresh, RequestOptions.DEFAULT); // Search for documents. SearchSourceBuilder sourceBuilder = new SearchSourceBuilder(); sourceBuilder.query(QueryBuilders.termQuery( name ,  dog )); SearchRequest searchRequest = new SearchRequest(); searchRequest.indices(index); searchRequest.source(sourceBuilder); SearchResponse search = client.search(searchRequest, RequestOptions.DEFAULT); for (SearchHit hit : search.getHits()) { String id = hit.getId(); final Map\u003cString, Object\u003e source = hit.getSourceAsMap(); logger.append(String.format( result id %s source: %s , id, } // Delete documents. for (String animal : animals) { client.delete(new DeleteRequest(index, type, animal), RequestOptions.DEFAULT); } } catch (IOException exp) { throw new RuntimeException( An error when execute Elasticsearch:   + exp.getMessage()); } return logger.toString(); } } package sh.platform.languages.sample; import org.apache.kafka.clients.consumer.Consumer; import org.apache.kafka.clients.consumer.ConsumerConfig; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.producer.Producer; import org.apache.kafka.clients.producer.ProducerConfig; import org.apache.kafka.clients.producer.ProducerRecord; import org.apache.kafka.clients.producer.RecordMetadata; import sh.platform.config.Config; import sh.platform.config.Kafka; import java.time.Duration; import java.util.HashMap; import java.util.Map; import java.util.function.Supplier; public class KafkaSample implements Supplier\u003cString\u003e { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); try { // Get the credentials to connect to the Kafka service. final Kafka kafka = config.getCredential( kafka , Kafka::new); Map\u003cString, Object\u003e configProducer = new HashMap\u003c\u003e(); configProducer.putIfAbsent(ProducerConfig.CLIENT_ID_CONFIG,  animals ); final Producer\u003cLong, String\u003e producer = kafka.getProducer(configProducer); // Sending data into the stream. RecordMetadata metadata = producer.send(new ProducerRecord\u003c\u003e( animals ,  lion )).get(); logger.append( Record sent with to partition  ).append(metadata.partition()) .append(  with offset metadata = producer.send(new ProducerRecord\u003c\u003e( animals ,  dog )).get(); logger.append( Record sent with to partition  ).append(metadata.partition()) .append(  with offset metadata = producer.send(new ProducerRecord\u003c\u003e( animals ,  cat )).get(); logger.append( Record sent with to partition  ).append(metadata.partition()) .append(  with offset // Consumer, read data from the stream. final HashMap\u003cString, Object\u003e configConsumer = new HashMap\u003c\u003e(); configConsumer.put(ConsumerConfig.GROUP_ID_CONFIG,  consumerGroup1 ); configConsumer.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,  earliest ); Consumer\u003cLong, String\u003e consumer = kafka.getConsumer(configConsumer,  animals ); ConsumerRecords\u003cLong, String\u003e consumerRecords = consumer.poll(Duration.ofSeconds(3)); // Print each record. consumerRecords.forEach(record -\u003e { logger.append( Record: Key   + record.key()); logger.append(  value   + record.value()); logger.append(  partition   + record.partition()); logger.append(  offset   + }); // Commits the offset of record to broker. consumer.commitSync(); return logger.toString(); } catch (Exception exp) { throw new RuntimeException( An error when execute Kafka , exp); } } } package sh.platform.languages.sample; import net.spy.memcached.MemcachedClient; import sh.platform.config.Config; import java.util.function.Supplier; import sh.platform.config.Memcached; public class MemcachedSample implements Supplier\u003cString\u003e { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); // Get the credentials to connect to the Memcached service. Memcached memcached = config.getCredential( memcached , Memcached::new); final MemcachedClient client = memcached.get(); String key =  cloud ; String value =  platformsh ; // Set a value. client.set(key, 0, value); // Read it back. Object test = client.get(key); logger.append(String.format( Found value %s for key %s. , test, key)); return logger.toString(); } } package sh.platform.languages.sample; import com.mongodb.MongoClient; import com.mongodb.client.MongoCollection; import com.mongodb.client.MongoDatabase; import org.bson.Document; import sh.platform.config.Config; import sh.platform.config.MongoDB; import java.util.function.Supplier; import static com.mongodb.client.model.Filters.eq; public class MongoDBSample implements Supplier\u003cString\u003e { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); // The 'database' relationship is generally the name of primary database of an application. // It could be anything, though, as in the case here here where it's called  mongodb . MongoDB database = config.getCredential( mongodb , MongoDB::new); MongoClient mongoClient = database.get(); final MongoDatabase mongoDatabase = mongoClient.getDatabase(database.getDatabase()); MongoCollection\u003cDocument\u003e collection = mongoDatabase.getCollection( scientist ); Document doc = new Document( name ,  Ada Lovelace ) .append( city ,  London ); collection.insertOne(doc); Document myDoc = collection.find(eq( _id , doc.get( _id ))).first(); logger.append(collection.deleteOne(eq( _id , doc.get( _id )))); return logger.toString(); } } package sh.platform.languages.sample; import sh.platform.config.Config; import sh.platform.config.MySQL; import javax.sql.DataSource; import java.sql.Connection; import java.sql.ResultSet; import java.sql.SQLException; import java.sql.Statement; import java.util.function.Supplier; public class MySQLSample implements Supplier\u003cString\u003e { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); // The 'database' relationship is generally the name of primary SQL database of an application. // That's not required, but much of our default automation code assumes it. MySQL database = config.getCredential( database , MySQL::new); DataSource dataSource = database.get(); // Connect to the database try (Connection connection = dataSource.getConnection()) { // Creating a table. String sql =  CREATE TABLE JAVA_PEOPLE (  +   id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY,  +  name VARCHAR(30) NOT NULL,  +  city VARCHAR(30) NOT NULL) ; final Statement statement = connection.createStatement(); statement.execute(sql); // Insert data. sql =  INSERT INTO JAVA_PEOPLE (name, city) VALUES  +  ('Neil Armstrong', 'Moon'),  +  ('Buzz Aldrin', 'Glen Ridge'),  +  ('Sally Ride', 'La Jolla') ; statement.execute(sql); // Show table. sql =  SELECT * FROM JAVA_PEOPLE ; final ResultSet resultSet = statement.executeQuery(sql); while (resultSet.next()) { int id = resultSet.getInt( id ); String name = resultSet.getString( name ); String city = resultSet.getString( city ); logger.append(String.format( the JAVA_PEOPLE id %d the name %s and city %s , id, name, city)); } statement.execute( DROP TABLE JAVA_PEOPLE ); return logger.toString(); } catch (SQLException exp) { throw new RuntimeException( An error when execute MySQL , exp); } } } package sh.platform.languages.sample; import sh.platform.config.Config; import sh.platform.config.MySQL; import sh.platform.config.PostgreSQL; import javax.sql.DataSource; import java.sql.Connection; import java.sql.ResultSet; import java.sql.SQLException; import java.sql.Statement; import java.util.function.Supplier; public class PostgreSQLSample implements Supplier\u003cString\u003e { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); // The 'database' relationship is generally the name of primary SQL database of an application. // It could be anything, though, as in the case here here where it's called  postgresql . PostgreSQL database = config.getCredential( postgresql , PostgreSQL::new); DataSource dataSource = database.get(); // Connect to the database try (Connection connection = dataSource.getConnection()) { // Creating a table. String sql =  CREATE TABLE JAVA_FRAMEWORKS (  +   id SERIAL PRIMARY KEY,  +  name VARCHAR(30) NOT NULL) ; final Statement statement = connection.createStatement(); statement.execute(sql); // Insert data. sql =  INSERT INTO JAVA_FRAMEWORKS (name) VALUES  +  ('Spring'),  +  ('Jakarta EE'),  +  ('Eclipse JNoSQL') ; statement.execute(sql); // Show table. sql =  SELECT * FROM JAVA_FRAMEWORKS ; final ResultSet resultSet = statement.executeQuery(sql); while (resultSet.next()) { int id = resultSet.getInt( id ); String name = resultSet.getString( name ); logger.append(String.format( the JAVA_FRAMEWORKS id %d the name %s  , id, name)); } statement.execute( DROP TABLE JAVA_FRAMEWORKS ); return logger.toString(); } catch (SQLException exp) { throw new RuntimeException( An error when execute PostgreSQL , exp); } } } package sh.platform.languages.sample; import sh.platform.config.Config; import sh.platform.config.RabbitMQ; import javax.jms.Connection; import javax.jms.ConnectionFactory; import javax.jms.MessageConsumer; import javax.jms.MessageProducer; import javax.jms.Queue; import javax.jms.Session; import javax.jms.TextMessage; import java.util.function.Supplier; public class RabbitMQSample implements Supplier\u003cString\u003e { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); try { // Get the credentials to connect to the RabbitMQ service. final RabbitMQ credential = config.getCredential( rabbitmq , RabbitMQ::new); final ConnectionFactory connectionFactory = credential.get(); // Connect to the RabbitMQ server. final Connection connection = connectionFactory.createConnection(); connection.start(); final Session session = connection.createSession(false, Session.AUTO_ACKNOWLEDGE); Queue queue = session.createQueue( cloud ); MessageConsumer consumer = session.createConsumer(queue); // Sending a message into the queue. TextMessage textMessage = session.createTextMessage( Platform.sh ); textMessage.setJMSReplyTo(queue); MessageProducer producer = session.createProducer(queue); producer.send(textMessage); // Receive the message. TextMessage replyMsg = (TextMessage) consumer.receive(100); logger.append( Message:  ).append(replyMsg.getText()); // close connections. producer.close(); consumer.close(); session.close(); connection.close(); return logger.toString(); } catch (Exception exp) { throw new RuntimeException( An error when execute RabbitMQ , exp); } } } package sh.platform.languages.sample; import redis.clients.jedis.Jedis; import redis.clients.jedis.JedisPool; import sh.platform.config.Config; import sh.platform.config.Redis; import java.util.Set; import java.util.function.Supplier; public class RedisSample implements Supplier\u003cString\u003e { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); // The 'database' relationship is generally the name of primary database of an application. // It could be anything, though, as in the case here here where it's called  redis . Redis database = config.getCredential( redis , Redis::new); JedisPool dataSource = database.get(); // Get a Redis Client final Jedis jedis = dataSource.getResource(); // Set a values jedis.sadd( cities ,  Salvador ); jedis.sadd( cities ,  London ); jedis.sadd( cities ,  São Paulo ); // Read it back. Set\u003cString\u003e cities = jedis.smembers( cities ); logger.append( cities:   + cities); jedis.del( cities ); return logger.toString(); } } package sh.platform.languages.sample; import org.apache.solr.client.solrj.SolrQuery; import org.apache.solr.client.solrj.SolrServerException; import org.apache.solr.client.solrj.impl.HttpSolrClient; import org.apache.solr.client.solrj.impl.XMLResponseParser; import org.apache.solr.client.solrj.response.QueryResponse; import org.apache.solr.client.solrj.response.UpdateResponse; import org.apache.solr.common.SolrDocumentList; import org.apache.solr.common.SolrInputDocument; import sh.platform.config.Config; import sh.platform.config.Solr; import java.io.IOException; import java.util.function.Supplier; public class SolrSample implements Supplier\u003cString\u003e { @Override public String get() { StringBuilder logger = new StringBuilder(); // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. Config config = new Config(); Solr solr = config.getCredential( solr , Solr::new); try { final HttpSolrClient solrClient = solr.get(); solrClient.setParser(new XMLResponseParser()); // Add a document SolrInputDocument document = new SolrInputDocument(); final String id =  123456 ; document.addField( id , id); document.addField( name ,  Ada Lovelace ); document.addField( city ,  London ); solrClient.add(document); final UpdateResponse response = solrClient.commit(); logger.append( Adding one document. Status (0 is success):  ) SolrQuery query = new SolrQuery(); query.set( q ,  city:London ); QueryResponse queryResponse = solrClient.query(query); SolrDocumentList results = queryResponse.getResults(); logger.append(String.format( Selecting documents (1 expected): %d results.getNumFound())); // Delete one document solrClient.deleteById(id); logger.append(String.format( Deleting one document. Status (0 is success): %s solrClient.commit().getStatus())); } catch (SolrServerException | IOException exp) { throw new RuntimeException( An error when execute Solr  , exp); } return logger.toString(); } } Project templates A number of project templates for major Java applications are available on GitHub. Not all of them are proactively maintained but all can be used as a starting point or reference for building your own website or web application. Apache Tomcat Apache Tomcat This project provides a starter kit for Apache Tomcat hosted on Platform.sh. Apache Tomcat is an open-source implementation of the Java Servlet, JavaServer Pages, Java Expression Language and WebSocket technologies. Services: Java 8 Maven Eclipse MicroProfile Apache Tomcat View the repository on GitHub. Apache TomEE Apache TomEE This project provides a starter kit for Apache TomEE Eclipse MicroProfile projects hosted on Platform.sh. Apache TomEE is the Eclipse MicroProfile implementation that uses several Apache Project flavors such as Apache Tomcat, Apache OpenWebBeans and so on. Services: Java 8 Maven Eclipse MicroProfile Apache TomEE View the repository on GitHub. Helidon Helidon This project provides a starter kit for Helidon Eclipse MicroProfile projects hosted on Platform.sh. Helidon is a collection of Java libraries for writing microservices that run on a fast web core powered by Netty. Helidon is designed to be simple to use, with tooling and examples to get you going quickly. Since Helidon is just a collection of libraries running on a fast Netty core, there is no extra overhead or bloat. Services: Java 8 Maven Eclipse MicroProfile Helidon View the repository on GitHub. Jenkins Jenkins This project provides a starter kit for Jenkins projects hosted on Platform.sh. Jenkins is an open source automation server written in Java. Jenkins helps to automate the non-human part of the software development process, with continuous integration and facilitating technical aspects of continuous delivery. Services: Java 8 Jenkins View the repository on GitHub. Jetty Jetty Eclipse Jetty provides a Web server and javax.servlet container, plus support for HTTP/2, WebSocket, OSGi, JMX, JNDI, JAAS and many other integrations. These components are open source and available for commercial use and distribution. Eclipse Jetty is used in a wide variety of projects and products, both in development and production. Jetty can be easily embedded in devices, tools, frameworks, application servers, and clusters. Services: Java 8 Maven Eclipse Jetty View the repository on GitHub. KumuluzEE KumuluzEE This project provides a starter kit for KumuluzEE Eclipse MicroProfile projects hosted on Platform.sh. KumuluzEE is a lightweight framework for developing microservices using standard Java, Java EE / Jakarta EE technologies and migrating existing Java applications to microservices. KumuluzEE packages microservices as standalone JARs. KumuluzEE microservices are lightweight and optimized for size and start-up time. Services: Java 8 Maven Eclipse MicroProfile KumuluzEE View the repository on GitHub. Micronaut Micronaut This project provides a starter kit for Micronaut projects hosted on Platform.sh. Micronaut is a modern, JVM-based, full-stack framework for building modular, easily testable microservice and serverless applications. Services: Java 8 Maven Micronaut View the repository on GitHub. Open Liberty Open Liberty This project provides a starter kit for Open Liberty Eclipse MicroProfile projects hosted on Platform.sh. Open Liberty is a highly composable, fast to start, dynamic application server runtime environment. Services: Java 8 Maven Eclipse MicroProfile Open Liberty View the repository on GitHub. Payara Micro Payara Micro This project provides a starter kit for Payara Micro projects hosted on Platform.sh. Payara Micro is an Open Source, lightweight Java EE (Jakarta EE) microservices deployments. Services: Java 8 Maven Eclipse MicroProfile Payara Micro View the repository on GitHub. Quarkus Quarkus QuarkusIO, the Supersonic Subatomic Java, promises to deliver small artifacts, extremely fast boot time, and lower time-to-first-request. Services: Java 8 Maven Eclipse MicroProfile Quarkus View the repository on GitHub. Spring Boot, Gradle, Mysql Spring Boot, Gradle, Mysql This project provides a starter kit for Spring Boot Gradle with MySQL projects hosted on Platform.sh. The Spring Framework is an application framework and inversion of control container for the Java platform. Services: Java 8 Gradle Spring Boot Oracle MySQL 8.0 View the repository on GitHub. Spring Boot, Maven, Mysql Spring Boot, Maven, Mysql This project provides a starter kit for Spring Boot Maven with MySQL projects hosted on Platform.sh. The Spring Framework is an application framework and inversion of control container for the Java platform. Services: Java 8 Maven Spring Boot Oracle MySQL 8.0 View the repository on GitHub. Spring MVC, Maven, MongoDB Spring MVC, Maven, MongoDB This project provides a starter kit for Spring MVC Maven with MongoDB projects hosted on Platform.sh. The Spring Framework is an application framework and inversion of control container for the Java platform. Services: Java 8 Maven Spring MVC MongoDB View the repository on GitHub. Spring, Kotlin, Maven Spring, Kotlin, Maven This project provides a starter kit for Spring Boot Maven with Kotlin projects hosted on Platform.sh. The Spring Framework is an application framework and inversion of control container for the Java platform. Services: Java 8 View the repository on GitHub. Thorntail Thorntail This project provides a starter kit for Thorntail Eclipse MicroProfile projects hosted on Platform.sh. Thorntail offers an innovative approach to packaging and running Java EE applications by packaging them with just enough of the server runtime to  java -jar  your application. It's MicroProfile compatible, too. Services: Java 8 Maven Eclipse MicroProfile Thorntail View the repository on GitHub. xwiki xwiki This project provides a starter kit for XWiki projects hosted on Platform.sh. XWiki is a free wiki software platform written in Java with a design emphasis on extensibility. XWiki is an enterprise wiki. It includes WYSIWYG editing, OpenDocument based document import/export, semantic annotations and tagging, and advanced permissions management. Services: Java 8 View the repository on GitHub.",
        "section": "Languages",
        "subsections": " Supported versions  OpenJDK versions:   Support libraries Support build automation Accessing services Project templates  ",
        "image": "",
        "url": "/languages/java.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "d5424971de8165dbf5a0caf913b7e384",
        "title": "Lisp",
        "description": "",
        "text": " Platform.sh supports building and deploying applications written in Lisp using Common Lisp (the SBCL version) with ASDF and Quick Lisp support. They are compiled during the Build phase, and support both committed dependencies and download-on-demand. Supported versions Grid Dedicated 1.5 None available To specify a Lisp container, use the type property in your .platform.app.yaml. type:\u0026#39;lisp:1.5\u0026#39; Assumptions Platform.sh is making assumptions about your application to provide a more streamlined experience. These assumptions are the following: Your .asd file is named like your system name. E.g. example.asd will have (defsystem example ...). Platform.sh will then run (asdf:make :example) on your system to build a binary. If you don’t want these assumptions, you can disable this behavior by specifying in your .platform.app.yaml: build:flavor:noneDependencies The recommended way to handle Lisp dependencies on Platform.sh is using ASDF. Commit a .asd file in your repository and the system will automatically download the dependencies using QuickLisp. QuickLisp options If you wish to change the distributions that QuickLisp is using, you can specify those as follows, specifying a distribution name, its URL and, an optional version: runtime:quicklisp:\u0026lt;distribution name\u0026gt;:url: ... version: ... For example: runtime:quicklisp:quicklisp:url:\u0026#39;http://beta.quicklisp.org/dist/quicklisp.txt\u0026#39;version:\u0026#39;2019-07-11\u0026#39;Platform.sh variables Platform.sh exposes relationships and other configuration as environment variables . Most notably, it allows a program to determine at runtime what HTTP port it should listen on and what the credentials are to access other services . To get the PORT environment variable (the port on which your web application is supposed to listen) you would: (parse-integer (uiop:getenv  PORT )) Building and running the application Assuming example.lisp and example.asd are present in your repository, the application will be automatically built on push. You can then start it from the web.commands.start directive. Note that the start command must run in the foreground. Should the program terminate for any reason it will be automatically restarted. In the example below we sleep for a very, very long time. You could also choose to join the thread of your web server, or use other methods to make sure the program does not terminate. The following basic .platform.app.yaml file is sufficient to run most Lisp applications. name:apptype:lisp:1.5web:commands:start:./examplelocations:/:allow:falsepassthru:truedisk:512Note that there will still be a proxy server in front of your application. If desired, certain paths may be served directly by our router without hitting your application (for static files, primarily) or you may route all requests to the Lisp application unconditionally, as in the example above. Accessing Services The services configuration is available in the environment variable PLATFORM_RELATIONSHIPS. To parse them, add the dependencies to your .asd file: :depends-on (:jsown :babel :s-base64) The following is an example of accessing a PostgreSQL instance: (defun relationships () (jsown:parse (babel:octets-to-string (with-input-from-string (in (uiop:getenv  PLATFORM_RELATIONSHIPS )) (s-base64:decode-base64-bytes in))))) Given a relationship defined in .platform.app.yaml: relationships:pg:postgresql:postgresqlThe following would access that relationship, and provide your Lisp program the credentials to connect to a PostgreSQL instance. Add this to your .asd file: :depends-on (:postmodern) Then in your program you could access the PostgreSQL instance as follows: (defvar *pg-spec* nil) (defun setup-postgresql () (let* ((pg-relationship (first (jsown:val (relationships)  pg ))) (database (jsown:val pg-relationship  path )) (username (jsown:val pg-relationship  username )) (password (jsown:val pg-relationship  password )) (host (jsown:val pg-relationship  host ))) (setf *pg-spec* (list database username password host))) (postmodern:with-connection *pg-spec* (unless (member  example_table  (postmodern:list-tables t) :test #\u0026#39;string=) (postmodern:execute  create table example_table ( a_field TEXT NOT NULL UNIQUE, another_field TEXT NOT NULL UNIQUE  )))) Project templates Platform.sh offers a project template for Lisp applications using the structure described above. It can be used as a starting point or reference for building your own website or web application. The following is a simple example of a Hunchentoot based web application (you can find the corresponding .asd and Platform.sh .yaml files in the linked Github repository): (defpackage #:example (:use :hunchentoot :cl-who :cl) (:export main)) (in-package #:example) (define-easy-handler (greet :uri  /hello ) (name) (with-html-output-to-string (s) (htm (:body (:h1  hello,   (str name)))))) (defun main () (let ((acceptor (make-instance \u0026#39;easy-acceptor :port (parse-integer (uiop:getenv  PORT ))))) (start acceptor) (sleep most-positive-fixnum))) Notice how we get the PORT from the environment, and how we sleep at the end, as (start acceptor) will immediately yield and Platform.sh requires applications to run in the foreground. Lisp Hunchentoot Lisp Hunchentoot This template is a simple Lisp Hunchentoot web server on Platform.sh. It includes a minimalist application for demonstration, but you are free to alter it as needed. Hunchentoot is a web server written in Common Lisp and at the same time a toolkit for building dynamic websites. Services: Lisp 1.5 View the repository on GitHub.",
        "section": "Languages",
        "subsections": " Supported versions Assumptions Dependencies QuickLisp options Platform.sh variables Building and running the application Accessing Services Project templates  ",
        "image": "",
        "url": "/languages/lisp.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "2efb1733847bc7285022ba88f1437dcb",
        "title": "New Relic",
        "description": "",
        "text": " Platform.sh supports New Relic APM for profiling PHP and Java applications. These instructions do not apply to other languages. On a Grid plan 1. Get your license key Sign up at https://newrelic.com and get your license key. 2. Add your license key Add your New Relic license key as a project level variable: platform variable:create --visible-build false php:newrelic.license --value \u0026#39;\u0026lt;your-new-relic-license-key\u0026gt;\u0026#39; PHP 3. Enable the New Relic extension Enable the New Relic extension in your .platform.app.yaml as follows: runtime: extensions: - newrelic Push the changes to your Platform.sh environment to enable New Relic as follows: git add .platform.app.yaml git commit -m  Enable New Relic.  git push That’s it! You need to wait a little bit for your New Relic dashboard to be generated. Java To set up New Relic in the Java project, we have two ways: Using the Maven project Download the code through .platform.app.yaml. Maven This section explains how to configure Maven to download and unzip the newrelic-java.zip file, which contains all New Relic Java agent components. To set up the application with New Relic, you have two options: Configuring and downloading from Maven Downloading on your own Configure your pom.xml to download newrelic-java.zip. For example: \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.newrelic.agent.java\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;newrelic-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;JAVA_AGENT_VERSION\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;type\u0026gt;zip\u0026lt;/type\u0026gt; \u0026lt;/dependency\u0026gt; Replace JAVA_AGENT_VERSION with the latest Java agent version 1 . Unzip newrelic-java.zip by configuring maven-dependency-plugin in your pom.xml. For example: \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-dependency-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.1\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;unpack-newrelic\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;unpack-dependencies\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;includeGroupIds\u0026gt;com.newrelic.agent.java\u0026lt;/includeGroupIds\u0026gt; \u0026lt;includeArtifactIds\u0026gt;newrelic-java\u0026lt;/includeArtifactIds\u0026gt; \u0026lt;excludes\u0026gt;**/newrelic.yml\u0026lt;/excludes\u0026gt; \u0026lt;overWriteReleases\u0026gt;false\u0026lt;/overWriteReleases\u0026gt; \u0026lt;overWriteSnapshots\u0026gt;false\u0026lt;/overWriteSnapshots\u0026gt; \u0026lt;overWriteIfNewer\u0026gt;true\u0026lt;/overWriteIfNewer\u0026gt; \u0026lt;outputDirectory\u0026gt;${project.build.directory}\u0026lt;/outputDirectory\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; The next step is to configure the .platform.app.yaml file to: Set the agent in the JVM parameters Overwrite the application file with the proper license key and application name. You can also do it using the API or the management console . Therefore this configuration will work outside the code very useful when the application is on a public repository. name:apptype:\u0026#39;java:8\u0026#39;disk:1024hooks:build:| mvn clean packagerm-rfnewrelic/mvtarget/newrelic/newrelic/mounts:\u0026#39;server/\u0026#39;:source:localsource_path:server_sourcevariables:env:NEW_RELIC_LICENSE_KEY:\u0026lt;LICENSE_KEY\u0026gt; NEW_RELIC_APP_NAME: \u0026lt;NAME_APPLICATION\u0026gt;web:commands:start:| java -jar Configuration To use this installation it is only required that you modify .platform.app.yaml , which will download and set the New Relic Java agent for you. name:apptype:\u0026#39;java:8\u0026#39;disk:1024variables:env:NEW_RELIC_URL:https://download.newrelic.com/newrelic/java-agent/newrelic-agent/current/newrelic-java.zipNEW_RELIC_LICENSE_KEY:\u0026lt;LICENSE_KEY\u0026gt; NEW_RELIC_APP_NAME: \u0026lt;NAME_APPLICATION\u0026gt;hooks:build:| mvn clean packagerm-rfnewreliccurl-O$NEW_RELIC_URLunzipnewrelic-java.zipweb:commands:start:| java -jar a Dedicated cluster Sign up at https://newrelic.com and get your license key. Then open a support ticket and let us know what your key is. Our support team will install it and let you know when it is complete. Troubleshoot Additionally, you can check that your application is properly connected to New Relic by looking at the /var/log/app.log file: platform log app 2017/04/19 14:00:16.706450 (93) Info: Reporting to: https://rpm.newrelic.com/accounts/xxx/applications/xxx 2017/04/19 14:00:16.706668 (93) Info: app \u0026#39;xxx-master-xxx.app\u0026#39; connected with run id \u0026#39;xxx\u0026#39;",
        "section": "Profiling",
        "subsections": " On a Grid plan  1. Get your license key 2. Add your license key PHP Java   On a Dedicated cluster Troubleshoot  ",
        "image": "",
        "url": "/integrations/profiling/new-relic.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "017ce07079a6d1ddd058801fae6a1278",
        "title": "Node.js",
        "description": "",
        "text": " Node.js is a popular JavaScript runtime built on Chrome’s V8 JavaScript engine. Platform.sh supports deploying Node.js applications quickly and easily. Using our Multi-App support you can build a micro-service oriented system mixing both Javascript and PHP applications. Supported versions Grid Dedicated 6 8 10 12 14 10 If you need other versions, take a look at our options for installing them with NVM . Deprecated versions Some versions with a minor (such as 8.9) are available but are not receiving security updates from upstream, so their use is not recommended. They will be removed at some point in the future. Grid Dedicated 0.12 4.7 4.8 6.1 6.9 8.2 9.8 Support libraries While it is possible to read the environment directly from your application, it is generally easier and more robust to use the platformsh-config NPM library which handles decoding of service credential information for you. Configuration To use Platform.sh and Node.js together, configure the .platform.app.yaml file with a few key settings, as described here (a complete example is included at the end). Specify the language of your application (available versions are listed above): type:'nodejs:14' Specify your dependencies under the nodejs key, like this: dependencies:nodejs:pm2: ^2.5.0 These are the global dependencies of your project (the ones you would have installed with npm install -g). Here we specify the pm2 process manager that will allow us to run the node process. Configure the command you use to start serving your application (this must be a foreground-running process) under the web section, e.g.: web:commands:start: PM2_HOME=/app/run pm2 start index.js --no-daemon If there is a package.json file present at the root of your repository, Platform.sh will automatically install the dependencies. We suggest including the platformsh-config helper npm module, which makes it trivial to access the running environment. {  dependencies : {  platformsh-config :  ^2.0.0  } } Note: If using the pm2 process manager to start your application, it is recommended that you do so directly in web.commands.start as described above, rather than by calling a separate script the contains that command. Calling pm2 start at web.commands.start from within a script, even with the --no-daemon flag, has been found to daemonize itself and block other processes (such as backups) with continuous respawns. Create any Read/Write mounts. The root file system is read only. You must explicitly describe writable mounts. In (3) we set the home of the process manager to /app/run so this needs to be writable. mounts:run:source:localsource_path:run Include any relevant commands needed to build and setup your application in the hooks section, e.g.: hooks:build:| npm installnpmrunbuildbowerinstall Setup the routes to your Node.js application in .platform/routes.yaml.  https://{default}/ :type:upstreamupstream: app:http  (Optional) If Platform.sh detects a package.json file in your repository, it will automatically include a default build flavor , that will run npm prune --userconfig .npmrc \u0026\u0026 npm install --userconfig .npmrc. You can modify that process to use an alternative package manager by including the following in your .platform.app.yaml file: build:flavor:noneConsult the documentation specific to Node.js builds for more information. Here’s a complete example that also serves static assets (.png from the /public directory): name:nodetype:nodejs:12web:commands:start: PM2_HOME=/app/run pm2 start index.js --no-daemon #in this setup you will find your application stdout and stderr in /app/run/logslocations: /public :passthru:falseroot: public # Whether to allow files not matching a rule or your application… Finally, make sure your Node.js application is configured to listen over the port given by the environment (here we use the platformsh helper and get it from config.port) that is available in the environment variable PORT. Here’s an example: // Load the http module to create an http server. const http = require('http'); // Load the Platform.sh configuration const config = require('platformsh-config').config(); const server = http.createServer(function (request, response) { response.writeHead(200, { Content-Type :  text/html }); response.end( \u003chtml\u003e\u003chead\u003e\u003ctitle\u003eHello Node.js\u003c/title\u003e\u003c/head\u003e\u003cbody\u003e\u003ch1\u003e\u003cimg src='public/js.png'\u003eHello Node.js\u003c/h1\u003e\u003ch3\u003ePlatform configuration:\u003c/h3\u003e\u003cpre\u003e +JSON.stringify(config, null, 4) +  \u003c/pre\u003e\u003c/body\u003e\u003c/html\u003e ); }); server.listen(config.port); Accessing services To access various services with Node.js, see the following examples. The individual service pages have more information on configuring each service. Elasticsearch Memcached MongoDB MySQL PostgreSQL Redis Solr const elasticsearch = require('elasticsearch'); const config = require( platformsh-config ).config(); exports.usageExample = async function() { const credentials = config.credentials('elasticsearch'); var client = new elasticsearch.Client({ host: `${credentials.host}:${credentials.port}`, }); let index = 'my_index'; let type = 'People'; // Index a few document. let names = ['Ada Lovelace', 'Alonzo Church', 'Barbara Liskov']; let message = { refresh:  wait_for , body: [] }; names.forEach((name) =\u003e { message.body.push({index: {_index: index, _type: type}}); message.body.push({name: name}); }); await client.bulk(message); // Search for documents. const response = await client.search({ index: index, q: 'name:Barbara Liskov' }); let output = ''; if(response.hits.total.value \u003e 0) { output += `\u003ctable\u003e \u003cthead\u003e \u003ctr\u003e\u003cth\u003eID\u003c/th\u003e\u003cth\u003eName\u003c/th\u003e\u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e`; response.hits.hits.forEach((record) =\u003e { output += }); output += } else { output =  No records found. ; } // Clean up after ourselves. response.hits.hits.forEach((record) =\u003e { client.delete({ index: index, type: type, id: record._id, }); }); return output; }; const Memcached = require('memcached'); const config = require( platformsh-config ).config(); const { promisify } = require('util'); exports.usageExample = async function() { const credentials = config.credentials('memcached'); let client = new Memcached(`${credentials.host}:${credentials.port}`); // The MemcacheD client is not Promise-aware, so make it so. const memcachedGet = promisify(client.get).bind(client); const memcachedSet = promisify(client.set).bind(client); let key = 'Deploy-day'; let value = 'Friday'; // Set a value. await memcachedSet(key, value, 10); // Read it back. let test = await memcachedGet(key); let output = `Found value \u003cstrong\u003e${test}\u003c/strong\u003e for key \u003cstrong\u003e${key}\u003c/strong\u003e.`; return output; }; const mongodb = require('mongodb'); const config = require( platformsh-config ).config(); exports.usageExample = async function() { const credentials = config.credentials('mongodb'); const MongoClient = mongodb.MongoClient; var client = await MongoClient.connect(config.formattedCredentials('mongodb', 'mongodb')); let db = client.db(credentials[ path ]); let collection = db.collection( startrek ); const documents = [ {'name': 'James Kirk', 'rank': 'Admiral'}, {'name': 'Jean-Luc Picard', 'rank': 'Captain'}, {'name': 'Benjamin Sisko', 'rank': 'Prophet'}, {'name': 'Katheryn Janeway', 'rank': 'Captain'}, ]; await collection.insert(documents, {w: 1}); let result = await collection.find({rank: Captain }).toArray(); let output = ''; output += `\u003ctable\u003e \u003cthead\u003e \u003ctr\u003e\u003cth\u003eName\u003c/th\u003e\u003cth\u003eRank\u003c/th\u003e\u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e`; Object.keys(result).forEach((key) =\u003e { output += }); output += // Clean up after ourselves. collection.remove(); return output; }; const mysql = require('mysql2/promise'); const config = require( platformsh-config ).config(); exports.usageExample = async function() { const credentials = config.credentials('database'); const connection = await mysql.createConnection({ host: credentials.host, port: credentials.port, user: credentials.username, password: credentials.password, database: credentials.path, }); let sql = ''; // Creating a table. sql = `CREATE TABLE IF NOT EXISTS People ( id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL )`; await connection.query(sql); // Insert data. sql = `INSERT INTO People (name, city) VALUES ('Neil Armstrong', 'Moon'), ('Buzz Aldrin', 'Glen Ridge'), ('Sally Ride', 'La Jolla');`; await connection.query(sql); // Show table. sql = `SELECT * FROM People`; let [rows] = await connection.query(sql); let output = ''; if (rows.length \u003e 0) { output +=`\u003ctable\u003e \u003cthead\u003e \u003ctr\u003e\u003cth\u003eName\u003c/th\u003e\u003cth\u003eCity\u003c/th\u003e\u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e`; rows.forEach((row) =\u003e { output += }); output += } // Drop table. sql = `DROP TABLE People`; await connection.query(sql); return output; }; const pg = require('pg'); const config = require( platformsh-config ).config(); exports.usageExample = async function() { const credentials = config.credentials('postgresql'); const client = new pg.Client({ host: credentials.host, port: credentials.port, user: credentials.username, password: credentials.password, database: credentials.path, }); client.connect(); let sql = ''; // Creating a table. sql = `CREATE TABLE IF NOT EXISTS People ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL )`; await client.query(sql); // Insert data. sql = `INSERT INTO People (name, city) VALUES ('Neil Armstrong', 'Moon'), ('Buzz Aldrin', 'Glen Ridge'), ('Sally Ride', 'La Jolla');`; await client.query(sql); // Show table. sql = `SELECT * FROM People`; let result = await client.query(sql); let output = ''; if (result.rows.length \u003e 0) { output +=`\u003ctable\u003e \u003cthead\u003e \u003ctr\u003e\u003cth\u003eName\u003c/th\u003e\u003cth\u003eCity\u003c/th\u003e\u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e`; result.rows.forEach((row) =\u003e { output += }); output += } // Drop table. sql = `DROP TABLE People`; await client.query(sql); return output; }; const redis = require('redis'); const config = require( platformsh-config ).config(); const { promisify } = require('util'); exports.usageExample = async function() { const credentials = config.credentials('redis'); var client = redis.createClient(credentials.port, credentials.host); // The Redis client is not Promise-aware, so make it so. const redisGet = promisify(client.get).bind(client); const redisSet = promisify(client.set).bind(client); let key = 'Deploy day'; let value = 'Friday'; // Set a value. await redisSet(key, value); // Read it back. let test = await redisGet(key); let output = `Found value \u003cstrong\u003e${test}\u003c/strong\u003e for key \u003cstrong\u003e${key}\u003c/strong\u003e.`; return output; }; const solr = require('solr-node'); const config = require( platformsh-config ).config(); exports.usageExample = async function() { let client = new solr(config.formattedCredentials('solr', 'solr-node')); let output = ''; // Add a document. let addResult = await client.update({ id: 123, name: 'Valentina Tereshkova', }); output +=  Adding one document. Status (0 is success):   + addResult.responseHeader.status +  \u003cbr // Flush writes so that we can query against them. await client.softCommit(); // Select one document: let strQuery = client.query().q(); let writeResult = await client.search(strQuery); output +=  Selecting documents (1 expected):   + writeResult.response.numFound +  \u003cbr // Delete one document. let deleteResult = await client.delete({id: 123}); output +=  Deleting one document. Status (0 is success):   + deleteResult.responseHeader.status +  \u003cbr return output; }; Project templates A number of project templates for Node.js applications and typical configurations are available on GitHub. Not all of them are proactively maintained but all can be used as a starting point or reference for building your own website or web application. Express Express This template builds the Express framework for Platform.sh. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. Express is a minimalist web framework written in Node.js. Services: Node.js 10 MariaDB 10.2 View the repository on GitHub. Gatsby Gatsby This template builds a simple application using Gatsby hosted on Platform.sh. Gatsby is a free and open source framework based on React that helps developers build blazing fast websites and apps. Services: Node.js View the repository on GitHub. Gatsby with Wordpress Gatsby with Wordpress This template builds a multi-app project using Gatsby as its frontend and a Wordpress to store content. Gatsby is a free and open source framework based on React that helps developers build statically-generated websites and apps, and WordPress is a blogging and lightweight CMS written in PHP. Services: Node.js 12 PHP 7.3 MariaDB 10.4 View the repository on GitHub. Koa Koa This template builds a Koa project on Platform.sh. Koa is a lightweight web microframework for Node.js. Services: Node.js 10 MariaDB 10.2 View the repository on GitHub. Node.js Node.js This template builds a simple application using the Node.js built-in `http` web server. It includes a minimalist application skeleton for demonstration, but you are free to alter it as needed. Node.js is an open-source JavaScript runtime built on Chrome's V8 JavaScript engine. Services: Node.js 10 MariaDB 10.4 View the repository on GitHub. Probot Probot This template builds a simple GitHub App using Probot. Probot is a framework for building GitHub Apps in Node.js. Services: Node.js 12 View the repository on GitHub. strapi strapi This template builds a Strapi backend for Platform.sh. It does not include a frontend application, but you can add one of your choice and access Strapi by defining it in a relationship in your frontend's .platform.app.yaml file. Strapi is a Headless CMS framework written in Node.js. Services: Node.js 12 PostgreSQL 11 View the repository on GitHub.",
        "section": "Languages",
        "subsections": " Supported versions Deprecated versions Support libraries Configuration In your application\u0026hellip; Accessing services Project templates  ",
        "image": "",
        "url": "/languages/nodejs.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "026e36b88aa3e09df907be8c1fdce24d",
        "title": "PHP",
        "description": "",
        "text": " Supported versions Grid Dedicated 7.2 7.3 7.4 7.2 7.3 Note that as of PHP 7.1 we use the Zend Thread Safe (ZTS) version of PHP. To specify a PHP container, use the type property in your .platform.app.yaml. type:'php:7.4' Deprecated versions The following versions are available but are not receiving security updates from upstream, so their use is not recommended. They will be removed at some point in the future. Grid Dedicated 5.4 5.5 5.6 7.0 7.1 5.6 7.0 7.1 Support libraries While it is possible to read the environment directly from your application, it is generally easier and more robust to use the platformsh/config-reader Composer library which handles decoding of service credential information for you. Alternate start commands PHP is most commonly run in a CGI mode, using PHP-FPM. That is the default on Platform.sh. However, you can also start alternative processes if desired, such as if you’re running an Async PHP daemon, a thread-based worker process, etc. To do so, simply specify an alternative start command in platform.app.yaml, similar to the following: web:commands:start:phprun.phpupstream:socket_family:tcpprotocol:httpThe above configuration will execute the run.php script in the application root when the container starts using the PHP-CLI SAPI, just before the deploy hook runs, but will not launch PHP-FPM. It will also tell the front-controller (Nginx) to connect to your application via a TCP socket, which will be specified in the PORT environment variable. Note that the start command must run in the foreground. If not specified, the effective default start command varies by PHP version: On PHP 5.x, it’s /usr/sbin/php5-fpm. On PHP 7.0, it’s /usr/sbin/php-fpm7.0. On PHP 7.1, it’s /usr/sbin/php-fpm7.1-zts. On PHP 7.2, it’s /usr/sbin/php-fpm7.2-zts. On PHP 7.3, it’s /usr/sbin/php-fpm7.3-zts. On PHP 7.4, it’s /usr/sbin/php-fpm7.4-zts. While you can call it manually that is generally not necessary. Note that PHP-FPM cannot run simultaneously along with another persistent process (such as ReactPHP or Amp). If you need both they will have to run in separate containers. Expanded dependencies In addition to the standard dependencies format, it is also possible to specify alternative repositories for use by Composer. The standard format like so: dependencies:php: platformsh/client :  dev-master is equivalent to composer require platform/client dev-master. However, you can also specify explicit require and repositories blocks: dependencies:php:require: platformsh/client :  dev-master repositories:- type:vcsurl: git@github.com:platformsh/platformsh-client-php.git That would install platformsh/client from the alternate repository specified, as a global dependency. That is, it is equivalent to the following composer.json file: {  repositories : [ {  type :  vcs ,  url :  git@github.com:platformsh/platformsh-client-php.git  } ],  require : {  platformsh/client :  dev-master  } } That allows you to install a forked version of a global dependency from a custom repository. Opcache preloading PHP 7.4 introduced a new feature called Opcache Preloading, which allows you to load selected files into shared memory when PHP-FPM starts. That means functions and classes in those files are always available and do not need to be autoloaded, at the cost of any changes to those files requiring a PHP-FPM restart. Since PHP-FPM restarts anyway when a new deploy happens this feature is a major win on Platform.sh, and we recommend using it aggressively. To enable preloading, add a php.ini value that specifies a preload script. Any php.ini mechanism will work, but using a variable in .platform.app.yaml is the recommended approach: variables:php:opcache.preload:'preload.php'The opcache.preload value is evaluated as a file path relative to the application root (where .platform.app.yaml is), and it may be any PHP script that calls opcache_compile_file(). The following example will preload all .php files anywhere in the vendor directory: $directory = new RecursiveDirectoryIterator(getenv('PLATFORM_APP_DIR') . '/vendor'); $iterator = new RecursiveIteratorIterator($directory); $regex = new RegexIterator($iterator, RecursiveRegexIterator::GET_MATCH); foreach ($regex as $key =\u003e $file) { // This is the important part! opcache_compile_file($file[0]); } Note: Preloading all .php files may not be optimal for your application, and may even introduce errors. Your application framework may provide recommendations or a pre-made presload script to use instead. Determining an optimal preloading strategy is the user’s responsibility. FFI PHP 7.4 introduced support for Foreign Function Interfaces (FFI), which allows user-space code to bridge to existing C-ABI-compatible libraries. FFI is fully supported on Platform.sh. Note: FFI is only intended for advanced use cases, and is rarely a net win for routine web requests. Use with caution. There are a few steps to leveraging FFI: Enable the FFI extension in .platform.app.yaml: runtime:extensions:- ffi Specify a preload file in which you can call FFI::load(). Using FFI::load() in preload will be considerably faster than loading the linked library on each request or script run. Ensure the library is available locally, but not in a web-accessible directory. .so files may included in your repository, downloaded i your build hook, or compiled in your build hook. If compiling C code, gcc is available by default. If compiling Rust code, you can download the Rust compiler in the build hook . For running FFI from the command line, you will need to enable the opcache for command line scripts in addition to the preloader. The standard pattern for the command would be php -d opcache.preload= your-preload-script.php  -d opcache.enable_cli=true your-cli-script.php. A working FFI example is available online for both C and Rust. Debug PHP-FPM If you want to inspect what’s going on with PHP-FPM, you can install this small CLI : dependencies:php:wizaplace/php-fpm-status-cli: ^1.0 Then when you are connected to your project over SSH you can run: $ php-fpm-status --socket=unix://$SOCKET --path=/-/status --full Accessing services To access various services with PHP, see the following examples. The individual service pages have more information on configuring each service. Elasticsearch Memcached MongoDB MySQL PostgreSQL RabbitMQ Redis Solr \u003c?php declare(strict_types=1); use use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // Get the credentials to connect to the Elasticsearch service. $credentials = $config-\u003ecredentials('elasticsearch'); try { // The Elasticsearch library lets you connect to multiple hosts. // On Platform.sh Standard there is only a single host so just // register that. $hosts = [ [ 'scheme' =\u003e $credentials['scheme'], 'host' =\u003e $credentials['host'], 'port' =\u003e $credentials['port'], ] ]; // Create an Elasticsearch client object. $builder = ClientBuilder::create(); $builder-\u003esetHosts($hosts); $client = $builder-\u003ebuild(); $index = 'my_index'; $type = 'People'; // Index a few document. $params = [ 'index' =\u003e $index, 'type' =\u003e $type, ]; $names = ['Ada Lovelace', 'Alonzo Church', 'Barbara Liskov']; foreach ($names as $name) { $params['body']['name'] = $name; $client-\u003eindex($params); } // Force just-added items to be indexed. $client-\u003eindices()-\u003erefresh(array('index' =\u003e $index)); // Search for documents. $result = $client-\u003esearch([ 'index' =\u003e $index, 'type' =\u003e $type, 'body' =\u003e [ 'query' =\u003e [ 'match' =\u003e [ 'name' =\u003e 'Barbara Liskov', ], ], ], ]); if (isset($result['hits']['hits'])) { print \u003c\u003c\u003cTABLE\u003ctable\u003e \u003cthead\u003e \u003ctr\u003e\u003cth\u003eID\u003c/th\u003e\u003cth\u003eName\u003c/th\u003e\u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e TABLE; foreach ($result['hits']['hits'] as $record) { $record['_id'], $record['_source']['name']); } print } // Delete documents. $params = [ 'index' =\u003e $index, 'type' =\u003e $type, ]; $ids = array_map(function($row) { return $row['_id']; }, $result['hits']['hits']); foreach ($ids as $id) { $params['id'] = $id; $client-\u003edelete($params); } } catch (Exception $e) { print $e-\u003egetMessage(); } \u003c?php declare(strict_types=1); use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // Get the credentials to connect to the Memcached service. $credentials = $config-\u003ecredentials('memcached'); try { // Connecting to Memcached server. $memcached = new Memcached(); $memcached-\u003eaddServer($credentials['host'], $credentials['port']); $memcached-\u003esetOption(Memcached::OPT_BINARY_PROTOCOL, true); $key =  Deploy day ; $value =  Friday ; // Set a value. $memcached-\u003eset($key, $value); // Read it back. $test = $memcached-\u003eget($key); printf('Found value \u003cstrong\u003e%s\u003c/strong\u003e for key \u003cstrong\u003e%s\u003c/strong\u003e.', $test, $key); } catch (Exception $e) { print $e-\u003egetMessage(); } \u003c?php declare(strict_types=1); use use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // The 'database' relationship is generally the name of primary database of an application. // It could be anything, though, as in the case here here where it's called  mongodb . $credentials = $config-\u003ecredentials('mongodb'); try { $server = sprintf('%s://%s:%s@%s:%d/%s', $credentials['scheme'], $credentials['username'], $credentials['password'], $credentials['host'], $credentials['port'], $credentials['path'] ); $client = new Client($server); $collection = $client-\u003emain-\u003estarwars; $result = $collection-\u003einsertOne([ 'name' =\u003e 'Rey', 'occupation' =\u003e 'Jedi', ]); $id = $result-\u003egetInsertedId(); $document = $collection-\u003efindOne([ '_id' =\u003e $id, ]); // Clean up after ourselves. $collection-\u003edrop(); printf( Found %s (%s)\u003cbr $document-\u003ename, $document-\u003eoccupation); } catch $e) { print $e-\u003egetMessage(); } \u003c?php declare(strict_types=1); use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // The 'database' relationship is generally the name of primary SQL database of an application. // That's not required, but much of our default automation code assumes it. $credentials = $config-\u003ecredentials('database'); try { // Connect to the database using PDO. If using some other abstraction layer you would // inject the values from $database into whatever your abstraction layer asks for. $dsn = sprintf('mysql:host=%s;port=%d;dbname=%s', $credentials['host'], $credentials['port'], $credentials['path']); $conn = new $credentials['username'], $credentials['password'], [ // Always use Exception error mode with PDO, as it's more reliable. =\u003e // So we don't have to mess around with cursors and unbuffered queries by default. =\u003e TRUE, // Make sure MySQL returns all matched rows on update queries including // rows that actually didn't have to be updated because the values didn't // change. This matches common behavior among other database systems. =\u003e TRUE, ]); // Creating a table. $sql =  CREATE TABLE People ( id INT(6) UNSIGNED AUTO_INCREMENT PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL ) ; $conn-\u003equery($sql); // Insert data. $sql =  INSERT INTO People (name, city) VALUES ('Neil Armstrong', 'Moon'), ('Buzz Aldrin', 'Glen Ridge'), ('Sally Ride', 'La Jolla'); ; $conn-\u003equery($sql); // Show table. $sql =  SELECT * FROM People ; $result = $conn-\u003equery($sql); if ($result) { print \u003c\u003c\u003cTABLE\u003ctable\u003e \u003cthead\u003e \u003ctr\u003e\u003cth\u003eName\u003c/th\u003e\u003cth\u003eCity\u003c/th\u003e\u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e TABLE; foreach ($result as $record) { $record-\u003ename, $record-\u003ecity); } print } // Drop table $sql =  DROP TABLE People ; $conn-\u003equery($sql); } catch $e) { print $e-\u003egetMessage(); } \u003c?php declare(strict_types=1); use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // The 'database' relationship is generally the name of primary SQL database of an application. // It could be anything, though, as in the case here here where it's called  postgresql . $credentials = $config-\u003ecredentials('postgresql'); try { // Connect to the database using PDO. If using some other abstraction layer you would // inject the values from $database into whatever your abstraction layer asks for. $dsn = sprintf('pgsql:host=%s;port=%d;dbname=%s', $credentials['host'], $credentials['port'], $credentials['path']); $conn = new $credentials['username'], $credentials['password'], [ // Always use Exception error mode with PDO, as it's more reliable. =\u003e // So we don't have to mess around with cursors and unbuffered queries by default. ]); $conn-\u003equery( DROP TABLE IF EXISTS People ); // Creating a table. $sql =  CREATE TABLE IF NOT EXISTS People ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL ) ; $conn-\u003equery($sql); // Insert data. $sql =  INSERT INTO People (name, city) VALUES ('Neil Armstrong', 'Moon'), ('Buzz Aldrin', 'Glen Ridge'), ('Sally Ride', 'La Jolla'); ; $conn-\u003equery($sql); // Show table. $sql =  SELECT * FROM People ; $result = $conn-\u003equery($sql); if ($result) { print \u003c\u003c\u003cTABLE\u003ctable\u003e \u003cthead\u003e \u003ctr\u003e\u003cth\u003eName\u003c/th\u003e\u003cth\u003eCity\u003c/th\u003e\u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e TABLE; foreach ($result as $record) { $record-\u003ename, $record-\u003ecity); } print } // Drop table. $sql =  DROP TABLE People ; $conn-\u003equery($sql); } catch $e) { print $e-\u003egetMessage(); } \u003c?php declare(strict_types=1); use use use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // Get the credentials to connect to the RabbitMQ service. $credentials = $config-\u003ecredentials('rabbitmq'); try { $queueName = 'deploy_days'; // Connect to the RabbitMQ server. $connection = new AMQPStreamConnection($credentials['host'], $credentials['port'], $credentials['username'], $credentials['password']); $channel = $connection-\u003echannel(); $channel-\u003equeue_declare($queueName, false, false, false, false); $msg = new AMQPMessage('Friday'); $channel-\u003ebasic_publish($msg, '', 'hello'); echo  [x] Sent // In a real application you't put the following in a separate script in a loop. $callback = function ($msg) { printf( [x] Deploying on %s\u003cbr $msg-\u003ebody); }; $channel-\u003ebasic_consume($queueName, '', false, true, false, false, $callback); // This blocks on waiting for an item from the queue, so comment it out in this demo script. //$channel-\u003ewait(); $channel-\u003eclose(); $connection-\u003eclose(); } catch (Exception $e) { print $e-\u003egetMessage(); } \u003c?php declare(strict_types=1); use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // Get the credentials to connect to the Redis service. $credentials = $config-\u003ecredentials('redis'); try { // Connecting to Redis server. $redis = new Redis(); $redis-\u003econnect($credentials['host'], $credentials['port']); $key =  Deploy day ; $value =  Friday ; // Set a value. $redis-\u003eset($key, $value); // Read it back. $test = $redis-\u003eget($key); printf('Found value \u003cstrong\u003e%s\u003c/strong\u003e for key \u003cstrong\u003e%s\u003c/strong\u003e.', $test, $key); } catch (Exception $e) { print $e-\u003egetMessage(); } \u003c?php declare(strict_types=1); use use // Create a new config object to ease reading the Platform.sh environment variables. // You can alternatively use getenv() yourself. $config = new Config(); // Get the credentials to connect to the Solr service. $credentials = $config-\u003ecredentials('solr'); try { $config = [ 'endpoint' =\u003e [ 'localhost' =\u003e [ 'host' =\u003e $credentials['host'], 'port' =\u003e $credentials['port'], 'path' =\u003e  /  . $credentials['path'], ] ] ]; $client = new Client($config); // Add a document $update = $client-\u003ecreateUpdate(); $doc1 = $update-\u003ecreateDocument(); $doc1-\u003eid = 123; $doc1-\u003ename = 'Valentina Tereshkova'; $update-\u003eaddDocuments(array($doc1)); $update-\u003eaddCommit(); $result = $client-\u003eupdate($update); print  Adding one document. Status (0 is success):   .$result-\u003egetStatus().  \u003cbr // Select one document $query = $client-\u003ecreateQuery($client::QUERY_SELECT); $resultset = $client-\u003eexecute($query); print  Selecting documents (1 expected):   .$resultset-\u003egetNumFound() .  \u003cbr // Delete one document $update = $client-\u003ecreateUpdate(); $update-\u003eaddDeleteById(123); $update-\u003eaddCommit(); $result = $client-\u003eupdate($update); print  Deleting one document. Status (0 is success):   .$result-\u003egetStatus().  \u003cbr } catch (Exception $e) { print $e-\u003egetMessage(); } Runtime configuration It is possible to change the PHP-FPM runtime configuration via the runtime block on your .platform.app.yaml. The PHP-FPM options below are configurable: request_terminate_timeout - The timeout for serving a single request after which the PHP-FPM worker process will be killed. That is separate from the PHP runtime’s max_execution_time ini option, which is preferred. This option may be used if the PHP process is dying without cleaning up properly and causing the FPM process to hang. runtime:request_terminate_timeout:300 Project templates A number of project templates for major PHP applications are available on GitHub. Not all of them are proactively maintained but all can be used as a starting point or reference for building your own website or web application. Backdrop Backdrop This template builds a Backdrop site, with the entire site committed to Git. Backdrop is a PHP-based CMS, originally forked from Drupal 7. Services: PHP 7.3 MariaDB 10.4 View the repository on GitHub. Basic PHP Basic PHP This template provides the most basic configuration for running a custom PHP project. PHP is a high-performance scripting language especially well suited to web development. Services: PHP 7.3 MariaDB 10.2 View the repository on GitHub. Drupal 8 Drupal 8 This template builds Drupal 8 using the  Drupal Recommended  Composer project. It also includes configuration to use Redis for caching, although that must be enabled post-install in `.platform.app.yaml`. Drupal is a flexible and extensible PHP-based CMS framework. Services: PHP 7.4 MariaDB 10.4 Redis 5.0 View the repository on GitHub. Drupal 8 Multisite Drupal 8 Multisite This template builds Drupal 8 in a multisite configuration using the  Drupal Recommended  Composer project. It also includes configuration to use Redis for caching, although that must be enabled post-install per-site. Drupal is a flexible and extensible PHP-based CMS framework capable of hosting multiple sites on a single code base. Services: PHP 7.4 MariaDB 10.4 Redis 5.0 View the repository on GitHub. Drupal 9 Drupal 9 This template builds Drupal 9 Beta using the  Drupal Recommended  Composer project. It also includes configuration to use Redis for caching, although that must be enabled post-install in `.platform.app.yaml`. Drupal is a flexible and extensible PHP-based CMS framework. Services: PHP 7.4 MariaDB 10.4 Redis 5.0 View the repository on GitHub. GovCMS 8 GovCMS 8 This template builds the Australian government's GovCMS Drupal 8 distribution using the Drupal Composer project for better flexibility. It also includes configuration to use Redis for caching, although that must be enabled post-install in .platform.app.yaml. GovCMS is a Drupal distribution built for the Australian government, and includes configuration optimized for managing government websites. Services: PHP 7.2 MariaDB 10.2 Redis 5.0 View the repository on GitHub. Laravel Laravel This template provides a basic Laravel skeleton. It comes pre-configured to use a MariaDB database and Redis for caching and sessions. Laravel is an opinionated, integrated rapid-application-development framework for PHP. Services: PHP 7.3 MariaDB 10.2 Redis 5.0 View the repository on GitHub. Magento 2 Community Edition Magento 2 Community Edition This template builds Magento 2 CE on Platform.sh. It includes additional scripts to customize Magento to run effectively in a build-and-deploy environment. Magento is a fully integrated ecommerce system and web store written in PHP. This is the Open Source version. Services: PHP 7.2 MariaDB 10.2 Redis 3.2 View the repository on GitHub. Mautic Mautic TThis template provides a basic Mautic installation. Mautic is an Open SOurce marketing automation tool built on Symfony. Services: PHP 7.2 MariaDB 10.4 RabbitMQ 3.7 View the repository on GitHub. Nextcloud Nextcloud This template builds Nextcloud on Platform.sh. Nextcloud is a PHP-based groupware server with installable apps, file synchronization, and federated storage. An adminstrative user will be created automatically. See the deploy log after the project is installed for the name and password. Services: PHP 7.4 MariaDB 10.4 Redis 5.0 View the repository on GitHub. Opigno Opigno This template builds the Opigno Drupal 8 distribution using the Drupal Composer project for better flexibility. It also includes configuration to use Redis for caching, although that must be enabled post-install in .platform.app.yaml. Opigno is a Learning Management system built as a Drupal distribution. Services: PHP 7.3 MariaDB 10.4 Redis 5.0 View the repository on GitHub. Pimcore Pimcore Pimcore Digital Platform for Enterprises Services: PHP 7.4 MariaDB 10.2 Redis 5.0 View the repository on GitHub. Sculpin Sculpin This template provides a basic Sculpin skeleton. All files are generated at build time, so at runtime only static files need to be served. Sculpin is a static site generator written in PHP and using the Twig templating engine. Services: PHP 7.3 View the repository on GitHub. Symfony 3 Symfony 3 This template provides a basic Symfony 3 skeleton. It is configured for Production mode by default so the usual Symfony  welcome  page will not appear. Symfony is a high-performance loosely-coupled PHP web development framework. Version 3 is the legacy support version. Services: PHP 7.2 MariaDB 10.2 View the repository on GitHub. Symfony 4 Symfony 4 This template provides a basic Symfony 4 skeleton. It is configured for Production mode by default so the usual Symfony  welcome  page will not appear. That can be adjusted in .platform.app.yaml. Symfony is a high-performance loosely-coupled PHP web development framework. Services: PHP 7.3 MariaDB 10.4 View the repository on GitHub. Symfony 5 Symfony 5 This template provides a basic Symfony 5 skeleton. It is configured for Production mode by default so the usual Symfony  welcome  page will not appear. That can be adjusted in .platform.app.yaml. Symfony is a high-performance loosely-coupled PHP web development framework. Services: PHP 7.3 MariaDB 10.4 View the repository on GitHub. TYPO3 TYPO3 This template provides a basic TYPO3 installation. TYPO3 is a PHP-based Content Management System Services: PHP 7.4 MariaDB 10.4 Redis 5.0 View the repository on GitHub. Wordpress Wordpress This template builds WordPress on Platform.sh using the johnbolch/wordpress  Composer Fork  of WordPress. Plugins and themes should be managed with Composer exclusively. WordPress is a blogging and lightweight CMS written in PHP. Services: PHP 7.3 MariaDB 10.2 View the repository on GitHub.",
        "section": "Languages",
        "subsections": " Supported versions Deprecated versions Support libraries Alternate start commands Expanded dependencies Opcache preloading FFI Debug PHP-FPM Accessing services Runtime configuration Project templates  ",
        "image": "",
        "url": "/languages/php.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "b8f4f5f57583a0b82bbac52256f544cf",
        "title": "Python",
        "description": "",
        "text": " Platform.sh supports deploying Python applications. Your application can use WSGI-based (Gunicorn / uWSGI) application server, Tornado, Twisted, or Python 3.5\u0026#43; asyncio server. Supported Grid Dedicated 2.7 3.5 3.6 3.7 3.8 None available Support libraries While it is possible to read the environment directly from your application, it is generally easier and more robust to use the platformshconfig pip library which handles decoding of service credential information for you. WSGI-based configuration In this example, we use Gunicorn to run our WSGI application. Configure the .platform.app.yaml file with a few key settings as listed below, a complete example is included at the end of this section. Specify the language of your application (available versions are listed above): type:\u0026#39;python:3.8\u0026#39; Build your application with the build hook. Assuming you have your pip dependencies stored in requirements.txt and a setup.py at the root of your application folder to execute build steps: hooks:build:| pip install -r requirements.txtpipinstall-e.pipinstallgunicornThese are installed as global dependencies in your environment. Configure the command you use to start serving your application (this must be a foreground-running process) under the web section, e.g.: web:commands:start: gunicorn -b 0.0.0.0:$PORT project.wsgi:application This assumes the WSGI file is project/wsgi.py and the WSGI application object is named application in the WSGI file. Define the web locations your application is using: web:locations: / :root:  passthru:trueallow:false /static :root: static/ allow:trueThis configuration asks our web server to handle HTTP requests at “/static” to serve static files stored in /app/static/ folder while everything else is forwarded to your application server. Create any Read/Write mounts. The root file system is read only. You must explicitly describe writable mounts. mounts:tmp:source:localsource_path:tmplogs:source:localsource_path:logsThis setting allows your application writing files to /app/tmp and have logs stored in /app/logs. Then, set up the routes to your application in .platform/routes.yaml.  https://{default}/ :type:upstreamupstream: app:http Here is the complete .platform.app.yaml file: name:apptype:python:2.7web:commands:start: gunicorn -b $PORT project.wsgi:application locations: / :root:  passthru:trueallow:false /static :root: static/ allow:truehooks:build:| pip install -r requirements.txtpipinstall-e.pipinstallgunicornmounts:tmp:source:localsource_path:tmplogs:source:localsource_path:logsdisk:512Using the asyncio module The above Gunicorn based WSGI example can be modified to use the Python 3.5\u0026#43; asyncio module. Change the type to python:3.6. Change the start command to use asyncio. web:commands:start: gunicorn -b $PORT -k gaiohttp project.wsgi:application  Add aiohttp as pip dependency in your build hook. hooks:build:| pip install -r requirements.txtpipinstall-e.pipinstallgunicornaiohttp Accessing services To access various services with Python, see the following examples. The individual service pages have more information on configuring each service. Elasticsearch Kafka Memcached MongoDB MySQL PostgreSQL RabbitMQ Redis Solr import elasticsearch from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the Elasticsearch service. credentials = config.credentials(\u0026#39;elasticsearch\u0026#39;) try: # The Elasticsearch library lets you connect to multiple hosts. # On Platform.sh Standard there is only a single host so just register that. hosts = {  scheme : credentials[\u0026#39;scheme\u0026#39;],  host : credentials[\u0026#39;host\u0026#39;],  port : credentials[\u0026#39;port\u0026#39;] } # Create an Elasticsearch client object. client = elasticsearch.Elasticsearch([hosts]) # Index a few documents es_index = \u0026#39;my_index\u0026#39; es_type = \u0026#39;People\u0026#39; params = {  index : es_index,  type : es_type,  body : { name : \u0026#39;\u0026#39;} } names = [\u0026#39;Ada Lovelace\u0026#39;, \u0026#39;Alonzo Church\u0026#39;, \u0026#39;Barbara Liskov\u0026#39;] ids = {} for name in names: params[\u0026#39;body\u0026#39;][\u0026#39;name\u0026#39;] = name ids[name] = client.index(index=params[ index ], doc_type=params[ type ], body=params[\u0026#39;body\u0026#39;]) # Force just-added items to be indexed. client.indices.refresh(index=es_index) # Search for documents. result = client.search(index=es_index, body={ \u0026#39;query\u0026#39;: { \u0026#39;match\u0026#39;: { \u0026#39;name\u0026#39;: \u0026#39;Barbara Liskov\u0026#39; } } }) table = \u0026#39;\u0026#39;\u0026#39;\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;ID\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt;\u0026#39;\u0026#39;\u0026#39; if result[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]: for record in result[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]: table \u0026#43;= record[\u0026#39;_source\u0026#39;][\u0026#39;name\u0026#39;]) table \u0026#43;= # Delete documents. params = {  index : es_index,  type : es_type, } for name in names: client.delete(index=params[\u0026#39;index\u0026#39;], doc_type=params[\u0026#39;type\u0026#39;], id=ids[name][\u0026#39;_id\u0026#39;]) return table except Exception as e: return e from json import dumps from json import loads from kafka import KafkaConsumer, KafkaProducer from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the Kafka service. credentials = config.credentials(\u0026#39;kafka\u0026#39;) try: kafka_server = \u0026#39;{}:{}\u0026#39;.format(credentials[\u0026#39;host\u0026#39;], credentials[\u0026#39;port\u0026#39;]) # Producer producer = KafkaProducer( bootstrap_servers=[kafka_server], value_serializer=lambda x: dumps(x).encode(\u0026#39;utf-8\u0026#39;) ) for e in range(10): data = {\u0026#39;number\u0026#39; : e} producer.send(\u0026#39;numtest\u0026#39;, value=data) # Consumer consumer = KafkaConsumer( bootstrap_servers=[kafka_server], auto_offset_reset=\u0026#39;earliest\u0026#39; ) consumer.subscribe([\u0026#39;numtest\u0026#39;]) output = \u0026#39;\u0026#39; # For demonstration purposes so it doesn\u0026#39;t block. for e in range(10): message = next(consumer) output \u0026#43;= str(loads(message.value.decode(\u0026#39;UTF-8\u0026#39;))[ number ]) \u0026#43; \u0026#39;, \u0026#39; # What a real implementation would do instead. # for message in consumer: # output \u0026#43;= loads(message.value.decode(\u0026#39;UTF-8\u0026#39;))[ number ] return output except Exception as e: return e import pymemcache from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the Memcached service. credentials = config.credentials(\u0026#39;memcached\u0026#39;) try: # Try connecting to Memached server. memcached = pymemcache.Client((credentials[\u0026#39;host\u0026#39;], credentials[\u0026#39;port\u0026#39;])) memcached.set(\u0026#39;Memcached::OPT_BINARY_PROTOCOL\u0026#39;, True) key =  Deploy_day  value =  Friday  # Set a value. memcached.set(key, value) # Read it back. test = memcached.get(key) return \u0026#39;Found value \u0026lt;strong\u0026gt;{0}\u0026lt;/strong\u0026gt; for key \u0026lt;strong\u0026gt;{1}\u0026lt;/strong\u0026gt;.\u0026#39;.format(test.decode( utf-8 ), key) except Exception as e: return e from pymongo import MongoClient from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # The \u0026#39;database\u0026#39; relationship is generally the name of primary SQL database of an application. # It could be anything, though, as in the case here here where it\u0026#39;s called  mongodb . credentials = config.credentials(\u0026#39;mongodb\u0026#39;) try: formatted = config.formatted_credentials(\u0026#39;mongodb\u0026#39;, \u0026#39;pymongo\u0026#39;) server = \u0026#39;{0}://{1}:{2}@{3}\u0026#39;.format( credentials[\u0026#39;scheme\u0026#39;], credentials[\u0026#39;username\u0026#39;], credentials[\u0026#39;password\u0026#39;], formatted ) client = MongoClient(server) collection = client.main.starwars post = {  name :  Rey ,  occupation :  Jedi  } post_id = collection.insert_one(post).inserted_id document = collection.find_one( { _id : post_id} ) # Clean up after ourselves. collection.drop() return \u0026#39;Found {0} ({1})\u0026lt;br /\u0026gt;\u0026#39;.format(document[\u0026#39;name\u0026#39;], document[\u0026#39;occupation\u0026#39;]) except Exception as e: return e import pymysql from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # The \u0026#39;database\u0026#39; relationship is generally the name of primary SQL database of an application. # That\u0026#39;s not required, but much of our default automation code assumes it.\u0026#39; credentials = config.credentials(\u0026#39;database\u0026#39;) try: # Connect to the database using PDO. If using some other abstraction layer you would inject the values # from `database` into whatever your abstraction layer asks for. conn = pymysql.connect(host=credentials[\u0026#39;host\u0026#39;], port=credentials[\u0026#39;port\u0026#39;], database=credentials[\u0026#39;path\u0026#39;], user=credentials[\u0026#39;username\u0026#39;], password=credentials[\u0026#39;password\u0026#39;]) sql = \u0026#39;\u0026#39;\u0026#39; CREATE TABLE People ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL ) \u0026#39;\u0026#39;\u0026#39; cur = conn.cursor() cur.execute(sql) sql = \u0026#39;\u0026#39;\u0026#39; INSERT INTO People (name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La Jolla\u0026#39;); \u0026#39;\u0026#39;\u0026#39; cur.execute(sql) # Show table. sql = \u0026#39;\u0026#39;\u0026#39;SELECT * FROM People\u0026#39;\u0026#39;\u0026#39; cur.execute(sql) result = cur.fetchall() table = \u0026#39;\u0026#39;\u0026#39;\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt;\u0026#39;\u0026#39;\u0026#39; if result: for record in result: table \u0026#43;= record[2]) table \u0026#43;= # Drop table sql = \u0026#39;\u0026#39;\u0026#39;DROP TABLE People\u0026#39;\u0026#39;\u0026#39; cur.execute(sql) # Close communication with the database cur.close() conn.close() return table except Exception as e: return e import psycopg2 from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # The \u0026#39;database\u0026#39; relationship is generally the name of primary SQL database of an application. # That\u0026#39;s not required, but much of our default automation code assumes it.\u0026#39; database = config.credentials(\u0026#39;postgresql\u0026#39;) try: # Connect to the database. conn_params = { \u0026#39;host\u0026#39;: database[\u0026#39;host\u0026#39;], \u0026#39;port\u0026#39;: database[\u0026#39;port\u0026#39;], \u0026#39;dbname\u0026#39;: database[\u0026#39;path\u0026#39;], \u0026#39;user\u0026#39;: database[\u0026#39;username\u0026#39;], \u0026#39;password\u0026#39;: database[\u0026#39;password\u0026#39;] } conn = psycopg2.connect(**conn_params) # Open a cursor to perform database operations. cur = conn.cursor() cur.execute( DROP TABLE IF EXISTS People ) # Creating a table. sql = \u0026#39;\u0026#39;\u0026#39; CREATE TABLE IF NOT EXISTS People ( id SERIAL PRIMARY KEY, name VARCHAR(30) NOT NULL, city VARCHAR(30) NOT NULL ) \u0026#39;\u0026#39;\u0026#39; cur.execute(sql) # Insert data. sql = \u0026#39;\u0026#39;\u0026#39; INSERT INTO People (name, city) VALUES (\u0026#39;Neil Armstrong\u0026#39;, \u0026#39;Moon\u0026#39;), (\u0026#39;Buzz Aldrin\u0026#39;, \u0026#39;Glen Ridge\u0026#39;), (\u0026#39;Sally Ride\u0026#39;, \u0026#39;La Jolla\u0026#39;); \u0026#39;\u0026#39;\u0026#39; cur.execute(sql) # Show table. sql = \u0026#39;\u0026#39;\u0026#39;SELECT * FROM People\u0026#39;\u0026#39;\u0026#39; cur.execute(sql) result = cur.fetchall() table = \u0026#39;\u0026#39;\u0026#39;\u0026lt;table\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;Name\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;City\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt;\u0026#39;\u0026#39;\u0026#39; if result: for record in result: table \u0026#43;= record[2]) table \u0026#43;= # Drop table sql =  DROP TABLE People  cur.execute(sql) # Close communication with the database cur.close() conn.close() return table except Exception as e: return e import pika from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the RabbitMQ service. credentials = config.credentials(\u0026#39;rabbitmq\u0026#39;) try: # Connect to the RabbitMQ server creds = pika.PlainCredentials(credentials[\u0026#39;username\u0026#39;], credentials[\u0026#39;password\u0026#39;]) parameters = pika.ConnectionParameters(credentials[\u0026#39;host\u0026#39;], credentials[\u0026#39;port\u0026#39;], credentials=creds) connection = pika.BlockingConnection(parameters) channel = connection.channel() # Check to make sure that the recipient queue exists channel.queue_declare(queue=\u0026#39;deploy_days\u0026#39;) # Try sending a message over the channel channel.basic_publish(exchange=\u0026#39;\u0026#39;, routing_key=\u0026#39;deploy_days\u0026#39;, body=\u0026#39;Friday!\u0026#39;) # Receive the message def callback(ch, method, properties, body): print(  [x] Received {} .format(body)) # Tell RabbitMQ that this particular function should receive messages from our \u0026#39;hello\u0026#39; queue channel.basic_consume(\u0026#39;deploy_days\u0026#39;, callback, auto_ack=False) # This blocks on waiting for an item from the queue, so comment it out in this demo script. # print(\u0026#39; [*] Waiting for messages. To exit press CTRL\u0026#43;C\u0026#39;) # channel.start_consuming() connection.close() return   [x] Sent \u0026#39;Friday!\u0026#39;\u0026lt;br/\u0026gt;  except Exception as e: return e from redis import Redis from platformshconfig import Config def usage_example(): # Create a new config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the Redis service. credentials = config.credentials(\u0026#39;redis\u0026#39;) try: redis = Redis(credentials[\u0026#39;host\u0026#39;], credentials[\u0026#39;port\u0026#39;]) key =  Deploy day  value =  Friday  # Set a value redis.set(key, value) # Read it back test = redis.get(key) return \u0026#39;Found value \u0026lt;strong\u0026gt;{0}\u0026lt;/strong\u0026gt; for key \u0026lt;strong\u0026gt;{1}\u0026lt;/strong\u0026gt;.\u0026#39;.format(test.decode( utf-8 ), key) except Exception as e: return e import pysolr from xml.etree import ElementTree as et from platformshconfig import Config def usage_example(): # Create a new Config object to ease reading the Platform.sh environment variables. # You can alternatively use os.environ yourself. config = Config() # Get the credentials to connect to the Solr service. credentials = config.credentials(\u0026#39;solr\u0026#39;) try: formatted_url = config.formatted_credentials(\u0026#39;solr\u0026#39;, \u0026#39;pysolr\u0026#39;) # Create a new Solr Client using config variables client = pysolr.Solr(formatted_url) # Add a document message = \u0026#39;\u0026#39; doc_1 = {  id : 123,  name :  Valentina Tereshkova  } result0 = client.add([doc_1]) client.commit() message \u0026#43;= \u0026#39;Adding one document. Status (0 is success): {0} \u0026lt;br /\u0026gt;\u0026#39;.format(et.fromstring(result0)[0][0].text) # Select one document query = client.search(\u0026#39;*:*\u0026#39;) message \u0026#43;= documents (1 expected): {0} \u0026lt;br /\u0026gt;\u0026#39;.format(str(query.hits)) # Delete one document result1 = client.delete(doc_1[\u0026#39;id\u0026#39;]) client.commit() message \u0026#43;= one document. Status (0 is success): {0}\u0026#39;.format(et.fromstring(result1)[0][0].text) return message except Exception as e: return e Project templates A number of project templates for Python applications are available on GitHub. Not all of them are proactively maintained but all can be used as a starting point or reference for building your own website or web application. Basic Python 2 Basic Python 2 This template provides the most basic configuration for running a custom Python 2.7 project. It launches a Python application directly rather than using a runner. Python is a general purpose scripting language often used in web development. Services: Python 2 MariaDB 10.2 Redis 5 View the repository on GitHub. Basic Python 3 Basic Python 3 This template provides the most basic configuration for running a custom Python 3.7 project. It launches a Python application directly rather than using a runner. Python is a general purpose scripting language often used in web development. Services: Python 3 MariaDB 10.2 Redis 5 View the repository on GitHub. Django 1 Django 1 This template builds Django 1 on Platform.sh, using the gunicorn application runner. New projects should be built using Django 2, but this project is a reference for existing migrating sites. Django is a Python-based web application framework with a built-in ORM. Version 1 is the legacy support version. Services: Python 2 PostgreSQL 10 View the repository on GitHub. Django 2 Django 2 This template builds Django 2 on Platform.sh, using the gunicorn application runner. Django is a Python-based web application framework with a built-in ORM. Services: Python 3.7 PostgreSQL 10 View the repository on GitHub. Django 3 Django 3 This template builds Django 3 on Platform.sh, using the gunicorn application runner. Django is a Python-based web application framework with a built-in ORM. Services: Python 3.7 PostgreSQL 10 View the repository on GitHub. Flask Flask This template builds a Flask project on Platform.sh, run natively without a separate runner. Flask is a lightweight web microframework for Python. Services: Python 3.7 MariaDB 10.2 Redis 5.0 View the repository on GitHub. MoinMoin MoinMoin This template builds a Moin Moin wiki on Platform.sh. The project doesn\u0026#39;t include Moin Moin itself; rather, it includes build and deploy scripts that will download Moin Moin on the fly. Moin Moin is a Python-based Wiki system that uses flat files on disk for storage. Services: Python 2 View the repository on GitHub. Pelican Pelican This template provides a basic Pelican skeleton. All files are generated at build time, so at runtime only static files need to be served. Pelican is a static site generator written in Python and using Jinja for templating. Services: Python 3.7 View the repository on GitHub. Pyramid Pyramid This template builds Pyramid on Platform.sh. It includes some basic example code to demonstrate how to connect to the database. Pyramid is a web framework written in Python. Services: Python 3.7 View the repository on GitHub. Python 3 running UWSGI Python 3 running UWSGI This template provides the most basic configuration for running a custom Python 3.7 project. It launches the application using the UWSGI application runner. Python is a general purpose scripting language often used in web development. Services: Python 3.7 MariaDB 10.2 Redis 5.0 View the repository on GitHub. Wagtail Wagtail This template builds the Wagtail CMS on Platform.sh, using the gunicorn application runner. Wagtail is a web CMS built using the Django framework for Python. Services: Python 3.7 PostgreSQL 10 View the repository on GitHub.",
        "section": "Languages",
        "subsections": " Supported Support libraries WSGI-based configuration Using the asyncio module Accessing services Project templates  ",
        "image": "",
        "url": "/languages/python.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "07b0599aecee035e107a19f17cdcc6d1",
        "title": "Ruby",
        "description": "",
        "text": " Platform.sh supports deploying any Ruby application. Your application can use any Ruby application server such as Unicorn or Puma and deploying a Rails or a Sinatra app is very straight forward. Supported versions Ruby MRI Grid Dedicated 2.3 2.4 2.5 2.6 2.7 None available Unicorn based Rails configuration In this example, we use Unicorn to run our Ruby application. You could use any Ruby application server such as Puma or Thin. Configure the .platform.app.yaml file with a few key settings as listed below, a complete example is included at the end of this section. Specify the language of your application (available versions are listed above): type:\u0026#39;ruby:2.7\u0026#39; Build your application with the build hook. Assuming you have your dependencies stored in the Gemfile at the root of your application folder to execute build steps: hooks:build:bundleinstall--withoutdevelopmenttestdeploy:RACK_ENV=productionbundleexecrakedb:migrateThese are installed as your project dependencies in your environment. You can also use the dependencies key to install global dependencies theses can be Ruby, Python, NodeJS or PHP libraries. Configure the command you use to start serving your application (this must be a foreground-running process) under the web section, e.g.: web:upstream:socket_family:unixcommands:start: unicorn -l $SOCKET -E production config.ru This assumes you have Unicorn as a dependency in your Gemfile # Use Unicorn as the app server group :production do gem \u0026#39;unicorn\u0026#39; end and that you have a rackup file config.ru at the root of your repository, for example for a rails application you would put: require  rubygems  require ::File.expand_path(\u0026#39;../config/environment\u0026#39;, __FILE__) run Rails.application Define the web locations your application is using: web:locations: / :root: public passthru:trueexpires:1hallow:trueThis configuration asks our web server to handle HTTP requests at “/static” to serve static files stored in /app/static/ folder while everything else are forwarded to your application server. Create any Read/Write mounts. The root file system is read only. You must explicitly describe writable mounts. mounts:tmp:source:localsource_path:tmplogs:source:localsource_path:logsThis setting allows your application writing files to /app/tmp and have logs stored in /app/logs. You can define other read/write mounts (your application code itself being deployed to a read-only file system). Note that the file system is persistent, and when you backup your cluster these mounts get backed-up too. Then, setup the routes to your application in .platform/routes.yaml.  https://{default}/ :type:upstream# the first part should be your project nameupstream: app:http Here is the complete .platform.app.yaml file: name:\u0026#39;app\u0026#39;type: ruby:2.7 web:upstream:socket_family:unixcommands:start: unicorn -l $SOCKET -E production config.ru locations: / :root: public passthru:trueexpires:1hallow:truerelationships:database: database:mysql disk:2048hooks:build:bundleinstall--withoutdevelopmenttestdeploy:RACK_ENV=productionbundleexecrakedb:migratemounts:tmp:source:localsource_path:tmplogs:source:localsource_path:logs Configuring services In this example we assue in the relationships key that we have a mysql instance. To configure it we need to create a .platform/services.yaml with for eample: database:type:mysql:10.4disk:2048 Connecting to services You can define services in your environment. And, link to the services using .platform.app.yaml: relationships:database: mysqldb:mysql By using the following ruby function calls, you can obtain the database details. require  base64  require  json  relationships= JSON.parse(Base64.decode64(ENV[\u0026#39;PLATFORM_RELATIONSHIPS\u0026#39;])) Which should give you something like: {  database  : [ {  path  :  main ,  query  : {  is_master  : true },  port  : 3306,  username  :  user ,  password  :   ,  host  :  database.internal ,  ip  :  246.0.241.50 ,  scheme  :  mysql  } ] } Project templates A number of project templates for Ruby applications and typical configurations are available on GitHub. Not all of them are proactively maintained but all can be used as a starting point or reference for building your own website or web application. Platform.sh also provides a helper library for Ruby applications that simplifies presenting environment information to your application. It is not required to run Ruby applications on Platform.sh but is recommended. Ruby on Rails Ruby on Rails This template builds Ruby on Rails 5 on Platform.sh. It includes a bridge library that will auto-configure most databases and services. Rails is an opinionated rapid application development framework written in Ruby. Services: Ruby 2.6 Postgresql 11 View the repository on GitHub.",
        "section": "Languages",
        "subsections": " Supported versions  Ruby MRI   Unicorn based Rails configuration Configuring services Connecting to services Project templates  ",
        "image": "",
        "url": "/languages/ruby.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "36f0c5b59b7cfdce4688bb02ab9eb45f",
        "title": "Tideways",
        "description": "",
        "text": " Platform.sh supports Tideways APM for PHP. This functionality is only available on PHP 7.0 and later. Note: The upstream now maintains two versions for Tideways, and both plugins are available on Platform.sh: tideways_xhprof: The open source version, therefore no licensing is required (On the downside, less integration services are available). You can use it in combination with xhprof UI. tideways: The bundle proprietary full version of the product and plugins, which the rest of the guide is mostly aimed to cover. Get Started 1. Get your license key Sign up at https://tideways.com and get your license key. 2. Add your license key Add your Tideways license key as a project level variable: platform variable:create --visible-build false php:tideways.api_key --value \u0026#39;\u0026lt;your-license-key\u0026gt;\u0026#39; 3. Enable the Tideways extension Enable the Tideways extension in your .platform.app.yaml as follows: runtime:extensions:- tidewaysEnabling the extension will also activate the Tideways background process. Push the changes to your Platform.sh environment to enable Tideways as follows: git add .platform.app.yaml git commit -m  Enable Tideways.  git push Tideways should now be enabled. Give it a few hours to a day to get a decent set of data before checking your Tideways dashboard. Deployment Integration Tideways integrates with Platform.sh deployment hooks and provides performance comparisons before and after deployments were released. You can find the Platform.sh CLI command to register this hook for your application in Tideways “Application Settings” screen under the section “Exports \u0026amp; Integrations”. Here is an example: platform integration:add --type=webhook --url= https://app.tideways.io/api/events/external/1234/abcdefghijklmnopqrstuvwxyz1234567890 ",
        "section": "Profiling",
        "subsections": " Get Started  1. Get your license key 2. Add your license key 3. Enable the Tideways extension   Deployment Integration  ",
        "image": "",
        "url": "/integrations/profiling/tideways.html",
        "relurl": ""
    },
    {
        "site": "docs",
        "source": "primary",
        "rank": 1,
        "documentId": "cd3db3fee01d619f1561605536c55ac2",
        "title": "Webhooks",
        "description": "",
        "text": " Webhooks allow you to host a script yourself externally that receives the same payload as an activity script and responds to the same events, but can be hosted on your own server in your own language. Setup platform integration:add --type=webhook --url=A-URL-THAT-CAN-RECEIVE-THE-POSTED-JSON The webhook URL will receive a POST message for every “Activity” that is triggered, and the message will contain complete information about the entire state of the project at that time. In practice most of the message can be ignored but is available if needed. The most commonly used values are documented below. It’s also possible to set the integration to only send certain activity types, or only activities on certain branches. The CLI will prompt you to specify which to include or exclude. Leave at the default values to get all events on all environments in a project. Webhook schema See the activity script reference for a description of the webhook payload. Validate the integration You can then verify that your integration is functioning properly using the CLI command platform integration:validate",
        "section": "Activity scripts",
        "subsections": " Setup Webhook schema Validate the integration  ",
        "image": "",
        "url": "/integrations/activity/webhooks.html",
        "relurl": ""
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "42a9b6c9ab0332d8918387c519417562b185340a",
        "title": "How to deny abusive IP addresses on Platform.sh",
        "description": "Goal To block a certain set of abusive IP addresses before they hit the application layer. This can be IPs identified to cause high load or otherwise harm the system, e.g. crawlers / bots, malicious scripts, hacking attempts, or IPs that should be blocked for any other operational reason. Assumptions Access to a project hosted on https://platform.sh/ Administrator rights on the project Knowledge on using the and the web UI Problems Sometimes a website can be stressed by traffic originating from IP addresses that are causing high load or any other negative impact on the application environment. In some cases it makes sense for those IP addresses to be added to a deny liset in order to stabilize system resource usage. In other cases certain IP ranges might need to be blocked for a different operational reason. http://platform.sh/ allows for such a deny list to be configured either through the CLI, or through the project’s web UI. Depending on personal taste or technical familiarity level, there are 2 ways to go about configuring IP deny list on the project. Steps (Via the CLI) If the Platform CLI is preferred, these are the steps to take in order to block IP addresses. 1. Log in to Platform CLI $ platform login 2. Find the project ID Note the project ID of the project to enable the deny list for: $ platform project:list 3. Find the environment ID Note the environment ID for the environment on the project that needs securing: $ platform environment:list -p 4. Create the deny list Run the following command to create the deny list and include the IP that needs to be blocked: platform httpaccess --access deny: -p -e (optional: --no-wait) Note: If multiple IPs are meant to be denied, this entire block has to be repeated: --access deny: Example: platform httpaccess --access deny: --access deny: ",
        "text": "Goal To block a certain set of abusive IP addresses before they hit the application layer. This can be IPs identified to cause high load or otherwise harm the system, e.g. crawlers / bots, malicious scripts, hacking attempts, or IPs that should be blocked for any other operational reason. Assumptions Access to a project hosted on https://platform.sh/ Administrator rights on the project Knowledge on using the and the web UI Problems Sometimes a website can be stressed by traffic originating from IP addresses that are causing high load or any other negative impact on the application environment. In some cases it makes sense for those IP addresses to be added to a deny liset in order to stabilize system resource usage. In other cases certain IP ranges might need to be blocked for a different operational reason. http://platform.sh/ allows for such a deny list to be configured either through the CLI, or through the project’s web UI. Depending on personal taste or technical familiarity level, there are 2 ways to go about configuring IP deny list on the project. Steps (Via the CLI) If the Platform CLI is preferred, these are the steps to take in order to block IP addresses. 1. Log in to Platform CLI $ platform login 2. Find the project ID Note the project ID of the project to enable the deny list for: $ platform project:list 3. Find the environment ID Note the environment ID for the environment on the project that needs securing: $ platform environment:list -p 4. Create the deny list Run the following command to create the deny list and include the IP that needs to be blocked: platform httpaccess --access deny: -p -e (optional: --no-wait) Note: If multiple IPs are meant to be denied, this entire block has to be repeated: --access deny: Example: platform httpaccess --access deny: --access deny: ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-deny-abusive-ip-addresses-on-platform-sh/169",
        "relurl": "/t/how-to-deny-abusive-ip-addresses-on-platform-sh/169"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "fda5eba3f5ac3251c123c5a2e1c3dbb0af771537",
        "title": "How to overwrite Spring Data MongoDB variable to access Platform.sh services",
        "description": "Goal In this tutorial, we’ll cover how you can overwrite https://spring.io/ https://spring.io/projects/spring-data-mongodb configurations to access the services in http://Platform.sh. Preparation This tutorial assumes you have A working Spring Data MongoDB application with Java 8 or higher A text editor of your choice. Problems http://Platform.sh has a https://github.com/platformsh/config-reader-java that provides a streamlined and easy to use way to interact with a http://Platform.sh environment and its https://docs.platform.sh/configuration/services.html. However, you can also use the application regularly and overwrite those configurations when you deploy your application on http://Platform.sh. That is useful when you either already have one app and want to move to http://Platform.sh or keep the default configuration to run locally. Steps To keep the configurations Spring has, by default, the application.properties file where you can set the settings that you wish on your application. Furthermore, . Give a Spring Data MongoDB application that you’re running locally with either an empty or non applications.properties file: You can overwrite those configurations on the platform.app.yaml the application configuration to http://Platform.sh. As shown in the configuration below. name: app type: \"java:11\" disk: 1024 hooks: build: mvn clean install relationships: database: 'mongodb:mongodb' # The configuration of app when it is exposed to the web. web: commands: start: | export USER=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].username\"` export PASSWORD=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].password\"` export HOST=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].host\"` export DATABASE=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].path\"` java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError \\ -Dspring.data.mongodb.database=$DATABASE \\ -Dspring.data.mongodb.host=$HOST \\ -Dspring.data.mongodb.username=$USER \\ -Dspring.data.mongodb.password=$PASSWORD \\ target/spring-boot.jar --server.port=$PORT Therefore, you can have the configuration or just migrate the application that already exists to http://Platform.sh. References https://spring.io/ https://spring.io/projects/spring-data-mongodb https://docs.spring.io/spring-boot/docs/current/reference/html/appendix-application-properties.html https://docs.platform.sh/frameworks/spring.html https://github.com/platformsh-examples/java-overwrite-configuration/tree/master/spring-mongodb ",
        "text": "Goal In this tutorial, we’ll cover how you can overwrite https://spring.io/ https://spring.io/projects/spring-data-mongodb configurations to access the services in http://Platform.sh. Preparation This tutorial assumes you have A working Spring Data MongoDB application with Java 8 or higher A text editor of your choice. Problems http://Platform.sh has a https://github.com/platformsh/config-reader-java that provides a streamlined and easy to use way to interact with a http://Platform.sh environment and its https://docs.platform.sh/configuration/services.html. However, you can also use the application regularly and overwrite those configurations when you deploy your application on http://Platform.sh. That is useful when you either already have one app and want to move to http://Platform.sh or keep the default configuration to run locally. Steps To keep the configurations Spring has, by default, the application.properties file where you can set the settings that you wish on your application. Furthermore, . Give a Spring Data MongoDB application that you’re running locally with either an empty or non applications.properties file: You can overwrite those configurations on the platform.app.yaml the application configuration to http://Platform.sh. As shown in the configuration below. name: app type: \"java:11\" disk: 1024 hooks: build: mvn clean install relationships: database: 'mongodb:mongodb' # The configuration of app when it is exposed to the web. web: commands: start: | export USER=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].username\"` export PASSWORD=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].password\"` export HOST=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].host\"` export DATABASE=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].path\"` java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError \\ -Dspring.data.mongodb.database=$DATABASE \\ -Dspring.data.mongodb.host=$HOST \\ -Dspring.data.mongodb.username=$USER \\ -Dspring.data.mongodb.password=$PASSWORD \\ target/spring-boot.jar --server.port=$PORT Therefore, you can have the configuration or just migrate the application that already exists to http://Platform.sh. References https://spring.io/ https://spring.io/projects/spring-data-mongodb https://docs.spring.io/spring-boot/docs/current/reference/html/appendix-application-properties.html https://docs.platform.sh/frameworks/spring.html https://github.com/platformsh-examples/java-overwrite-configuration/tree/master/spring-mongodb ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-overwrite-spring-data-mongodb-variable-to-access-platform-sh-services/528",
        "relurl": "/t/how-to-overwrite-spring-data-mongodb-variable-to-access-platform-sh-services/528"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "ba3b8c6cf146e8c479811339d69b6cdf41e2c1a4",
        "title": "How to flush the Fastly cache",
        "description": "Goal To purge all cached assets from the CDN layer sitting in front of your site. Assumptions You will need: To have Fastly cache running in front of your site The ability to run curl from your local terminal, Working access to the platform command for your project OR Read access to the Variables on your project through the web console. Problems For most Enterprise or Premium plans, The http://Platform.sh setup team has configured Fastly CDN for you, and manages it on your behalf with a built-in account. This means you do not have direct administration access using the Fastly web UI, where a ‘Purge’ button can be found. However, you have been issued with an “API” key that allows you access to many actions on your account, and you can initiate a purge using this. To purge a single page Fastly lets you . curl -X PURGE $url That’s it! To Purge all You need use the API key. All projects with Fastly configured should have had the env:FASTLY_API_TOKEN set in their /master/settings/variables Some older dedicated plans do not have it here, and the API token is instead stored in a text file on the server. If you can’t find the value in the environment variables, check for a file called /mnt/shared/fastly_tokens.txt on your server. This key can be used to construct a purge_all API request. The to construct a request like this: POST /service/SU1Z0isxPaozGVKXdv0eY/purge_all HTTP/1.1 Fastly-Key: YOUR_FASTLY_TOKEN Accept: application/json So, from a local CLI, run this sequence to retrieve your keys: (or you can just set the variables to the values you can see in the web console yourself) FASTLY_SERVICE_ID=$(platform --project=$PROJECTID --environment=master variable:get env:FASTLY_SERVICE_ID --pipe) FASTLY_API_TOKEN=$(platform --project=$PROJECTID --environment=master variable:get env:FASTLY_API_TOKEN --pipe) # Legacy: If these environment variables do not exist on your installation, try looking for a file called `/mnt/shared/fastly_tokens.txt` instead. FASTLY_API_URL=\"https://api.fastly.com\" curl -H \"Fastly-Key: $FASTLY_API_TOKEN\" \\ -X POST \"${FASTLY_API_URL}/service/${FASTLY_SERVICE_ID}/purge_all\" eg: curl -H Fastly-Key: nfDDFUahPwV4wQiJOMbLwJsCBy3Wh4TQ -X POST https://api.fastly.com/service/nOBmPqIK5M71NgZMhLYTZk/purge_all {\"status\":\"ok\"} Verify it really worked To empirically test, you could retrieve a page from your site and see if it looks right , or immediately make a network request and check the cache headers and dates: curl -I -s $url HTTP/2 200 ... cache-control: max-age=3600, public last-modified: Fri, 06 Mar 2020 01:36:16 GMT date: Fri, 06 Mar 2020 02:26:54 GMT ... via: 1.1 varnish age: 0 x-served-by: cache-syd10142-SYD, cache-hkg17927-HKG x-cache: MISS, MISS The x-served-by and via confirm that a cache is working. Unless someone else has already requested this page just ahead of you, you should see a MISS in the x-cache. Either way, the age (in seconds) should be low, and the date should be fresh. Ignore the last-modified Verify via API (just FYI) To verify the command ran as expected, you can retrieve a list of the , with a command like: curl -H \"Fastly-Key: $FASTLY_API_TOKEN\" -X GET \"${FASTLY_API_URL}/events?page[number]=1\u0026page[size]=10\" …which should return a response including something like: { \"id\": \"miaXP6k1OLwUdOIXcKkiHG\", \"type\": \"event\", \"attributes\": { \"customer_id\": \"rSam6DCAgeyDU7gSoC5pJi\", \"description\": \"Purge all was performed\", \"event_type\": \"service.purge_all\", \"ip\": \"118.173.60.218, 118.173.60.218\", \"metadata\": {}, \"service_id\": \"ZknOBmPqIK5M71NgZMhLYT\", \"user_id\": \"TdqCvp120Tm185r3gn4DYR\", \"created_at\": \"2020-03-06T01:35:35Z\", \"admin\": false } }, Conclusion This is a quick method to flush your CDN cache as needed. There are many other actions possible through the https://docs.fastly.com/api/ though not all will be available to the account granted with that API key. If you are using a CMS like Drupal, then a https://www.drupal.org/project/fastly can be installed to integrate cache management into your CMS. This provides an easier interface to actions like this. This approach also lets the CMS invoke a CDN purge when things change, so is recommended if you need to tune things finer.",
        "text": "Goal To purge all cached assets from the CDN layer sitting in front of your site. Assumptions You will need: To have Fastly cache running in front of your site The ability to run curl from your local terminal, Working access to the platform command for your project OR Read access to the Variables on your project through the web console. Problems For most Enterprise or Premium plans, The http://Platform.sh setup team has configured Fastly CDN for you, and manages it on your behalf with a built-in account. This means you do not have direct administration access using the Fastly web UI, where a ‘Purge’ button can be found. However, you have been issued with an “API” key that allows you access to many actions on your account, and you can initiate a purge using this. To purge a single page Fastly lets you . curl -X PURGE $url That’s it! To Purge all You need use the API key. All projects with Fastly configured should have had the env:FASTLY_API_TOKEN set in their /master/settings/variables Some older dedicated plans do not have it here, and the API token is instead stored in a text file on the server. If you can’t find the value in the environment variables, check for a file called /mnt/shared/fastly_tokens.txt on your server. This key can be used to construct a purge_all API request. The to construct a request like this: POST /service/SU1Z0isxPaozGVKXdv0eY/purge_all HTTP/1.1 Fastly-Key: YOUR_FASTLY_TOKEN Accept: application/json So, from a local CLI, run this sequence to retrieve your keys: (or you can just set the variables to the values you can see in the web console yourself) FASTLY_SERVICE_ID=$(platform --project=$PROJECTID --environment=master variable:get env:FASTLY_SERVICE_ID --pipe) FASTLY_API_TOKEN=$(platform --project=$PROJECTID --environment=master variable:get env:FASTLY_API_TOKEN --pipe) # Legacy: If these environment variables do not exist on your installation, try looking for a file called `/mnt/shared/fastly_tokens.txt` instead. FASTLY_API_URL=\"https://api.fastly.com\" curl -H \"Fastly-Key: $FASTLY_API_TOKEN\" \\ -X POST \"${FASTLY_API_URL}/service/${FASTLY_SERVICE_ID}/purge_all\" eg: curl -H Fastly-Key: nfDDFUahPwV4wQiJOMbLwJsCBy3Wh4TQ -X POST https://api.fastly.com/service/nOBmPqIK5M71NgZMhLYTZk/purge_all {\"status\":\"ok\"} Verify it really worked To empirically test, you could retrieve a page from your site and see if it looks right , or immediately make a network request and check the cache headers and dates: curl -I -s $url HTTP/2 200 ... cache-control: max-age=3600, public last-modified: Fri, 06 Mar 2020 01:36:16 GMT date: Fri, 06 Mar 2020 02:26:54 GMT ... via: 1.1 varnish age: 0 x-served-by: cache-syd10142-SYD, cache-hkg17927-HKG x-cache: MISS, MISS The x-served-by and via confirm that a cache is working. Unless someone else has already requested this page just ahead of you, you should see a MISS in the x-cache. Either way, the age (in seconds) should be low, and the date should be fresh. Ignore the last-modified Verify via API (just FYI) To verify the command ran as expected, you can retrieve a list of the , with a command like: curl -H \"Fastly-Key: $FASTLY_API_TOKEN\" -X GET \"${FASTLY_API_URL}/events?page[number]=1\u0026page[size]=10\" …which should return a response including something like: { \"id\": \"miaXP6k1OLwUdOIXcKkiHG\", \"type\": \"event\", \"attributes\": { \"customer_id\": \"rSam6DCAgeyDU7gSoC5pJi\", \"description\": \"Purge all was performed\", \"event_type\": \"service.purge_all\", \"ip\": \"118.173.60.218, 118.173.60.218\", \"metadata\": {}, \"service_id\": \"ZknOBmPqIK5M71NgZMhLYT\", \"user_id\": \"TdqCvp120Tm185r3gn4DYR\", \"created_at\": \"2020-03-06T01:35:35Z\", \"admin\": false } }, Conclusion This is a quick method to flush your CDN cache as needed. There are many other actions possible through the https://docs.fastly.com/api/ though not all will be available to the account granted with that API key. If you are using a CMS like Drupal, then a https://www.drupal.org/project/fastly can be installed to integrate cache management into your CMS. This provides an easier interface to actions like this. This approach also lets the CMS invoke a CDN purge when things change, so is recommended if you need to tune things finer.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-flush-the-fastly-cache/484",
        "relurl": "/t/how-to-flush-the-fastly-cache/484"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "b36b50ada8f8f87e2781ab63f8b134cc299e82e0",
        "title": "How to Check my Fastly CDN peformance (Cache hit ratio)",
        "description": "Goal To review how effective the cache layer is, you may want to check the “Hit Ratio” of the content it is serving. A busy read-only site with its cache preferences properly configured can be seen to serve percentages in the high 90’s; while if you see a cache ratio down below 60%, the server is probably working pretty hard, and some things can be done - either at the app level, of just in the Response-Headers, to improve cache-ability and responsiveness. Assumptions You will need: A http://Platform.sh plan with managed Fastly cache enabled. This usually means an “Enterprise Dedicated” plan. Read access to your hosting server - the ability to ssh in and read a file there. This is best done by installing and using the platform CLI tool, though plain ssh work as well. Access to the curl tool. if you don’t have it installed locally, you can use it when ssh-ed in to your instance. Challenge For managed-CDN plans, http://Platform.sh looks after your Fastly subscriptions. Which is great, because cache management is hard. But this does mean that you don’t get direct access to some of the reporting tools that Fastly provides. While you may not get dashboard access, you do get to use the https://developer.fastly.com/reference/api/ with an access token that’s been attached to your account. Steps 1. SSH in to your production instance platform ssh --environment=production 2. Retrieve your API token This will have been added to your production instance, conventionally in the location /mnt/shared/fastly_tokens.txt $ cat /mnt/shared/fastly_tokens.txt This should return something like: Service: myprojectname Service ID: C8L8ykr4PCC65vQ8TK97td API Token: 74919c95e59ec799bc82641811e4e3b4 So, save that as variables: $ FASTLY_SERVICE_ID=\"C8L8ykr4PCC65vQ8TK97td\" $ FASTLY_API_TOKEN=\"74919c95e59ec799bc82641811e4e3b4\" 3. Use the Fastly API to ask for your statistics Historical stats can be retrieved from the https://developer.fastly.com/reference/api/metrics-stats/historical-stats/ curl \"https://api.fastly.com/stats/service/${FASTLY_SERVICE_ID}/field/hit_ratio?from=2+days+ago\" \\ -XGET \\ -H \"Fastly-Key: ${FASTLY_API_TOKEN}\" \\ -H \"Accept: Application/json\" \\ -k 4. Check the result It’s machine-readable, not so much human-readable, but: Hopefully you’ll see something like: { \"msg\" : null, \"status\" : \"success\", \"data\" : [ { \"service_id\" : \"C8L8ykr4PCC65vQ8TK97td\", \"hit_ratio\" : 0.818625522455654, \"start_time\" : 1589846900 } ], \"meta\" : { \"to\" : \"2020-05-22 12:21:00 UTC\", \"by\" : \"day\", \"from\" : \"2020-05-24 12:21:00 UTC\", \"region\" : \"all\" } } the hit_ratio there indicates that 81% of all requests were cacheable, and being returned by the CDN. Conclusion http://Platform.sh plans with managed Fastly CDN accounts do provide you access to an amount of management options, but you have to work through the API to get at them. This example gives you the recent “hit ratio”, but there are https://developer.fastly.com/reference/api/metrics-stats/historical-stats/ also, that can be retrieved in a similar way. See also https://community.platform.sh/t/how-to-flush-the-fastly-cache/484 ",
        "text": "Goal To review how effective the cache layer is, you may want to check the “Hit Ratio” of the content it is serving. A busy read-only site with its cache preferences properly configured can be seen to serve percentages in the high 90’s; while if you see a cache ratio down below 60%, the server is probably working pretty hard, and some things can be done - either at the app level, of just in the Response-Headers, to improve cache-ability and responsiveness. Assumptions You will need: A http://Platform.sh plan with managed Fastly cache enabled. This usually means an “Enterprise Dedicated” plan. Read access to your hosting server - the ability to ssh in and read a file there. This is best done by installing and using the platform CLI tool, though plain ssh work as well. Access to the curl tool. if you don’t have it installed locally, you can use it when ssh-ed in to your instance. Challenge For managed-CDN plans, http://Platform.sh looks after your Fastly subscriptions. Which is great, because cache management is hard. But this does mean that you don’t get direct access to some of the reporting tools that Fastly provides. While you may not get dashboard access, you do get to use the https://developer.fastly.com/reference/api/ with an access token that’s been attached to your account. Steps 1. SSH in to your production instance platform ssh --environment=production 2. Retrieve your API token This will have been added to your production instance, conventionally in the location /mnt/shared/fastly_tokens.txt $ cat /mnt/shared/fastly_tokens.txt This should return something like: Service: myprojectname Service ID: C8L8ykr4PCC65vQ8TK97td API Token: 74919c95e59ec799bc82641811e4e3b4 So, save that as variables: $ FASTLY_SERVICE_ID=\"C8L8ykr4PCC65vQ8TK97td\" $ FASTLY_API_TOKEN=\"74919c95e59ec799bc82641811e4e3b4\" 3. Use the Fastly API to ask for your statistics Historical stats can be retrieved from the https://developer.fastly.com/reference/api/metrics-stats/historical-stats/ curl \"https://api.fastly.com/stats/service/${FASTLY_SERVICE_ID}/field/hit_ratio?from=2+days+ago\" \\ -XGET \\ -H \"Fastly-Key: ${FASTLY_API_TOKEN}\" \\ -H \"Accept: Application/json\" \\ -k 4. Check the result It’s machine-readable, not so much human-readable, but: Hopefully you’ll see something like: { \"msg\" : null, \"status\" : \"success\", \"data\" : [ { \"service_id\" : \"C8L8ykr4PCC65vQ8TK97td\", \"hit_ratio\" : 0.818625522455654, \"start_time\" : 1589846900 } ], \"meta\" : { \"to\" : \"2020-05-22 12:21:00 UTC\", \"by\" : \"day\", \"from\" : \"2020-05-24 12:21:00 UTC\", \"region\" : \"all\" } } the hit_ratio there indicates that 81% of all requests were cacheable, and being returned by the CDN. Conclusion http://Platform.sh plans with managed Fastly CDN accounts do provide you access to an amount of management options, but you have to work through the API to get at them. This example gives you the recent “hit ratio”, but there are https://developer.fastly.com/reference/api/metrics-stats/historical-stats/ also, that can be retrieved in a similar way. See also https://community.platform.sh/t/how-to-flush-the-fastly-cache/484 ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-check-my-fastly-cdn-peformance-cache-hit-ratio/566",
        "relurl": "/t/how-to-check-my-fastly-cdn-peformance-cache-hit-ratio/566"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "8e59e86395060653ab9d1ba14659202021de2199",
        "title": "How to use the latest npm version",
        "description": "Goal NPM is a package manager for the JavaScript programming language. This guide will explain how to keep the latest version. Assumptions You will need: an active https://platform.sh/ project A text editor of your choice. Steps Explicitly, set the npm as the latest version as the code below shows. dependencies: nodejs: npm: latest References https://docs.platform.sh/languages/nodejs.html https://docs.platform.sh/languages/nodejs/nvm.html ",
        "text": "Goal NPM is a package manager for the JavaScript programming language. This guide will explain how to keep the latest version. Assumptions You will need: an active https://platform.sh/ project A text editor of your choice. Steps Explicitly, set the npm as the latest version as the code below shows. dependencies: nodejs: npm: latest References https://docs.platform.sh/languages/nodejs.html https://docs.platform.sh/languages/nodejs/nvm.html ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-use-the-latest-npm-version/562",
        "relurl": "/t/how-to-use-the-latest-npm-version/562"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "75c9cbef75a0b5e57b7cac8efc696438f8da11eb",
        "title": "How to access MySQL/MariaDB credentials on Platform.sh",
        "description": "Goal Access credentials for MySQL/MariaDB from within a http://Platform.sh application using Node.js. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/mysql.html .platform/services.yaml for the given service on account if developing locally MySQL installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html MySQL/MariaDB relationship like so, in .platform.app.yaml: relationships: database: \"db:mysql\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-nodejs Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-nodejs for minimum requirements. $ npm install platformsh-config --save 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. const config = require(\"platformsh-config\").config(); 3. Read the credentials // Get the credentials to connect to the MySQL/MariaDB service. const credentials = config.credentials('database'); Steps (Manual) 1. Load the environment variable relationships = JSON.parse(new Buffer(process.env['PLATFORM_RELATIONSHIPS'], 'base64').toString()); 2. Read the credentials const credentials = relationships['database']; Use path is the database name that will be needed to connect. Pass the needed properties to your MySQL library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is a an object matching the relationship JSON object, which includes the appropriate user, password, host, database name, and other pertinent information. { \"username\": \"user\", \"scheme\": \"mysql\", \"service\": \"db\", \"fragment\": null, \"ip\": \"169.254.147.122\", \"hostname\": \"czwb2d7zzunu67lh77infwkm6i.mysql.service._.eu-3.platformsh.site\", \"public\": false, \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"database.internal\", \"rel\": \"mysql\", \"query\": { \"is_master\": true }, \"path\": \"main\", \"password\": \"\", \"type\": \"mysql:10.2\", \"port\": 3306 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get MySQL/MariaDB credentials in Node.js. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-nodejs can be useful for inspecting the project environment: // Checks whether the code is running in a build environment config.inBuild() // Checks whether the code is running in a runtime environment config.inRuntime() and for reading environment variables as attributes of config: // Available in Build and at Runtime config.appDir; // Available at Runtime only config.branch; config.smtpHost; The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "text": "Goal Access credentials for MySQL/MariaDB from within a http://Platform.sh application using Node.js. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/mysql.html .platform/services.yaml for the given service on account if developing locally MySQL installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html MySQL/MariaDB relationship like so, in .platform.app.yaml: relationships: database: \"db:mysql\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-nodejs Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-nodejs for minimum requirements. $ npm install platformsh-config --save 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. const config = require(\"platformsh-config\").config(); 3. Read the credentials // Get the credentials to connect to the MySQL/MariaDB service. const credentials = config.credentials('database'); Steps (Manual) 1. Load the environment variable relationships = JSON.parse(new Buffer(process.env['PLATFORM_RELATIONSHIPS'], 'base64').toString()); 2. Read the credentials const credentials = relationships['database']; Use path is the database name that will be needed to connect. Pass the needed properties to your MySQL library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is a an object matching the relationship JSON object, which includes the appropriate user, password, host, database name, and other pertinent information. { \"username\": \"user\", \"scheme\": \"mysql\", \"service\": \"db\", \"fragment\": null, \"ip\": \"169.254.147.122\", \"hostname\": \"czwb2d7zzunu67lh77infwkm6i.mysql.service._.eu-3.platformsh.site\", \"public\": false, \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"database.internal\", \"rel\": \"mysql\", \"query\": { \"is_master\": true }, \"path\": \"main\", \"password\": \"\", \"type\": \"mysql:10.2\", \"port\": 3306 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get MySQL/MariaDB credentials in Node.js. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-nodejs can be useful for inspecting the project environment: // Checks whether the code is running in a build environment config.inBuild() // Checks whether the code is running in a runtime environment config.inRuntime() and for reading environment variables as attributes of config: // Available in Build and at Runtime config.appDir; // Available at Runtime only config.branch; config.smtpHost; The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-access-mysql-mariadb-credentials-on-platform-sh/149",
        "relurl": "/t/how-to-access-mysql-mariadb-credentials-on-platform-sh/149"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e4e26a648cd947662617792a323599f918c1f549",
        "title": "How to access Memcached credentials on Platform.sh",
        "description": "Goal Access credentials for Memcached from within a http://Platform.sh application using Node.js. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/memcached.html .platform/services.yaml for the given service on account if developing locally Memcached installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html Memcached relationship like so, in .platform.app.yaml: relationships: memcachedcache: \"cachemc:memcached\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-nodejs Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-nodejs for minimum requirements. $ npm install platformsh-config --save 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. const config = require(\"platformsh-config\").config(); 3. Read the credentials // Get the credentials to connect to the Memcached service. const credentials = config.credentials('memcachedcache'); Steps (Manual) 1. Load the environment variable relationships = JSON.parse(new Buffer(process.env['PLATFORM_RELATIONSHIPS'], 'base64').toString()); 2. Read the credentials credentials = relationships['memcachedcache']; Use In most cases, you will need only the host and port properties to connect to Memcached. Pass those to your Memecached library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is a an object matching the relationship JSON object, which includes the appropriate user, password, host, database name, and other pertinent information. { \"service\": \"cachemc\", \"ip\": \"169.254.192.212\", \"hostname\": \"tmimatsnz2dv26qlpuespenf3u.memcached.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"memcachedcache.internal\", \"rel\": \"memcached\", \"scheme\": \"memcached\", \"type\": \"memcached:1.4\", \"port\": 11211 } Conclusions: Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get Memcached credentials in Node.js. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-nodejs can be useful for inspecting the project environment: // Checks whether the code is running in a build environment config.inBuild() // Checks whether the code is running in a runtime environment config.inRuntime() and for reading environment variables as attributes of config: // Available in Build and at Runtime config.appDir; // Available at Runtime only config.branch; config.smtpHost; The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "text": "Goal Access credentials for Memcached from within a http://Platform.sh application using Node.js. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/memcached.html .platform/services.yaml for the given service on account if developing locally Memcached installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html Memcached relationship like so, in .platform.app.yaml: relationships: memcachedcache: \"cachemc:memcached\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-nodejs Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-nodejs for minimum requirements. $ npm install platformsh-config --save 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. const config = require(\"platformsh-config\").config(); 3. Read the credentials // Get the credentials to connect to the Memcached service. const credentials = config.credentials('memcachedcache'); Steps (Manual) 1. Load the environment variable relationships = JSON.parse(new Buffer(process.env['PLATFORM_RELATIONSHIPS'], 'base64').toString()); 2. Read the credentials credentials = relationships['memcachedcache']; Use In most cases, you will need only the host and port properties to connect to Memcached. Pass those to your Memecached library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is a an object matching the relationship JSON object, which includes the appropriate user, password, host, database name, and other pertinent information. { \"service\": \"cachemc\", \"ip\": \"169.254.192.212\", \"hostname\": \"tmimatsnz2dv26qlpuespenf3u.memcached.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"memcachedcache.internal\", \"rel\": \"memcached\", \"scheme\": \"memcached\", \"type\": \"memcached:1.4\", \"port\": 11211 } Conclusions: Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get Memcached credentials in Node.js. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-nodejs can be useful for inspecting the project environment: // Checks whether the code is running in a build environment config.inBuild() // Checks whether the code is running in a runtime environment config.inRuntime() and for reading environment variables as attributes of config: // Available in Build and at Runtime config.appDir; // Available at Runtime only config.branch; config.smtpHost; The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-access-memcached-credentials-on-platform-sh/147",
        "relurl": "/t/how-to-access-memcached-credentials-on-platform-sh/147"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "83a7ccb4fdbab0a788c48c32fb2f5b8861de7a60",
        "title": "How to access RabbitMQ credentials on Platform.sh",
        "description": "Goal: Access credentials for RabbitMQ from within a http://Platform.sh application using Python. Assumptions: an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/rabbitmq.html .platform/services.yaml for the given service on account if developing locally RabbitMQ installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html RabbitMQ relationship like so, in .platform.app.yaml: relationships: rabbitmqqueue: \"queuerabbit:rabbitmq\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-python Accessing environment variables manually Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-python for minimum requirements. $ pip install platformshconfig 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. from platformshconfig import Config config = Config() 3. Read the credentials # Get the credentials to connect to the RabbitMQ service. credentials = config.credentials('rabbitmqqueue') Steps (Manual) 1. Load the environment variable import os, json, base64 relationships = json.loads(base64.b64decode(os.environ[\"PLATFORM_RELATIONSHIPS\"])) 2. Read the credentials credentials = relationships['rabbitmqqueue'] Use In most cases, you will need only the host and port properties to connect to RabbitMQ. Pass those to your RabbitMQ library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is now a dictionary matching the relationship JSON object. { \"username\": \"guest\", \"scheme\": \"amqp\", \"service\": \"queuerabbit\", \"ip\": \"169.254.107.244\", \"hostname\": \"qmx5lbdjoolco7dqjr7cx26r7q.rabbitmq.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"rabbitmqqueue.internal\", \"rel\": \"rabbitmq\", \"password\": \"guest\", \"type\": \"rabbitmq:3.5\", \"port\": 5672 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get RabbitMQ credentials in Python. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-python can be useful for inspecting the project environment: # Checks whether the code is running in a build environment config.inBuild() # Checks whether the code is running in a runtime environment config.inRuntime() and for reading environment variables as attributes of config: # Available in Build and at Runtime config.appDir # Available at Runtime only config.branch config.smtpHost The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "text": "Goal: Access credentials for RabbitMQ from within a http://Platform.sh application using Python. Assumptions: an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/rabbitmq.html .platform/services.yaml for the given service on account if developing locally RabbitMQ installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html RabbitMQ relationship like so, in .platform.app.yaml: relationships: rabbitmqqueue: \"queuerabbit:rabbitmq\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-python Accessing environment variables manually Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-python for minimum requirements. $ pip install platformshconfig 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. from platformshconfig import Config config = Config() 3. Read the credentials # Get the credentials to connect to the RabbitMQ service. credentials = config.credentials('rabbitmqqueue') Steps (Manual) 1. Load the environment variable import os, json, base64 relationships = json.loads(base64.b64decode(os.environ[\"PLATFORM_RELATIONSHIPS\"])) 2. Read the credentials credentials = relationships['rabbitmqqueue'] Use In most cases, you will need only the host and port properties to connect to RabbitMQ. Pass those to your RabbitMQ library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is now a dictionary matching the relationship JSON object. { \"username\": \"guest\", \"scheme\": \"amqp\", \"service\": \"queuerabbit\", \"ip\": \"169.254.107.244\", \"hostname\": \"qmx5lbdjoolco7dqjr7cx26r7q.rabbitmq.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"rabbitmqqueue.internal\", \"rel\": \"rabbitmq\", \"password\": \"guest\", \"type\": \"rabbitmq:3.5\", \"port\": 5672 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get RabbitMQ credentials in Python. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-python can be useful for inspecting the project environment: # Checks whether the code is running in a build environment config.inBuild() # Checks whether the code is running in a runtime environment config.inRuntime() and for reading environment variables as attributes of config: # Available in Build and at Runtime config.appDir # Available at Runtime only config.branch config.smtpHost The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-access-rabbitmq-credentials-on-platform-sh/151",
        "relurl": "/t/how-to-access-rabbitmq-credentials-on-platform-sh/151"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "578d4e3ececc995c4be735729c380b7a4eb2461e",
        "title": "How to access Redis credentials on Platform.sh",
        "description": "Goal Access credentials for Redis from within a http://Platform.sh application using PHP. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/redis.html .platform/services.yaml for the given service on account if developing locally Redis installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html Redis relationship like so, in .platform.app.yaml: relationships: rediscache: \"cache:redis\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-php Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-php for minimum requirements. $ composer install platformsh/config-reader 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. use Platformsh\\ConfigReader\\Config; $config = new Config(); 3. Read the credentials // Get the credentials to connect to the Redis service. $credentials = $config-credentials('rediscache'); Steps (Manual) 1. Load the environment variable $relationships = json_decode(base64_decode(getenv('PLATFORM_RELATIONSHIPS')), TRUE); 2. Read the credentials $credentials = $relationships['rediscache']; Use In most cases, you will need only the host and port properties to connect to Redis. Pass those to your Redis library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is now an array matching the relationship JSON object. { \"username\": null, \"scheme\": \"redis\", \"service\": \"cache\", \"fragment\": null, \"ip\": \"169.254.22.205\", \"hostname\": \"2wye4dawv22vhvozyec3o5hxfm.redis.service._.eu-3.platformsh.site\", \"public\": false, \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"rediscache.internal\", \"rel\": \"redis\", \"query\": [ ], \"path\": null, \"password\": null, \"type\": \"redis:5.0\", \"port\": 6379 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get Redis credentials in PHP. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-php can be useful for inspecting the project environment: // Checks whether the code is running in a build environment $config-inBuild(); // Checks whether the code is running in a runtime environment $config-inRuntime(); and for reading environment variables as attributes of config: // Available in Build and at Runtime $config-appDir; // Available at Runtime only $config-branch; $config-smtpHost; The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "text": "Goal Access credentials for Redis from within a http://Platform.sh application using PHP. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/redis.html .platform/services.yaml for the given service on account if developing locally Redis installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html Redis relationship like so, in .platform.app.yaml: relationships: rediscache: \"cache:redis\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-php Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-php for minimum requirements. $ composer install platformsh/config-reader 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. use Platformsh\\ConfigReader\\Config; $config = new Config(); 3. Read the credentials // Get the credentials to connect to the Redis service. $credentials = $config-credentials('rediscache'); Steps (Manual) 1. Load the environment variable $relationships = json_decode(base64_decode(getenv('PLATFORM_RELATIONSHIPS')), TRUE); 2. Read the credentials $credentials = $relationships['rediscache']; Use In most cases, you will need only the host and port properties to connect to Redis. Pass those to your Redis library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is now an array matching the relationship JSON object. { \"username\": null, \"scheme\": \"redis\", \"service\": \"cache\", \"fragment\": null, \"ip\": \"169.254.22.205\", \"hostname\": \"2wye4dawv22vhvozyec3o5hxfm.redis.service._.eu-3.platformsh.site\", \"public\": false, \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"rediscache.internal\", \"rel\": \"redis\", \"query\": [ ], \"path\": null, \"password\": null, \"type\": \"redis:5.0\", \"port\": 6379 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get Redis credentials in PHP. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-php can be useful for inspecting the project environment: // Checks whether the code is running in a build environment $config-inBuild(); // Checks whether the code is running in a runtime environment $config-inRuntime(); and for reading environment variables as attributes of config: // Available in Build and at Runtime $config-appDir; // Available at Runtime only $config-branch; $config-smtpHost; The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-access-redis-credentials-on-platform-sh/152",
        "relurl": "/t/how-to-access-redis-credentials-on-platform-sh/152"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "4f91d2c8a0031e327247e117b949cf722dd93a57",
        "title": "How to access Elasticsearch credentials on Platform.sh",
        "description": "Goal Access credentials for Elasticsearch from within a http://Platform.sh application using Python. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/elasticsearch.html .platform/services.yaml for the given service on account if developing locally Elasticsearch installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html Elasticsearch relationship like so, in .platform.app.yaml: relationships: essearch: \"searchelastic:elasticsearch\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-python Accessing environment variables manually Steps (Config Reader) 1. Install the library Install the http://Platform.sh Configuration Reader library. See the https://github.com/platformsh/config-reader-python for minimum requirements. $ pip install platformshconfig 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. from platformshconfig import Config config = Config() 3. Read the credentials # Get the credentials to connect to the Elasticsearch service. credentials = config.credentials('essearch') If the relationship is named something else, modify the credentials() call accordingly. Steps (Manual) 1. Load and decode the environment variable import os, json, base64 relationships = json.loads(base64.b64decode(os.environ[\"PLATFORM_RELATIONSHIPS\"])) 2. Get the credentials credentials = relationships['essearch'] If the relationship is named something else, modify that line accordingly. Use In most cases, you will need only the scheme, host, and port properties to connect to Elasticsearch. Pass those to your Elasticsearch library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is now a dictionary matching the relationship JSON object. { \"username\": null, \"scheme\": \"http\", \"service\": \"searchelastic\", \"fragment\": null, \"ip\": \"169.254.101.149\", \"hostname\": \"j2dkzht3gs2yr66fb2brhoj4zu.elasticsearch.service._.eu-3.platformsh.site\", \"public\": false, \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"essearch.internal\", \"rel\": \"elasticsearch\", \"query\": [ ], \"path\": null, \"password\": null, \"type\": \"elasticsearch:5.4\", \"port\": 9200 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get Elasticsearch credentials in Python. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-python can be useful for inspecting the project environment: # Checks whether the code is running in a build environment config.inBuild() # Checks whether the code is running in a runtime environment config.inRuntime() and for reading environment variables as attributes of config: # Available in Build and at Runtime config.appDir # Available at Runtime only config.branch config.smtpHost The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "text": "Goal Access credentials for Elasticsearch from within a http://Platform.sh application using Python. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/elasticsearch.html .platform/services.yaml for the given service on account if developing locally Elasticsearch installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html Elasticsearch relationship like so, in .platform.app.yaml: relationships: essearch: \"searchelastic:elasticsearch\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-python Accessing environment variables manually Steps (Config Reader) 1. Install the library Install the http://Platform.sh Configuration Reader library. See the https://github.com/platformsh/config-reader-python for minimum requirements. $ pip install platformshconfig 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. from platformshconfig import Config config = Config() 3. Read the credentials # Get the credentials to connect to the Elasticsearch service. credentials = config.credentials('essearch') If the relationship is named something else, modify the credentials() call accordingly. Steps (Manual) 1. Load and decode the environment variable import os, json, base64 relationships = json.loads(base64.b64decode(os.environ[\"PLATFORM_RELATIONSHIPS\"])) 2. Get the credentials credentials = relationships['essearch'] If the relationship is named something else, modify that line accordingly. Use In most cases, you will need only the scheme, host, and port properties to connect to Elasticsearch. Pass those to your Elasticsearch library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is now a dictionary matching the relationship JSON object. { \"username\": null, \"scheme\": \"http\", \"service\": \"searchelastic\", \"fragment\": null, \"ip\": \"169.254.101.149\", \"hostname\": \"j2dkzht3gs2yr66fb2brhoj4zu.elasticsearch.service._.eu-3.platformsh.site\", \"public\": false, \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"essearch.internal\", \"rel\": \"elasticsearch\", \"query\": [ ], \"path\": null, \"password\": null, \"type\": \"elasticsearch:5.4\", \"port\": 9200 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get Elasticsearch credentials in Python. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-python can be useful for inspecting the project environment: # Checks whether the code is running in a build environment config.inBuild() # Checks whether the code is running in a runtime environment config.inRuntime() and for reading environment variables as attributes of config: # Available in Build and at Runtime config.appDir # Available at Runtime only config.branch config.smtpHost The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-access-elasticsearch-credentials-on-platform-sh/146",
        "relurl": "/t/how-to-access-elasticsearch-credentials-on-platform-sh/146"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "884bc41074079bab17248fe65e762901d3e6179d",
        "title": "How to access MongoDB credentials on Platform.sh",
        "description": "Goal Access credentials for MongoDB from within a http://Platform.sh application using Node.js. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/mongodb.html .platform/services.yaml for the given service on account if developing locally MongoDB installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html MongoDB relationship like so, in .platform.app.yaml: relationships: database: \"dbmongo:mongodb\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-nodejs Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-nodejs for minimum requirements. $ npm install platformsh-config --save 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. const config = require(\"platformsh-config\").config(); 3. Read the credentials // Get the credentials to connect to the MongoDB service. const credentials = config.credentials('database'); Steps (Manual) 1. Load and decode the environment variable relationships = JSON.parse(new Buffer(process.env['PLATFORM_RELATIONSHIPS'], 'base64').toString()); 2. Read the credentials credentials = relationships['database']; Use In most cases, you will need only the host and port properties to connect to MongoDB. Pass those to your MongoDB library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is a an object matching the relationship JSON object. { \"username\": \"main\", \"scheme\": \"mongodb\", \"service\": \"dbmongo\", \"ip\": \"169.254.78.213\", \"hostname\": \"okqb7mbggt4wbyiszla2zy4boq.mongodb.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"database.internal\", \"rel\": \"mongodb\", \"path\": \"main\", \"query\": { \"is_master\": true }, \"password\": \"main\", \"type\": \"mongodb:3.6\", \"port\": 27017 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get MongoDB credentials in Node.js. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-nodejs can be useful for connecting to MongoDB without creating the full DSN string using formattedCredentials: // Define formatted credentials var client = await MongoClient.connect(config.formattedCredentials('database', 'mongodb')); The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "text": "Goal Access credentials for MongoDB from within a http://Platform.sh application using Node.js. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/mongodb.html .platform/services.yaml for the given service on account if developing locally MongoDB installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html MongoDB relationship like so, in .platform.app.yaml: relationships: database: \"dbmongo:mongodb\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-nodejs Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-nodejs for minimum requirements. $ npm install platformsh-config --save 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. const config = require(\"platformsh-config\").config(); 3. Read the credentials // Get the credentials to connect to the MongoDB service. const credentials = config.credentials('database'); Steps (Manual) 1. Load and decode the environment variable relationships = JSON.parse(new Buffer(process.env['PLATFORM_RELATIONSHIPS'], 'base64').toString()); 2. Read the credentials credentials = relationships['database']; Use In most cases, you will need only the host and port properties to connect to MongoDB. Pass those to your MongoDB library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is a an object matching the relationship JSON object. { \"username\": \"main\", \"scheme\": \"mongodb\", \"service\": \"dbmongo\", \"ip\": \"169.254.78.213\", \"hostname\": \"okqb7mbggt4wbyiszla2zy4boq.mongodb.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"database.internal\", \"rel\": \"mongodb\", \"path\": \"main\", \"query\": { \"is_master\": true }, \"password\": \"main\", \"type\": \"mongodb:3.6\", \"port\": 27017 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get MongoDB credentials in Node.js. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-nodejs can be useful for connecting to MongoDB without creating the full DSN string using formattedCredentials: // Define formatted credentials var client = await MongoClient.connect(config.formattedCredentials('database', 'mongodb')); The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-access-mongodb-credentials-on-platform-sh/145",
        "relurl": "/t/how-to-access-mongodb-credentials-on-platform-sh/145"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "ddc9f9bafab94adf778bb07f7759c42990245dce",
        "title": "How to access PostgreSQL credentials on Platform.sh",
        "description": "Goal: Access credentials for PostgreSQL from within a http://Platform.sh application using PHP. Assumptions: an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/postgresql.html .platform/services.yaml for the given service on account if developing locally PostgreSQL installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html PostgreSQL relationship like so, in .platform.app.yaml: relationships: database: \"dbpostgres:postgresql\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-php Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-php for minimum requirements. $ composer install platformsh/config-reader 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. use Platformsh\\ConfigReader\\Config; $config = new Config(); 3. Read the credentials // Get the credentials to connect to the PostgreSQL service. $credentials = $config-credentials('database'); Steps (Manual) 1. Load and decode the environment variable $relationships = json_decode(base64_decode(getenv('PLATFORM_RELATIONSHIPS')), TRUE); 2. Read the credentials $credentials = $relationships['database']; Use: path is the database name that will be needed to connect. Pass the needed properties to your PostgreSQL library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is now an object matching the relationship JSON object. { \"username\": \"main\", \"scheme\": \"pgsql\", \"service\": \"dbpostgres\", \"ip\": \"169.254.231.161\", \"hostname\": \"tkydopuhmksuhglz7ox4x4de6m.postgresql.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"database.internal\", \"rel\": \"postgresql\", \"path\": \"main\", \"query\": { \"is_master\": true }, \"password\": \"main\", \"type\": \"postgresql:11\", \"port\": 5432 } Conclusions: Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get PostgreSQL credentials in PHP. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-php can be useful for connecting to PostgreSQL without creating the full DSN string using FormattedCredentials: // Define formatted credentials $formatted = $config-FormattedCredentials(\"database\", \"pdo_pgsql\") The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "text": "Goal: Access credentials for PostgreSQL from within a http://Platform.sh application using PHP. Assumptions: an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/postgresql.html .platform/services.yaml for the given service on account if developing locally PostgreSQL installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html PostgreSQL relationship like so, in .platform.app.yaml: relationships: database: \"dbpostgres:postgresql\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-php Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-php for minimum requirements. $ composer install platformsh/config-reader 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. use Platformsh\\ConfigReader\\Config; $config = new Config(); 3. Read the credentials // Get the credentials to connect to the PostgreSQL service. $credentials = $config-credentials('database'); Steps (Manual) 1. Load and decode the environment variable $relationships = json_decode(base64_decode(getenv('PLATFORM_RELATIONSHIPS')), TRUE); 2. Read the credentials $credentials = $relationships['database']; Use: path is the database name that will be needed to connect. Pass the needed properties to your PostgreSQL library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is now an object matching the relationship JSON object. { \"username\": \"main\", \"scheme\": \"pgsql\", \"service\": \"dbpostgres\", \"ip\": \"169.254.231.161\", \"hostname\": \"tkydopuhmksuhglz7ox4x4de6m.postgresql.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"database.internal\", \"rel\": \"postgresql\", \"path\": \"main\", \"query\": { \"is_master\": true }, \"password\": \"main\", \"type\": \"postgresql:11\", \"port\": 5432 } Conclusions: Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get PostgreSQL credentials in PHP. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-php can be useful for connecting to PostgreSQL without creating the full DSN string using FormattedCredentials: // Define formatted credentials $formatted = $config-FormattedCredentials(\"database\", \"pdo_pgsql\") The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-access-postgresql-credentials-on-platform-sh/150",
        "relurl": "/t/how-to-access-postgresql-credentials-on-platform-sh/150"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "a45103b61ac17e5cfda8e6f66e015164e27b2d03",
        "title": "How to access Solr credentials on Platform.sh",
        "description": "Goal Access credentials for Solr from within a http://Platform.sh application using Python. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/solr.html .platform/services.yaml for the given service on account if developing locally Solr installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html Solr relationship like so, in .platform.app.yaml: relationships: solrsearch: \"searchsolr:solr\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-python Accessing environment variables manually Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-python for minimum requirements. $ pip install platformshconfig 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. from platformshconfig import Config config = Config() 3. Read the credentials # Get the credentials to connect to the Solr service. credentials = config.credentials('solrsearch') Steps (Manual) 1. Load the environment variable import os, json, base64 relationships = json.loads(base64.b64decode(os.environ[\"PLATFORM_RELATIONSHIPS\"])) 2. Read the credentials credentials = relationships['solrsearch'] Use In most cases, you will need only the host and port properties to connect to Solr. Pass those to your Solr library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is a dictionary matching the relationship JSON object. { \"service\": \"searchsolr\", \"ip\": \"169.254.103.29\", \"hostname\": \"swhnppdg3ugcd5sktkga73wjoi.solr.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"solrsearh.internal\", \"rel\": \"solr\", \"path\": \"solr\\/collection1\", \"scheme\": \"solr\", \"type\": \"solr:7.6\", \"port\": 8080 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get Solr credentials in Python. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-python can be useful for connecting to Solr without creating the full DSN string using FormattedCredentials: # Define formatted credentials formatted_url = config.formatted_credentials('solrsearch', 'pysolr') The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "text": "Goal Access credentials for Solr from within a http://Platform.sh application using Python. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/solr.html .platform/services.yaml for the given service on account if developing locally Solr installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html Solr relationship like so, in .platform.app.yaml: relationships: solrsearch: \"searchsolr:solr\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-python Accessing environment variables manually Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-python for minimum requirements. $ pip install platformshconfig 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. from platformshconfig import Config config = Config() 3. Read the credentials # Get the credentials to connect to the Solr service. credentials = config.credentials('solrsearch') Steps (Manual) 1. Load the environment variable import os, json, base64 relationships = json.loads(base64.b64decode(os.environ[\"PLATFORM_RELATIONSHIPS\"])) 2. Read the credentials credentials = relationships['solrsearch'] Use In most cases, you will need only the host and port properties to connect to Solr. Pass those to your Solr library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is a dictionary matching the relationship JSON object. { \"service\": \"searchsolr\", \"ip\": \"169.254.103.29\", \"hostname\": \"swhnppdg3ugcd5sktkga73wjoi.solr.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"solrsearh.internal\", \"rel\": \"solr\", \"path\": \"solr\\/collection1\", \"scheme\": \"solr\", \"type\": \"solr:7.6\", \"port\": 8080 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get Solr credentials in Python. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-python can be useful for connecting to Solr without creating the full DSN string using FormattedCredentials: # Define formatted credentials formatted_url = config.formatted_credentials('solrsearch', 'pysolr') The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-access-solr-credentials-on-platform-sh/153",
        "relurl": "/t/how-to-access-solr-credentials-on-platform-sh/153"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "eaf0b7ad65f79cd63b7b9491f8b0f0ad53ff964a",
        "title": "How to Configure Load Balancer in a multiple applications",
        "description": "Goal This guide will explain how to configure a simple load balancing setup in a Single Application at http://Platform.sh. The critical point is that this Load balancer configuration happens transparently, that is, the application does not know about the LB. For that to work, it is very important that your application follows good practices. For example, ensure that your application is stateless. https://thenewstack.io/best-practices-for-developing-cloud-native-applications-and-microservice-architectures/ is a good practice for having your application in the cloud. One of the advantages of stateless services is that you can bring up multiple instances of your application. Since there’s no per-instance state, any instance can handle any request. This is a natural fit for load balancers, which can be leveraged to help scale the service. They’re also readily available on cloud platforms. Assumptions You either have java Applications, and you want to run at http://platform.sh or you already have a Java application running at http://platform.sh A text editor of your choice. Steps https://community.platform.sh/t/how-to-configure-multi-applications-with-applications-yaml/552 Given that you already have a multi-application running in with .platform/applications.yaml The next step is “copy/paste” the information that you want the replicated machine. In this sample, we’ll only overwrite the name. - \u0026appdef name: app type: 'java:8' disk: 1024 source: root: app hooks: build: mvn clean install mounts: 'server/': source: local source_path: server_source web: commands: start: | cp target/dependency/webapp-runner.jar server/webapp-runner.jar cp target/tomcat.war server/tomcat.war cd server \u0026\u0026 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError webapp-runner.jar --port $PORT tomcat.war - ",
        "text": "Goal This guide will explain how to configure a simple load balancing setup in a Single Application at http://Platform.sh. The critical point is that this Load balancer configuration happens transparently, that is, the application does not know about the LB. For that to work, it is very important that your application follows good practices. For example, ensure that your application is stateless. https://thenewstack.io/best-practices-for-developing-cloud-native-applications-and-microservice-architectures/ is a good practice for having your application in the cloud. One of the advantages of stateless services is that you can bring up multiple instances of your application. Since there’s no per-instance state, any instance can handle any request. This is a natural fit for load balancers, which can be leveraged to help scale the service. They’re also readily available on cloud platforms. Assumptions You either have java Applications, and you want to run at http://platform.sh or you already have a Java application running at http://platform.sh A text editor of your choice. Steps https://community.platform.sh/t/how-to-configure-multi-applications-with-applications-yaml/552 Given that you already have a multi-application running in with .platform/applications.yaml The next step is “copy/paste” the information that you want the replicated machine. In this sample, we’ll only overwrite the name. - \u0026appdef name: app type: 'java:8' disk: 1024 source: root: app hooks: build: mvn clean install mounts: 'server/': source: local source_path: server_source web: commands: start: | cp target/dependency/webapp-runner.jar server/webapp-runner.jar cp target/tomcat.war server/tomcat.war cd server \u0026\u0026 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError webapp-runner.jar --port $PORT tomcat.war - ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-configure-load-balancer-in-a-multiple-applications/554",
        "relurl": "/t/how-to-configure-load-balancer-in-a-multiple-applications/554"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "ef183595513b2be194048ce8bf3e715d36d83ad3",
        "title": "How to Configure Load Balancer in a Single Application",
        "description": "Goal This guide will explain how to configure a simple load balancing setup in a Single Application at http://Platform.sh. The critical point is that this Load balancer configuration happens transparently, that is, the application does not know about the LB. For that to work, it is very important that your application follows good practices. For example, ensure that your application is stateless. https://thenewstack.io/best-practices-for-developing-cloud-native-applications-and-microservice-architectures/ is a good practice for having your application in the cloud. One of the advantages of stateless services is that you can bring up multiple instances of your application. Since there’s no per-instance state, any instance can handle any request. This is a natural fit for load balancers, which can be leveraged to help scale the service. They’re also readily available on cloud platforms. What is load balancing? At a basic level, load balancing works to distribute web traffic requests among different servers to ensure high availability and optimal traffic management while avoiding overload of any one server and defending against denial of service attacks. Load balancers increase capacity and reliability. How does the Load Balancer work at http://Platform.sh? To make a Load balancer possible, http://Platform.sh provides https://docs.platform.sh/configuration/services/varnish.html, an HTTP accelerator designed for content-heavy dynamic web sites as well as APIs. In Varnish you will find three different methods (“directors”) for load balancing: round robin fallback random Assumptions You either have an application, and you want to run at http://platform.sh or you already have an application running at http://platform.sh A text editor of your choice. The example below uses a Java application, but any stateless application in any language should work the same. Steps 1. Use network storage Because this configuration involves multiple application instances, they will each have their own local storage. To share files between applications you must use a https://docs.platform.sh/configuration/services/network-storage.html service. See the documentation for specific instructions. 2. Define the application Define your application in the https://docs.platform.sh/configuration/app/multi-app.html file, rather than in .platform.app.yaml as usual. The syntax is the same, but represented as a YAML array. The application should also be defined as a . - \u0026appdef name: app type: 'java:8' disk: 1024 source: root: / hooks: build: mvn clean install mounts: 'server/': source: local source_path: server_source web: commands: start: | cp target/dependency/webapp-runner.jar server/webapp-runner.jar cp target/tomcat.war server/tomcat.war cd server \u0026\u0026 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError webapp-runner.jar --port $PORT tomcat.war Next, add a second definition in the same file by aliasing the first definition. Add the following lines to applications.yaml: - ",
        "text": "Goal This guide will explain how to configure a simple load balancing setup in a Single Application at http://Platform.sh. The critical point is that this Load balancer configuration happens transparently, that is, the application does not know about the LB. For that to work, it is very important that your application follows good practices. For example, ensure that your application is stateless. https://thenewstack.io/best-practices-for-developing-cloud-native-applications-and-microservice-architectures/ is a good practice for having your application in the cloud. One of the advantages of stateless services is that you can bring up multiple instances of your application. Since there’s no per-instance state, any instance can handle any request. This is a natural fit for load balancers, which can be leveraged to help scale the service. They’re also readily available on cloud platforms. What is load balancing? At a basic level, load balancing works to distribute web traffic requests among different servers to ensure high availability and optimal traffic management while avoiding overload of any one server and defending against denial of service attacks. Load balancers increase capacity and reliability. How does the Load Balancer work at http://Platform.sh? To make a Load balancer possible, http://Platform.sh provides https://docs.platform.sh/configuration/services/varnish.html, an HTTP accelerator designed for content-heavy dynamic web sites as well as APIs. In Varnish you will find three different methods (“directors”) for load balancing: round robin fallback random Assumptions You either have an application, and you want to run at http://platform.sh or you already have an application running at http://platform.sh A text editor of your choice. The example below uses a Java application, but any stateless application in any language should work the same. Steps 1. Use network storage Because this configuration involves multiple application instances, they will each have their own local storage. To share files between applications you must use a https://docs.platform.sh/configuration/services/network-storage.html service. See the documentation for specific instructions. 2. Define the application Define your application in the https://docs.platform.sh/configuration/app/multi-app.html file, rather than in .platform.app.yaml as usual. The syntax is the same, but represented as a YAML array. The application should also be defined as a . - \u0026appdef name: app type: 'java:8' disk: 1024 source: root: / hooks: build: mvn clean install mounts: 'server/': source: local source_path: server_source web: commands: start: | cp target/dependency/webapp-runner.jar server/webapp-runner.jar cp target/tomcat.war server/tomcat.war cd server \u0026\u0026 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError webapp-runner.jar --port $PORT tomcat.war Next, add a second definition in the same file by aliasing the first definition. Add the following lines to applications.yaml: - ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-configure-load-balancer-in-a-single-application/553",
        "relurl": "/t/how-to-configure-load-balancer-in-a-single-application/553"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "2eb87e9ab2fc0e657b26e8ed97c05458e7004548",
        "title": "How to configure multi-applications with applications.yaml",
        "description": "Goal This “how to” guide will explain about create multiple applications with applications.yaml file with Java. Assumptions You either have a Java application, and you want to run at http://platform.sh or you already have a Java application running at http://platform.sh A text editor of your choice. Steps Create a .platform/applications.yml file at the root directory of your project, this file will have the each application’s configuration. In this file, we’re going to use YAML’s built-in “anchors” to share configuration typically found in a .platform.app.yaml file between multiple applications. When we talk about YAML achor there are two important points: The anchor ‘\u0026’ which defines a chunk of configuration The alias ‘*’ used to refer to that chunk elsewhere In the code below, we define the anchorappdef that contains the settings of the first application, app, and we use the alias for the second application, app2. That becomes the basis of the first application, which we can then overwrite with information, such as the app’s unique name. - \u0026appdef name: app type: 'java:8' disk: 1024 source: root: app hooks: build: mvn clean install mounts: 'server/': source: local source_path: server_source web: commands: start: | cp target/dependency/webapp-runner.jar server/webapp-runner.jar cp target/tomcat.war server/tomcat.war cd server \u0026\u0026 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError webapp-runner.jar --port $PORT tomcat.war - ",
        "text": "Goal This “how to” guide will explain about create multiple applications with applications.yaml file with Java. Assumptions You either have a Java application, and you want to run at http://platform.sh or you already have a Java application running at http://platform.sh A text editor of your choice. Steps Create a .platform/applications.yml file at the root directory of your project, this file will have the each application’s configuration. In this file, we’re going to use YAML’s built-in “anchors” to share configuration typically found in a .platform.app.yaml file between multiple applications. When we talk about YAML achor there are two important points: The anchor ‘\u0026’ which defines a chunk of configuration The alias ‘*’ used to refer to that chunk elsewhere In the code below, we define the anchorappdef that contains the settings of the first application, app, and we use the alias for the second application, app2. That becomes the basis of the first application, which we can then overwrite with information, such as the app’s unique name. - \u0026appdef name: app type: 'java:8' disk: 1024 source: root: app hooks: build: mvn clean install mounts: 'server/': source: local source_path: server_source web: commands: start: | cp target/dependency/webapp-runner.jar server/webapp-runner.jar cp target/tomcat.war server/tomcat.war cd server \u0026\u0026 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError webapp-runner.jar --port $PORT tomcat.war - ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-configure-multi-applications-with-applications-yaml/552",
        "relurl": "/t/how-to-configure-multi-applications-with-applications-yaml/552"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "565c67842415ac7af245608af259d2a0df191268",
        "title": "How to Migrate my Java Application to Platform.sh",
        "description": "Goal This tutorial will cover how to migrate your Java application to http://Platform.sh. Preparation This tutorial assumes you have A working Java 8 or higher application A text editor of your choice. Problems The Java Application is ready, and now the team wants to migrate to http://Platform.sh and enjoy the benefits of cloud computing naturally. Steps https://docs.platform.sh/ https://docs.platform.sh/gettingstarted/own-code/create-project.html Create https://docs.platform.sh/configuration/routes.html , https://docs.platform.sh/configuration/services.html and https://docs.platform.sh/configuration/app-containers.html . You can integrate to existing , , or just use the remote repository that http://Platform.sh will provide to you. Furthermore, we also have those guides to help you to migrate with your Spring or Jakarta EE application: https://community.platform.sh/t/how-to-overwrite-spring-data-variable-to-access-platform-sh-services/518 https://community.platform.sh/t/how-to-overwrite-spring-data-mongodb-variable-to-access-platform-sh-services/528 https://community.platform.sh/t/how-to-overwrite-variables-to-payara-jpa-access-platform-sh-sql-services/519 https://community.platform.sh/t/how-to-overwrite-variables-to-payara-jpa-access-platform-sh-sql-services/519 https://community.platform.sh/t/how-to-migrate-your-java-application-from-heroku-to-platform-sh/441 References https://community.platform.sh/t/tips-about-java-commands-to-run-your-application-at-platform-sh/531 https://platform.sh/blog/2019/java-hello-world-at-platform.sh/ https://github.com/platformsh/config-reader-java https://docs.platform.sh/languages/java.html https://docs.platform.sh/languages/java/frameworks.html https://github.com/platformsh/java-quick-start ",
        "text": "Goal This tutorial will cover how to migrate your Java application to http://Platform.sh. Preparation This tutorial assumes you have A working Java 8 or higher application A text editor of your choice. Problems The Java Application is ready, and now the team wants to migrate to http://Platform.sh and enjoy the benefits of cloud computing naturally. Steps https://docs.platform.sh/ https://docs.platform.sh/gettingstarted/own-code/create-project.html Create https://docs.platform.sh/configuration/routes.html , https://docs.platform.sh/configuration/services.html and https://docs.platform.sh/configuration/app-containers.html . You can integrate to existing , , or just use the remote repository that http://Platform.sh will provide to you. Furthermore, we also have those guides to help you to migrate with your Spring or Jakarta EE application: https://community.platform.sh/t/how-to-overwrite-spring-data-variable-to-access-platform-sh-services/518 https://community.platform.sh/t/how-to-overwrite-spring-data-mongodb-variable-to-access-platform-sh-services/528 https://community.platform.sh/t/how-to-overwrite-variables-to-payara-jpa-access-platform-sh-sql-services/519 https://community.platform.sh/t/how-to-overwrite-variables-to-payara-jpa-access-platform-sh-sql-services/519 https://community.platform.sh/t/how-to-migrate-your-java-application-from-heroku-to-platform-sh/441 References https://community.platform.sh/t/tips-about-java-commands-to-run-your-application-at-platform-sh/531 https://platform.sh/blog/2019/java-hello-world-at-platform.sh/ https://github.com/platformsh/config-reader-java https://docs.platform.sh/languages/java.html https://docs.platform.sh/languages/java/frameworks.html https://github.com/platformsh/java-quick-start ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-migrate-my-java-application-to-platform-sh/529",
        "relurl": "/t/how-to-migrate-my-java-application-to-platform-sh/529"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "b10571b969602f927f2b07117ccd443adaf6d72c",
        "title": "How to use the Platform.sh CLI from the web container",
        "description": "Goal Have the http://Platform.SH CLI installed and authenticated in the web container. Assumptions Access to a project hosted on http://Platform.sh Your project account has administrator rights Knowledge on using the project web interface or Problems Using the http://Platform.sh CLI directly from the web (application) container can be required to automate certain tasks, like renewing the SSL certificate or triggering a snapshot based on a cron schedule. Due to the non-interactive deployment flow, using a regular username and password authentication is not possible. We will use token-based authentication to have the CLI installed and configured on each deployment. Steps 0. Create a dedicated account to use for automated tasks This step is optional, but _ strongly recommended_: for automated tasks, a dedicated user should be created and added to the project. Adding an API token to a project is a security risk, as it means the token will be visible to the other project members who have SSH access, so they will then also have access to the token’s account. 1. Create an API token Log in and navigate to Account settings API tokens, available https://accounts.platform.sh/user/api-tokens. Click on Create API token. https://community.platform.sh/uploads/default/718bd2e55a0aa3257ee8850e654a66b6af264b75 You will be asked for the token name - enter a name to easily identify your token in the future, in case of multiple tokens (CLI automated is one example). Click on Create API token to save the token. https://community.platform.sh/uploads/default/8da6eff40eec38ba02da0bc02f7cd149e49bfc03 Once done, the newly created token will be displayed at the top of the page, and can be copied to the clipboard using the Copy button. After this, you will not be able to view the API token again. https://community.platform.sh/uploads/default/9c567702a32fc496e8c7e4d3b8ea0cc5e7546b09 2. Add the token as an environment variable in the project Once the API token is added as an environment variable in the project, it will be automatically detected and used by the CLI tool. After the installation is done, you should see the PLATFORMSH_CLI_TOKEN variable when running env. Option 1: Using the web interface Open the project web interface and navigate to the environment in which you want to use the CLI, then click on Configure environment and go to the Variables tab. Add a variable named env:PLATFORMSH_CLI_TOKEN and set its value to the previously created token. https://community.platform.sh/uploads/default/49b4472ba7c3fd557210a40fe39e4ca89a75e902 Option 2: Using the CLI If you have the CLI tool installed and configured, you can add the variable from the command line: platform variable:create -e --level environment --name env:PLATFORMSH_CLI_TOKEN --sensitive true --value Replace the and with the correct values for your use case. 3. Install the CLI tool in the application container Once the variable has been added to the environment, it is required to download and install the CLI tool in a build hook. Modify .platform.app.yaml in the project to include: hooks: build: | curl -sS https://platform.sh/cli/installer | php This will download the CLI to a known directory, .platformsh/bin , which will be added to the PATH at runtime (via the .environment file). Redeploy your environment and log into the application container with SSH, then run platform. You should see the welcome prompt and a list of your current projects. Conclusion Having the CLI tool set up in an environment opens up further automation possibilities - renewing the SSL certificate, triggering snapshots, etc.",
        "text": "Goal Have the http://Platform.SH CLI installed and authenticated in the web container. Assumptions Access to a project hosted on http://Platform.sh Your project account has administrator rights Knowledge on using the project web interface or Problems Using the http://Platform.sh CLI directly from the web (application) container can be required to automate certain tasks, like renewing the SSL certificate or triggering a snapshot based on a cron schedule. Due to the non-interactive deployment flow, using a regular username and password authentication is not possible. We will use token-based authentication to have the CLI installed and configured on each deployment. Steps 0. Create a dedicated account to use for automated tasks This step is optional, but _ strongly recommended_: for automated tasks, a dedicated user should be created and added to the project. Adding an API token to a project is a security risk, as it means the token will be visible to the other project members who have SSH access, so they will then also have access to the token’s account. 1. Create an API token Log in and navigate to Account settings API tokens, available https://accounts.platform.sh/user/api-tokens. Click on Create API token. https://community.platform.sh/uploads/default/718bd2e55a0aa3257ee8850e654a66b6af264b75 You will be asked for the token name - enter a name to easily identify your token in the future, in case of multiple tokens (CLI automated is one example). Click on Create API token to save the token. https://community.platform.sh/uploads/default/8da6eff40eec38ba02da0bc02f7cd149e49bfc03 Once done, the newly created token will be displayed at the top of the page, and can be copied to the clipboard using the Copy button. After this, you will not be able to view the API token again. https://community.platform.sh/uploads/default/9c567702a32fc496e8c7e4d3b8ea0cc5e7546b09 2. Add the token as an environment variable in the project Once the API token is added as an environment variable in the project, it will be automatically detected and used by the CLI tool. After the installation is done, you should see the PLATFORMSH_CLI_TOKEN variable when running env. Option 1: Using the web interface Open the project web interface and navigate to the environment in which you want to use the CLI, then click on Configure environment and go to the Variables tab. Add a variable named env:PLATFORMSH_CLI_TOKEN and set its value to the previously created token. https://community.platform.sh/uploads/default/49b4472ba7c3fd557210a40fe39e4ca89a75e902 Option 2: Using the CLI If you have the CLI tool installed and configured, you can add the variable from the command line: platform variable:create -e --level environment --name env:PLATFORMSH_CLI_TOKEN --sensitive true --value Replace the and with the correct values for your use case. 3. Install the CLI tool in the application container Once the variable has been added to the environment, it is required to download and install the CLI tool in a build hook. Modify .platform.app.yaml in the project to include: hooks: build: | curl -sS https://platform.sh/cli/installer | php This will download the CLI to a known directory, .platformsh/bin , which will be added to the PATH at runtime (via the .environment file). Redeploy your environment and log into the application container with SSH, then run platform. You should see the welcome prompt and a list of your current projects. Conclusion Having the CLI tool set up in an environment opens up further automation possibilities - renewing the SSL certificate, triggering snapshots, etc.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-use-the-platform-sh-cli-from-the-web-container/126",
        "relurl": "/t/how-to-use-the-platform-sh-cli-from-the-web-container/126"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e31b14ceb71374402b78c16df608ead85d567d75",
        "title": "How to make a Vue.js single page application (SPA) on Platform.sh",
        "description": "Goal This guide shows how to deploy a https://vuejs.org/ application on http://Platform.sh, using https://cli.vuejs.org/ . Assumptions To go through this guide, you will need: An empty http://Platform.sh project (you can https://accounts.platform.sh/platform/trial/general/setup to start your free trial) Your SSH key loaded in your https://linux.die.net/man/1/ssh-agent and configured in the https://accounts.platform.sh/user The https://github.com/platformsh/platformsh-cli installed. The https://cli.vuejs.org/guide/installation.html installed. Steps 1. Create a Vue.js application Create a new Vue.js project with Vue CLI (ignore this step if you want to deploy an existing Vue.js application on http://Platform.sh): vue create vuejs-platformsh cd vuejs-platformsh Set the platform Git remote: platform project:set-remote Add the https://npmjs.com/package/vue-cli-plugin-platformsh : vue add platformsh This plugin will add the http://Platform.sh configuration files to your project and extract the http://Platform.sh environment variables. 2. Deploy your application to http://Platform.sh Commit and push your code to deploy your application: git add --all git commit -m \"Vue.js on Platform.sh.\" git push platform master 3. Test your application on http://Platform.sh platform url This opens a browser tab with your Vue.js application running. 4. Fetch http://Platform.sh environment variables The plugin will automatically extract the http://Platform.sh environment variables. To fetch those, you simply need to import the following package: import platformshVar from 'platformsh_variables' Conclusion Using the Vue CLI, it’s very easy to setup and deploy a new or an existing Vue.js application on http://Platform.sh.",
        "text": "Goal This guide shows how to deploy a https://vuejs.org/ application on http://Platform.sh, using https://cli.vuejs.org/ . Assumptions To go through this guide, you will need: An empty http://Platform.sh project (you can https://accounts.platform.sh/platform/trial/general/setup to start your free trial) Your SSH key loaded in your https://linux.die.net/man/1/ssh-agent and configured in the https://accounts.platform.sh/user The https://github.com/platformsh/platformsh-cli installed. The https://cli.vuejs.org/guide/installation.html installed. Steps 1. Create a Vue.js application Create a new Vue.js project with Vue CLI (ignore this step if you want to deploy an existing Vue.js application on http://Platform.sh): vue create vuejs-platformsh cd vuejs-platformsh Set the platform Git remote: platform project:set-remote Add the https://npmjs.com/package/vue-cli-plugin-platformsh : vue add platformsh This plugin will add the http://Platform.sh configuration files to your project and extract the http://Platform.sh environment variables. 2. Deploy your application to http://Platform.sh Commit and push your code to deploy your application: git add --all git commit -m \"Vue.js on Platform.sh.\" git push platform master 3. Test your application on http://Platform.sh platform url This opens a browser tab with your Vue.js application running. 4. Fetch http://Platform.sh environment variables The plugin will automatically extract the http://Platform.sh environment variables. To fetch those, you simply need to import the following package: import platformshVar from 'platformsh_variables' Conclusion Using the Vue CLI, it’s very easy to setup and deploy a new or an existing Vue.js application on http://Platform.sh.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-make-a-vue-js-single-page-application-spa-on-platform-sh/125",
        "relurl": "/t/how-to-make-a-vue-js-single-page-application-spa-on-platform-sh/125"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "5e5b7c74d6538b26edc10e307d3c05ed914960a7",
        "title": "How to monitor long running scripts with pv",
        "description": "Goal Sometimes you want to run a long running process without feedback. https://linux.die.net/man/1/pv is a tool that allows you to do just that. Example Running a mysqldump mysqldump | pv --progress --size 100m /tmp/dump.sql Importing a mysql dump pv dump.sql | mysql Copying a large file pv /originallocation/largefile.bin /otherlocation/largefile.bin Create a zip file pv largefile.txt | zip largefile.zip Create a tar archive tar -czf - ./foldertotar | (pv -p --timer --rate --bytes backup.tgz) Problem Sounds cool, but pv isn’t available by default. Solution Luckily, there is an easy solution. Simply add the pv package in your .platform.app.yaml file. This will automatically install pv. So you can use it. dependencies: nodejs: # Specify one NPM package per line. pv: '~1.0.1' Note: you can do this in any container type. PHP, nodejs, golang, … This works because there is a https://github.com/roccomuso/pv that does the heavy lifting for us. For more information on dependencies, check ",
        "text": "Goal Sometimes you want to run a long running process without feedback. https://linux.die.net/man/1/pv is a tool that allows you to do just that. Example Running a mysqldump mysqldump | pv --progress --size 100m /tmp/dump.sql Importing a mysql dump pv dump.sql | mysql Copying a large file pv /originallocation/largefile.bin /otherlocation/largefile.bin Create a zip file pv largefile.txt | zip largefile.zip Create a tar archive tar -czf - ./foldertotar | (pv -p --timer --rate --bytes backup.tgz) Problem Sounds cool, but pv isn’t available by default. Solution Luckily, there is an easy solution. Simply add the pv package in your .platform.app.yaml file. This will automatically install pv. So you can use it. dependencies: nodejs: # Specify one NPM package per line. pv: '~1.0.1' Note: you can do this in any container type. PHP, nodejs, golang, … This works because there is a https://github.com/roccomuso/pv that does the heavy lifting for us. For more information on dependencies, check ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-monitor-long-running-scripts-with-pv/537",
        "relurl": "/t/how-to-monitor-long-running-scripts-with-pv/537"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "ed74c8a025b1e01a544851e27045c6002576f27b",
        "title": "How to configure Java with New Relic At Platform.sh",
        "description": "Goal This tutorial will explain how to configure New Relic into a Java application at http://Platform.sh. Assumptions You either have a Java application, and you want to run at http://Platform.sh or you already have a Java application running at http://Platform.sh A text editor of your choice. Problems New Relic is a technology company that develops cloud-based software to help website and application owners track the performance of their services. That might be useful because you can track everything from performance issues to tiny errors within your code. Every minute the agent posts metric time slice and event data to the New Relic user interface, where the owner of that data can sign in and use the data to see how their website is performing. Steps To set up new-relic in the Java project, we have two ways: Using the maven project Download the code through application.app.yaml. Maven This section explains how to configure Maven to download and unzip the newrelic-java.zip file, which contains all New Relic Java agent components. To set up the application with New Relic, you have two options: To set up the application with New Relic, you have two options: Configuring and download from Maven Downloading on your own Configure your pom.xml to download newrelic-java.zip. For example: com.newrelic.agent.java JAVA_AGENT_VERSION provided zip Replace JAVA_AGENT_VERSION with the https://docs.newrelic.com/docs/agents/java-agent/getting-started/java-release-notes . Unzip newrelic-java.zip by configuring maven-dependency-plugin in your pom.xml. For example: org.apache.maven.plugins 3.1.1 unpack-newrelic package unpack-dependencies com.newrelic.agent.java newrelic-java **/newrelic.yml false false true ${project.build.directory} The next step is to configure the https://docs.platform.sh/configuration/app-containers.html file to: Set the agent in the JVM parameters Overwrite the application file with the proper license key and application name. You can also do it using the https://docs.platform.sh/development/variables.html or . Therefore this configuration will work outside the code very useful when the application is on a public repository. name: app type: 'java:8' disk: 1024 hooks: build: | mvn clean package rm -rf newrelic/ mv target/newrelic/ newrelic/ mounts: 'server/': source: local source_path: server_source variables: env: NEW_RELIC_LICENSE_KEY: NEW_RELIC_APP_NAME: web: commands: start: | cp target/dependency/webapp-runner.jar server/webapp-runner.jar cp target/tomcat.war server/tomcat.war java -jar \\ -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError \\ -javaagent:/app/newrelic/newrelic.jar //the other parameters here Manual installation To use this installation it is only required that you modify .platform.app.yaml , which will download and set the New Relic Java agent for you. name: app type: 'java:8' disk: 1024 variables: env: NEW_RELIC_URL: https://download.newrelic.com/newrelic/java-agent/newrelic-agent/current/newrelic-java.zip NEW_RELIC_LICENSE_KEY: NEW_RELIC_APP_NAME: hooks: build: | mvn clean package rm -rf newrelic curl -O $NEW_RELIC_URL unzip newrelic-java.zip web: commands: start: | java -jar \\ -Xmx$(jq .info.limits.memory /run/config.json)m \\ -XX:+ExitOnOutOfMemoryError \\ -javaagent:/app/newrelic/newrelic.jar //the left of the commands here References https://newrelic.com/ https://docs.newrelic.com/docs/agents/java-agent/configuration/java-agent-configuration-config-file https://docs.newrelic.com/docs/agents/java-agent/additional-installation/install-java-agent-using-maven https://docs.newrelic.com/docs/agents/java-agent/installation/install-java-agent https://github.com/platformsh-examples/new-relic-java ",
        "text": "Goal This tutorial will explain how to configure New Relic into a Java application at http://Platform.sh. Assumptions You either have a Java application, and you want to run at http://Platform.sh or you already have a Java application running at http://Platform.sh A text editor of your choice. Problems New Relic is a technology company that develops cloud-based software to help website and application owners track the performance of their services. That might be useful because you can track everything from performance issues to tiny errors within your code. Every minute the agent posts metric time slice and event data to the New Relic user interface, where the owner of that data can sign in and use the data to see how their website is performing. Steps To set up new-relic in the Java project, we have two ways: Using the maven project Download the code through application.app.yaml. Maven This section explains how to configure Maven to download and unzip the newrelic-java.zip file, which contains all New Relic Java agent components. To set up the application with New Relic, you have two options: To set up the application with New Relic, you have two options: Configuring and download from Maven Downloading on your own Configure your pom.xml to download newrelic-java.zip. For example: com.newrelic.agent.java JAVA_AGENT_VERSION provided zip Replace JAVA_AGENT_VERSION with the https://docs.newrelic.com/docs/agents/java-agent/getting-started/java-release-notes . Unzip newrelic-java.zip by configuring maven-dependency-plugin in your pom.xml. For example: org.apache.maven.plugins 3.1.1 unpack-newrelic package unpack-dependencies com.newrelic.agent.java newrelic-java **/newrelic.yml false false true ${project.build.directory} The next step is to configure the https://docs.platform.sh/configuration/app-containers.html file to: Set the agent in the JVM parameters Overwrite the application file with the proper license key and application name. You can also do it using the https://docs.platform.sh/development/variables.html or . Therefore this configuration will work outside the code very useful when the application is on a public repository. name: app type: 'java:8' disk: 1024 hooks: build: | mvn clean package rm -rf newrelic/ mv target/newrelic/ newrelic/ mounts: 'server/': source: local source_path: server_source variables: env: NEW_RELIC_LICENSE_KEY: NEW_RELIC_APP_NAME: web: commands: start: | cp target/dependency/webapp-runner.jar server/webapp-runner.jar cp target/tomcat.war server/tomcat.war java -jar \\ -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError \\ -javaagent:/app/newrelic/newrelic.jar //the other parameters here Manual installation To use this installation it is only required that you modify .platform.app.yaml , which will download and set the New Relic Java agent for you. name: app type: 'java:8' disk: 1024 variables: env: NEW_RELIC_URL: https://download.newrelic.com/newrelic/java-agent/newrelic-agent/current/newrelic-java.zip NEW_RELIC_LICENSE_KEY: NEW_RELIC_APP_NAME: hooks: build: | mvn clean package rm -rf newrelic curl -O $NEW_RELIC_URL unzip newrelic-java.zip web: commands: start: | java -jar \\ -Xmx$(jq .info.limits.memory /run/config.json)m \\ -XX:+ExitOnOutOfMemoryError \\ -javaagent:/app/newrelic/newrelic.jar //the left of the commands here References https://newrelic.com/ https://docs.newrelic.com/docs/agents/java-agent/configuration/java-agent-configuration-config-file https://docs.newrelic.com/docs/agents/java-agent/additional-installation/install-java-agent-using-maven https://docs.newrelic.com/docs/agents/java-agent/installation/install-java-agent https://github.com/platformsh-examples/new-relic-java ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-configure-java-with-new-relic-at-platform-sh/533",
        "relurl": "/t/how-to-configure-java-with-new-relic-at-platform-sh/533"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "76a4d76b36e0fd06b9aed49d73de38907faba264",
        "title": "Improving GraphQL performance by caching POST requests",
        "description": "At http://Platform.sh we recently came across a project that utilized GraphQL to pull in data for its front-end, but was experiencing performance issues with slow GraphQL queries. We found a way to conveniently cache those query results and greatly speed up the project as a result. Queries to GraphQL are normally done as POST requests, and caching the results of POST requests is typically bad practice. In our use case, some queries are taking upwards of 5s to execute, which results in a rather poor user experience. Combining caching with a short TTL, (say, 60 seconds) the slightly-outdated content is preferable to an incredibly slow user experience. We start at https://fastly.com, and create a custom CDN endpoint to the GraphQL URI. This passes all requests (to that new endpoint) through the Fastly service, which uses Varnish to cache requests. Fastly allows the user to upload a custom VCL, using the as a starting point. The only change we need to make is to ensure that POST requests are treated in the same manner as GET requests. This results in two small changes: The block if (req.method != \"HEAD\" \u0026\u0026 req.method != \"GET\" \u0026\u0026 req.method != \"FASTLYPURGE\") { return(pass); } … should be changed to: if (req.method != \"HEAD\" \u0026\u0026 req.method != \"GET\" \u0026\u0026 req.method != \"POST\" \u0026\u0026 req.method != \"FASTLYPURGE\") { return(pass); } And similarly, the block if ((beresp.status == 500 || beresp.status == 503) \u0026\u0026 req.restarts ",
        "text": "At http://Platform.sh we recently came across a project that utilized GraphQL to pull in data for its front-end, but was experiencing performance issues with slow GraphQL queries. We found a way to conveniently cache those query results and greatly speed up the project as a result. Queries to GraphQL are normally done as POST requests, and caching the results of POST requests is typically bad practice. In our use case, some queries are taking upwards of 5s to execute, which results in a rather poor user experience. Combining caching with a short TTL, (say, 60 seconds) the slightly-outdated content is preferable to an incredibly slow user experience. We start at https://fastly.com, and create a custom CDN endpoint to the GraphQL URI. This passes all requests (to that new endpoint) through the Fastly service, which uses Varnish to cache requests. Fastly allows the user to upload a custom VCL, using the as a starting point. The only change we need to make is to ensure that POST requests are treated in the same manner as GET requests. This results in two small changes: The block if (req.method != \"HEAD\" \u0026\u0026 req.method != \"GET\" \u0026\u0026 req.method != \"FASTLYPURGE\") { return(pass); } … should be changed to: if (req.method != \"HEAD\" \u0026\u0026 req.method != \"GET\" \u0026\u0026 req.method != \"POST\" \u0026\u0026 req.method != \"FASTLYPURGE\") { return(pass); } And similarly, the block if ((beresp.status == 500 || beresp.status == 503) \u0026\u0026 req.restarts ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/improving-graphql-performance-by-caching-post-requests/530",
        "relurl": "/t/improving-graphql-performance-by-caching-post-requests/530"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "b7e514098b82f3c01c55e3005005cf8dcab4b94e",
        "title": "How to Overwrite Spring Data variable to access Platform.sh services",
        "description": "Goal In this tutorial, we’ll cover how you can overwrite https://spring.io/ configurations to access the services in http://Platform.sh. Preparation This tutorial assumes you have A working Spring application with Java 8 or higher A text editor of your choice. Problems http://Platform.sh has a https://github.com/platformsh/config-reader-java library that provides a streamlined and easy to interact with a http://Platform.sh environment and its https://docs.platform.sh/configuration/services.html. However, you can also use the application regularly and overwrite those configurations when you deploy your application on http://Platform.sh. That is useful when you either already have one app and want to move to http://Platform.sh or keep the default configuration to run locally. Steps To keep the configurations Spring has by default, the application.properties file is where you can set the settings that you wish on your application. Furthermore, . E.g.: Give a Spring Data JPA application that you’re running locally PostgreSQL with those properties on the applications.properties: ## default connection pool spring.datasource.hikari.connectionTimeout=20000 spring.datasource.hikari.maximumPoolSize=5 ## PostgreSQL spring.datasource.url=jdbc:postgresql://localhost:5432/people spring.datasource.username=postgres spring.datasource.password=password spring.jpa.hibernate.ddl-auto=update You can overwrite those configurations on the platform.app.yaml, the application configuration file for http://Platform.sh: name: app type: \"java:11\" disk: 1024 hooks: build: mvn clean install relationships: database: \"dbpostgres:postgresql\" web: commands: start: | export DB_PORT=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].port\"` export USER=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].username\"` export PASSWORD=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].password\"` export HOST=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].host\"` export DATABASE=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].path\"` export URL=\"jdbc:postgresql://${HOST}:${DB_PORT}/${DATABASE}\" java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError \\ -Dspring.datasource.url=$URL \\ -Dspring.datasource.username=$USER \\ -Dspring.datasource.password=$PASSWORD \\ target/spring-boot.jar --server.port=$PORT Therefore, you can have the configuration or just migrate the application that already exists to http://Platform.sh. References https://spring.io/ https://docs.spring.io/spring-boot/docs/current/reference/html/appendix-application-properties.html https://docs.platform.sh/frameworks/spring.html https://github.com/platformsh-examples/java-overwrite-configuration/tree/master/spring-jpa ",
        "text": "Goal In this tutorial, we’ll cover how you can overwrite https://spring.io/ configurations to access the services in http://Platform.sh. Preparation This tutorial assumes you have A working Spring application with Java 8 or higher A text editor of your choice. Problems http://Platform.sh has a https://github.com/platformsh/config-reader-java library that provides a streamlined and easy to interact with a http://Platform.sh environment and its https://docs.platform.sh/configuration/services.html. However, you can also use the application regularly and overwrite those configurations when you deploy your application on http://Platform.sh. That is useful when you either already have one app and want to move to http://Platform.sh or keep the default configuration to run locally. Steps To keep the configurations Spring has by default, the application.properties file is where you can set the settings that you wish on your application. Furthermore, . E.g.: Give a Spring Data JPA application that you’re running locally PostgreSQL with those properties on the applications.properties: ## default connection pool spring.datasource.hikari.connectionTimeout=20000 spring.datasource.hikari.maximumPoolSize=5 ## PostgreSQL spring.datasource.url=jdbc:postgresql://localhost:5432/people spring.datasource.username=postgres spring.datasource.password=password spring.jpa.hibernate.ddl-auto=update You can overwrite those configurations on the platform.app.yaml, the application configuration file for http://Platform.sh: name: app type: \"java:11\" disk: 1024 hooks: build: mvn clean install relationships: database: \"dbpostgres:postgresql\" web: commands: start: | export DB_PORT=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].port\"` export USER=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].username\"` export PASSWORD=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].password\"` export HOST=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].host\"` export DATABASE=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].path\"` export URL=\"jdbc:postgresql://${HOST}:${DB_PORT}/${DATABASE}\" java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError \\ -Dspring.datasource.url=$URL \\ -Dspring.datasource.username=$USER \\ -Dspring.datasource.password=$PASSWORD \\ target/spring-boot.jar --server.port=$PORT Therefore, you can have the configuration or just migrate the application that already exists to http://Platform.sh. References https://spring.io/ https://docs.spring.io/spring-boot/docs/current/reference/html/appendix-application-properties.html https://docs.platform.sh/frameworks/spring.html https://github.com/platformsh-examples/java-overwrite-configuration/tree/master/spring-jpa ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-overwrite-spring-data-variable-to-access-platform-sh-services/518",
        "relurl": "/t/how-to-overwrite-spring-data-variable-to-access-platform-sh-services/518"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "b1004e7b9065e8b447509de310e6ddf03e199021",
        "title": "How to Migrate your Java 8 application to Java 11 on Platform.sh",
        "description": "Goal In this tutorial, we’ll cover the how and why of upgrading a Java 8 project to Java 11 on http://Platform.sh. Preparation This tutorial assumes you have A working Java 8 application already deployed on http://Platform.sh. A text editor of your choice. Problems Java 8, the old Java LTS release, is no longer maintained. As an unmaintained version it will no longer receive security fixes, and so over time there will be more and more known-but-unfixed security issues found with it. Java 11 is the new LTS release, and is both more secure and more performant. https://twitter.com/leonardopanga ’s https://medium.com/criciumadev/its-time-migrating-to-java-11-5eb3868354f9 covers the major new features, including: Next LTS version Full support for containers Support parallel full garbage collection on G1. Free Application Class-Data Sharing feature. Heap allocation on alternative memory devices. New default set of root authority certificates. New https://wiki.openjdk.java.net/display/zgc/Main and http://openjdk.java.net/jeps/318 garbage collectors. Ahead-of-time compilation and GraalVM. Transport Layer Security (TLS) 1.3. http://openjdk.java.net/jeps/254 http://openjdk.java.net/jeps/248 Several benchmarks on the JVM improvements https://www.optaplanner.org/blog/2019/01/17/HowMuchFasterIsJava11.html . Furthermore, code written for Java 8 doesn’t need to be updated to run on the Java 11 JVM, https://caff.de/posts/java-11-vs-8-performance/ . To keep your code working you need to be aware of https://openjdk.java.net/jeps/320 . If you are using those modules you’ll need to re-add them in your dependency management tool. See . Steps To update the application, you need to update a single file: the .platform.app.yaml file. On a development branch (not production), find the type property: name: app type: \"java:8\" Update this line to Java 11: name: app type: \"java:11\" Commit the changes and push. http://Platform.sh will then build a new container using Java 11 instead of Java 8. The new version will be used for both the build process (compilation) and the running environment. Once you’re satisfied that the update is safe to complete, merge this new branch to master and push. References https://docs.platform.sh/languages/java.html ",
        "text": "Goal In this tutorial, we’ll cover the how and why of upgrading a Java 8 project to Java 11 on http://Platform.sh. Preparation This tutorial assumes you have A working Java 8 application already deployed on http://Platform.sh. A text editor of your choice. Problems Java 8, the old Java LTS release, is no longer maintained. As an unmaintained version it will no longer receive security fixes, and so over time there will be more and more known-but-unfixed security issues found with it. Java 11 is the new LTS release, and is both more secure and more performant. https://twitter.com/leonardopanga ’s https://medium.com/criciumadev/its-time-migrating-to-java-11-5eb3868354f9 covers the major new features, including: Next LTS version Full support for containers Support parallel full garbage collection on G1. Free Application Class-Data Sharing feature. Heap allocation on alternative memory devices. New default set of root authority certificates. New https://wiki.openjdk.java.net/display/zgc/Main and http://openjdk.java.net/jeps/318 garbage collectors. Ahead-of-time compilation and GraalVM. Transport Layer Security (TLS) 1.3. http://openjdk.java.net/jeps/254 http://openjdk.java.net/jeps/248 Several benchmarks on the JVM improvements https://www.optaplanner.org/blog/2019/01/17/HowMuchFasterIsJava11.html . Furthermore, code written for Java 8 doesn’t need to be updated to run on the Java 11 JVM, https://caff.de/posts/java-11-vs-8-performance/ . To keep your code working you need to be aware of https://openjdk.java.net/jeps/320 . If you are using those modules you’ll need to re-add them in your dependency management tool. See . Steps To update the application, you need to update a single file: the .platform.app.yaml file. On a development branch (not production), find the type property: name: app type: \"java:8\" Update this line to Java 11: name: app type: \"java:11\" Commit the changes and push. http://Platform.sh will then build a new container using Java 11 instead of Java 8. The new version will be used for both the build process (compilation) and the running environment. Once you’re satisfied that the update is safe to complete, merge this new branch to master and push. References https://docs.platform.sh/languages/java.html ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-migrate-your-java-8-application-to-java-11-on-platform-sh/499",
        "relurl": "/t/how-to-migrate-your-java-8-application-to-java-11-on-platform-sh/499"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "4f41fc199792a82171e62fa0f8a345bca5b641e4",
        "title": "How to Overwrite variables to Payara JPA access Platform.sh SQL services",
        "description": "Goal In this tutorial, we’ll cover how you can overwrite https://www.payara.fish/ configurations to access the services in http://Platform.sh. Preparation This tutorial assumes you have A working Payara application with Java 8 or higher A text editor of your choice. Problems http://Platform.sh has a https://github.com/platformsh/config-reader-java library that provides a streamlined and easy to use way to interact with a http://Platform.sh environment and https://docs.platform.sh/configuration/services.html . However, you can also use the application regularly and overwrite those configurations when you deploy your application on http://Platform.sh. That is useful when you either already have one app and want to move to http://Platform.sh or keep the default configuration to run locally. Steps First, to use a database as a data source in Payara Micro, you’ll need to have the database already running. Once you have that in place, download the JDBC driver for the database and put it into your WEB-INF/lib directory. In your web.xml add the following: java:global/JPAExampleDataSource org.postgresql.ds.PGSimpleDataSource ${server.host} 5432 ${server.database} ${server.user} ${server.password} You can overwrite those configurations on the platform.app.yaml the application configuration to http://Platform.sh. As shown in the configuration below. name: app type: \"java:8\" disk: 1024 hooks: build: mvn clean package payara-micro:bundle relationships: database: \"db:postgresql\" web: commands: start: | export HOST=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].host\"` export PASSWORD=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].password\"` export USER=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].username\"` export DATABASE=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].path\"` java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError \\ -Dserver.host=$HOST \\ -Dserver.database=$DATABASE \\ -Dserver.user=$USER \\ -Dserver.password=$PASSWORD \\ target/microprofile-microbundle.jar --port $PORT Therefore, you can have the configuration or just migrate the application that already exists to http://Platform.sh. References https://www.payara.fish/ https://github.com/payara/Payara-Examples/tree/master/payara-micro/jpa-datasource-example https://blog.payara.fish/setting-up-a-data-source-in-payara-micro https://docs.platform.sh/frameworks/jakarta.html https://github.com/platformsh-examples/java-overwrite-configuration/blob/master/payara/README.md ",
        "text": "Goal In this tutorial, we’ll cover how you can overwrite https://www.payara.fish/ configurations to access the services in http://Platform.sh. Preparation This tutorial assumes you have A working Payara application with Java 8 or higher A text editor of your choice. Problems http://Platform.sh has a https://github.com/platformsh/config-reader-java library that provides a streamlined and easy to use way to interact with a http://Platform.sh environment and https://docs.platform.sh/configuration/services.html . However, you can also use the application regularly and overwrite those configurations when you deploy your application on http://Platform.sh. That is useful when you either already have one app and want to move to http://Platform.sh or keep the default configuration to run locally. Steps First, to use a database as a data source in Payara Micro, you’ll need to have the database already running. Once you have that in place, download the JDBC driver for the database and put it into your WEB-INF/lib directory. In your web.xml add the following: java:global/JPAExampleDataSource org.postgresql.ds.PGSimpleDataSource ${server.host} 5432 ${server.database} ${server.user} ${server.password} You can overwrite those configurations on the platform.app.yaml the application configuration to http://Platform.sh. As shown in the configuration below. name: app type: \"java:8\" disk: 1024 hooks: build: mvn clean package payara-micro:bundle relationships: database: \"db:postgresql\" web: commands: start: | export HOST=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].host\"` export PASSWORD=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].password\"` export USER=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].username\"` export DATABASE=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].path\"` java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError \\ -Dserver.host=$HOST \\ -Dserver.database=$DATABASE \\ -Dserver.user=$USER \\ -Dserver.password=$PASSWORD \\ target/microprofile-microbundle.jar --port $PORT Therefore, you can have the configuration or just migrate the application that already exists to http://Platform.sh. References https://www.payara.fish/ https://github.com/payara/Payara-Examples/tree/master/payara-micro/jpa-datasource-example https://blog.payara.fish/setting-up-a-data-source-in-payara-micro https://docs.platform.sh/frameworks/jakarta.html https://github.com/platformsh-examples/java-overwrite-configuration/blob/master/payara/README.md ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-overwrite-variables-to-payara-jpa-access-platform-sh-sql-services/519",
        "relurl": "/t/how-to-overwrite-variables-to-payara-jpa-access-platform-sh-sql-services/519"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "6e5e4cf509e1096d0f6721cbc9fd394f3cca2465",
        "title": "How to overwrite configuration to Jakarta/MicroProfile to access Platform.sh services",
        "description": "Goal In this tutorial, we’ll cover how you can overwrite Jakarta EE/MicroProfile configurations to access the services in http://Platform.sh. Preparation This tutorial assumes you have: A working Jakarta EE/MicroProfile application with Java 8 or higher A text editor of your choice. Problems http://Platform.sh has a https://github.com/platformsh/config-reader-java library that provides a streamlined and easy to interact with a http://Platform.sh environment and its https://docs.platform.sh/configuration/services.html. However, you can also use the application regularly and overwrite those configurations when you deploy your application on http://Platform.sh. That is useful when you either already have one app and want to move to http://Platform.sh or keep the default configuration to run locally. Steps For https://microprofile.io// https://jakarta.ee/ the configurations and settings are set by https://github.com/eclipse/microprofile-config . It is as a convention and not as standard, which by default it uses the microprofile-config.properties file where you can set the settings and overwrite it. To have a https://projects.eclipse.org/projects/ee4j.nosql application that connects a https://www.mongodb.com/ database locally, the configuration will be like this: document=document document.database=conferences document.settings.jakarta.nosql.host=localhost:27017 document.provider=org.eclipse.jnosql.diana.mongodb.document.MongoDBDocumentConfiguration You can overwrite those configurations in platform.app.yaml, the application configuration file for http://Platform.sh: name: app type: \"java:8\" disk: 800 hooks: build: mvn -U -DskipTests clean package payara-micro:bundle relationships: mongodb: 'mongodb:mongodb' web: commands: start: | export MONGO_PORT=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".mongodb[0].port\"` export MONGO_HOST=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".mongodb[0].host\"` export MONGO_ADDRESS=\"${MONGO_HOST}:${MONGO_PORT}\" export MONGO_PASSWORD=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".mongodb[0].password\"` export MONGO_USER=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".mongodb[0].username\"` export MONGO_DATABASE=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".mongodb[0].path\"` java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -Ddocument.settings.jakarta.nosql.host=$MONGO_ADDRESS \\ -Ddocument.database=$MONGO_DATABASE -Ddocument.settings.jakarta.nosql.user=$MONGO_USER \\ -Ddocument.settings.jakarta.nosql.password=$MONGO_PASSWORD \\ -Ddocument.settings.mongodb.authentication.source=$MONGO_DATABASE \\ target/heroes-microbundle.jar --port $PORT Therefore, you can have the configuration or just migrate the application that already exists to http://Platform.sh. References https://jakarta.ee/ https://microprofile.io/ https://github.com/eclipse/microprofile-config https://www.jnosql.org/ https://docs.platform.sh/frameworks/jakarta.html https://github.com/platformsh-examples/java-overwrite-configuration/tree/master/jakarta-nosql ",
        "text": "Goal In this tutorial, we’ll cover how you can overwrite Jakarta EE/MicroProfile configurations to access the services in http://Platform.sh. Preparation This tutorial assumes you have: A working Jakarta EE/MicroProfile application with Java 8 or higher A text editor of your choice. Problems http://Platform.sh has a https://github.com/platformsh/config-reader-java library that provides a streamlined and easy to interact with a http://Platform.sh environment and its https://docs.platform.sh/configuration/services.html. However, you can also use the application regularly and overwrite those configurations when you deploy your application on http://Platform.sh. That is useful when you either already have one app and want to move to http://Platform.sh or keep the default configuration to run locally. Steps For https://microprofile.io// https://jakarta.ee/ the configurations and settings are set by https://github.com/eclipse/microprofile-config . It is as a convention and not as standard, which by default it uses the microprofile-config.properties file where you can set the settings and overwrite it. To have a https://projects.eclipse.org/projects/ee4j.nosql application that connects a https://www.mongodb.com/ database locally, the configuration will be like this: document=document document.database=conferences document.settings.jakarta.nosql.host=localhost:27017 document.provider=org.eclipse.jnosql.diana.mongodb.document.MongoDBDocumentConfiguration You can overwrite those configurations in platform.app.yaml, the application configuration file for http://Platform.sh: name: app type: \"java:8\" disk: 800 hooks: build: mvn -U -DskipTests clean package payara-micro:bundle relationships: mongodb: 'mongodb:mongodb' web: commands: start: | export MONGO_PORT=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".mongodb[0].port\"` export MONGO_HOST=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".mongodb[0].host\"` export MONGO_ADDRESS=\"${MONGO_HOST}:${MONGO_PORT}\" export MONGO_PASSWORD=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".mongodb[0].password\"` export MONGO_USER=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".mongodb[0].username\"` export MONGO_DATABASE=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".mongodb[0].path\"` java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -Ddocument.settings.jakarta.nosql.host=$MONGO_ADDRESS \\ -Ddocument.database=$MONGO_DATABASE -Ddocument.settings.jakarta.nosql.user=$MONGO_USER \\ -Ddocument.settings.jakarta.nosql.password=$MONGO_PASSWORD \\ -Ddocument.settings.mongodb.authentication.source=$MONGO_DATABASE \\ target/heroes-microbundle.jar --port $PORT Therefore, you can have the configuration or just migrate the application that already exists to http://Platform.sh. References https://jakarta.ee/ https://microprofile.io/ https://github.com/eclipse/microprofile-config https://www.jnosql.org/ https://docs.platform.sh/frameworks/jakarta.html https://github.com/platformsh-examples/java-overwrite-configuration/tree/master/jakarta-nosql ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-overwrite-configuration-to-jakarta-microprofile-to-access-platform-sh-services/520",
        "relurl": "/t/how-to-overwrite-configuration-to-jakarta-microprofile-to-access-platform-sh-services/520"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "01dec78a8c614f5c91236814c7bd2b7a58d94e48",
        "title": "How to Configure your Java Application with Log4J at Platform.sh",
        "description": "Goal In this tutorial, we’ll cover the how and why to use log in your application using https://logging.apache.org/log4j/2.x/ http://platform.sh. Preparation This tutorial assumes you have A working Java 8 application already deployed on http://platform.sh. A text editor of your choice. Problems Logging is the process of writing log messages during the execution of a program to a central place. This logging allows you to report and persist error and warning messages as well as info messages (e.g., runtime statistics) so that the messages can later be retrieved and analyzed. Steps 1. http://Platform.sh log file In http://Platform.sh we have two options to log your Java application. The first one is to use the stdout where http://Platform.sh will handle the folder it includes to avoid the oversized in the disk issue. This log will be at the /var/log/app.log: https://docs.platform.sh/development/logs.html In your log4j.properties you can set it, e.g.: log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.Target=System.out log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n All log files are trimmed to 100 MB automatically. But if you need to have complete logs, you can set up cron which will upload them to third-party storage. https://www.contextualcode.com/ made a https://gitlab.com/contextualcode/platformsh-store-logs-at-s3 how to achieve it. 2. The custom log folder The second option is to create a new mount directory and then use it to handle your logs. The advantage is that we can create multiple records as you wish; however, you will then be responsible for purging old log files to avoid filling up the disk. In your log4j.properties you can set, e.g.: log4j.rootLogger=DEBUG, R log4j.appender.R=org.apache.log4j.RollingFileAppender log4j.appender.R.MaxFileSize=1MB log4j.appender.R.MaxBackupIndex=20 log4j.appender.R.File=/app/log/app.log log4j.appender.R.Append=true log4j.appender.R.DatePattern='.'yyyy-MM-dd'.log' log4j.appender.R.layout=org.apache.log4j.PatternLayout log4j.appender.R.layout.ConversionPattern=%d{MM/dd/yyyy HH:mm:ss,SSS} %-5p %c - %m%n This time we need to create a to create your log, so we need to go in the https://docs.platform.sh/configuration/app-containers.html to define a writable folder to the log. mounts: 'log/': source: local source_path: log_source Commit the changes and push to redeploy. References https://dzone.com/articles/logging-with-log4j-in-java https://www.vogella.com/tutorials/Logging/article.html https://docs.platform.sh/languages/java.html https://docs.platform.sh/development/logs.html ",
        "text": "Goal In this tutorial, we’ll cover the how and why to use log in your application using https://logging.apache.org/log4j/2.x/ http://platform.sh. Preparation This tutorial assumes you have A working Java 8 application already deployed on http://platform.sh. A text editor of your choice. Problems Logging is the process of writing log messages during the execution of a program to a central place. This logging allows you to report and persist error and warning messages as well as info messages (e.g., runtime statistics) so that the messages can later be retrieved and analyzed. Steps 1. http://Platform.sh log file In http://Platform.sh we have two options to log your Java application. The first one is to use the stdout where http://Platform.sh will handle the folder it includes to avoid the oversized in the disk issue. This log will be at the /var/log/app.log: https://docs.platform.sh/development/logs.html In your log4j.properties you can set it, e.g.: log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.Target=System.out log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n All log files are trimmed to 100 MB automatically. But if you need to have complete logs, you can set up cron which will upload them to third-party storage. https://www.contextualcode.com/ made a https://gitlab.com/contextualcode/platformsh-store-logs-at-s3 how to achieve it. 2. The custom log folder The second option is to create a new mount directory and then use it to handle your logs. The advantage is that we can create multiple records as you wish; however, you will then be responsible for purging old log files to avoid filling up the disk. In your log4j.properties you can set, e.g.: log4j.rootLogger=DEBUG, R log4j.appender.R=org.apache.log4j.RollingFileAppender log4j.appender.R.MaxFileSize=1MB log4j.appender.R.MaxBackupIndex=20 log4j.appender.R.File=/app/log/app.log log4j.appender.R.Append=true log4j.appender.R.DatePattern='.'yyyy-MM-dd'.log' log4j.appender.R.layout=org.apache.log4j.PatternLayout log4j.appender.R.layout.ConversionPattern=%d{MM/dd/yyyy HH:mm:ss,SSS} %-5p %c - %m%n This time we need to create a to create your log, so we need to go in the https://docs.platform.sh/configuration/app-containers.html to define a writable folder to the log. mounts: 'log/': source: local source_path: log_source Commit the changes and push to redeploy. References https://dzone.com/articles/logging-with-log4j-in-java https://www.vogella.com/tutorials/Logging/article.html https://docs.platform.sh/languages/java.html https://docs.platform.sh/development/logs.html ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-configure-your-java-application-with-log4j-at-platform-sh/514",
        "relurl": "/t/how-to-configure-your-java-application-with-log4j-at-platform-sh/514"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "c7a1b88d335d55ff3c8d8b614183f4a86a8f3e34",
        "title": "How to migrate your Java Application from Heroku to Platform.sh",
        "description": "Goal Migrate a Heroku Java Application to a http://Platform.sh service. Assumptions You will need: an active application on http://Platform.sh configured to an empty database an active application on Heroku Steps 1. Git clone the Heroku Repository git clone heroku_repository.git 2. Create the three basic http://Platform.sh file .platform/routes.yaml : http://Platform.sh allows you to define the https://docs.platform.sh/configuration/routes.html. .platform/services.yaml: http://Platform.sh allows you to completely define and configure the topology and https://docs.platform.sh/configuration/services.html . .platform.app.yaml: You control your application and the way it will be built and deployed on http://Platform.sh https://docs.platform.sh/configuration/app-containers.html . 3. Set the Application File At the .platform.app.yaml we’ll set the minimum configuration to run a plain configuration be aware that it does not include database services such as MySQL, MariaDB and so on. # This file describes an application. You can have multiple applications # in the same project. # # See https://docs.platform.sh/user_guide/reference/platform-app-yaml.html # The name of this app. Must be unique within a project. name: app # The runtime the application uses. type: 'java:8' disk: 1024 # The hooks executed at various points in the lifecycle of the application. hooks: build: mvn clean install mounts: 'server/': source: local source_path: server_source # The relationships of the application with services or other applications. # # The left-hand side is the name of the relationship as it will be exposed # to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand # side is in the form ` : `. # The configuration of app when it is exposed to the web. web: commands: start: | cp target/dependency/webapp-runner.jar server/webapp-runner.jar cp target/tomcat.war server/tomcat.war cd server \u0026\u0026 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError webapp-runner.jar --port $PORT tomcat.war 4. Add http://Platform.sh Remote Repository git remote add platform platform_repository.git 5. Push the changes to the Repository git push platform master Conclusion By adding a .platform.app.yaml file to a project, a Java application’s build process can be migrated from Heroku. In order to fully migrate, the next step is to use the Heroku CLI to dump your database and import it to a service defined on your http://Platform.sh project. Additional resources: https://community.platform.sh/t/how-to-migrate-a-postgresql-database-from-heroku-to-platform-sh/301 https://github.com/platformsh-examples/tomcat-webapp-runner ",
        "text": "Goal Migrate a Heroku Java Application to a http://Platform.sh service. Assumptions You will need: an active application on http://Platform.sh configured to an empty database an active application on Heroku Steps 1. Git clone the Heroku Repository git clone heroku_repository.git 2. Create the three basic http://Platform.sh file .platform/routes.yaml : http://Platform.sh allows you to define the https://docs.platform.sh/configuration/routes.html. .platform/services.yaml: http://Platform.sh allows you to completely define and configure the topology and https://docs.platform.sh/configuration/services.html . .platform.app.yaml: You control your application and the way it will be built and deployed on http://Platform.sh https://docs.platform.sh/configuration/app-containers.html . 3. Set the Application File At the .platform.app.yaml we’ll set the minimum configuration to run a plain configuration be aware that it does not include database services such as MySQL, MariaDB and so on. # This file describes an application. You can have multiple applications # in the same project. # # See https://docs.platform.sh/user_guide/reference/platform-app-yaml.html # The name of this app. Must be unique within a project. name: app # The runtime the application uses. type: 'java:8' disk: 1024 # The hooks executed at various points in the lifecycle of the application. hooks: build: mvn clean install mounts: 'server/': source: local source_path: server_source # The relationships of the application with services or other applications. # # The left-hand side is the name of the relationship as it will be exposed # to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand # side is in the form ` : `. # The configuration of app when it is exposed to the web. web: commands: start: | cp target/dependency/webapp-runner.jar server/webapp-runner.jar cp target/tomcat.war server/tomcat.war cd server \u0026\u0026 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError webapp-runner.jar --port $PORT tomcat.war 4. Add http://Platform.sh Remote Repository git remote add platform platform_repository.git 5. Push the changes to the Repository git push platform master Conclusion By adding a .platform.app.yaml file to a project, a Java application’s build process can be migrated from Heroku. In order to fully migrate, the next step is to use the Heroku CLI to dump your database and import it to a service defined on your http://Platform.sh project. Additional resources: https://community.platform.sh/t/how-to-migrate-a-postgresql-database-from-heroku-to-platform-sh/301 https://github.com/platformsh-examples/tomcat-webapp-runner ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-migrate-your-java-application-from-heroku-to-platform-sh/441",
        "relurl": "/t/how-to-migrate-your-java-application-from-heroku-to-platform-sh/441"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "7ac02fd7f0e887eaff88fe68dc56af15532a2c6e",
        "title": "How to set up Wordpress to use the HTTP cache of Platform.sh",
        "description": "Goal Configure Wordpress to send proper cache-control headers, so the can be used. Assumptions A WordPress installation on http://Platform.sh that uses Composer, for example one that uses the https://github.com/platformsh/template-wordpress and is configured for a https://community.platform.sh/t/how-to-install-wordpress-plugins-and-themes-with-composer An https://docs.platform.sh/development/ssh.html configured on the project account https://getcomposer.org/ installed locally Problems By default, Wordpress doesn’t set proper headers to allow the build-in to work efficiently. You can test if your site is cached based on the x-platform-cache response header which will probably be a MISS with the default Wordpress installation. Steps 1. Install and enable the cache-control plugin The main configuration that the HTTP cache needs to work properly, is Cache-Control header. Since Wordpress does not include an option to set those headers in their core, you need to use a 3rd party plugin. The example used here is https://wordpress.org/plugins/cache-control/. If you have set up your project to use a composer-based installation of plugins ( https://community.platform.sh/t/how-to-install-wordpress-plugins-and-themes-with-composer ), you can use composer require wpackagist-plugin/cache-control locally to update your composer files. After that commit and push them so the plugin gets deployed to your project. In the Admin-Dashboard go to Plugins, find Cache-Control and click Activate. 2. Verify and adjust the header values In the plugin settings, you can adjust the time different sites is being cached (e.g home page, post sites, etc). You can adjust that as necessary, following good practises. https://community.platform.sh/uploads/default/83103faec078789e4fa3c155e364e73aa38d72aa 3. Settings for static assets For static assets, the Wordpress template includes a default Cache-Control of 600 in your .platform.app.yaml file. To adjust that (if needed), you find the expires setting in the . 4. Verify your site is being cached To check if the response is cached, you can use curl -I to retrieve the headers. While the first response might be a MISS (because the page has not been cached yet), as soon as you send the same request again, it should return HIT. curl -I https://master-7rqtwti-af6kbo7ndasc2.eu-3.platformsh.site/ HTTP/2 200 cache-control: max-age=300 … x-platform-cache: HIT … Conclusion With this setup, your Wordpress site can leverage the http://Platform.sh HTTP Cache and you have control over the caching behaviour for different parts of your Wordpress site.",
        "text": "Goal Configure Wordpress to send proper cache-control headers, so the can be used. Assumptions A WordPress installation on http://Platform.sh that uses Composer, for example one that uses the https://github.com/platformsh/template-wordpress and is configured for a https://community.platform.sh/t/how-to-install-wordpress-plugins-and-themes-with-composer An https://docs.platform.sh/development/ssh.html configured on the project account https://getcomposer.org/ installed locally Problems By default, Wordpress doesn’t set proper headers to allow the build-in to work efficiently. You can test if your site is cached based on the x-platform-cache response header which will probably be a MISS with the default Wordpress installation. Steps 1. Install and enable the cache-control plugin The main configuration that the HTTP cache needs to work properly, is Cache-Control header. Since Wordpress does not include an option to set those headers in their core, you need to use a 3rd party plugin. The example used here is https://wordpress.org/plugins/cache-control/. If you have set up your project to use a composer-based installation of plugins ( https://community.platform.sh/t/how-to-install-wordpress-plugins-and-themes-with-composer ), you can use composer require wpackagist-plugin/cache-control locally to update your composer files. After that commit and push them so the plugin gets deployed to your project. In the Admin-Dashboard go to Plugins, find Cache-Control and click Activate. 2. Verify and adjust the header values In the plugin settings, you can adjust the time different sites is being cached (e.g home page, post sites, etc). You can adjust that as necessary, following good practises. https://community.platform.sh/uploads/default/83103faec078789e4fa3c155e364e73aa38d72aa 3. Settings for static assets For static assets, the Wordpress template includes a default Cache-Control of 600 in your .platform.app.yaml file. To adjust that (if needed), you find the expires setting in the . 4. Verify your site is being cached To check if the response is cached, you can use curl -I to retrieve the headers. While the first response might be a MISS (because the page has not been cached yet), as soon as you send the same request again, it should return HIT. curl -I https://master-7rqtwti-af6kbo7ndasc2.eu-3.platformsh.site/ HTTP/2 200 cache-control: max-age=300 … x-platform-cache: HIT … Conclusion With this setup, your Wordpress site can leverage the http://Platform.sh HTTP Cache and you have control over the caching behaviour for different parts of your Wordpress site.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-set-up-wordpress-to-use-the-http-cache-of-platform-sh/508",
        "relurl": "/t/how-to-set-up-wordpress-to-use-the-http-cache-of-platform-sh/508"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "5bcf53ac327f598f8d0723b98a3bee07469bd3ed",
        "title": "How to install Wordpress plugins and themes with Composer",
        "description": "Goal Add external plugins or themes from the official directory to your Wordpress installation using composer. Assumptions A WordPress installation on http://Platform.sh that uses Composer, for example one that uses the https://github.com/platformsh/template-wordpress . An https://docs.platform.sh/development/ssh.html configured on the project account https://getcomposer.org/ installed locally Problems Since http://Platform.sh deploys the code read-only (and thus makes many Wordpress sites more secure by default), the integrated mechanism to add and update both plugins and themes doesn’t work. Luckily, https://wpackagist.org/ mirrors the WordPress plugin and theme directories as a Composer repository. This means you can add all plugins and themes covered by the directory using composer. Steps 1. Add the external repository Edit your composer.json file and add wpackagist as a repository. \"repositories\":[ { \"type\":\"composer\", \"url\":\"https://wpackagist.org\" } ], 2. Add a plugin or theme via composer You can use composer require to add a plugin or a theme as a dependency. For plugins, use wpackagist-plugin as the vendor name, for themes use wpackagist-theme. Examples: Plugin: composer require wpackagist-plugin/cache-control Theme: composer require wpackagist-theme/neve Composer will update your composer.json and composer.lock files accordingly. 3. Push to your repository By pushing those changes to http://Platform.sh, the build process will automatically install the themes and plugins in the right folder. git add composer.json composer.lock git commit -m 'adding themes/plugins' git push 4. Enable plugins/themes in the WP-Admin Dashboard The admin interface will show the plugins/themes and will allow you to enable them directly via the interface. https://community.platform.sh/uploads/default/190a0168bbfa12f70e4697173dde9e705688e27c https://community.platform.sh/uploads/default/d5cb437818dd8dfec2045255c7074593fea81340 Conclusion By using composer to install 3rd party plugins and themes, you get a reliable build process and make sure the code is deployed safely. This avoids committing plugins and themes into your git repository. It also makes it easy to keep plugins and themes up to date. You can follow https://community.platform.sh/t/how-to-upgrade-wordpress-core-and-dependencies-with-composer guide on how to keep your Wordpress site updated.",
        "text": "Goal Add external plugins or themes from the official directory to your Wordpress installation using composer. Assumptions A WordPress installation on http://Platform.sh that uses Composer, for example one that uses the https://github.com/platformsh/template-wordpress . An https://docs.platform.sh/development/ssh.html configured on the project account https://getcomposer.org/ installed locally Problems Since http://Platform.sh deploys the code read-only (and thus makes many Wordpress sites more secure by default), the integrated mechanism to add and update both plugins and themes doesn’t work. Luckily, https://wpackagist.org/ mirrors the WordPress plugin and theme directories as a Composer repository. This means you can add all plugins and themes covered by the directory using composer. Steps 1. Add the external repository Edit your composer.json file and add wpackagist as a repository. \"repositories\":[ { \"type\":\"composer\", \"url\":\"https://wpackagist.org\" } ], 2. Add a plugin or theme via composer You can use composer require to add a plugin or a theme as a dependency. For plugins, use wpackagist-plugin as the vendor name, for themes use wpackagist-theme. Examples: Plugin: composer require wpackagist-plugin/cache-control Theme: composer require wpackagist-theme/neve Composer will update your composer.json and composer.lock files accordingly. 3. Push to your repository By pushing those changes to http://Platform.sh, the build process will automatically install the themes and plugins in the right folder. git add composer.json composer.lock git commit -m 'adding themes/plugins' git push 4. Enable plugins/themes in the WP-Admin Dashboard The admin interface will show the plugins/themes and will allow you to enable them directly via the interface. https://community.platform.sh/uploads/default/190a0168bbfa12f70e4697173dde9e705688e27c https://community.platform.sh/uploads/default/d5cb437818dd8dfec2045255c7074593fea81340 Conclusion By using composer to install 3rd party plugins and themes, you get a reliable build process and make sure the code is deployed safely. This avoids committing plugins and themes into your git repository. It also makes it easy to keep plugins and themes up to date. You can follow https://community.platform.sh/t/how-to-upgrade-wordpress-core-and-dependencies-with-composer guide on how to keep your Wordpress site updated.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-install-wordpress-plugins-and-themes-with-composer/507",
        "relurl": "/t/how-to-install-wordpress-plugins-and-themes-with-composer/507"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "4b3f5c7dd6da631abb6463f7ac91bb628c70d289",
        "title": "How to use environment variables with nuxtjs on Platform.sh",
        "description": "Goal To use environment variables in a https://nuxtjs.org app on http://Platform.sh Problem Nuxt currently provides a very https://nuxtjs.org/api/configuration-env which uses webpack substitution to inject your env vars at build time. This works most of the time, but on http://Platform.Sh the build process is environment-agnostic. A build can, and will be reused on different environments. This means that environment variables are not available in the build hook. So nuxt is not able to compile them. Building nuxt in the deploy hook, is also not possible, since the file system is read only. Work around: nuxt-env Luckily, there is a work around. There’s a nuxtjs plugin called https://github.com/samtgarson/nuxt-env. This allows you to define the environment variables that you want to access at runtime. First, make sure you have nuxt-env installed by running yarn add nuxt-env on your local pc, and commit that yarn.lock file to your repository Then, you can add the env vars you want to injects to your nuxt.config.js file // nuxt.config.js // Tell nuxt-env which env vars you want to inject modules: [ 'other-nuxt-module', ['nuxt-env', { keys: [ 'TEST_ENV_VAR', // Basic usage—equivalent of { key: 'TEST_ENV_VAR' } { key: 'OTHER_ENV_VAR', default: 'defaultValue' } // Specify a default value { key: 'THIRD_ENV_VAR', secret: true } // Only inject the var server side { key: 'ANOTHER_ENV_VAR', name: 'MY_ENV_VAR' } // Rename the variable ] }] ] ",
        "text": "Goal To use environment variables in a https://nuxtjs.org app on http://Platform.sh Problem Nuxt currently provides a very https://nuxtjs.org/api/configuration-env which uses webpack substitution to inject your env vars at build time. This works most of the time, but on http://Platform.Sh the build process is environment-agnostic. A build can, and will be reused on different environments. This means that environment variables are not available in the build hook. So nuxt is not able to compile them. Building nuxt in the deploy hook, is also not possible, since the file system is read only. Work around: nuxt-env Luckily, there is a work around. There’s a nuxtjs plugin called https://github.com/samtgarson/nuxt-env. This allows you to define the environment variables that you want to access at runtime. First, make sure you have nuxt-env installed by running yarn add nuxt-env on your local pc, and commit that yarn.lock file to your repository Then, you can add the env vars you want to injects to your nuxt.config.js file // nuxt.config.js // Tell nuxt-env which env vars you want to inject modules: [ 'other-nuxt-module', ['nuxt-env', { keys: [ 'TEST_ENV_VAR', // Basic usage—equivalent of { key: 'TEST_ENV_VAR' } { key: 'OTHER_ENV_VAR', default: 'defaultValue' } // Specify a default value { key: 'THIRD_ENV_VAR', secret: true } // Only inject the var server side { key: 'ANOTHER_ENV_VAR', name: 'MY_ENV_VAR' } // Rename the variable ] }] ] ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-use-environment-variables-with-nuxtjs-on-platform-sh/505",
        "relurl": "/t/how-to-use-environment-variables-with-nuxtjs-on-platform-sh/505"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "215a7641ac7bf5885b62eb096983e40b8f1d0035",
        "title": "How to deploy Nextcloud to Platform.sh",
        "description": "Goal https://nextcloud.com/ is a full, self-hosted alternative to tools such as G Suite or Microsoft 360. It includes a wide range of features, such as file management and sharing, calendars, document editing, voice and video conferencing, chat, and more. This how-to guide will show a basic Nextcloud installation on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account The http://Platform.sh https://github.com/platformsh/platformsh-cli installed. Steps Create a new http://Platform.sh project Install Nextcloud from the CLI Log into Nextcloud Set up integrations 1. Create a new project You can create a new project using the http://Platform.sh CLI tool: platform create After following the instructions, you will get a confirmation that a new project has been created. Copy the new project ID. 2. Initialize your project with your new project ID Replace with the ID from the previous step and execute this command: platform environment:init -p -e master https://github.com/platformsh-templates/nextcloud 3. Log into Nextcloud The username and password for the administration account were created for you during deployment, and can be viewed in the deploy log: platform log deploy -p Using the URL from Step 1, and the credentials listed in the logs, visit your new Nextcloud installation to complete the setup. 4. Set up integrations Depending on how you intend to use Nextcloud, you will want to enhance the basic installation with some integrations. Common integrations that will give Nextcloud full functionality include: IMAP server for https://docs.nextcloud.com/server/18/admin_manual/configuration_server/email_configuration.html?highlight=imap https://docs.nextcloud.com/server/18/admin_manual/configuration_files/external_storage_configuration_gui.html?highlight=dropbox (eg. Dropbox, S3, SFTP) for enabling file exchange https://nextcloud.com/collaboraonline/ or https://nextcloud.com/onlyoffice/ integration for collaborative document editing Conclusion The basic installation of Nextcloud on http://Platform.sh is straightforward using the CLI and the provided template. Nextcloud users will still need to look after some integrations before the full potential of Nextcloud is available.",
        "text": "Goal https://nextcloud.com/ is a full, self-hosted alternative to tools such as G Suite or Microsoft 360. It includes a wide range of features, such as file management and sharing, calendars, document editing, voice and video conferencing, chat, and more. This how-to guide will show a basic Nextcloud installation on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account The http://Platform.sh https://github.com/platformsh/platformsh-cli installed. Steps Create a new http://Platform.sh project Install Nextcloud from the CLI Log into Nextcloud Set up integrations 1. Create a new project You can create a new project using the http://Platform.sh CLI tool: platform create After following the instructions, you will get a confirmation that a new project has been created. Copy the new project ID. 2. Initialize your project with your new project ID Replace with the ID from the previous step and execute this command: platform environment:init -p -e master https://github.com/platformsh-templates/nextcloud 3. Log into Nextcloud The username and password for the administration account were created for you during deployment, and can be viewed in the deploy log: platform log deploy -p Using the URL from Step 1, and the credentials listed in the logs, visit your new Nextcloud installation to complete the setup. 4. Set up integrations Depending on how you intend to use Nextcloud, you will want to enhance the basic installation with some integrations. Common integrations that will give Nextcloud full functionality include: IMAP server for https://docs.nextcloud.com/server/18/admin_manual/configuration_server/email_configuration.html?highlight=imap https://docs.nextcloud.com/server/18/admin_manual/configuration_files/external_storage_configuration_gui.html?highlight=dropbox (eg. Dropbox, S3, SFTP) for enabling file exchange https://nextcloud.com/collaboraonline/ or https://nextcloud.com/onlyoffice/ integration for collaborative document editing Conclusion The basic installation of Nextcloud on http://Platform.sh is straightforward using the CLI and the provided template. Nextcloud users will still need to look after some integrations before the full potential of Nextcloud is available.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-deploy-nextcloud-to-platform-sh/504",
        "relurl": "/t/how-to-deploy-nextcloud-to-platform-sh/504"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "76c4705b4f68c18b7f6c4d4b2aaf4e735e4e65a7",
        "title": "How to deploy Mattermost (Slack alternative) on Platform.sh",
        "description": "Goal This how-to guide will guide you through deploying https://mattermost.com on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account The http://Platform.sh https://github.com/platformsh/platformsh-cli installed. Steps Create a new http://Platform.sh project Install Mattermost from the CLI Create a first account and team Connect it to your Desktop and Mobile Mattermost clients 1. Create a new project You can create a new project using the http://Platform.sh CLI tool: platform create After following the instructions, you will get a confirmation that a new project has been created. Copy the new project ID. 2. Initialize your project with your new project ID Replace with the ID from the previous step and execute this command: platform environment:init -p -e master https://github.com/platformsh-templates/mattermost 3. Create a first account and team Use the URL from the first step to visit your new Mattermost website, and complete the steps in the setup wizard. https://community.platform.sh/uploads/default/a1e2299cda49fda087cb063991ba338e0a87c55a https://community.platform.sh/uploads/default/c74db48c0d2b5d42f2244554d37782db21822e51 4. Connect it to your Desktop and Mobile Mattermost clients Once you have created an account and a team on your Mattermost server, you can log into the server from your desktop client or mobile app. See https://mattermost.com/download/ on how to get those. The server URL is the same URL that was displayed in Step 1 of this how-to guide, the URL to your running Mattermost server. The authentication that follows is the account you created in Step 3. https://community.platform.sh/uploads/default/3f93f8e58b15fafa55425e916a45e7f25b6e86e8 Conclusion Using the http://Platform.sh CLI and the Mattermost template provided by http://Platform.sh, it is a straightforward process to set up a Mattermost server.",
        "text": "Goal This how-to guide will guide you through deploying https://mattermost.com on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account The http://Platform.sh https://github.com/platformsh/platformsh-cli installed. Steps Create a new http://Platform.sh project Install Mattermost from the CLI Create a first account and team Connect it to your Desktop and Mobile Mattermost clients 1. Create a new project You can create a new project using the http://Platform.sh CLI tool: platform create After following the instructions, you will get a confirmation that a new project has been created. Copy the new project ID. 2. Initialize your project with your new project ID Replace with the ID from the previous step and execute this command: platform environment:init -p -e master https://github.com/platformsh-templates/mattermost 3. Create a first account and team Use the URL from the first step to visit your new Mattermost website, and complete the steps in the setup wizard. https://community.platform.sh/uploads/default/a1e2299cda49fda087cb063991ba338e0a87c55a https://community.platform.sh/uploads/default/c74db48c0d2b5d42f2244554d37782db21822e51 4. Connect it to your Desktop and Mobile Mattermost clients Once you have created an account and a team on your Mattermost server, you can log into the server from your desktop client or mobile app. See https://mattermost.com/download/ on how to get those. The server URL is the same URL that was displayed in Step 1 of this how-to guide, the URL to your running Mattermost server. The authentication that follows is the account you created in Step 3. https://community.platform.sh/uploads/default/3f93f8e58b15fafa55425e916a45e7f25b6e86e8 Conclusion Using the http://Platform.sh CLI and the Mattermost template provided by http://Platform.sh, it is a straightforward process to set up a Mattermost server.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-deploy-mattermost-slack-alternative-on-platform-sh/503",
        "relurl": "/t/how-to-deploy-mattermost-slack-alternative-on-platform-sh/503"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "b46491195d8dc786efc05650514b0d95782c7abc",
        "title": "How to create, initialize, and delete a Platform.sh project from the command line",
        "description": "Goal To fully manage the life cycle of sites on http://Platform.sh, including the creation of new projects, initializing from a hosted GitHub repository, and deleting projects. Assumptions on account The https://github.com/platformsh/platformsh-cli installed Problems You will need a public GitHub project that already has the required https://docs.platform.sh/configuration/yaml.html to work with http://Platform.sh. Steps 1. Creating projects To create a new project, use the platform create command. https://community.platform.sh/uploads/default/eebff1e7f8af01f1ef3a797f23cabb821edfd799 2. Initializing projects from a GitHub repository The platform environment:init command requires both the Project ID of the newly created project and the URL of the GitHub project. Note that as of version 3.40.8 of the http://Platform.sh CLI, the environment:init command is considered beta and not listed via platform list. platform environment:init -p -e master Note that the GitHub URL is the https: URL from the browser, not the git: URL. https://community.platform.sh/uploads/default/bc695ccb5ad79b9a1f838d93a8189883d057f4ce Open the new site in your browser with the following command: platform url -p platform url -p ov4iu6fcb35jg Environment ID [master]: Enter a number to open a URL [0] https://master-7rqtwti-ov4iu6fcb35jg.de-2.platformsh.site/ [1] https://www.master-7rqtwti-ov4iu6fcb35jg.de-2.platformsh.site/ [2] http://master-7rqtwti-ov4iu6fcb35jg.de-2.platformsh.site/ [3] http://www.master-7rqtwti-ov4iu6fcb35jg.de-2.platformsh.site/ 0 https://master-7rqtwti-ov4iu6fcb35jg.de-2.platformsh.site/ https://community.platform.sh/uploads/default/7c010a9f1063622054a23af85d7ac144ce8dedfb 3. Deleting projects Delete the project with the following command: platform project:delete -p You will be prompted to type the name of the project as a security confirmation, since deleting a project is irreversible. platform project:delete -p ov4iu6fcb35jg You are about to delete the project: New Drupal Site (ov4iu6fcb35jg) * This action is irreversible. * Your site will no longer be accessible. * All data associated with this project will be deleted, including backups. * You will be charged at the end of the month for any remaining project costs. Are you sure you want to delete this project? [y/N] y Type the project title to confirm: New Drupal Site Conclusion The http://Platform.sh CLI enables full life cycle management of your sites. It is not even necessary to use Git directly when creating, initializing, and deleting sites.",
        "text": "Goal To fully manage the life cycle of sites on http://Platform.sh, including the creation of new projects, initializing from a hosted GitHub repository, and deleting projects. Assumptions on account The https://github.com/platformsh/platformsh-cli installed Problems You will need a public GitHub project that already has the required https://docs.platform.sh/configuration/yaml.html to work with http://Platform.sh. Steps 1. Creating projects To create a new project, use the platform create command. https://community.platform.sh/uploads/default/eebff1e7f8af01f1ef3a797f23cabb821edfd799 2. Initializing projects from a GitHub repository The platform environment:init command requires both the Project ID of the newly created project and the URL of the GitHub project. Note that as of version 3.40.8 of the http://Platform.sh CLI, the environment:init command is considered beta and not listed via platform list. platform environment:init -p -e master Note that the GitHub URL is the https: URL from the browser, not the git: URL. https://community.platform.sh/uploads/default/bc695ccb5ad79b9a1f838d93a8189883d057f4ce Open the new site in your browser with the following command: platform url -p platform url -p ov4iu6fcb35jg Environment ID [master]: Enter a number to open a URL [0] https://master-7rqtwti-ov4iu6fcb35jg.de-2.platformsh.site/ [1] https://www.master-7rqtwti-ov4iu6fcb35jg.de-2.platformsh.site/ [2] http://master-7rqtwti-ov4iu6fcb35jg.de-2.platformsh.site/ [3] http://www.master-7rqtwti-ov4iu6fcb35jg.de-2.platformsh.site/ 0 https://master-7rqtwti-ov4iu6fcb35jg.de-2.platformsh.site/ https://community.platform.sh/uploads/default/7c010a9f1063622054a23af85d7ac144ce8dedfb 3. Deleting projects Delete the project with the following command: platform project:delete -p You will be prompted to type the name of the project as a security confirmation, since deleting a project is irreversible. platform project:delete -p ov4iu6fcb35jg You are about to delete the project: New Drupal Site (ov4iu6fcb35jg) * This action is irreversible. * Your site will no longer be accessible. * All data associated with this project will be deleted, including backups. * You will be charged at the end of the month for any remaining project costs. Are you sure you want to delete this project? [y/N] y Type the project title to confirm: New Drupal Site Conclusion The http://Platform.sh CLI enables full life cycle management of your sites. It is not even necessary to use Git directly when creating, initializing, and deleting sites.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-create-initialize-and-delete-a-platform-sh-project-from-the-command-line/188",
        "relurl": "/t/how-to-create-initialize-and-delete-a-platform-sh-project-from-the-command-line/188"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "30fd4bbad0a3a7e72af39de3a66f009724485caf",
        "title": "How to access InfluxDB credentials on Platform.sh",
        "description": "Goal Access credentials for InfluxDB from within a http://Platform.sh application using PHP. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/influxdb.html .platform/services.yaml for the given service on account if developing locally InfluxDB installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html InfluxDB relationship like so, in .platform.app.yaml: relationships: database: \"timedb:influxdb\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-php Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-php for minimum requirements. $ composer install platformsh/config-reader 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. use Platformsh\\ConfigReader\\Config; $config = new Config(); 3. Read the credentials // Get the credentials to connect to the InfluxDB service. $credentials = $config-credentials('database'); Steps (Manual) 1. Load and decode the environment variable $relationships = json_decode(base64_decode(getenv('PLATFORM_RELATIONSHIPS')), TRUE); 2. Read the credentials $credentials = $relationships['database']; Use: In most cases, you will need only the host and port properties to connect to InfluxDB. Pass those to your InfluxDB library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is now an array matching the relationship JSON object. { \"service\": \"timedb\", \"ip\": \"169.254.113.144\", \"hostname\": \"haz5rys6n2dsnjusqa54os3ii4.influxdb.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"database.internal\", \"rel\": \"influxdb\", \"scheme\": \"http\", \"type\": \"influxdb:1.3\", \"port\": 8086 } Conclusions: Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get InfluxDB credentials in PHP. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-php can be useful for inspecting the project environment: // Checks whether the code is running in a build environment $config-inBuild(); // Checks whether the code is running in a runtime environment $config-inRuntime(); and for reading environment variables as attributes of config: // Available in Build and at Runtime $config-appDir; // Available at Runtime only $config-branch; $config-smtpHost; The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "text": "Goal Access credentials for InfluxDB from within a http://Platform.sh application using PHP. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/influxdb.html .platform/services.yaml for the given service on account if developing locally InfluxDB installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html InfluxDB relationship like so, in .platform.app.yaml: relationships: database: \"timedb:influxdb\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-php Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-php for minimum requirements. $ composer install platformsh/config-reader 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. use Platformsh\\ConfigReader\\Config; $config = new Config(); 3. Read the credentials // Get the credentials to connect to the InfluxDB service. $credentials = $config-credentials('database'); Steps (Manual) 1. Load and decode the environment variable $relationships = json_decode(base64_decode(getenv('PLATFORM_RELATIONSHIPS')), TRUE); 2. Read the credentials $credentials = $relationships['database']; Use: In most cases, you will need only the host and port properties to connect to InfluxDB. Pass those to your InfluxDB library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is now an array matching the relationship JSON object. { \"service\": \"timedb\", \"ip\": \"169.254.113.144\", \"hostname\": \"haz5rys6n2dsnjusqa54os3ii4.influxdb.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"database.internal\", \"rel\": \"influxdb\", \"scheme\": \"http\", \"type\": \"influxdb:1.3\", \"port\": 8086 } Conclusions: Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get InfluxDB credentials in PHP. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-php can be useful for inspecting the project environment: // Checks whether the code is running in a build environment $config-inBuild(); // Checks whether the code is running in a runtime environment $config-inRuntime(); and for reading environment variables as attributes of config: // Available in Build and at Runtime $config-appDir; // Available at Runtime only $config-branch; $config-smtpHost; The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-access-influxdb-credentials-on-platform-sh/148",
        "relurl": "/t/how-to-access-influxdb-credentials-on-platform-sh/148"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "35d22561178c1d9924ed7eeb6e4aedc851234607",
        "title": "How to run Grocy on Platform.sh",
        "description": "Goal https://grocy.info/ is an awesome app one can use to manage groceries, food stock, recipes, chores and more. It’s like a personal ERP, I am using it to hoard responsibly without wasting food and other household items. Here’s how it looks like: https://community.platform.sh/uploads/default/d8d9662e07dddf277803bfac89bca2b18fb5ffdd It is very easy to run Grocy on http://Platform.sh, and I will show you here what it takes. You can always https://github.com/kotnik/grocy-platformsh , and have it running in seconds. Assumptions You will need: http://Platform.sh project, it can be development sized one too. Steps Clone your http://Platform.sh project locally first: platform get It should be fresh and empty project. Now, let’s add Grocy as submodule so we can upgrade easily when new version is released: cd git submodule add https://github.com/grocy/grocy.git git submodule init git submodule update cd grocy git checkout v2.6.1 cd .. git add . git commit -am 'Added Grocy 2.6.1' OK, latest Grocy is ready. Now, let’s add some info so http://Platform.sh knows how to run it. Grocy itself is not using any additional services, all the data is kept in a file, so let’s create empty .platform/services.yaml file: # No need for services Routes can be default ones, have this in .platform/routes.yaml: \"https://{default}/\": type: upstream upstream: \"app:http\" \"http://{default}/\": type: redirect to: \"https://{default}/\" Good. Now, let’s create .platform.app.yaml. You can take https://github.com/kotnik/grocy-platformsh/blob/master/.platform.app.yaml , but here we’ll cover it section by section. Grocy, at the time this is written, needs PHP 7.2 so let’s use that: name: 'app' type: 'php:7.2' It keeps all the data in single file with db extension it searches for in data/ directory. So, let’s give it writable permanent mount there: disk: 512 mounts: '/data': source: local source_path: 'data' For build it only requires yarn around, so let’s get it: build: flavor: none dependencies: nodejs: yarn: \"*\" Now, this is the build hook: hooks: build: | set -e mkdir -p www cd www ln -s ../data/data data cd .. mv grocy/public www mv grocy/controllers www mv grocy/helpers www mv grocy/localization www mv grocy/middleware www mv grocy/migrations www mv grocy/publication_assets www mv grocy/services www mv grocy/views www mv grocy/composer.* www mv grocy/.yarnrc www mv grocy/yarn.* www mv grocy/*.php www mv grocy/*.json www cd www composer install yarn install This results with built app in www sub-directory. Notice the trick we did with the data directory. By Grocy’s own installation documentation, /data directory keeps cache and plugins as well (structure is provided in the repo of the app itself), not just the data created by user. So we link it to the permanent mount and then copy the provided plugins in the deploy hook: hooks: deploy: | set -e mkdir -p data/data/viewcache cp -rp grocy/data/* data/data cp config.php data/data Deploy hook also provides configuration file config.php. You can look at the defaults, but at least you should disable PHP’s error reporting since it breaks session management: ",
        "text": "Goal https://grocy.info/ is an awesome app one can use to manage groceries, food stock, recipes, chores and more. It’s like a personal ERP, I am using it to hoard responsibly without wasting food and other household items. Here’s how it looks like: https://community.platform.sh/uploads/default/d8d9662e07dddf277803bfac89bca2b18fb5ffdd It is very easy to run Grocy on http://Platform.sh, and I will show you here what it takes. You can always https://github.com/kotnik/grocy-platformsh , and have it running in seconds. Assumptions You will need: http://Platform.sh project, it can be development sized one too. Steps Clone your http://Platform.sh project locally first: platform get It should be fresh and empty project. Now, let’s add Grocy as submodule so we can upgrade easily when new version is released: cd git submodule add https://github.com/grocy/grocy.git git submodule init git submodule update cd grocy git checkout v2.6.1 cd .. git add . git commit -am 'Added Grocy 2.6.1' OK, latest Grocy is ready. Now, let’s add some info so http://Platform.sh knows how to run it. Grocy itself is not using any additional services, all the data is kept in a file, so let’s create empty .platform/services.yaml file: # No need for services Routes can be default ones, have this in .platform/routes.yaml: \"https://{default}/\": type: upstream upstream: \"app:http\" \"http://{default}/\": type: redirect to: \"https://{default}/\" Good. Now, let’s create .platform.app.yaml. You can take https://github.com/kotnik/grocy-platformsh/blob/master/.platform.app.yaml , but here we’ll cover it section by section. Grocy, at the time this is written, needs PHP 7.2 so let’s use that: name: 'app' type: 'php:7.2' It keeps all the data in single file with db extension it searches for in data/ directory. So, let’s give it writable permanent mount there: disk: 512 mounts: '/data': source: local source_path: 'data' For build it only requires yarn around, so let’s get it: build: flavor: none dependencies: nodejs: yarn: \"*\" Now, this is the build hook: hooks: build: | set -e mkdir -p www cd www ln -s ../data/data data cd .. mv grocy/public www mv grocy/controllers www mv grocy/helpers www mv grocy/localization www mv grocy/middleware www mv grocy/migrations www mv grocy/publication_assets www mv grocy/services www mv grocy/views www mv grocy/composer.* www mv grocy/.yarnrc www mv grocy/yarn.* www mv grocy/*.php www mv grocy/*.json www cd www composer install yarn install This results with built app in www sub-directory. Notice the trick we did with the data directory. By Grocy’s own installation documentation, /data directory keeps cache and plugins as well (structure is provided in the repo of the app itself), not just the data created by user. So we link it to the permanent mount and then copy the provided plugins in the deploy hook: hooks: deploy: | set -e mkdir -p data/data/viewcache cp -rp grocy/data/* data/data cp config.php data/data Deploy hook also provides configuration file config.php. You can look at the defaults, but at least you should disable PHP’s error reporting since it breaks session management: ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-run-grocy-on-platform-sh/498",
        "relurl": "/t/how-to-run-grocy-on-platform-sh/498"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "4c63027166447ff170ee0cd627011ecf64160bc0",
        "title": "How to run CryptPad on Platform.sh",
        "description": "Goal Here you can find what is needed to run https://cryptpad.fr/what-is-cryp.htpadtml on http://Platform.sh. You will see soon that it doesn’t take much at all, and very quickly you will be able to collaborate on slides, sheets, and other types of documents in secure and private manner. TL;DR Clone CryptPad, https://github.com/kotnik/cryptpad/commit/72a8fd5f7c5b8781c95d6fa31691d3b439f80dfe , push to fresh http://Platform.sh project, profffit. Here’s what you can collaborate on with CryptPad: https://community.platform.sh/uploads/default/4e3dcd0eb8384ed8d1d13689faf1f9a597808d5d It’s like Google Docs or Microsoft Office365, but on your own: you own the data, and it is protected at rest. Interested? Let’s go. Assumptions You will need: Basic knowledge of how http://Platform.sh works. You have run at least one project on http://Platform.sh, so we don’t have to go over git and other primitives here. Steps Before we go to steps you can pick up this complete https://github.com/kotnik/cryptpad/tree/platform , push it to your http://Platform.sh project, and you will have fully working CryptPad copy. You can see this repo https://master-7rqtwti-6sbs6vfvslbwc.eu-3.platformsh.site/ . And now, here are all the steps I took to make it happen. 1. Fork it - Clone it - Branch it So I forked https://github.com/xwiki-labs/cryptpad to my own Github account and cloned it. I will paste my own command, but please adjust it for your own username and URL: git clone git@github.com:kotnik/cryptpad.git cd cryptpad Ok, origin remote is Github one. Sweet. Next, I wanted to use the latest CryptPad release, which at the time of writing this is 3.12.0, and name that branch platform, you can guess why. Let’s go: git checkout -b platform tags/3.12.0 Easier to do than to describe. 2. Create http://Platform.sh project So go and create new project. It can be trial too. It’s fast, I’ll wait. 3. Hook it up - Basics As you already probably know, there are only a few things http://Platform.sh needs to know before it is able to run your application: Your app description: what it runs on, how to build it, how to serve it, etc. It’s that .platform.app.yaml file. Services your app is using. It’s easy here because CryptPad uses no external services and keeps everything on the disk. Your routes, how to access your application. And only here we have one small gotcha that is crucial, we’ll cover it below. But first, let’s add http://Platform.sh project remote to our repo: platform project:set-remote 4. Hook it up - Describing the app Here it is, full and fully commented .platform.app.yaml file: name: cryptpad # CryptPad needs to run on Node 12. type: nodejs:12 variables: env: # This tells the app how to create encryption key. Refer its documentation for more info. FRESH: 1 # No, don't sweat it, you're behind proxy and we got TLS all figured out :slight_smile:. USE_SSL: \"false\" web: commands: # I love Express framework apps, they're so easy to run! start: \"node ./server.js\" build: # Let me handle it, don't be smart. flavor: none hooks: build: | npm install # Make our life easier with official Platform.sh helper. npm install platformsh-config@2.3.1 bower install # This is the configuration file, more about this one later. cp config.platformsh.js config/config.js dependencies: nodejs: bower: \"^1.8.8\" mounts: datastore: source: local source_path: datastore data: source: local source_path: data block: source: local source_path: block blob: source: local source_path: blob # Starting with half of a gigabyte. It's easy to bump it up, so you choose. disk: 512 5. Hook it up - Services and routes Now we need to tell http://Platform.sh what we need for the application to run, and where one can find it. As for services, there’s nothing we need, so use this for .platform/services.yaml file: # No needs You do need it, even an empty one. Next, here are the routes you need to put in your .platform/routes.yaml file: \"https://{default}/\": type: upstream upstream: \"cryptpad:http\" \"https://{default}/cryptpad_websocket\": type: upstream upstream: \"cryptpad:http\" cache: enabled: false And this is the only tricky part of this how-to, the one I figured out the hard way: CryptPad uses WebSockets to talk with browsers and you must have that second entry in routes.yaml to disable caching it, or your app will simply not work. We but CryptPad people should have this endpoint mentioned on their installation page. 6. Hook it up - Configure app itself CryptPad looks at config/config.js file by default for configuration instructions. This file is rather long so I will not include it here, you can https://github.com/kotnik/cryptpad/blob/72a8fd5f7c5b8781c95d6fa31691d3b439f80dfe/config.platformsh.js and save it to config.platform.js. It’s well documented so you should have no problems figuring out what to do. 7. Run it This is it, after defining your application (with .platform.app.yaml, .platform/services.yaml and .platform/routes.yaml in place) and instructing how your CryptPad should behave (file config.platform.js is ready), you can simply commit all this and push it to your http://Platform.sh project: git add .platform.app.yaml .platform/services.yaml .platform/routes.yaml config.platform.js git commit -m 'Platformize' git push platform -u platform:master After building and deploying your application, your project will inform you where your CryptPad is and you can start using it right away! Happy hacking on http://Platform.sh!",
        "text": "Goal Here you can find what is needed to run https://cryptpad.fr/what-is-cryp.htpadtml on http://Platform.sh. You will see soon that it doesn’t take much at all, and very quickly you will be able to collaborate on slides, sheets, and other types of documents in secure and private manner. TL;DR Clone CryptPad, https://github.com/kotnik/cryptpad/commit/72a8fd5f7c5b8781c95d6fa31691d3b439f80dfe , push to fresh http://Platform.sh project, profffit. Here’s what you can collaborate on with CryptPad: https://community.platform.sh/uploads/default/4e3dcd0eb8384ed8d1d13689faf1f9a597808d5d It’s like Google Docs or Microsoft Office365, but on your own: you own the data, and it is protected at rest. Interested? Let’s go. Assumptions You will need: Basic knowledge of how http://Platform.sh works. You have run at least one project on http://Platform.sh, so we don’t have to go over git and other primitives here. Steps Before we go to steps you can pick up this complete https://github.com/kotnik/cryptpad/tree/platform , push it to your http://Platform.sh project, and you will have fully working CryptPad copy. You can see this repo https://master-7rqtwti-6sbs6vfvslbwc.eu-3.platformsh.site/ . And now, here are all the steps I took to make it happen. 1. Fork it - Clone it - Branch it So I forked https://github.com/xwiki-labs/cryptpad to my own Github account and cloned it. I will paste my own command, but please adjust it for your own username and URL: git clone git@github.com:kotnik/cryptpad.git cd cryptpad Ok, origin remote is Github one. Sweet. Next, I wanted to use the latest CryptPad release, which at the time of writing this is 3.12.0, and name that branch platform, you can guess why. Let’s go: git checkout -b platform tags/3.12.0 Easier to do than to describe. 2. Create http://Platform.sh project So go and create new project. It can be trial too. It’s fast, I’ll wait. 3. Hook it up - Basics As you already probably know, there are only a few things http://Platform.sh needs to know before it is able to run your application: Your app description: what it runs on, how to build it, how to serve it, etc. It’s that .platform.app.yaml file. Services your app is using. It’s easy here because CryptPad uses no external services and keeps everything on the disk. Your routes, how to access your application. And only here we have one small gotcha that is crucial, we’ll cover it below. But first, let’s add http://Platform.sh project remote to our repo: platform project:set-remote 4. Hook it up - Describing the app Here it is, full and fully commented .platform.app.yaml file: name: cryptpad # CryptPad needs to run on Node 12. type: nodejs:12 variables: env: # This tells the app how to create encryption key. Refer its documentation for more info. FRESH: 1 # No, don't sweat it, you're behind proxy and we got TLS all figured out :slight_smile:. USE_SSL: \"false\" web: commands: # I love Express framework apps, they're so easy to run! start: \"node ./server.js\" build: # Let me handle it, don't be smart. flavor: none hooks: build: | npm install # Make our life easier with official Platform.sh helper. npm install platformsh-config@2.3.1 bower install # This is the configuration file, more about this one later. cp config.platformsh.js config/config.js dependencies: nodejs: bower: \"^1.8.8\" mounts: datastore: source: local source_path: datastore data: source: local source_path: data block: source: local source_path: block blob: source: local source_path: blob # Starting with half of a gigabyte. It's easy to bump it up, so you choose. disk: 512 5. Hook it up - Services and routes Now we need to tell http://Platform.sh what we need for the application to run, and where one can find it. As for services, there’s nothing we need, so use this for .platform/services.yaml file: # No needs You do need it, even an empty one. Next, here are the routes you need to put in your .platform/routes.yaml file: \"https://{default}/\": type: upstream upstream: \"cryptpad:http\" \"https://{default}/cryptpad_websocket\": type: upstream upstream: \"cryptpad:http\" cache: enabled: false And this is the only tricky part of this how-to, the one I figured out the hard way: CryptPad uses WebSockets to talk with browsers and you must have that second entry in routes.yaml to disable caching it, or your app will simply not work. We but CryptPad people should have this endpoint mentioned on their installation page. 6. Hook it up - Configure app itself CryptPad looks at config/config.js file by default for configuration instructions. This file is rather long so I will not include it here, you can https://github.com/kotnik/cryptpad/blob/72a8fd5f7c5b8781c95d6fa31691d3b439f80dfe/config.platformsh.js and save it to config.platform.js. It’s well documented so you should have no problems figuring out what to do. 7. Run it This is it, after defining your application (with .platform.app.yaml, .platform/services.yaml and .platform/routes.yaml in place) and instructing how your CryptPad should behave (file config.platform.js is ready), you can simply commit all this and push it to your http://Platform.sh project: git add .platform.app.yaml .platform/services.yaml .platform/routes.yaml config.platform.js git commit -m 'Platformize' git push platform -u platform:master After building and deploying your application, your project will inform you where your CryptPad is and you can start using it right away! Happy hacking on http://Platform.sh!",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-run-cryptpad-on-platform-sh/486",
        "relurl": "/t/how-to-run-cryptpad-on-platform-sh/486"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "2569574f10d19de4ba0d9dca8f591363d57bdb39",
        "title": "How to fail a build on failing unit tests",
        "description": "Goal Every time a push is made, run all unit tests. If tests fail, the build should fail and not be deployed. Assumptions During the build phase no services are available. That means only tests that can run with only your code (no database, no web requests, etc.) can be run. Most testing frameworks have a way to define test suites and run only a selected set of them. This guide assumes you have defined a test suite that is safe to run in isolation. Although this example uses PHP the process is essentially identical for any language, provide its test runner follows standard Unix error conventions. Steps 1. Ensure that your test framework is included in your list of project dependencies For a PHP application that would mean composer.json. For a Node.js application it would mean package.json. For PHP: { \"requires-dev\": { \"phpunit/phpunit\": \"^7.5\" } } (By default http://Platform.sh installs dev dependencies as well, so it will be downloaded by default.) 2. Add set -e as the first line of your build hook In .platform.app.yaml, locate your build hook and add set -e as the first line. If one does not exist go ahead and create it. hooks: build: set -e That tells the system to fail the build if any command in the build hook returns an error code. 3. Add your test command as the last step of your build hook After all other tasks in your build hook have run, add a line to run your tests. For PHP your build hook will look like this: hooks: build: set -e # Possibly other stuff here vendor/bin/phpunit For Node.js it will look like this: hooks: build: set -e # Possibly other stuff here npm test 4. Commit the result and push git add composer.json .platform.app.yaml git commit -m \"Enable build tests\" Conclusion If the test command fails a test it will return a non-0 error code to the shell. The set -e flag means that error will cause the whole build process to fail and the container will not be deployed. If no tests fail then the test command will return 0, allowing the build to proceed. The output of the test runner will be available in the build log.",
        "text": "Goal Every time a push is made, run all unit tests. If tests fail, the build should fail and not be deployed. Assumptions During the build phase no services are available. That means only tests that can run with only your code (no database, no web requests, etc.) can be run. Most testing frameworks have a way to define test suites and run only a selected set of them. This guide assumes you have defined a test suite that is safe to run in isolation. Although this example uses PHP the process is essentially identical for any language, provide its test runner follows standard Unix error conventions. Steps 1. Ensure that your test framework is included in your list of project dependencies For a PHP application that would mean composer.json. For a Node.js application it would mean package.json. For PHP: { \"requires-dev\": { \"phpunit/phpunit\": \"^7.5\" } } (By default http://Platform.sh installs dev dependencies as well, so it will be downloaded by default.) 2. Add set -e as the first line of your build hook In .platform.app.yaml, locate your build hook and add set -e as the first line. If one does not exist go ahead and create it. hooks: build: set -e That tells the system to fail the build if any command in the build hook returns an error code. 3. Add your test command as the last step of your build hook After all other tasks in your build hook have run, add a line to run your tests. For PHP your build hook will look like this: hooks: build: set -e # Possibly other stuff here vendor/bin/phpunit For Node.js it will look like this: hooks: build: set -e # Possibly other stuff here npm test 4. Commit the result and push git add composer.json .platform.app.yaml git commit -m \"Enable build tests\" Conclusion If the test command fails a test it will return a non-0 error code to the shell. The set -e flag means that error will cause the whole build process to fail and the container will not be deployed. If no tests fail then the test command will return 0, allowing the build to proceed. The output of the test runner will be available in the build log.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-fail-a-build-on-failing-unit-tests/57",
        "relurl": "/t/how-to-fail-a-build-on-failing-unit-tests/57"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "7c9e418c901c5b3455a910819f44d89b7335231a",
        "title": "How to determine database usage of an environment",
        "description": "Goal Determine approximate database usage of a http://platform.sh/ project using the https://github.com/platformsh/platformsh-cli tool. Assumptions One or more http://platform.sh/ projects https://github.com/platformsh/platformsh-cli tool installed locally https://docs.platform.sh/development/ssh.html configured with the project account(s) Problems As a general rule, the database service should have at least 15% of disk space available on a http://Platform.sh project environment, so it is useful to be able to approximate the current disk usage of a project’s database(s). Steps 1. Log in to Platform CLI $ platform login 2. Find the project ID Note the project ID of the project you’re checking: $ platform project:list 3. Find the environment ID Note the environment ID for the environment you’re checking. $ platform environment:list -p 4. Run the database size command $ platform db:size -p -e The command will return something like: +----------------+-----------------+--------+ | Allocated disk | Estimated usage | % used | +----------------+-----------------+--------+ | 14.6 GiB | 9.4 GiB | ~ 64% | +----------------+-----------------+--------+ Warning This is an estimate of the databases disk usage. It does not represent its real size on disk. Conclusion Obtaining an approximate measure of your database usage can be done from the command line using the https://github.com/platformsh/platformsh-cli tool.",
        "text": "Goal Determine approximate database usage of a http://platform.sh/ project using the https://github.com/platformsh/platformsh-cli tool. Assumptions One or more http://platform.sh/ projects https://github.com/platformsh/platformsh-cli tool installed locally https://docs.platform.sh/development/ssh.html configured with the project account(s) Problems As a general rule, the database service should have at least 15% of disk space available on a http://Platform.sh project environment, so it is useful to be able to approximate the current disk usage of a project’s database(s). Steps 1. Log in to Platform CLI $ platform login 2. Find the project ID Note the project ID of the project you’re checking: $ platform project:list 3. Find the environment ID Note the environment ID for the environment you’re checking. $ platform environment:list -p 4. Run the database size command $ platform db:size -p -e The command will return something like: +----------------+-----------------+--------+ | Allocated disk | Estimated usage | % used | +----------------+-----------------+--------+ | 14.6 GiB | 9.4 GiB | ~ 64% | +----------------+-----------------+--------+ Warning This is an estimate of the databases disk usage. It does not represent its real size on disk. Conclusion Obtaining an approximate measure of your database usage can be done from the command line using the https://github.com/platformsh/platformsh-cli tool.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-determine-database-usage-of-an-environment/180",
        "relurl": "/t/how-to-determine-database-usage-of-an-environment/180"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "73211ec386367945926c44387c419d97efb10ac8",
        "title": "How to compile CSS using SASS on Platform.sh",
        "description": "Goal Have your SCSS Code compiled to CSS during deployment on http://Platform.sh Assumptions An empty http://Platform.sh project A local git repository, with Platform.as as a remote Knowledge of https://community.platform.sh/t/how-to-serve-a-static-html-page-on-platform-sh/51 Steps 1. Create a HTML file and link style.css. ./web/index.html \r\n\r\nFoobar\r\n\r\n lorem ipsum\r\n\r\n 2. Create a SCSS file . ./scss/style.scss $colors: ( background: rgb(26, 25, 43), text: rgb(255, 255, 255), ); body { background-color: map-get($colors, 'background'); color: map-get($colors, 'text'); } 3. Define default routes. ./.platform/routes.yaml https://{default}/: type: upstream upstream: sasshowto:http 4. Add empty services (we don’t need any in this example). ./.platform/services.yaml # empty 5. Add .platform.app.yaml configuration. ./.platform.app.yaml name: sasshowto # Any type will work as we just serve static HTML type: \"php:7.3\" # Install sass from npm dependencies: nodejs: sass: '~1.17.2' # Compile sass to css during the build hook and save output to web/style.css with compressed css for production hooks: build: | sass --style compressed scss/style.scss web/style.css disk: 256 web: locations: \"/\": # This tells Nginx to serve from the base directory root: \"web\" index: - \"index.html\" 6. Add, commit, and push these files to your empty http://Platform.sh project. git add . git commit -m \"Compile SASS to CSS\" git push -u platform master 7. Test by visiting the URL of your environment. Check that white text on a dark blue background is visible on the site. platform url Conclusion Every time the project is pushed to http://Platform.sh, compressed CSS will be generated from your SCSS-File and put into web/style.css.",
        "text": "Goal Have your SCSS Code compiled to CSS during deployment on http://Platform.sh Assumptions An empty http://Platform.sh project A local git repository, with Platform.as as a remote Knowledge of https://community.platform.sh/t/how-to-serve-a-static-html-page-on-platform-sh/51 Steps 1. Create a HTML file and link style.css. ./web/index.html \r\n\r\nFoobar\r\n\r\n lorem ipsum\r\n\r\n 2. Create a SCSS file . ./scss/style.scss $colors: ( background: rgb(26, 25, 43), text: rgb(255, 255, 255), ); body { background-color: map-get($colors, 'background'); color: map-get($colors, 'text'); } 3. Define default routes. ./.platform/routes.yaml https://{default}/: type: upstream upstream: sasshowto:http 4. Add empty services (we don’t need any in this example). ./.platform/services.yaml # empty 5. Add .platform.app.yaml configuration. ./.platform.app.yaml name: sasshowto # Any type will work as we just serve static HTML type: \"php:7.3\" # Install sass from npm dependencies: nodejs: sass: '~1.17.2' # Compile sass to css during the build hook and save output to web/style.css with compressed css for production hooks: build: | sass --style compressed scss/style.scss web/style.css disk: 256 web: locations: \"/\": # This tells Nginx to serve from the base directory root: \"web\" index: - \"index.html\" 6. Add, commit, and push these files to your empty http://Platform.sh project. git add . git commit -m \"Compile SASS to CSS\" git push -u platform master 7. Test by visiting the URL of your environment. Check that white text on a dark blue background is visible on the site. platform url Conclusion Every time the project is pushed to http://Platform.sh, compressed CSS will be generated from your SCSS-File and put into web/style.css.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-compile-css-using-sass-on-platform-sh/129",
        "relurl": "/t/how-to-compile-css-using-sass-on-platform-sh/129"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "563413d4acae78b93350a4c6adebacf58672992c",
        "title": "How to set up automated environment tasks",
        "description": "Goal To have one more more environments automatically backed up through snapshots triggered without manual intervention. Assumptions Access to a project hosted on http://platform.sh/ Your project account has administrator rights Knowledge on using the project web interface or the CLI tool set up in the environment, as described https://community.platform.sh/t/how-to-use-the-platform-sh-cli-from-the-web-container/ Problems The snapshot process is not automated and requires a manual trigger using the CLI or web interface. Steps 1. Check the CLI tool is correctly installed in the environment After logging in with SSH in the environment, you should be able to run platform and see the welcome prompt, together with a list of your projects. 2. Add a cron for automated environment snapshots Edit your .platform.app.yaml file and add the snapshot command in a cron. Example: crons: auto_snapshot: # The cron task will run everyday at 4 am (UTC) spec: '0 4 * * *' cmd: | if [ \"$PLATFORM_BRANCH\" = master ]; then platform backup:create --yes --no-wait fi The PLATFORM_BRANCH variable check ensures automatic snapshots are done only on the master environment. The --yes flag skips user interaction for the snapshot command. Ensure the --no-wait parameter is added to the command, in order to make the operation non-blocking (otherwise, your site will be down until the snapshot operation is completed). 3. Add a cron for automated SSL certificate renewal The provided Let’s Encrypt certificates have to be renewed every 3 months. The renewal is done automatically on every deployment, but if you do not deploy that often it is possible to have the SSL certificate expire. In order to ensure this does not happen, you can configure a cron job to automatically redeploy your environment. If there are no code changes, this will happen very fast and ensure your SSL certificate is refreshed. To do this, you need to edit your .platform.app.yaml just like before and add a cron job like in this example: crons: auto_renewcert: # Force a redeploy at 10 am (UTC) on the 1st and 15th of every month. spec: '0 10 1,15 * *' cmd: | if [ \"$PLATFORM_BRANCH\" = master ]; then platform redeploy --yes --no-wait fi Like above, PLATFORM_BRANCH variable check ensures the redeployment is done only on the master environment. Conclusion After setting up the CLI tool in the environment, we learned how to use it in order to trigger various maintenance tasks for the project.",
        "text": "Goal To have one more more environments automatically backed up through snapshots triggered without manual intervention. Assumptions Access to a project hosted on http://platform.sh/ Your project account has administrator rights Knowledge on using the project web interface or the CLI tool set up in the environment, as described https://community.platform.sh/t/how-to-use-the-platform-sh-cli-from-the-web-container/ Problems The snapshot process is not automated and requires a manual trigger using the CLI or web interface. Steps 1. Check the CLI tool is correctly installed in the environment After logging in with SSH in the environment, you should be able to run platform and see the welcome prompt, together with a list of your projects. 2. Add a cron for automated environment snapshots Edit your .platform.app.yaml file and add the snapshot command in a cron. Example: crons: auto_snapshot: # The cron task will run everyday at 4 am (UTC) spec: '0 4 * * *' cmd: | if [ \"$PLATFORM_BRANCH\" = master ]; then platform backup:create --yes --no-wait fi The PLATFORM_BRANCH variable check ensures automatic snapshots are done only on the master environment. The --yes flag skips user interaction for the snapshot command. Ensure the --no-wait parameter is added to the command, in order to make the operation non-blocking (otherwise, your site will be down until the snapshot operation is completed). 3. Add a cron for automated SSL certificate renewal The provided Let’s Encrypt certificates have to be renewed every 3 months. The renewal is done automatically on every deployment, but if you do not deploy that often it is possible to have the SSL certificate expire. In order to ensure this does not happen, you can configure a cron job to automatically redeploy your environment. If there are no code changes, this will happen very fast and ensure your SSL certificate is refreshed. To do this, you need to edit your .platform.app.yaml just like before and add a cron job like in this example: crons: auto_renewcert: # Force a redeploy at 10 am (UTC) on the 1st and 15th of every month. spec: '0 10 1,15 * *' cmd: | if [ \"$PLATFORM_BRANCH\" = master ]; then platform redeploy --yes --no-wait fi Like above, PLATFORM_BRANCH variable check ensures the redeployment is done only on the master environment. Conclusion After setting up the CLI tool in the environment, we learned how to use it in order to trigger various maintenance tasks for the project.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-set-up-automated-environment-tasks/127",
        "relurl": "/t/how-to-set-up-automated-environment-tasks/127"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "857da84ef00e2fabd0700a1594a1b917ce872c4a",
        "title": "How to use wkhtmltopdf on platform.sh",
        "description": "Goal https://wkhtmltopdf.org/ is a tool to generate PDF files from HTML. It can also be used from within https://www.drupal.org/project/wkhtmltopdf. This article will explain how to install it on http://platform.sh Installing You can install the wkhtmltopdf-binary using the https://rubygems.org/gems/wkhtmltopdf-binary/ . You do not need to run ruby to be able to use this. Add this to your .platform.app.yaml dependencies to install: dependencies: ruby: \"wkhtmltopdf-binary\": \" 0.12.5.1\" Note that you can specify the binary version. Check the https://rubygems.org/gems/wkhtmltopdf-binary/ for a full list of all versions. Important: Make sure you also add wkhtmltopdf -V to your build hook. This is needed because of the way the ruby gem works. The first time it runs, it unpacks the correct binary into your /app folder. And since we have a read-only files system by design, this can only be done in the build hook. hooks: build: | wkhtmltopdf -V Usage When you’re using the https://www.drupal.org/project/wkhtmltopdf it should already be working. You can also use a wrapper e.g. https://github.com/mikehaertl/phpwkhtmltopdf https://github.com/mikehaertl/phpwkhtmltopdf The default examples should work use mikehaertl\\wkhtmlto\\Pdf; // You can pass a filename, a HTML string, an URL or an options array to the constructor $pdf = new Pdf('/path/to/page.html'); if (!$pdf-saveAs('/path/to/page.pdf')) { $error = $pdf-getError(); // ... handle error here } If you have any other way of using it, make sure you are calling the alias wkhtmltopdf. Don’t use /app/.global/gems/wkhtmltopdf-binary-0.12.5/bin/wkhtmltopdf because that will cause problems upon upgrading the version. e.g. 0.0\" to this: dependencies: ruby: \"wkhtmltopdf-binary\": \" 0.12.5\" or any other version you specifically require.",
        "text": "Goal https://wkhtmltopdf.org/ is a tool to generate PDF files from HTML. It can also be used from within https://www.drupal.org/project/wkhtmltopdf. This article will explain how to install it on http://platform.sh Installing You can install the wkhtmltopdf-binary using the https://rubygems.org/gems/wkhtmltopdf-binary/ . You do not need to run ruby to be able to use this. Add this to your .platform.app.yaml dependencies to install: dependencies: ruby: \"wkhtmltopdf-binary\": \" 0.12.5.1\" Note that you can specify the binary version. Check the https://rubygems.org/gems/wkhtmltopdf-binary/ for a full list of all versions. Important: Make sure you also add wkhtmltopdf -V to your build hook. This is needed because of the way the ruby gem works. The first time it runs, it unpacks the correct binary into your /app folder. And since we have a read-only files system by design, this can only be done in the build hook. hooks: build: | wkhtmltopdf -V Usage When you’re using the https://www.drupal.org/project/wkhtmltopdf it should already be working. You can also use a wrapper e.g. https://github.com/mikehaertl/phpwkhtmltopdf https://github.com/mikehaertl/phpwkhtmltopdf The default examples should work use mikehaertl\\wkhtmlto\\Pdf; // You can pass a filename, a HTML string, an URL or an options array to the constructor $pdf = new Pdf('/path/to/page.html'); if (!$pdf-saveAs('/path/to/page.pdf')) { $error = $pdf-getError(); // ... handle error here } If you have any other way of using it, make sure you are calling the alias wkhtmltopdf. Don’t use /app/.global/gems/wkhtmltopdf-binary-0.12.5/bin/wkhtmltopdf because that will cause problems upon upgrading the version. e.g. 0.0\" to this: dependencies: ruby: \"wkhtmltopdf-binary\": \" 0.12.5\" or any other version you specifically require.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-use-wkhtmltopdf-on-platform-sh/445",
        "relurl": "/t/how-to-use-wkhtmltopdf-on-platform-sh/445"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "25d431e2584a4013b37088f9dcc7d770845986e2",
        "title": "Set up XDebug on Dedicated (Pro) server clusters",
        "description": "Use your local development environment to do breakpoint debugging on your remote server(s) This document is mostly about the nuts-and-bolts of establishing communications between your local development environment and the remote servers. It’s not a HOWTO use your IDE, it’s about how to diagnose network or service issues that are specific to the Platform hosting environment. Assumptions You will need: A local copy of your project site, with all code files available An IDE such as PHPStorm with integrated XDebug support. XDebug extensions installed and activated on your environments Conceptual overview Before getting started, it’s helpful to understand what happens at every step in an xdebugging process. You normally don’t have to worry about some of these layers, but if any one of them goes wrong, nothing will work, so this may help to narrow in on what does and doesn’t work, when you are setting up for the first time. Once all the server configurations are in place, and your IDE and the tunnels are set up to listen, what happens is this: You initiate an xdebug session from your browser, by requesting http://your.site?XDEBUG_SESSION_START=yourxdebugkey This request is routed to the outer (cache) layer of the hosting platform, where the presence of the XDEBUG_SESSION_START key should tell the system not to cache it. The request next passes to the load-balancing router, which forwards it to one of three web heads. It reaches the (nginx) webserver, which also recognises the XDEBUG_SESSION_START key, and so passes it to a dedicated xdebug-enabled php service for processing. This is to avoid the performance cost of running xdebug resident or on-demand inside a production PHP service. PHP script execution begins, and this starts sending stack trace messages to a socket on that server ( /run/platform/${PROJECTID}${SUFFIX}/xdebug.sock ). Your development environment has a tunnel open to each of the three web heads, which listens to that remote socket(s), and relays these messages to a local socket (port 9000) on your development machine. Your IDE is configured to listen to that port, so the incoming message triggers IDE debugging. Your IDE is configured to map the paths of php code files on the server to your local project folders, so it’s able to show you where in the local code, the remote process is currently stepping through. You can use your IDE to step through the execution stack, evaluate state, or run to breakpoints. This sends messages back over the tunnel to the running process, which then executes on the server. Eventually, execution and page build completes, and the response is sent back from the server, and your browser displays the page. For subsequent browser requests, an XDEBUG_SESSION cookie should have been set, and should provide the same effect as the XDEBUG_SESSION_START parameter for subsequent requests. That is what is supposed to happen when all is well. The routing, tunneling and the multi-head delegation of requests are the quirks specific to this hosting environment that you may need to know. Other tutorials on https://devdocs.magento.com/guides/v2.3/cloud/howtos/debug.html or https://crosp.net/blog/software-development/web/php/understanding-and-using-xdebug-with-phpstorm-and-magento-remotely/ should be referred to for deeper HOWTOs. For reference, the config settings on the server that make this happen can be inspected on the server at /etc/platform/$USER/php-fpm.xdebug.conf /etc/platform/$USER/php.xdebug.ini Activity logs are kept separate from the usual access logs, and are seen at /var/log/platform/$USER/xdebug.access.log /var/log/platform/$USER/php5-fpm-xdebug.log Remember, you are sometimes talking to three different servers at once. A https://docs.platform.sh/dedicated/overview.html (Previously known as “Pro”, “Platform Enterprise” or “PE”) site with integrated deployment management has several web heads in the . This makes connecting to “the server” indeterminate, as any one of three may be the server for a request, so keep this in mind as we go forwards. Getting started Getting the server configured For a “Dedicated” cluster, you need to have https://accounts.platform.sh/support . This may already be done for you, so please check before raising another ticket. They will have set xdebug_enable: true on your project, and provided you with a unique xdebug_key to use to initiate the session. Your xdebug_key is usually different between your production and your staging environments. Take note of which you are using. If xdebug has already been enabled, a record of your key may have been helpfully left in a text file /mnt/shared/ for your reference. If it’s not there, you can usually retrieve the xdebug key with a command like: platform ssh --environment=staging 'grep -A 3 XDebug /etc/platform/$USER/nginx.conf' # XDebug Configuration ## map \"$cookie_xdebug_session$arg_xdebug_session_start$arg_xdebug_session_stop\" $php_backend { \"Gd6QdPZaqnnSet32\" \"unix:///run/platform/myproject_stg/php5-xdebug.sock\"; = Your XDEBUGKEY=Gd6QdPZaqnnSet32 If you can’t find that key in your nginx.conf file either, then xdebug is probably not yet enabled for you, and you should raise the request. Local environment Assume any debugging should be happening on staging in the first case. PROJECTID=[projectID from ticket] BRANCH='staging' Or if you already have the project cloned locally: PROJECTID=$(platform project:info id) HOSTNAME=$(platform environment:info edge_hostname) XDebug talking to multiple webheads To listen to multiple possible sources of incoming xdebug connections, XDebug communication from all three heads need to be tunneled back to a port (9000) on our local environment. Most public XDebug tutorials won’t allow for this multiple-head issue. Open tunnels to the servers Here’s a small script that sets up 3 simultaneous tunnels: # Set these: PROJECTID=\"xxxxxxxxxxxxx\" XDEBUGKEY=\"yyyyyyyyyyyyy\" # Optionally change these: BRANCH=\"staging\"; # or 'master' PORT=9000 # These are the per-instance configurations you may need to change. # Review these URL=$( platform --project=${PROJECTID} --environment=${BRANCH} route:get 'https://{default}/' --property=url ) [ $BRANCH = 'staging' ] \u0026\u0026 SUFFIX='_stg' || SUFFIX='' SOCKETPATH=\"/run/platform/${PROJECTID}${SUFFIX}/xdebug.sock\" WEBHEADS=( $(platform --project=${PROJECTID} --environment=${BRANCH} ssh --pipe --all) ) # These are settings to be used for setting up the tunnels # Now open the tunnels for WEBHEAD in $WEBHEADS ; do echo \"Will listen to xdebug on webhead ${WEBHEAD}\" echo \"Clearing old xdebug socket on instance.\" ssh ${WEBHEAD} \"rm ${SOCKETPATH}\" echo \"Opening port forwarding to xdebug socket. Listening on port ${PORT} in the background.\" ssh -R ${SOCKETPATH}:localhost:${PORT} -N ${WEBHEAD} \u0026 done At this point, the remote server(s) should be sending you messages back down that tunnel. You next need to connect a listener on your end to do something with that info. To close the tunnels kill $(jobs -p) If using zsh, then “jobs -p” doesn’t work as expected. Instead, “kill %1 %2 %3 %4” may work. In my experience, the ssh tunnels time out on their own in about 10 minutes if idle. Timeout, server ssh.platform.cloud not responding. [1] Exit 255 ssh -R ${SOCKETPATH}:localhost:${PORT} -N ${WEBHEAD} To start debugging. Launch your browser with the key in the URL open \"${URL}?XDEBUG_SESSION_START=${XDEBUGKEY}\" Diagnostic: Ensure the request is hitting the server(s) correctly When that key is used (and is correct) during a browser session, transactions will be getting logged in xdebug.access.log on the server(s). You can check activity in these files to ensure that the request is even getting through - that it’s not being cached, and that the parameter is being passed through the router without being stripped. The following command will summarize the most recent xdebug.access.log from all webheads at once. WEBHEADS=$(platform ssh --project=$PROJECTID --environment=staging --pipe --all) echo $WEBHEADS | xargs -I% ssh % 'tail /var/log/platform/$USER/xdebug.access.log' Reasons the XDebug session may not be getting logged in xdebug.access.log You have three web heads, the load balancer may be sending your request to any one of them. You need to check all logs on all instances at once, not just one. Your $XDEBUGKEY is wrong. Double-check against the value in /etc/platform/$USER/nginx.conf. The URL you used was for a different branch than the one you are looking at The outside cache layer is intercepting the request. You can check if it’s cached using wget -I The inside router is not recognising or honoring the XDEBUGKEY Nginx is performing a redirect or an access denied before the request can be routed to php. You’ll need your IP to be whitelisted if using HTTP access controls. The xdebug php service (site-$USER-xdebug-php) is not running or responding. You should see it in the server process list (ps -axf). Investigating Things don’t always go smoothly, so here is a process of elimination to ensure that all things are set up as expected. To verify that xdebug configs have been deployed on the host(s) You can see the settings on the servers in /etc/platform/${PROJECTID}${SUFFIX}/php.xdebug.ini, looking like xdebug.remote_host = unix:///run/platform/xxxxxxxxxxxxx_stg/xdebug.sock To verify that xdebug is being loaded by PHP You may be able to check out a phpinfo diagnostic from within your web application and confirm xdebug is running. In Drupal this can be found underneath reports. Don’t verify using php -m command Note: Running basic diagnostics like commandline php -i on the server may not show that xdebug is enabled as php can be configured to use different settings for commandline than it does for web requests. The files php-fpm.conf, php-cli.conf, and php-fpm.xdebug.conf (deployed into in the apps etc folder) are different in that way, and are each used depending upon context of the request. To verify that the xdebug process is active xdebug.access.log The xdebug.conf tells us that the logs are at /var/log/platform/$USER/xdebug.access.log Tailing that log should show some current activity when a browser session activates xdebug with the xdebug key. When connected to an instance, tail -f /var/log/platform/${USER}/xdebug.access.log Or called directly from your environment: echo tail /var/log/platform/${PROJECTID}${SUFFIX}/xdebug.access.log | platform --project=${PROJECTID} --environment=${BRANCH} ssh echo tail /var/log/platform/${PROJECTID}${SUFFIX}/error.log | platform --project=${PROJECTID} --environment=${BRANCH} ssh Beware, the requests will actually happen on more than one instance, so you will only see some of the requests. Use tmux or similar tools to watch them all simultaneously. Gotcha if using non-standard project names Most dedicated hosting plans name your docroot after your project ID. Such as qazqaz234qaz or qazqaz234qaz_stg. Thus is the assumption used in the tunnel script that is configured to listen to the xdebug socket /run/platform/qazqaz234qaz/php5-xdebug.sock . However, if you are using a non-standard or legacy docroot name, some of these paths need to be updated. The socket may instead be something like /run/platform/shoppingsite/php5-xdebug.sock. Diagnosing if the cache layer is interfering If your outer cache layer (eg Fastly) is returning a previously cached version of the page, you will seem to be getting debuggable transactions at first, but later requests will fail to debug, sometimes unpredictably. Use curl -I \"${URL}?XDEBUG_SESSION_START=${XDEBUGKEY}\" a couple of times in a row, and look for x-cache: HIT, HIT in the response. If this is happening, you need to bypass the cache. This may be possible by disabling your cache, or configuring it to use the presence of the XDEBUG_SESSION_START argument to prevent caching - using the cache management configurations if you have access to them. To verify that the socket is open for communication with the debugger client After the client sets up a tunnel from their development side, the socket file mentioned in the configs should be seen on each of the servers. ls -la /run/platform/${USER}/xdebug.sock You should check to see that the last-modified date on it is recent - that reflects the last time the socket was set up. Beware timezones on the server are likely to be quite different to your own! Compare the date against the server time! echo $(( $(date +%s) - $(stat -c%Y \"/run/platform/${USER}/xdebug.sock\") )) seconds old.` Note: Don’t get distracted by php5-xdebug.sock seen in the same directory, that’s an internal socket used for communication between php-fpm and nginx To verify that messages are being sent down the pipe to the debugger client Listen for a bit When the tunnels are active, port 9000 on the developers machine is a window into the xdebug process. If it seems that xdebug is not firing at all, on the server you may sniff what’s happening on port 9000 with something like As a very basic test, running netcat --listen --local-port 9000 on the developer machine, and then visiting the website with the XDEBUG_SESSION_START key in the URL (or the XDEBUG_CONFIG set in a CLI environment) should result in the first raw xdebug message being shown on your console. https://xdebug.org/docs/remote . Doing this will stall the server, as you will not be able to respond with the expected sort of acknowledgements (just exit out) but if you get any sort of initial packet sent to that port, it shows that something xdebug is happening, and you need to work on the tunnels. It seems that unless you acknowledge that first message appropriately, no subsequent ones will be sent, and the server will hang there until you kill one end of the conversation, so there is a limit to what can be done without a real debugging tool, but this may at least prove that messages are getting through to the developers desktop. https://hackernoon.com/how-debug-php-applications-with-dephpugger-98cc234d917c is a quick CLI tool for this, but you probably want to just go straight to using a real IDE. Using an IDE to listen to xdebug messages With something like PHPStorm, you can just ‘start listening’ to port 9000 and when the first message arrives from the server, the wizard will ask you to match the incoming request (eg /app/${PROJECTID}_stg/index.php ) to a local file to begin breakpoint debugging. If you don’t have a local checkout of the project, well, you need to go get one to proceed now. XDebug on cli Note that xdebugging on the CLI does NOT log into the access log (not even the xdebug.access.log which is for web requests) so looking for clues there will not help. You can trigger xdebug behaviour on the CLI using a custom php.ini, by setting an environment variable export XDEBUG_CONFIG=\"remote_enable=1\" or by specifying everything up front in the commandline arguments …though all methods ALSO need XDEBUG_CONFIG to at least be set to SOMETHING. To test XDebug is working in a snippet As a single command is the most straightforward for testing, XDebug can be triggered minimally with: # On the host: SOCKETPATH=\"unix:///run/platform/${USER}/xdebug.sock\" export XDEBUG_CONFIG=\"remote_enable=1 remote_host=$SOCKETPATH\" php \\ -dzend_extension=xdebug.so \\ index.php If you have a listener open on port 9000 on your local dev, it’ll start getting messages. I haven’t been able to find a way to get logs of these transactions, so it’s up to you to be listening correctly. To use the php.xdebug.ini To work as designed however, a php.xdebug.ini has been provided. To use that, you should invoke php, source the special ini, and also must set XDEBUG_CONFIG to non-null in your session. PHPXDEBUG=/etc/platform/${USER}/php.xdebug.ini export XDEBUG_CONFIG=true php -c $PHPXDEBUG index.php … and stuff should be coming down the socket. Interesting snippets: PHPXDEBUG=/etc/platform/$USER/php.xdebug.ini php -c $PHPXDEBUG -r ‘echo(ini_get(“xdebug.idekey”));’ Go and trace your project The real fun begins after the tunnels are set up and the XDebug communications are happening. Other tutorials from your IDE will probably be more helpful than can be covered here.",
        "text": "Use your local development environment to do breakpoint debugging on your remote server(s) This document is mostly about the nuts-and-bolts of establishing communications between your local development environment and the remote servers. It’s not a HOWTO use your IDE, it’s about how to diagnose network or service issues that are specific to the Platform hosting environment. Assumptions You will need: A local copy of your project site, with all code files available An IDE such as PHPStorm with integrated XDebug support. XDebug extensions installed and activated on your environments Conceptual overview Before getting started, it’s helpful to understand what happens at every step in an xdebugging process. You normally don’t have to worry about some of these layers, but if any one of them goes wrong, nothing will work, so this may help to narrow in on what does and doesn’t work, when you are setting up for the first time. Once all the server configurations are in place, and your IDE and the tunnels are set up to listen, what happens is this: You initiate an xdebug session from your browser, by requesting http://your.site?XDEBUG_SESSION_START=yourxdebugkey This request is routed to the outer (cache) layer of the hosting platform, where the presence of the XDEBUG_SESSION_START key should tell the system not to cache it. The request next passes to the load-balancing router, which forwards it to one of three web heads. It reaches the (nginx) webserver, which also recognises the XDEBUG_SESSION_START key, and so passes it to a dedicated xdebug-enabled php service for processing. This is to avoid the performance cost of running xdebug resident or on-demand inside a production PHP service. PHP script execution begins, and this starts sending stack trace messages to a socket on that server ( /run/platform/${PROJECTID}${SUFFIX}/xdebug.sock ). Your development environment has a tunnel open to each of the three web heads, which listens to that remote socket(s), and relays these messages to a local socket (port 9000) on your development machine. Your IDE is configured to listen to that port, so the incoming message triggers IDE debugging. Your IDE is configured to map the paths of php code files on the server to your local project folders, so it’s able to show you where in the local code, the remote process is currently stepping through. You can use your IDE to step through the execution stack, evaluate state, or run to breakpoints. This sends messages back over the tunnel to the running process, which then executes on the server. Eventually, execution and page build completes, and the response is sent back from the server, and your browser displays the page. For subsequent browser requests, an XDEBUG_SESSION cookie should have been set, and should provide the same effect as the XDEBUG_SESSION_START parameter for subsequent requests. That is what is supposed to happen when all is well. The routing, tunneling and the multi-head delegation of requests are the quirks specific to this hosting environment that you may need to know. Other tutorials on https://devdocs.magento.com/guides/v2.3/cloud/howtos/debug.html or https://crosp.net/blog/software-development/web/php/understanding-and-using-xdebug-with-phpstorm-and-magento-remotely/ should be referred to for deeper HOWTOs. For reference, the config settings on the server that make this happen can be inspected on the server at /etc/platform/$USER/php-fpm.xdebug.conf /etc/platform/$USER/php.xdebug.ini Activity logs are kept separate from the usual access logs, and are seen at /var/log/platform/$USER/xdebug.access.log /var/log/platform/$USER/php5-fpm-xdebug.log Remember, you are sometimes talking to three different servers at once. A https://docs.platform.sh/dedicated/overview.html (Previously known as “Pro”, “Platform Enterprise” or “PE”) site with integrated deployment management has several web heads in the . This makes connecting to “the server” indeterminate, as any one of three may be the server for a request, so keep this in mind as we go forwards. Getting started Getting the server configured For a “Dedicated” cluster, you need to have https://accounts.platform.sh/support . This may already be done for you, so please check before raising another ticket. They will have set xdebug_enable: true on your project, and provided you with a unique xdebug_key to use to initiate the session. Your xdebug_key is usually different between your production and your staging environments. Take note of which you are using. If xdebug has already been enabled, a record of your key may have been helpfully left in a text file /mnt/shared/ for your reference. If it’s not there, you can usually retrieve the xdebug key with a command like: platform ssh --environment=staging 'grep -A 3 XDebug /etc/platform/$USER/nginx.conf' # XDebug Configuration ## map \"$cookie_xdebug_session$arg_xdebug_session_start$arg_xdebug_session_stop\" $php_backend { \"Gd6QdPZaqnnSet32\" \"unix:///run/platform/myproject_stg/php5-xdebug.sock\"; = Your XDEBUGKEY=Gd6QdPZaqnnSet32 If you can’t find that key in your nginx.conf file either, then xdebug is probably not yet enabled for you, and you should raise the request. Local environment Assume any debugging should be happening on staging in the first case. PROJECTID=[projectID from ticket] BRANCH='staging' Or if you already have the project cloned locally: PROJECTID=$(platform project:info id) HOSTNAME=$(platform environment:info edge_hostname) XDebug talking to multiple webheads To listen to multiple possible sources of incoming xdebug connections, XDebug communication from all three heads need to be tunneled back to a port (9000) on our local environment. Most public XDebug tutorials won’t allow for this multiple-head issue. Open tunnels to the servers Here’s a small script that sets up 3 simultaneous tunnels: # Set these: PROJECTID=\"xxxxxxxxxxxxx\" XDEBUGKEY=\"yyyyyyyyyyyyy\" # Optionally change these: BRANCH=\"staging\"; # or 'master' PORT=9000 # These are the per-instance configurations you may need to change. # Review these URL=$( platform --project=${PROJECTID} --environment=${BRANCH} route:get 'https://{default}/' --property=url ) [ $BRANCH = 'staging' ] \u0026\u0026 SUFFIX='_stg' || SUFFIX='' SOCKETPATH=\"/run/platform/${PROJECTID}${SUFFIX}/xdebug.sock\" WEBHEADS=( $(platform --project=${PROJECTID} --environment=${BRANCH} ssh --pipe --all) ) # These are settings to be used for setting up the tunnels # Now open the tunnels for WEBHEAD in $WEBHEADS ; do echo \"Will listen to xdebug on webhead ${WEBHEAD}\" echo \"Clearing old xdebug socket on instance.\" ssh ${WEBHEAD} \"rm ${SOCKETPATH}\" echo \"Opening port forwarding to xdebug socket. Listening on port ${PORT} in the background.\" ssh -R ${SOCKETPATH}:localhost:${PORT} -N ${WEBHEAD} \u0026 done At this point, the remote server(s) should be sending you messages back down that tunnel. You next need to connect a listener on your end to do something with that info. To close the tunnels kill $(jobs -p) If using zsh, then “jobs -p” doesn’t work as expected. Instead, “kill %1 %2 %3 %4” may work. In my experience, the ssh tunnels time out on their own in about 10 minutes if idle. Timeout, server ssh.platform.cloud not responding. [1] Exit 255 ssh -R ${SOCKETPATH}:localhost:${PORT} -N ${WEBHEAD} To start debugging. Launch your browser with the key in the URL open \"${URL}?XDEBUG_SESSION_START=${XDEBUGKEY}\" Diagnostic: Ensure the request is hitting the server(s) correctly When that key is used (and is correct) during a browser session, transactions will be getting logged in xdebug.access.log on the server(s). You can check activity in these files to ensure that the request is even getting through - that it’s not being cached, and that the parameter is being passed through the router without being stripped. The following command will summarize the most recent xdebug.access.log from all webheads at once. WEBHEADS=$(platform ssh --project=$PROJECTID --environment=staging --pipe --all) echo $WEBHEADS | xargs -I% ssh % 'tail /var/log/platform/$USER/xdebug.access.log' Reasons the XDebug session may not be getting logged in xdebug.access.log You have three web heads, the load balancer may be sending your request to any one of them. You need to check all logs on all instances at once, not just one. Your $XDEBUGKEY is wrong. Double-check against the value in /etc/platform/$USER/nginx.conf. The URL you used was for a different branch than the one you are looking at The outside cache layer is intercepting the request. You can check if it’s cached using wget -I The inside router is not recognising or honoring the XDEBUGKEY Nginx is performing a redirect or an access denied before the request can be routed to php. You’ll need your IP to be whitelisted if using HTTP access controls. The xdebug php service (site-$USER-xdebug-php) is not running or responding. You should see it in the server process list (ps -axf). Investigating Things don’t always go smoothly, so here is a process of elimination to ensure that all things are set up as expected. To verify that xdebug configs have been deployed on the host(s) You can see the settings on the servers in /etc/platform/${PROJECTID}${SUFFIX}/php.xdebug.ini, looking like xdebug.remote_host = unix:///run/platform/xxxxxxxxxxxxx_stg/xdebug.sock To verify that xdebug is being loaded by PHP You may be able to check out a phpinfo diagnostic from within your web application and confirm xdebug is running. In Drupal this can be found underneath reports. Don’t verify using php -m command Note: Running basic diagnostics like commandline php -i on the server may not show that xdebug is enabled as php can be configured to use different settings for commandline than it does for web requests. The files php-fpm.conf, php-cli.conf, and php-fpm.xdebug.conf (deployed into in the apps etc folder) are different in that way, and are each used depending upon context of the request. To verify that the xdebug process is active xdebug.access.log The xdebug.conf tells us that the logs are at /var/log/platform/$USER/xdebug.access.log Tailing that log should show some current activity when a browser session activates xdebug with the xdebug key. When connected to an instance, tail -f /var/log/platform/${USER}/xdebug.access.log Or called directly from your environment: echo tail /var/log/platform/${PROJECTID}${SUFFIX}/xdebug.access.log | platform --project=${PROJECTID} --environment=${BRANCH} ssh echo tail /var/log/platform/${PROJECTID}${SUFFIX}/error.log | platform --project=${PROJECTID} --environment=${BRANCH} ssh Beware, the requests will actually happen on more than one instance, so you will only see some of the requests. Use tmux or similar tools to watch them all simultaneously. Gotcha if using non-standard project names Most dedicated hosting plans name your docroot after your project ID. Such as qazqaz234qaz or qazqaz234qaz_stg. Thus is the assumption used in the tunnel script that is configured to listen to the xdebug socket /run/platform/qazqaz234qaz/php5-xdebug.sock . However, if you are using a non-standard or legacy docroot name, some of these paths need to be updated. The socket may instead be something like /run/platform/shoppingsite/php5-xdebug.sock. Diagnosing if the cache layer is interfering If your outer cache layer (eg Fastly) is returning a previously cached version of the page, you will seem to be getting debuggable transactions at first, but later requests will fail to debug, sometimes unpredictably. Use curl -I \"${URL}?XDEBUG_SESSION_START=${XDEBUGKEY}\" a couple of times in a row, and look for x-cache: HIT, HIT in the response. If this is happening, you need to bypass the cache. This may be possible by disabling your cache, or configuring it to use the presence of the XDEBUG_SESSION_START argument to prevent caching - using the cache management configurations if you have access to them. To verify that the socket is open for communication with the debugger client After the client sets up a tunnel from their development side, the socket file mentioned in the configs should be seen on each of the servers. ls -la /run/platform/${USER}/xdebug.sock You should check to see that the last-modified date on it is recent - that reflects the last time the socket was set up. Beware timezones on the server are likely to be quite different to your own! Compare the date against the server time! echo $(( $(date +%s) - $(stat -c%Y \"/run/platform/${USER}/xdebug.sock\") )) seconds old.` Note: Don’t get distracted by php5-xdebug.sock seen in the same directory, that’s an internal socket used for communication between php-fpm and nginx To verify that messages are being sent down the pipe to the debugger client Listen for a bit When the tunnels are active, port 9000 on the developers machine is a window into the xdebug process. If it seems that xdebug is not firing at all, on the server you may sniff what’s happening on port 9000 with something like As a very basic test, running netcat --listen --local-port 9000 on the developer machine, and then visiting the website with the XDEBUG_SESSION_START key in the URL (or the XDEBUG_CONFIG set in a CLI environment) should result in the first raw xdebug message being shown on your console. https://xdebug.org/docs/remote . Doing this will stall the server, as you will not be able to respond with the expected sort of acknowledgements (just exit out) but if you get any sort of initial packet sent to that port, it shows that something xdebug is happening, and you need to work on the tunnels. It seems that unless you acknowledge that first message appropriately, no subsequent ones will be sent, and the server will hang there until you kill one end of the conversation, so there is a limit to what can be done without a real debugging tool, but this may at least prove that messages are getting through to the developers desktop. https://hackernoon.com/how-debug-php-applications-with-dephpugger-98cc234d917c is a quick CLI tool for this, but you probably want to just go straight to using a real IDE. Using an IDE to listen to xdebug messages With something like PHPStorm, you can just ‘start listening’ to port 9000 and when the first message arrives from the server, the wizard will ask you to match the incoming request (eg /app/${PROJECTID}_stg/index.php ) to a local file to begin breakpoint debugging. If you don’t have a local checkout of the project, well, you need to go get one to proceed now. XDebug on cli Note that xdebugging on the CLI does NOT log into the access log (not even the xdebug.access.log which is for web requests) so looking for clues there will not help. You can trigger xdebug behaviour on the CLI using a custom php.ini, by setting an environment variable export XDEBUG_CONFIG=\"remote_enable=1\" or by specifying everything up front in the commandline arguments …though all methods ALSO need XDEBUG_CONFIG to at least be set to SOMETHING. To test XDebug is working in a snippet As a single command is the most straightforward for testing, XDebug can be triggered minimally with: # On the host: SOCKETPATH=\"unix:///run/platform/${USER}/xdebug.sock\" export XDEBUG_CONFIG=\"remote_enable=1 remote_host=$SOCKETPATH\" php \\ -dzend_extension=xdebug.so \\ index.php If you have a listener open on port 9000 on your local dev, it’ll start getting messages. I haven’t been able to find a way to get logs of these transactions, so it’s up to you to be listening correctly. To use the php.xdebug.ini To work as designed however, a php.xdebug.ini has been provided. To use that, you should invoke php, source the special ini, and also must set XDEBUG_CONFIG to non-null in your session. PHPXDEBUG=/etc/platform/${USER}/php.xdebug.ini export XDEBUG_CONFIG=true php -c $PHPXDEBUG index.php … and stuff should be coming down the socket. Interesting snippets: PHPXDEBUG=/etc/platform/$USER/php.xdebug.ini php -c $PHPXDEBUG -r ‘echo(ini_get(“xdebug.idekey”));’ Go and trace your project The real fun begins after the tunnels are set up and the XDebug communications are happening. Other tutorials from your IDE will probably be more helpful than can be covered here.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/set-up-xdebug-on-dedicated-pro-server-clusters/403",
        "relurl": "/t/set-up-xdebug-on-dedicated-pro-server-clusters/403"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "464345847de3070ff40f5ba7bcf9209ca1ae6a07",
        "title": "How to configure a php mail library (like Drupal 7 phpmailer) to use the correct server",
        "description": "Goal To send mail, using a third party PHP library, In a Drupal environment. Assumptions This is for Drupal 7, and its https://www.drupal.org/project/phpmailer. However, the concepts may apply to any PHP app that needs SMTP mail server details provided to it during configuration. You have admin access to your Drupal site. You have write access to your sites settings.php (by way of git deployments). You have read the initial , and taken notes that you need to explicitly enable emails on dev environments if needed. Problems Mail doesn’t send, because I’m using an advanced library. Test emails are not delivered. Watchdog reports, or drush wd-show shows messages like Error sending e-mail SMTP Error: SMTP connect() failed. By default, using mail() on a http://Platform.sh environment is already configured to just work, as The PHP runtime is configured to send email automatically via the assigned SendGrid sub-account. However, if using additional mail libraries like SwiftMailer, Drupals https://www.drupal.org/project/smtp or https://www.drupal.org/project/phpmailer, the basic PHP mail() is deliberately bypassed, and additional SMTP settings are exposed, and must now be configured by you. Steps If using Drupal 7 phpmailer: 1. Check the settings At the admin page found at /admin/config/system/phpmailer check the Primary SMTP server. It should not be localhost. It may need to be set to whatever the value of the Platform.sh-provided environment variable PLATFORM_SMTP_HOST is for your hosting environment. This may be different depending on region or plan. . Hint: you can see the environment variables available to your runtime by visiting your phpinfo page at /admin/reports/status/php - though it’s also available by ssh-ing in also. platform ssh ‘echo PLATFORM_SMTP_HOST is $PLATFORM_SMTP_HOST’ Copy that host value (an IP) into your Primary SMTP server setting at /admin/config/system/phpmailer. The SMTP port should remain 25. Test that mail works by entering your email address in the Test Configuration section when you save. 2. Configure these settings in your settings.php file, not just the UI For protection against database overwrites, and to ensure the settings remain correct if you have to move servers around, it can be better to copy this environment variable out of the live environment, and into Drupals configuration settings directly. Inspecting the settings ( drush vget smtp ) tells us that the configuration key we need to update is smtp_host. Edit your sites settings.php or equivalent, and add a line like: $conf['smtp_host'] = getenv('PLATFORM_SMTP_HOST'); Test email sending works. Conclusion The mail delivery subsystem is now configured to send mail using the named server explicitly. If things change, the correct SMTP server should be updated also. As noted earlier, the default behaviour with no special mail handling subsystem, is expected to work automatically. It’s only the more configurable ones that may have incorrect placeholders, or may require you to provide a value explicitly.",
        "text": "Goal To send mail, using a third party PHP library, In a Drupal environment. Assumptions This is for Drupal 7, and its https://www.drupal.org/project/phpmailer. However, the concepts may apply to any PHP app that needs SMTP mail server details provided to it during configuration. You have admin access to your Drupal site. You have write access to your sites settings.php (by way of git deployments). You have read the initial , and taken notes that you need to explicitly enable emails on dev environments if needed. Problems Mail doesn’t send, because I’m using an advanced library. Test emails are not delivered. Watchdog reports, or drush wd-show shows messages like Error sending e-mail SMTP Error: SMTP connect() failed. By default, using mail() on a http://Platform.sh environment is already configured to just work, as The PHP runtime is configured to send email automatically via the assigned SendGrid sub-account. However, if using additional mail libraries like SwiftMailer, Drupals https://www.drupal.org/project/smtp or https://www.drupal.org/project/phpmailer, the basic PHP mail() is deliberately bypassed, and additional SMTP settings are exposed, and must now be configured by you. Steps If using Drupal 7 phpmailer: 1. Check the settings At the admin page found at /admin/config/system/phpmailer check the Primary SMTP server. It should not be localhost. It may need to be set to whatever the value of the Platform.sh-provided environment variable PLATFORM_SMTP_HOST is for your hosting environment. This may be different depending on region or plan. . Hint: you can see the environment variables available to your runtime by visiting your phpinfo page at /admin/reports/status/php - though it’s also available by ssh-ing in also. platform ssh ‘echo PLATFORM_SMTP_HOST is $PLATFORM_SMTP_HOST’ Copy that host value (an IP) into your Primary SMTP server setting at /admin/config/system/phpmailer. The SMTP port should remain 25. Test that mail works by entering your email address in the Test Configuration section when you save. 2. Configure these settings in your settings.php file, not just the UI For protection against database overwrites, and to ensure the settings remain correct if you have to move servers around, it can be better to copy this environment variable out of the live environment, and into Drupals configuration settings directly. Inspecting the settings ( drush vget smtp ) tells us that the configuration key we need to update is smtp_host. Edit your sites settings.php or equivalent, and add a line like: $conf['smtp_host'] = getenv('PLATFORM_SMTP_HOST'); Test email sending works. Conclusion The mail delivery subsystem is now configured to send mail using the named server explicitly. If things change, the correct SMTP server should be updated also. As noted earlier, the default behaviour with no special mail handling subsystem, is expected to work automatically. It’s only the more configurable ones that may have incorrect placeholders, or may require you to provide a value explicitly.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-configure-a-php-mail-library-like-drupal-7-phpmailer-to-use-the-correct-server/437",
        "relurl": "/t/how-to-configure-a-php-mail-library-like-drupal-7-phpmailer-to-use-the-correct-server/437"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "712240881c66b83d9fdcaf337719d6ead3239b78",
        "title": "How do I get a postgres connection string for prisma2 framework",
        "description": "Goal To create a postgresql connection string from the PLATFORM_RELATIONSHIPS environment variable. Assumptions You have nodejs installed in your container. You have postgresql You know how to work with prisma2 Problem You can get database credentials using the $PLATFORM_RELATIONSHIPS variable echo $PLATFORM_RELATIONSHIPS | base64 --decode | json_pp But… the prisma2 framework assumes you have a postgresql connection string https://github.com/prisma/prisma2/blob/master/docs/core/connectors/postgresql.md . Proposed solution We can make a simple nodejs script that takes the base64 decoded string from PLATFORM_RELATIONSHIPS, and converts it into a postgresql connection string. In a bash script, that would look like this: $ DECODED_RELATIONSHIPS=$(echo $PLATFORM_RELATIONSHIPS | base64 --decode) $ nodejs get_postgres_querystring.js $DECODED_RELATIONSHIPS postgresdatabase postgres://main:main@postgresdatabase.internal:5432/main Where the last variable, postgresdatabase, is the name of the relationship. And get_postgres_querystring.js looks like this: var myArgs = process.argv.slice(2); var relationships = myArgs[0]; var db = myArgs[1]; var json_rel = JSON.parse(relationships); var json_db = json_rel[db][0]; var url = \"postgres://\"+json_db[\"username\"]+\":\"+json_db[\"password\"]+\"@\"+json_db[\"host\"]+\":\"+json_db[\"port\"]+\"/\"+json_db[\"path\"]; console.log(url); Integrate into your environment Create a .environment file According to the documentation we can create an that allows us to set environment variables. Create a .environment file and put what we have learned above in it, but add the output to an environment variable using export $ DECODED_RELATIONSHIPS=$(echo $PLATFORM_RELATIONSHIPS | base64 --decode) $ connect_string=$(nodejs qs.js $DECODED_RELATIONSHIPS postgresdatabase) $ export PS_CON_STRING=\"$connect_string\" Using the environment variable You can now use env(\"PS_CON_STRING\") in your https://github.com/prisma/prisma2/blob/master/docs/core/connectors/postgresql.md .",
        "text": "Goal To create a postgresql connection string from the PLATFORM_RELATIONSHIPS environment variable. Assumptions You have nodejs installed in your container. You have postgresql You know how to work with prisma2 Problem You can get database credentials using the $PLATFORM_RELATIONSHIPS variable echo $PLATFORM_RELATIONSHIPS | base64 --decode | json_pp But… the prisma2 framework assumes you have a postgresql connection string https://github.com/prisma/prisma2/blob/master/docs/core/connectors/postgresql.md . Proposed solution We can make a simple nodejs script that takes the base64 decoded string from PLATFORM_RELATIONSHIPS, and converts it into a postgresql connection string. In a bash script, that would look like this: $ DECODED_RELATIONSHIPS=$(echo $PLATFORM_RELATIONSHIPS | base64 --decode) $ nodejs get_postgres_querystring.js $DECODED_RELATIONSHIPS postgresdatabase postgres://main:main@postgresdatabase.internal:5432/main Where the last variable, postgresdatabase, is the name of the relationship. And get_postgres_querystring.js looks like this: var myArgs = process.argv.slice(2); var relationships = myArgs[0]; var db = myArgs[1]; var json_rel = JSON.parse(relationships); var json_db = json_rel[db][0]; var url = \"postgres://\"+json_db[\"username\"]+\":\"+json_db[\"password\"]+\"@\"+json_db[\"host\"]+\":\"+json_db[\"port\"]+\"/\"+json_db[\"path\"]; console.log(url); Integrate into your environment Create a .environment file According to the documentation we can create an that allows us to set environment variables. Create a .environment file and put what we have learned above in it, but add the output to an environment variable using export $ DECODED_RELATIONSHIPS=$(echo $PLATFORM_RELATIONSHIPS | base64 --decode) $ connect_string=$(nodejs qs.js $DECODED_RELATIONSHIPS postgresdatabase) $ export PS_CON_STRING=\"$connect_string\" Using the environment variable You can now use env(\"PS_CON_STRING\") in your https://github.com/prisma/prisma2/blob/master/docs/core/connectors/postgresql.md .",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-get-a-postgres-connection-string-for-prisma2-framework/427",
        "relurl": "/t/how-do-i-get-a-postgres-connection-string-for-prisma2-framework/427"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "5888f35719f97cac1eb7fa44e5a5736c5ced5b2e",
        "title": "How to back up environment variables",
        "description": "Introduction You always need to store some sensitive data for your application to work. But you should never store them in your git repository. For this, you can use environment variables to define per-environment configuration settings and other sensitive information. Each project depends on those settings and will not work without the correct value in each variable. The default ‘restore from backup’ procedure on http://platform.sh will restore your code and data but it will not store environment variables because of safety concerns. This article describes how you can backup your variables in a secure and easy way. Note: This article is based on a https://www.contextualcode.com/Blog/backup-environment-variables-on-platform.sh written by Ivan Ternovtsiy from Contextual Code. Contextual Code relies heavily on http://Platform.Sh to host their customer’s websites. Feel free to check out their original blog post and website. Prerequisites You will need: https://www.php.net/manual/en/openssl.installation.php PHP extension. Installed on http://platform.sh by default. https://aws.amazon.com/cli/ installed and configured with access to the backup bucket dependencies: python2: awscli: '*' installed and configured with access to project variables Set your token in the PLATFORMSH_CLI_TOKEN environment variable. update .platform.app.yaml hooks: build: | if [ ! -z \"$PLATFORMSH_CLI_TOKEN\" ]; then curl -sS https://platform.sh/cli/installer | php fi Backup To configure a daily backup, follow these steps. It will add a daily backup with 7 days of retention. Storage data files are named PROJECT_BRANCH_EnvVars_DAY.json. The command MUST be executed on the http://platform.sh instance, otherwise, it will not be able to access sensitive variables value. Set http://Platform.sh variables: platform variable:create --level=project --name=BACKUP_ENVVAR_S3_DIRECTORY --value=\"s3://your-bucket/platformsh-env-variables\" --json=false --sensitive=false --prefix=env --visible-build=false --visible-runtime=true Copy backup utility tool from https://gitlab.com/contextualcode/platformsh-migration/raw/master/utils/backup_environment_variables.php cd /bin/php wget https://gitlab.com/contextualcode/platformsh-migration/raw/master/utils/backup_environment_variables.php Add backup_env_vars_to_s3 cron ( .platform.app.yaml): backup_env_vars_to_s3: spec: '45 23 * * *' cmd: php bin/php/backup_environment_variables.php Commit the changes: git add .platform.app.yaml bin/php/backup_environment_variables.php git commit -m \"Add backup environment variables to AWS S3\" git push Under the hood Below is a summary of what the code is actually doing, but feel free to check the source code. It’s always a good idea to inspect third party code before running it, especially when sensitive information is at risk. read list of environment variables using “platform variables” command. read actual value of sensitive variable using php “getenv()” function. read variable options (level, is sensitive, is inheritable, is json) build json with ‘method’, ‘iv’, ‘data’, ‘tag’. ‘method’ - encryption method. AES-256-GCM for PHP7.1+ and AES-256-CBC otherwise. ‘iv’ - random initialization vector, used during encryption. ‘tag’ - encryption tag. Used for AES-256-GCM, empty for AES-256-CBC. ‘data’ - encrypted json with variables list. store json to file and push it to S3 using aws s3 cp command. Restore The ‘restore’ command can be triggered from a http://platform.sh instance or outside. Keep in mind that execution outside of http://platform.sh will not be able to perform “same exists” checks for sensitive variables. As a result, it will trigger the update of all variable disregard existing variable values. To restore values from http://Platform.sh. On http://platform.sh, copy file from https://gitlab.com/contextualcode/platformsh-migration/raw/master/utils/restore_environment_variables.php cd /tmp wget https://gitlab.com/contextualcode/platformsh-migration/raw/master/utils/restore_environment_variables.php Check the latest backup filename and restore variables from it. Use --dry-run to see what will be created/updated. aws s3 ls s3://your-bucket/platformsh-env-variables/ php /tmp/restore_environment_variables.php --dry-run --s3dir=s3://your-bucket/platformsh-env-variables --s3filename=projectname_stageEnvVars_FRI.json --override-existing --secret=ChangeTheSecret After changing per environment variable, http://platform.sh will trigger a redeploy, so you’ll need to re-execute step 2 a few times to get all the variables restored. Under the hood Below is a summary of what the code is actually doing, but feel free to check the source code. It’s always a good idea to inspect third party code before running it, especially when sensitive information is at risk. download file from S3 using aws s3 cp. decrypt it with PHP “openssl_decrypt” function compare variables in file with existing variables create variables using platform variable:create command. ignore variables with the same value as already defined in the environment skip existing variables unless --override-existing option specified. update variables using platform variable:update command. Conclusion That’s it! You should now have a backup and restore process for your environment variables!",
        "text": "Introduction You always need to store some sensitive data for your application to work. But you should never store them in your git repository. For this, you can use environment variables to define per-environment configuration settings and other sensitive information. Each project depends on those settings and will not work without the correct value in each variable. The default ‘restore from backup’ procedure on http://platform.sh will restore your code and data but it will not store environment variables because of safety concerns. This article describes how you can backup your variables in a secure and easy way. Note: This article is based on a https://www.contextualcode.com/Blog/backup-environment-variables-on-platform.sh written by Ivan Ternovtsiy from Contextual Code. Contextual Code relies heavily on http://Platform.Sh to host their customer’s websites. Feel free to check out their original blog post and website. Prerequisites You will need: https://www.php.net/manual/en/openssl.installation.php PHP extension. Installed on http://platform.sh by default. https://aws.amazon.com/cli/ installed and configured with access to the backup bucket dependencies: python2: awscli: '*' installed and configured with access to project variables Set your token in the PLATFORMSH_CLI_TOKEN environment variable. update .platform.app.yaml hooks: build: | if [ ! -z \"$PLATFORMSH_CLI_TOKEN\" ]; then curl -sS https://platform.sh/cli/installer | php fi Backup To configure a daily backup, follow these steps. It will add a daily backup with 7 days of retention. Storage data files are named PROJECT_BRANCH_EnvVars_DAY.json. The command MUST be executed on the http://platform.sh instance, otherwise, it will not be able to access sensitive variables value. Set http://Platform.sh variables: platform variable:create --level=project --name=BACKUP_ENVVAR_S3_DIRECTORY --value=\"s3://your-bucket/platformsh-env-variables\" --json=false --sensitive=false --prefix=env --visible-build=false --visible-runtime=true Copy backup utility tool from https://gitlab.com/contextualcode/platformsh-migration/raw/master/utils/backup_environment_variables.php cd /bin/php wget https://gitlab.com/contextualcode/platformsh-migration/raw/master/utils/backup_environment_variables.php Add backup_env_vars_to_s3 cron ( .platform.app.yaml): backup_env_vars_to_s3: spec: '45 23 * * *' cmd: php bin/php/backup_environment_variables.php Commit the changes: git add .platform.app.yaml bin/php/backup_environment_variables.php git commit -m \"Add backup environment variables to AWS S3\" git push Under the hood Below is a summary of what the code is actually doing, but feel free to check the source code. It’s always a good idea to inspect third party code before running it, especially when sensitive information is at risk. read list of environment variables using “platform variables” command. read actual value of sensitive variable using php “getenv()” function. read variable options (level, is sensitive, is inheritable, is json) build json with ‘method’, ‘iv’, ‘data’, ‘tag’. ‘method’ - encryption method. AES-256-GCM for PHP7.1+ and AES-256-CBC otherwise. ‘iv’ - random initialization vector, used during encryption. ‘tag’ - encryption tag. Used for AES-256-GCM, empty for AES-256-CBC. ‘data’ - encrypted json with variables list. store json to file and push it to S3 using aws s3 cp command. Restore The ‘restore’ command can be triggered from a http://platform.sh instance or outside. Keep in mind that execution outside of http://platform.sh will not be able to perform “same exists” checks for sensitive variables. As a result, it will trigger the update of all variable disregard existing variable values. To restore values from http://Platform.sh. On http://platform.sh, copy file from https://gitlab.com/contextualcode/platformsh-migration/raw/master/utils/restore_environment_variables.php cd /tmp wget https://gitlab.com/contextualcode/platformsh-migration/raw/master/utils/restore_environment_variables.php Check the latest backup filename and restore variables from it. Use --dry-run to see what will be created/updated. aws s3 ls s3://your-bucket/platformsh-env-variables/ php /tmp/restore_environment_variables.php --dry-run --s3dir=s3://your-bucket/platformsh-env-variables --s3filename=projectname_stageEnvVars_FRI.json --override-existing --secret=ChangeTheSecret After changing per environment variable, http://platform.sh will trigger a redeploy, so you’ll need to re-execute step 2 a few times to get all the variables restored. Under the hood Below is a summary of what the code is actually doing, but feel free to check the source code. It’s always a good idea to inspect third party code before running it, especially when sensitive information is at risk. download file from S3 using aws s3 cp. decrypt it with PHP “openssl_decrypt” function compare variables in file with existing variables create variables using platform variable:create command. ignore variables with the same value as already defined in the environment skip existing variables unless --override-existing option specified. update variables using platform variable:update command. Conclusion That’s it! You should now have a backup and restore process for your environment variables!",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-back-up-environment-variables/430",
        "relurl": "/t/how-to-back-up-environment-variables/430"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e0d8fb131896d9cbf64c9994144674381a27772f",
        "title": "How to use cmake in a Platform.sh build hook",
        "description": "Goal Sometimes a library may require cmake in order to compile. By default you only have make available. An example would be libgit2 how would you get this to work on http://platform.sh ? Assumptions You will need: An SSH key configured on your http://Platform.sh account A http://Platform.sh project and a .platform.app.yaml to edit Steps 1. Add at the beginning of your build hook in .platform.app.yaml Setting VERSION to the version of cmake you want. export VERSION=\"3.16.0-rc4\" wget https://github.com/Kitware/CMake/releases/download/v$VERSION/cmake-$VERSION-Linux-x86_64.tar.gz tar xzf cmake-$VERSION-Linux-x86_64.tar.gz export PATH=$PATH:/$PWD/cmake-$VERSION-Linux-x86_64/bin 2. Potentially cleanup at the end of the build step if you don’t want cmake in production rm -rf /$PWD/cmake-$VERSION-Linux-x86_64 Conclusion Any dependencies that require cmake in their build should very probably work now.",
        "text": "Goal Sometimes a library may require cmake in order to compile. By default you only have make available. An example would be libgit2 how would you get this to work on http://platform.sh ? Assumptions You will need: An SSH key configured on your http://Platform.sh account A http://Platform.sh project and a .platform.app.yaml to edit Steps 1. Add at the beginning of your build hook in .platform.app.yaml Setting VERSION to the version of cmake you want. export VERSION=\"3.16.0-rc4\" wget https://github.com/Kitware/CMake/releases/download/v$VERSION/cmake-$VERSION-Linux-x86_64.tar.gz tar xzf cmake-$VERSION-Linux-x86_64.tar.gz export PATH=$PATH:/$PWD/cmake-$VERSION-Linux-x86_64/bin 2. Potentially cleanup at the end of the build step if you don’t want cmake in production rm -rf /$PWD/cmake-$VERSION-Linux-x86_64 Conclusion Any dependencies that require cmake in their build should very probably work now.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-use-cmake-in-a-platform-sh-build-hook/405",
        "relurl": "/t/how-to-use-cmake-in-a-platform-sh-build-hook/405"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "51e57f5aa87d82f80ed2381075eb46173d0f463c",
        "title": "How do I check the disk usage for elastic search?",
        "description": "Goal Get the available disk size of elasticsearch Assumptions You already have elasticsearch set-up and running. Steps 1. Log in to the app container using ssh 2. Query the elastic search api Run this curl snippet assuming your elasticsearch relationship is called elasticsearch. If the name is different, change the snippet below to reflect that. curl -XGET 'elasticsearch.internal:9200/_cat/allocation?v\u0026pretty' shards disk.indices disk.used disk.avail disk.total disk.percent host ip node 4 124.7kb 22.6mb 1.8gb 1.9gb 1 127.0.0.0 127.0.0.0 2kv5ycbzprs4bomyaeu6zkvoh4 ",
        "text": "Goal Get the available disk size of elasticsearch Assumptions You already have elasticsearch set-up and running. Steps 1. Log in to the app container using ssh 2. Query the elastic search api Run this curl snippet assuming your elasticsearch relationship is called elasticsearch. If the name is different, change the snippet below to reflect that. curl -XGET 'elasticsearch.internal:9200/_cat/allocation?v\u0026pretty' shards disk.indices disk.used disk.avail disk.total disk.percent host ip node 4 124.7kb 22.6mb 1.8gb 1.9gb 1 127.0.0.0 127.0.0.0 2kv5ycbzprs4bomyaeu6zkvoh4 ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-check-the-disk-usage-for-elastic-search/395",
        "relurl": "/t/how-do-i-check-the-disk-usage-for-elastic-search/395"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "37dfd98155449c271f605e29309ca66291acff16",
        "title": "How to reset a project's code and data",
        "description": "Goal To reset a project entirely to a new code base. Assumptions You will need: A working http://Platform.sh project Administrator access to it Git and SSH configured and accessible Problems Normally the easiest way to “start fresh” with a project on http://Platform.sh is to create a new project and optionally delete the old one. However, there are some cases where that may not be viable: When using a trial project, deleting the project created with the trial will also delete the trial, even if the trial period has not expired. You may want to retain non-code configuration on the project (users, access control, etc.) but still reset all code and data on the project. These steps will “reset” a project to a fresh state. Warning: By design, these instructions will result in code and data loss. Do not proceed unless code and data loss is really what you want to do. Steps 1. Remove all services and application data Edit the services.yaml file and remove all lines. Just leave it as an empty file. Then update your .platform.app.yaml file. Remove the relationships block, all mount declarations and change the name key to any new value. Optionally you may remove the hooks section to make the deploy a bit faster but that is not necessary. If you have multiple .platform.app.yaml files, do the same for all of them. Commit these changes and git push to the master branch. The environment will rebuild with no services (and thus deleting all of the previously-specified services) and with a blank application container. Additionally, delete any non-master branches in Git as they will not work correctly after this process is complete. 2. Select a new code base to deploy If you have an existing Git repository you wish to deploy, ensure it has the https://docs.platform.sh/gettingstarted/own-code/project-configuration.html . If you would like to use a one of http://Platform.sh’s pre-made templates, git clone the appropriate repository from GitHub to your local computer. Now reset the Git history in the repository. cd into the directory you just cloned, then run: $ rm -rf .git git init git add . git commit -m \"Add Platform.sh template.\" 3. Force push to the project Add a Git remote for the project to the local Git repository you just created. You can find the Git URL to use in the Web Console by going to the master environment and selecting the “Git” dropdown. Just copy the remote URL itself, not the full command. Add that remote to the project (using the Git URL you just copied): git remote add platform abc123@git.eu-3.platform.sh:abc123git Then “Force push” to the master branch of the project: git push --force -u platform master That will completely overwrite the master branch on http://Platform.sh with the code in your new repository, and set your local branch to track the project’s master branch so you don’t need to specify it in the future. The new code will build and deploy a new master environment with the configuration in Git. Conclusion The project will now have a fresh Git history with new code, and completely empty services based on what was defined in the new code base. Any users or environment variables that had been defined previously, however, will remain intact.",
        "text": "Goal To reset a project entirely to a new code base. Assumptions You will need: A working http://Platform.sh project Administrator access to it Git and SSH configured and accessible Problems Normally the easiest way to “start fresh” with a project on http://Platform.sh is to create a new project and optionally delete the old one. However, there are some cases where that may not be viable: When using a trial project, deleting the project created with the trial will also delete the trial, even if the trial period has not expired. You may want to retain non-code configuration on the project (users, access control, etc.) but still reset all code and data on the project. These steps will “reset” a project to a fresh state. Warning: By design, these instructions will result in code and data loss. Do not proceed unless code and data loss is really what you want to do. Steps 1. Remove all services and application data Edit the services.yaml file and remove all lines. Just leave it as an empty file. Then update your .platform.app.yaml file. Remove the relationships block, all mount declarations and change the name key to any new value. Optionally you may remove the hooks section to make the deploy a bit faster but that is not necessary. If you have multiple .platform.app.yaml files, do the same for all of them. Commit these changes and git push to the master branch. The environment will rebuild with no services (and thus deleting all of the previously-specified services) and with a blank application container. Additionally, delete any non-master branches in Git as they will not work correctly after this process is complete. 2. Select a new code base to deploy If you have an existing Git repository you wish to deploy, ensure it has the https://docs.platform.sh/gettingstarted/own-code/project-configuration.html . If you would like to use a one of http://Platform.sh’s pre-made templates, git clone the appropriate repository from GitHub to your local computer. Now reset the Git history in the repository. cd into the directory you just cloned, then run: $ rm -rf .git git init git add . git commit -m \"Add Platform.sh template.\" 3. Force push to the project Add a Git remote for the project to the local Git repository you just created. You can find the Git URL to use in the Web Console by going to the master environment and selecting the “Git” dropdown. Just copy the remote URL itself, not the full command. Add that remote to the project (using the Git URL you just copied): git remote add platform abc123@git.eu-3.platform.sh:abc123git Then “Force push” to the master branch of the project: git push --force -u platform master That will completely overwrite the master branch on http://Platform.sh with the code in your new repository, and set your local branch to track the project’s master branch so you don’t need to specify it in the future. The new code will build and deploy a new master environment with the configuration in Git. Conclusion The project will now have a fresh Git history with new code, and completely empty services based on what was defined in the new code base. Any users or environment variables that had been defined previously, however, will remain intact.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-reset-a-projects-code-and-data/382",
        "relurl": "/t/how-to-reset-a-projects-code-and-data/382"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "0e5d13e6ac4b9bd49722778f95fe3682c71e5c1e",
        "title": "How to deploy a Vue.js Single Page Application (SPA) with a Golang API on Platform.sh",
        "description": "Goal Deploy a Vue.js Single Page Application (SPA) with a Golang API backend on http://Platform.sh Assumptions an empty http://Platform.sh project with Medium plan Node.js installed locally Vue.js CLI installed locally (npm install -g @vue/cli): https://cli.vuejs.org/ Problems Two strategies are possible when building an SPA with http://Platform.sh: creating two separate projects for Vue.js frontend and Golang API backend hosting both apps within a single multi-app Platform project (requires at least a Medium plan) This How-to shows the second option. Steps (Multi-app SPA) 1. Project structure Ultimately, the project structure will look like the following (one common .platform directory, and one .platform.app.yaml file per app): vuespa/ .git/ .platform/ routes.yaml services.yaml hello_world_backend/ .platform.app.yaml hello_world.go hello_world_frontend/ .platform.app.yaml node_modules/ public/ src/ App.vue main.js babel.config.js package-lock.json package.json 2. Set Up Golang App 1. Create the Go Project Create and enter a project directory vuespa. Create a hello_world_backend directory, and then create the following hello_world.go file within that directory: // vuespa/hello_world_backend/hello_world.go package main import ( \"encoding/json\" \"fmt\" \"net/http\" psh \"github.com/platformsh/gohelper\" \"github.com/rs/cors\" ) // Greetings is a basic Greetings to user type Greetings struct { Message string `json:\"message\"` } // sayHello returns greetings to user in JSON format func sayHello(w http.ResponseWriter, r *http.Request) { greetings := Greetings{Message: \"Hello World!\"} returnedJSON, err := json.Marshal(greetings) if err != nil { http.Error(w, \"Internal server error\", http.StatusInternalServerError) } w.Header().Set(\"Content-Type\", \"application/json\") fmt.Fprintf(w, \"%s\", returnedJSON) } func main() { // Load Platform.sh environment variables in order to retrieve correct port p, err := psh.NewPlatformInfo() if err != nil { panic(\"Not in a Platform.sh Environment.\") } // Initialize HTTP server mux := http.NewServeMux() // Set up the /say-hello API endpoint mux.HandleFunc(\"/say-hello\", sayHello) // Enable CORS and Whitelist the frontend domain in order to comply with // CORS Allowed Origins policy handler := cors.New(cors.Options{ AllowedOrigins: []string{\"https://your-platformsh-frontend-url\"}, }).Handler(mux) // Launch HTTP server with custom port retrieved in Platform.sh env var http.ListenAndServe(\":\"+p.Port, handler) } The above exposes a say-hello REST endpoint returning a greetings message to user. CORS need to be properly handled as both frontend and backend apps are not hosted at the same URLs. Ideally the frontend URL whitelisted in CORS (https://your-platformsh-frontend-url) should be retrieved through an environment variable. For more information about CORS please visit: https://wikipedia.org/wiki/Cross-origin_resource_sharing The frontend url will not be made available until the project is pushed to http://Platform.sh for the first time, so it may be necessary to push the code once to establish those routes, and then commit the url once they are defined. In general, they will take the forms https://https://master-7rqtwti- . .platformsh.site for the frontend url and https://https://backend.master-7rqtwti- . .platformsh.site for the backend url. 2. Set up the http://Platform.sh configuration Create a .platform.app.yaml within the directory: # vuespa/hello_world_backend/.platform.app.yaml name: go-backend # Use the Golang 1.12 image type: golang:1.12 # A Medium plan is necessary for multi-app size: M hooks: # Get dependencies and build Go app build: | go get ./... go build -o bin/app web: upstream: socket_family: tcp protocol: http # Launch the Go server commands: start: ./bin/app locations: /: allow: false passthru: true disk: 1024 3. Set Up Vue.js App 1. Initialize the Vue.js project Create the Vue.js base project with the Vue CLI, selecting the default install option when prompted: $ vue create hello_world_frontend cd into hello_world_frontend and install the axios dependency: $ npm install axios 2. Update The Vue.js Project Remove everything inside the src directory except the main.js file and add the following App.vue file: Retrieved the following greetings message from API Go backend: {{ msg }} \r\n\r\n Error while getting message from Go API backend: {{ msgError }} \r\n\r\n ",
        "text": "Goal Deploy a Vue.js Single Page Application (SPA) with a Golang API backend on http://Platform.sh Assumptions an empty http://Platform.sh project with Medium plan Node.js installed locally Vue.js CLI installed locally (npm install -g @vue/cli): https://cli.vuejs.org/ Problems Two strategies are possible when building an SPA with http://Platform.sh: creating two separate projects for Vue.js frontend and Golang API backend hosting both apps within a single multi-app Platform project (requires at least a Medium plan) This How-to shows the second option. Steps (Multi-app SPA) 1. Project structure Ultimately, the project structure will look like the following (one common .platform directory, and one .platform.app.yaml file per app): vuespa/ .git/ .platform/ routes.yaml services.yaml hello_world_backend/ .platform.app.yaml hello_world.go hello_world_frontend/ .platform.app.yaml node_modules/ public/ src/ App.vue main.js babel.config.js package-lock.json package.json 2. Set Up Golang App 1. Create the Go Project Create and enter a project directory vuespa. Create a hello_world_backend directory, and then create the following hello_world.go file within that directory: // vuespa/hello_world_backend/hello_world.go package main import ( \"encoding/json\" \"fmt\" \"net/http\" psh \"github.com/platformsh/gohelper\" \"github.com/rs/cors\" ) // Greetings is a basic Greetings to user type Greetings struct { Message string `json:\"message\"` } // sayHello returns greetings to user in JSON format func sayHello(w http.ResponseWriter, r *http.Request) { greetings := Greetings{Message: \"Hello World!\"} returnedJSON, err := json.Marshal(greetings) if err != nil { http.Error(w, \"Internal server error\", http.StatusInternalServerError) } w.Header().Set(\"Content-Type\", \"application/json\") fmt.Fprintf(w, \"%s\", returnedJSON) } func main() { // Load Platform.sh environment variables in order to retrieve correct port p, err := psh.NewPlatformInfo() if err != nil { panic(\"Not in a Platform.sh Environment.\") } // Initialize HTTP server mux := http.NewServeMux() // Set up the /say-hello API endpoint mux.HandleFunc(\"/say-hello\", sayHello) // Enable CORS and Whitelist the frontend domain in order to comply with // CORS Allowed Origins policy handler := cors.New(cors.Options{ AllowedOrigins: []string{\"https://your-platformsh-frontend-url\"}, }).Handler(mux) // Launch HTTP server with custom port retrieved in Platform.sh env var http.ListenAndServe(\":\"+p.Port, handler) } The above exposes a say-hello REST endpoint returning a greetings message to user. CORS need to be properly handled as both frontend and backend apps are not hosted at the same URLs. Ideally the frontend URL whitelisted in CORS (https://your-platformsh-frontend-url) should be retrieved through an environment variable. For more information about CORS please visit: https://wikipedia.org/wiki/Cross-origin_resource_sharing The frontend url will not be made available until the project is pushed to http://Platform.sh for the first time, so it may be necessary to push the code once to establish those routes, and then commit the url once they are defined. In general, they will take the forms https://https://master-7rqtwti- . .platformsh.site for the frontend url and https://https://backend.master-7rqtwti- . .platformsh.site for the backend url. 2. Set up the http://Platform.sh configuration Create a .platform.app.yaml within the directory: # vuespa/hello_world_backend/.platform.app.yaml name: go-backend # Use the Golang 1.12 image type: golang:1.12 # A Medium plan is necessary for multi-app size: M hooks: # Get dependencies and build Go app build: | go get ./... go build -o bin/app web: upstream: socket_family: tcp protocol: http # Launch the Go server commands: start: ./bin/app locations: /: allow: false passthru: true disk: 1024 3. Set Up Vue.js App 1. Initialize the Vue.js project Create the Vue.js base project with the Vue CLI, selecting the default install option when prompted: $ vue create hello_world_frontend cd into hello_world_frontend and install the axios dependency: $ npm install axios 2. Update The Vue.js Project Remove everything inside the src directory except the main.js file and add the following App.vue file: Retrieved the following greetings message from API Go backend: {{ msg }} \r\n\r\n Error while getting message from Go API backend: {{ msgError }} \r\n\r\n ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-deploy-a-vue-js-single-page-application-spa-with-a-golang-api-on-platform-sh/182",
        "relurl": "/t/how-to-deploy-a-vue-js-single-page-application-spa-with-a-golang-api-on-platform-sh/182"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "cae4fea0e0859ea12fcfccb106b8ad7d89d8bdc9",
        "title": "How to transfer files between two different projects",
        "description": "Transfer files between two different projects with SSH or SCP Goal The goal is to transfer files between two separate projects. Assumptions You will need two projects set up on http://Platform.sh: One project (undntpvafhdn4 in this guide) that acts as the data source. One project (xksjd6v6od7iq) that is the data sink (where we want to copy data to). Problems Communications between projects relies on SSH, so managing the keys needs to be automated. Steps 1. Preparing the data sink On the data sink, add .ssh as a writeable mount (this is to allow SSH to write into known_hosts). Your .platform.app.yaml has to contain this block: mounts: \"/.ssh\": source: local source_path: .ssh In the build hook, https://community.platform.sh/t/how-to-use-the-platform-sh-cli-from-the-web-container/126 . This is used to dynamically retrieve the data source’s SSH connection string. If the remote will not change, you can also hardcode the path and avoid this step. hooks: build: | echo \"### INSTALLING Platform.sh CLI ... ###\" curl -sS https://platform.sh/cli/installer | php 2. Transferring files Connect to the data sink project using the -A option for SSH (this allows SSH key forwarding and connections to the data source project from the data sink project): ssh -A \"$(platform ssh -p xksjd6v6od7iq --pipe)\" ___ _ _ __ _ | _ \\ |__ _| |_ / _|___ _ _ _ __ __| |_ | _/ / _` | _| _/ _ \\ '_| ' \\ _(_- ssh-keygen -f id_rsa_transfer Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in id_rsa_transfer. Your public key has been saved in id_rsa_transfer.pub. The key fingerprint is: SHA256: The key's randomart image is: Add the generated public key into .ssh/authorized_keys in the data source project and commit the change: cat id_rsa_transfer.pub /.ssh/authorized_keys cd git commit -m \"Add SSH key\" git push Add the key in the data sink project: cat id_rsa_transfer /.ssh/id_rsa cat id_rsa_transfer.pub /.ssh/id_rsa.pub cd Edit the build hook to set the correct permissions on the keys: hooks: build: | chmod 0600 .ssh/id_rsa chmod 0600 .ssh/id_rsa.pub Commit the changes: git commit -m \"Add SSH key\" git push On the data source project, add a separate http://Platform.sh user in the Console and add the newly generated public key in the new account. Provide the least privileges possible (Viewer), for security reasons. Now, connect in the data sink project without the -A switch: ssh xksjd6v6od7iq-master-7rqtwti--app@ssh.eu-3.platform.sh ___ _ _ __ _ | _ \\ |__ _| |_ / _|___ _ _ _ __ __| |_ | _/ / _` | _| _/ _ \\ '_| ' \\ _(_- ",
        "text": "Transfer files between two different projects with SSH or SCP Goal The goal is to transfer files between two separate projects. Assumptions You will need two projects set up on http://Platform.sh: One project (undntpvafhdn4 in this guide) that acts as the data source. One project (xksjd6v6od7iq) that is the data sink (where we want to copy data to). Problems Communications between projects relies on SSH, so managing the keys needs to be automated. Steps 1. Preparing the data sink On the data sink, add .ssh as a writeable mount (this is to allow SSH to write into known_hosts). Your .platform.app.yaml has to contain this block: mounts: \"/.ssh\": source: local source_path: .ssh In the build hook, https://community.platform.sh/t/how-to-use-the-platform-sh-cli-from-the-web-container/126 . This is used to dynamically retrieve the data source’s SSH connection string. If the remote will not change, you can also hardcode the path and avoid this step. hooks: build: | echo \"### INSTALLING Platform.sh CLI ... ###\" curl -sS https://platform.sh/cli/installer | php 2. Transferring files Connect to the data sink project using the -A option for SSH (this allows SSH key forwarding and connections to the data source project from the data sink project): ssh -A \"$(platform ssh -p xksjd6v6od7iq --pipe)\" ___ _ _ __ _ | _ \\ |__ _| |_ / _|___ _ _ _ __ __| |_ | _/ / _` | _| _/ _ \\ '_| ' \\ _(_- ssh-keygen -f id_rsa_transfer Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in id_rsa_transfer. Your public key has been saved in id_rsa_transfer.pub. The key fingerprint is: SHA256: The key's randomart image is: Add the generated public key into .ssh/authorized_keys in the data source project and commit the change: cat id_rsa_transfer.pub /.ssh/authorized_keys cd git commit -m \"Add SSH key\" git push Add the key in the data sink project: cat id_rsa_transfer /.ssh/id_rsa cat id_rsa_transfer.pub /.ssh/id_rsa.pub cd Edit the build hook to set the correct permissions on the keys: hooks: build: | chmod 0600 .ssh/id_rsa chmod 0600 .ssh/id_rsa.pub Commit the changes: git commit -m \"Add SSH key\" git push On the data source project, add a separate http://Platform.sh user in the Console and add the newly generated public key in the new account. Provide the least privileges possible (Viewer), for security reasons. Now, connect in the data sink project without the -A switch: ssh xksjd6v6od7iq-master-7rqtwti--app@ssh.eu-3.platform.sh ___ _ _ __ _ | _ \\ |__ _| |_ / _|___ _ _ _ __ __| |_ | _/ / _` | _| _/ _ \\ '_| ' \\ _(_- ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-transfer-files-between-two-different-projects/373",
        "relurl": "/t/how-to-transfer-files-between-two-different-projects/373"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "c1cf7d083669e62203e7081e3e140e5cab538941",
        "title": "How to export database table data to a CSV file with platform-cli",
        "description": "Goal You want to export a database table (or a part of it) to a CSV file. Assumptions You will need: Platform CLI Platform SQL The platform CLI tool allows you to connect to your database using the command platform sql You can also specify which query you want to run. This will show you all the tables in your database. platform sql \"SHOW TABLES\" Exporting data With this, piping it to a file then becomes trivial. Just add --raw to your command to make the output machine readable. If there is more than one column, it will be separated by a TAB character. platform sql \"SHOW TABLES\" --raw all_tables.csv To export data from the table items, in the database main simply do: platform sql \"SELECT id, title FROM items LIMIT 10\" --raw --schema=main tab_separated_output.csv The resulting file can be opened with any spreadsheet software (libreoffice, openoffice, MS Office). Simply select TAB as a column separator. Warning Note that these queries will run on your database server. If the tables are large, exporting them can take a while. More information For more information on what is possible with platform sql try platform sql --help or checkout the platform cli git repository https://github.com/platformsh/platformsh-cli",
        "text": "Goal You want to export a database table (or a part of it) to a CSV file. Assumptions You will need: Platform CLI Platform SQL The platform CLI tool allows you to connect to your database using the command platform sql You can also specify which query you want to run. This will show you all the tables in your database. platform sql \"SHOW TABLES\" Exporting data With this, piping it to a file then becomes trivial. Just add --raw to your command to make the output machine readable. If there is more than one column, it will be separated by a TAB character. platform sql \"SHOW TABLES\" --raw all_tables.csv To export data from the table items, in the database main simply do: platform sql \"SELECT id, title FROM items LIMIT 10\" --raw --schema=main tab_separated_output.csv The resulting file can be opened with any spreadsheet software (libreoffice, openoffice, MS Office). Simply select TAB as a column separator. Warning Note that these queries will run on your database server. If the tables are large, exporting them can take a while. More information For more information on what is possible with platform sql try platform sql --help or checkout the platform cli git repository https://github.com/platformsh/platformsh-cli",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-export-database-table-data-to-a-csv-file-with-platform-cli/370",
        "relurl": "/t/how-to-export-database-table-data-to-a-csv-file-with-platform-cli/370"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "bf74fa534d67c25445dd96e9866e498e6529abd0",
        "title": "How to serve a static HTML page on Platform.sh",
        "description": "Goal To serve a static HTML page on http://Platform.sh. Assumptions You will need: An empty http://Platform.sh project SSH key configured to the http://Platform.sh account Problems In addition to your HTML file, http://Platform.sh needs three YAML files to configure an application in a project. These files define the routing, configure the web server, and set minimal defaults for everything else. Steps 1. Set git remote to http://Platform.sh project $ git init $ platform project:set-remote 2. Create your HTML file: ./web/index.html \r\n\r\nHello World\r\n\r\n 3. Define routes ./.platform/routes.yaml https://{default}/: type: upstream upstream: htmlhowto:http 4. Add empty services ./.platform/services.yaml # empty 5. Add .platform.app.yaml ./.platform.app.yaml # The name of this app. Must be unique within a project. name: htmlhowto # Any type will work. There is no \"plain HTML\" type. type: \"python:3.7\" # There is no need for a writable file mount, so set it to the smallest possible size. disk: 256 # Configure the web server to serve our static site. web: commands: # Run a no-op process that uses no CPU resources, since this is a static site. start: sleep infinity locations: # This tells Nginx to serve from the base directory \"/\": root: \"web\" index: - \"index.html\" This is a stripped down application configuration for serving a static file on http://Platform.sh. You can find more information about additional commands that can be included in .platform.app.yaml in the . 6. Add, commit, and push these files to your empty http://Platform.sh project git add . git commit -m \"Adding configuration to serve a static html file\" git push platform master 7. Test by visiting the URL of your environment. Conclusion By adding a route to routes.yaml, and by adding the proper web server configuration to .platform.app.yaml, a project is able to serve static HTML files.",
        "text": "Goal To serve a static HTML page on http://Platform.sh. Assumptions You will need: An empty http://Platform.sh project SSH key configured to the http://Platform.sh account Problems In addition to your HTML file, http://Platform.sh needs three YAML files to configure an application in a project. These files define the routing, configure the web server, and set minimal defaults for everything else. Steps 1. Set git remote to http://Platform.sh project $ git init $ platform project:set-remote 2. Create your HTML file: ./web/index.html \r\n\r\nHello World\r\n\r\n 3. Define routes ./.platform/routes.yaml https://{default}/: type: upstream upstream: htmlhowto:http 4. Add empty services ./.platform/services.yaml # empty 5. Add .platform.app.yaml ./.platform.app.yaml # The name of this app. Must be unique within a project. name: htmlhowto # Any type will work. There is no \"plain HTML\" type. type: \"python:3.7\" # There is no need for a writable file mount, so set it to the smallest possible size. disk: 256 # Configure the web server to serve our static site. web: commands: # Run a no-op process that uses no CPU resources, since this is a static site. start: sleep infinity locations: # This tells Nginx to serve from the base directory \"/\": root: \"web\" index: - \"index.html\" This is a stripped down application configuration for serving a static file on http://Platform.sh. You can find more information about additional commands that can be included in .platform.app.yaml in the . 6. Add, commit, and push these files to your empty http://Platform.sh project git add . git commit -m \"Adding configuration to serve a static html file\" git push platform master 7. Test by visiting the URL of your environment. Conclusion By adding a route to routes.yaml, and by adding the proper web server configuration to .platform.app.yaml, a project is able to serve static HTML files.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-serve-a-static-html-page-on-platform-sh/51",
        "relurl": "/t/how-to-serve-a-static-html-page-on-platform-sh/51"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "db73646ee7ba37254005122c1b7feaf44bc42317",
        "title": "How to automatically update Composer dependencies with Source Operations",
        "description": "Goal This guide details how to automatically update Composer dependencies on a specific environment, so that you can test the changes before deploying to production. Assumptions You will need: A Composer based http://Platform.sh application (you can start with one of our many templates for https://github.com/platformsh/template-drupal8 , https://github.com/platformsh/template-wordpress, https://github.com/platformsh/template-magento2ce, https://github.com/platformsh/template-symfony4 , https://github.com/platformsh/template-laravel) An SSH key loaded in an SSH agent and configured in the https://accounts.platform.sh/user The http://Platform.sh CLI https://docs.platform.sh/gettingstarted/cli.html Problems Keeping your code base and its dependencies up to date is critical for so many reasons, and it is always possible to forget and miss a security upgrade. Even though http://Platform.sh makes it easy to update dependencies, thanks to its support for all the famous package managers (Composer, npm, gem…), it is better to automate that process, so that this issue never happens. That is the goal of this how-to guide. Steps 1. Install the CLI on the application container Create a machine user that you invite to your project. Get an API token from this machine user account (read the https://docs.platform.sh/gettingstarted/cli/api-tokens.html ) and run the following command: cd my-platformsh-project/ platform variable:create -e master --level environment --name env:PLATFORMSH_CLI_TOKEN --sensitive true --value 'your API token' Your local CLI will automatically detect the current project and add the env:PLATFORMSH_CLI_TOKEN environment variable to your project. Then install the CLI on your application container via a new build hook defined in .platform.app.yaml: hooks: build: | curl -sS https://platform.sh/cli/installer | php 2. Enable Source Operations Create a dedicated update-dependencies branch where we will automatically run and test Composer updates. platform branch update-dependencies -e master On that newly created branch, add the following lines in the .platform.app.yaml: source: operations: update: command: | composer update git add composer.lock git commit -m \"Update Composer dependencies.\" This configuration defines an arbitrary update source operation which will run the composer update command and commit the changes to the composer.lock file, before redeploying the environment on which it has been triggered. 3. Automatically trigger the update source operation Define a new cron entry to automatically trigger the update source operation in .platform.app.yaml: crons: update: # Trigger the update source operation every day at 00:00. spec: '0 0 * * *' cmd: | if [ \"$PLATFORM_BRANCH\" = update-dependencies ]; then platform environment:sync code data --no-wait --yes platform source-operation:run update --no-wait --yes fi Every day, this cron will synchronize the update-dependencies environment with its parent master, and trigger the update source operation on it. 4. Deploy the changes Use Git to deploy the changes: git add .platform.app.yaml git commit -m \"Enable automated Composer updates on the update-dependencies branch via cron.\" git push platform update-dependencies Conclusion This is how easy it is to automate the update of Composer dependencies (or any other package manager dependencies if you are not using PHP) on http://Platform.sh. The next step should be to enable a http://Platform.sh notification alert (Email, Slack…) so that you know when the environment has been updated and you can test the changes before deploying those to production.",
        "text": "Goal This guide details how to automatically update Composer dependencies on a specific environment, so that you can test the changes before deploying to production. Assumptions You will need: A Composer based http://Platform.sh application (you can start with one of our many templates for https://github.com/platformsh/template-drupal8 , https://github.com/platformsh/template-wordpress, https://github.com/platformsh/template-magento2ce, https://github.com/platformsh/template-symfony4 , https://github.com/platformsh/template-laravel) An SSH key loaded in an SSH agent and configured in the https://accounts.platform.sh/user The http://Platform.sh CLI https://docs.platform.sh/gettingstarted/cli.html Problems Keeping your code base and its dependencies up to date is critical for so many reasons, and it is always possible to forget and miss a security upgrade. Even though http://Platform.sh makes it easy to update dependencies, thanks to its support for all the famous package managers (Composer, npm, gem…), it is better to automate that process, so that this issue never happens. That is the goal of this how-to guide. Steps 1. Install the CLI on the application container Create a machine user that you invite to your project. Get an API token from this machine user account (read the https://docs.platform.sh/gettingstarted/cli/api-tokens.html ) and run the following command: cd my-platformsh-project/ platform variable:create -e master --level environment --name env:PLATFORMSH_CLI_TOKEN --sensitive true --value 'your API token' Your local CLI will automatically detect the current project and add the env:PLATFORMSH_CLI_TOKEN environment variable to your project. Then install the CLI on your application container via a new build hook defined in .platform.app.yaml: hooks: build: | curl -sS https://platform.sh/cli/installer | php 2. Enable Source Operations Create a dedicated update-dependencies branch where we will automatically run and test Composer updates. platform branch update-dependencies -e master On that newly created branch, add the following lines in the .platform.app.yaml: source: operations: update: command: | composer update git add composer.lock git commit -m \"Update Composer dependencies.\" This configuration defines an arbitrary update source operation which will run the composer update command and commit the changes to the composer.lock file, before redeploying the environment on which it has been triggered. 3. Automatically trigger the update source operation Define a new cron entry to automatically trigger the update source operation in .platform.app.yaml: crons: update: # Trigger the update source operation every day at 00:00. spec: '0 0 * * *' cmd: | if [ \"$PLATFORM_BRANCH\" = update-dependencies ]; then platform environment:sync code data --no-wait --yes platform source-operation:run update --no-wait --yes fi Every day, this cron will synchronize the update-dependencies environment with its parent master, and trigger the update source operation on it. 4. Deploy the changes Use Git to deploy the changes: git add .platform.app.yaml git commit -m \"Enable automated Composer updates on the update-dependencies branch via cron.\" git push platform update-dependencies Conclusion This is how easy it is to automate the update of Composer dependencies (or any other package manager dependencies if you are not using PHP) on http://Platform.sh. The next step should be to enable a http://Platform.sh notification alert (Email, Slack…) so that you know when the environment has been updated and you can test the changes before deploying those to production.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-automatically-update-composer-dependencies-with-source-operations/337",
        "relurl": "/t/how-to-automatically-update-composer-dependencies-with-source-operations/337"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e4a83949ff3e1b3bad0fd690e845962bc7e16d5e",
        "title": "How to generate PDFs using Puppeteer and Headless Chrome",
        "description": "Goal To use https://github.com/GoogleChrome/puppeteer and https://developers.google.com/web/updates/2017/04/headless-chrome to create an https://expressjs.com/ application that generates PDFs of web sites on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account The https://docs.platform.sh/gettingstarted/cli.html installed locally Node.js and npm installed locally Problems Using headless Chrome to generate PDFs of a website requires properly connecting the chrome-headless service container to the Node library https://github.com/GoogleChrome/puppeteer by passing its credentials using the Node.js libary. Steps The project will ultimately have the following structure: . ├── .platform │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml ├── index.js ├── package.json ├── package-lock.json └── pdfs.js 1. Initialize the project Create an empty project on http://Platform.sh using the CLI. $ platform create Create a new project directory for the application on your local machine called pdfs and cd into it. Initialize the directory as a Git repository and set its remote to the newly created http://Platform.sh project using the outputted project ID. $ git init $ platform project:set-remote 2. Create the http://Platform.sh configuration files .platform/services.yaml Define the chrome-headless container using the supported version outlined in the https://docs.platform.sh/configuration/services/headless-chrome.html . headless: type: chrome-headless:73 .platform.app.yaml Configure the application nodejs: name: nodejs type: nodejs:10 relationships: headless: \"headless:http\" crons: cleanup: spec: '*/30 * * * *' cmd: rm pdfs/* web: commands: start: \"nodejs index.js\" mounts: \"/pdfs\": \"shared:files/pdfs\" disk: 512 The configuration uses nodejs 10, since it is required to use the Config Reader library with Puppeteer. It defines the mount pdfs that will act as a writable directory to save the PDFs the application generates. In order to prevent pdfs/ from filling up as people use it, a cron job is also defined that removes its contents every 30 minutes. .platform/routes.yaml Lastly, set up a basic routes configuration file, using the name of the application nodejs \"https://{default}/\": id: main type: upstream upstream: \"nodejs:http\" \"https://www.{default}/\": type: redirect to: \"https://{default}/\" 3. Write the pdfs.js file Create a file in the project directory called pdfs.js with the following contents: const puppeteer = require('puppeteer'); const platformsh = require('platformsh-config'); var exports = module.exports = {}; // Create an async function exports.makePDF = async function (url, pdfID) { try { // Connect to chrome-headless using pre-formatted puppeteer credentials let config = platformsh.config(); const formattedURL = config.formattedCredentials(\"headless\", \"puppeteer\"); const browser = await puppeteer.connect({browserURL: formattedURL}); // Open a new page to the given url and create the PDF const page = await browser.newPage(); await page.goto(url, {waitUntil: 'networkidle2'}); await page.pdf({ path: `pdfs/${pdfID}.pdf`, printBackground: true }); await browser.close(); return browser } catch (e) { return Promise.reject(e); } }; It defines an async function called makePDF as a module export. The Node.js Config Reader retrieves the library’s formatted credentials for Puppeteer to create the formattedURL string. path defines the saved location of the PDF, while printBackground allows background images on the page to be included in the generated PDF. Additional parameters for page.pdf() can be found in the . 4. Define index.js Create the file index.js that defines the ExpressJS application app: const fs = require('fs'); const uuidv4 = require('uuid/v4') const platformsh = require('platformsh-config'); const express = require('express'); // Require pdf file and its function var pdfs = require(\"./pdfs.js\"); // Build the application var app = express(); // Define the index route app.get('/', (req, res) = { res.writeHead(200, {\"Content-Type\": \"text/html\"}); res.write(` \r\n\r\nHeadless Chrome on Platform.sh\r\n\r\n Generate a PDF of a page\r\n\r\n Click submit to generate a PDF of the https://platform.sh/ , or paste in another URL. `); res.end(``); }) // Define PDF result route app.get('/result', async function(req, res){ // Create a randomly generated ID number for the current PDF var pdfID = uuidv4(); // Generate the PDF await pdfs.makePDF(req.query['pdfURL'], pdfID) // Define and download the file const file = `pdfs/${pdfID}.pdf`; res.download(file); }); // Create config object to get Platform.sh PORT credentials let config = platformsh.config(); // Start the server. app.listen(config.port, function() { console.log(`Listening on port ${config.port}`) }); In addition to the home route, index.js defines a /results path that calls makePDF() and passes a randomly generated ID that will become part of the name for the generated PDF file. 5. Define the application’s dependencies Include the application’s dependencies in package.json: { \"name\": \"chrome_headless\", \"version\": \"1.0.0\", \"description\": \"A simple example for taking screenshots with Puppeteer and headless Chrome on Platform.sh\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" \u0026\u0026 exit 1\" }, \"author\": \"Chad Carlson\", \"license\": \"MIT\", \"dependencies\": { \"platformsh-config\": \"^2.0.0\", \"puppeteer\": \"^1.14.0\", \"express\": \"^4.16.4\", \"uuid\": \"^3.3.2\" } } Then create the package-lock.json file by running $ npm install 6. Push to http://Platform.sh Commit the changes and push master to http://Platform.sh $ git add . $ git commit -m \"Create PDF generator application.\" $ git push platform master 7. Verify Use the command platform url when the build process has completed to visit the site. Click submit to generate a PDF of the http://Platform.sh website, or copy in another url to test the application. Conclusion Using ExpressJS, Puppeteer, and http://Platform.sh headless Chrome as a service, a simple application can be made that generates PDFs of an inputted url.",
        "text": "Goal To use https://github.com/GoogleChrome/puppeteer and https://developers.google.com/web/updates/2017/04/headless-chrome to create an https://expressjs.com/ application that generates PDFs of web sites on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account The https://docs.platform.sh/gettingstarted/cli.html installed locally Node.js and npm installed locally Problems Using headless Chrome to generate PDFs of a website requires properly connecting the chrome-headless service container to the Node library https://github.com/GoogleChrome/puppeteer by passing its credentials using the Node.js libary. Steps The project will ultimately have the following structure: . ├── .platform │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml ├── index.js ├── package.json ├── package-lock.json └── pdfs.js 1. Initialize the project Create an empty project on http://Platform.sh using the CLI. $ platform create Create a new project directory for the application on your local machine called pdfs and cd into it. Initialize the directory as a Git repository and set its remote to the newly created http://Platform.sh project using the outputted project ID. $ git init $ platform project:set-remote 2. Create the http://Platform.sh configuration files .platform/services.yaml Define the chrome-headless container using the supported version outlined in the https://docs.platform.sh/configuration/services/headless-chrome.html . headless: type: chrome-headless:73 .platform.app.yaml Configure the application nodejs: name: nodejs type: nodejs:10 relationships: headless: \"headless:http\" crons: cleanup: spec: '*/30 * * * *' cmd: rm pdfs/* web: commands: start: \"nodejs index.js\" mounts: \"/pdfs\": \"shared:files/pdfs\" disk: 512 The configuration uses nodejs 10, since it is required to use the Config Reader library with Puppeteer. It defines the mount pdfs that will act as a writable directory to save the PDFs the application generates. In order to prevent pdfs/ from filling up as people use it, a cron job is also defined that removes its contents every 30 minutes. .platform/routes.yaml Lastly, set up a basic routes configuration file, using the name of the application nodejs \"https://{default}/\": id: main type: upstream upstream: \"nodejs:http\" \"https://www.{default}/\": type: redirect to: \"https://{default}/\" 3. Write the pdfs.js file Create a file in the project directory called pdfs.js with the following contents: const puppeteer = require('puppeteer'); const platformsh = require('platformsh-config'); var exports = module.exports = {}; // Create an async function exports.makePDF = async function (url, pdfID) { try { // Connect to chrome-headless using pre-formatted puppeteer credentials let config = platformsh.config(); const formattedURL = config.formattedCredentials(\"headless\", \"puppeteer\"); const browser = await puppeteer.connect({browserURL: formattedURL}); // Open a new page to the given url and create the PDF const page = await browser.newPage(); await page.goto(url, {waitUntil: 'networkidle2'}); await page.pdf({ path: `pdfs/${pdfID}.pdf`, printBackground: true }); await browser.close(); return browser } catch (e) { return Promise.reject(e); } }; It defines an async function called makePDF as a module export. The Node.js Config Reader retrieves the library’s formatted credentials for Puppeteer to create the formattedURL string. path defines the saved location of the PDF, while printBackground allows background images on the page to be included in the generated PDF. Additional parameters for page.pdf() can be found in the . 4. Define index.js Create the file index.js that defines the ExpressJS application app: const fs = require('fs'); const uuidv4 = require('uuid/v4') const platformsh = require('platformsh-config'); const express = require('express'); // Require pdf file and its function var pdfs = require(\"./pdfs.js\"); // Build the application var app = express(); // Define the index route app.get('/', (req, res) = { res.writeHead(200, {\"Content-Type\": \"text/html\"}); res.write(` \r\n\r\nHeadless Chrome on Platform.sh\r\n\r\n Generate a PDF of a page\r\n\r\n Click submit to generate a PDF of the https://platform.sh/ , or paste in another URL. `); res.end(``); }) // Define PDF result route app.get('/result', async function(req, res){ // Create a randomly generated ID number for the current PDF var pdfID = uuidv4(); // Generate the PDF await pdfs.makePDF(req.query['pdfURL'], pdfID) // Define and download the file const file = `pdfs/${pdfID}.pdf`; res.download(file); }); // Create config object to get Platform.sh PORT credentials let config = platformsh.config(); // Start the server. app.listen(config.port, function() { console.log(`Listening on port ${config.port}`) }); In addition to the home route, index.js defines a /results path that calls makePDF() and passes a randomly generated ID that will become part of the name for the generated PDF file. 5. Define the application’s dependencies Include the application’s dependencies in package.json: { \"name\": \"chrome_headless\", \"version\": \"1.0.0\", \"description\": \"A simple example for taking screenshots with Puppeteer and headless Chrome on Platform.sh\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" \u0026\u0026 exit 1\" }, \"author\": \"Chad Carlson\", \"license\": \"MIT\", \"dependencies\": { \"platformsh-config\": \"^2.0.0\", \"puppeteer\": \"^1.14.0\", \"express\": \"^4.16.4\", \"uuid\": \"^3.3.2\" } } Then create the package-lock.json file by running $ npm install 6. Push to http://Platform.sh Commit the changes and push master to http://Platform.sh $ git add . $ git commit -m \"Create PDF generator application.\" $ git push platform master 7. Verify Use the command platform url when the build process has completed to visit the site. Click submit to generate a PDF of the http://Platform.sh website, or copy in another url to test the application. Conclusion Using ExpressJS, Puppeteer, and http://Platform.sh headless Chrome as a service, a simple application can be made that generates PDFs of an inputted url.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-generate-pdfs-using-puppeteer-and-headless-chrome/306",
        "relurl": "/t/how-to-generate-pdfs-using-puppeteer-and-headless-chrome/306"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "d49f923782a6cf2c04a948924ef990e0ecc6a1ba",
        "title": "How to set up and deploy a Ruby on Rails application on Platform.sh",
        "description": "Goal This guide shows how to deploy a toy https://rubyonrails.org/ 5 application on https://platform.sh running on Sqlite (please do not use Sqlite in production, ever). In a later post I’ll show how to deploy a robust, production-ready Rails with Postgres and Redis. Assumptions An empty https://platform.sh project - https://docs.platform.sh/gettingstarted/first-project.html An SSH key loaded in an SSH agent and configured in the https://accounts.platform.sh/user The http://Platform.sh CLI https://docs.platform.sh/gettingstarted/cli.html Some supported version of Ruby and the Rails gem installed. You can follow http://installrails.com/ to get both installed. Your best experience would be to use the same version of Ruby you will be running on the server (for example 2.6). Steps 1. Create a new Rails application using the CLI $ rails new rails-platformsh $ cd rails-platformsh 2. Configure the application for https://platform.sh Set the platform Git remote. $ platform project:set-remote Update the Ruby version directive in the Gemfile not to care about patch level, because locking patch-level is evil. # rails-platformsh/Gemfile ruby '~ 2.6.0' Then update the Gemfile.lock $ bundle update 3. Add the https://platform.sh configuration files Create the .platform.app.yaml https://docs.platform.sh/configuration/app-containers.html . $ touch .platform.app.yaml Add a basic https://docs.platform.sh/languages/ruby.html to the file. # rails-platformsh/.platform.app.yaml name: \"ruby_example\" type: \"ruby:2.6\" disk: 1024 hooks: build: | bundle install rake assets:precompile deploy: | rails db:migrate mounts: \"/log\": \"shared:files/log\" \"/tmp\": \"shared:files/tmp\" \"/db\": \"shared:files/db\" web: commands: start: 'rails server -p $PORT' Create the .platform/services.yaml https://docs.platform.sh/configuration/services.html and .platform/routes.yaml https://docs.platform.sh/configuration/routes.html files. $ touch .platform/routes.yaml $ touch .platform/services.yaml Add basic routes configuration: # rails-platformsh/.platform/routes.yaml \"https://{default}\": type: upstream upstream: \"ruby_example:http\" you can leave .platform/services.yaml empty for the moment. Later if you would like a Postgres or a MySQL… this is where you will put it. 4. Deploy the application to https://platform.sh Commit everything to the repository: git add --all git commit -m \"Initial commit\" Push the working branch to the platform remote: git push platform master Conclusion The Rails gem can be used in conjunction with the Platform CLI to easily create and configure a Ruby on Rails application for deployment on https://platform.sh.",
        "text": "Goal This guide shows how to deploy a toy https://rubyonrails.org/ 5 application on https://platform.sh running on Sqlite (please do not use Sqlite in production, ever). In a later post I’ll show how to deploy a robust, production-ready Rails with Postgres and Redis. Assumptions An empty https://platform.sh project - https://docs.platform.sh/gettingstarted/first-project.html An SSH key loaded in an SSH agent and configured in the https://accounts.platform.sh/user The http://Platform.sh CLI https://docs.platform.sh/gettingstarted/cli.html Some supported version of Ruby and the Rails gem installed. You can follow http://installrails.com/ to get both installed. Your best experience would be to use the same version of Ruby you will be running on the server (for example 2.6). Steps 1. Create a new Rails application using the CLI $ rails new rails-platformsh $ cd rails-platformsh 2. Configure the application for https://platform.sh Set the platform Git remote. $ platform project:set-remote Update the Ruby version directive in the Gemfile not to care about patch level, because locking patch-level is evil. # rails-platformsh/Gemfile ruby '~ 2.6.0' Then update the Gemfile.lock $ bundle update 3. Add the https://platform.sh configuration files Create the .platform.app.yaml https://docs.platform.sh/configuration/app-containers.html . $ touch .platform.app.yaml Add a basic https://docs.platform.sh/languages/ruby.html to the file. # rails-platformsh/.platform.app.yaml name: \"ruby_example\" type: \"ruby:2.6\" disk: 1024 hooks: build: | bundle install rake assets:precompile deploy: | rails db:migrate mounts: \"/log\": \"shared:files/log\" \"/tmp\": \"shared:files/tmp\" \"/db\": \"shared:files/db\" web: commands: start: 'rails server -p $PORT' Create the .platform/services.yaml https://docs.platform.sh/configuration/services.html and .platform/routes.yaml https://docs.platform.sh/configuration/routes.html files. $ touch .platform/routes.yaml $ touch .platform/services.yaml Add basic routes configuration: # rails-platformsh/.platform/routes.yaml \"https://{default}\": type: upstream upstream: \"ruby_example:http\" you can leave .platform/services.yaml empty for the moment. Later if you would like a Postgres or a MySQL… this is where you will put it. 4. Deploy the application to https://platform.sh Commit everything to the repository: git add --all git commit -m \"Initial commit\" Push the working branch to the platform remote: git push platform master Conclusion The Rails gem can be used in conjunction with the Platform CLI to easily create and configure a Ruby on Rails application for deployment on https://platform.sh.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-set-up-and-deploy-a-ruby-on-rails-application-on-platform-sh/176",
        "relurl": "/t/how-to-set-up-and-deploy-a-ruby-on-rails-application-on-platform-sh/176"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "f9e82243619cf2ce9f474c18711722530deea06b",
        "title": "How to enable commandline autocomplete support for the `platform` CLI tool",
        "description": "The https://docs.platform.sh/gettingstarted/cli.html contains a large library of commands, and trying to list them each time you can’t quite remember the names or options can fill up your screen really fast. Tab-completion will be able to help us out here. Goal We want autocomplete support (press tab to get command and option suggestions) for the platform CLI tool. Assumptions As it’s a symfony component, these commands can be programmatically extracted in a standard way. The https://github.com/bamarni/symfony-console-autocomplete tool does this for us. Instructions here for OSX, the https://github.com/bamarni/symfony-console-autocomplete Prerequisites If you don’t have bash-completion installed by default, get that first. Using brew on OSX. *nix sessions probably already have it. brew install bash-completion Add the startup line to your ~/.bash_profile like it advises. Scripts found in /usr/local/etc/bash_completion.d/ can support individual commands now. Steps Install symfony-console-autocomplete composer global require bamarni/symfony-console-autocomplete Now we can generate a dump of all symfony-console-based CLI commands, and use that dump to generate the autocomplete prompts. Auto-generate autocomplete hints for platform symfony-autocomplete platform --script-options=list /usr/local/etc/bash_completion.d/platform Restart your shell session (or at least source ~/.bash_profile) and autocomplete should be available for platform cli now. The location /usr/local/etc/bash_completion.d/ will differ depending on your OS, version, and preferred shell. . The only difference from the examples there is that we have to add --script-options=list to extract the commands for platform. Test Pressing [TAB] part-way through entering platform commands will now provide either autocompletion or a list of suggestions, for both commands and options to those commands. Examples: $ platform pr[TAB] platform project: $ platform project:[TAB] clear-build-cache curl get list variable:delete variable:set create delete info set-remote variable:get $ platform project:l[TAB] platform project:list $ platform project:list --[TAB] --columns --help --my --no-header --quiet --reverse --title --version --format --host --no --pipe --refresh --sort --verbose --yes ",
        "text": "The https://docs.platform.sh/gettingstarted/cli.html contains a large library of commands, and trying to list them each time you can’t quite remember the names or options can fill up your screen really fast. Tab-completion will be able to help us out here. Goal We want autocomplete support (press tab to get command and option suggestions) for the platform CLI tool. Assumptions As it’s a symfony component, these commands can be programmatically extracted in a standard way. The https://github.com/bamarni/symfony-console-autocomplete tool does this for us. Instructions here for OSX, the https://github.com/bamarni/symfony-console-autocomplete Prerequisites If you don’t have bash-completion installed by default, get that first. Using brew on OSX. *nix sessions probably already have it. brew install bash-completion Add the startup line to your ~/.bash_profile like it advises. Scripts found in /usr/local/etc/bash_completion.d/ can support individual commands now. Steps Install symfony-console-autocomplete composer global require bamarni/symfony-console-autocomplete Now we can generate a dump of all symfony-console-based CLI commands, and use that dump to generate the autocomplete prompts. Auto-generate autocomplete hints for platform symfony-autocomplete platform --script-options=list /usr/local/etc/bash_completion.d/platform Restart your shell session (or at least source ~/.bash_profile) and autocomplete should be available for platform cli now. The location /usr/local/etc/bash_completion.d/ will differ depending on your OS, version, and preferred shell. . The only difference from the examples there is that we have to add --script-options=list to extract the commands for platform. Test Pressing [TAB] part-way through entering platform commands will now provide either autocompletion or a list of suggestions, for both commands and options to those commands. Examples: $ platform pr[TAB] platform project: $ platform project:[TAB] clear-build-cache curl get list variable:delete variable:set create delete info set-remote variable:get $ platform project:l[TAB] platform project:list $ platform project:list --[TAB] --columns --help --my --no-header --quiet --reverse --title --version --format --host --no --pipe --refresh --sort --verbose --yes ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-enable-commandline-autocomplete-support-for-the-platform-cli-tool/312",
        "relurl": "/t/how-to-enable-commandline-autocomplete-support-for-the-platform-cli-tool/312"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "5b4e445b48f71f8bf68746295ff24a9da5a0e694",
        "title": "How to take screenshots using Puppeteer and Headless Chrome",
        "description": "Goal To use https://github.com/GoogleChrome/puppeteer and https://developers.google.com/web/updates/2017/04/headless-chrome to create an https://expressjs.com/ application that takes website screenshots on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account The https://docs.platform.sh/gettingstarted/cli.html installed locally Node.js and npm installed locally Problems Using headless Chrome to take screenshots of a website requires properly connecting the chrome-headless service container to the Node library https://github.com/GoogleChrome/puppeteer by passing its credentials using the Node.js libary. Steps The project will ultimately have the following structure: . ├── .platform │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml ├── index.js ├── package.json ├── package-lock.json └── screenshots.js 1. Initialize the project Create an empty project on http://Platform.sh using the CLI. $ platform create Create a new project directory for the application on your local machine called screenshots and cd into it. Initialize the directory as a Git repository and set its remote to the newly created http://Platform.sh project using the outputted project ID. $ git init $ platform project:set-remote 2. Create the http://Platform.sh configuration files .platform/services.yaml Define the chrome-headless container using the supported version outlined in the https://docs.platform.sh/configuration/services/headless-chrome.html . headless: type: chrome-headless:73 .platform.app.yaml Configure the application nodejs: name: nodejs type: nodejs:10 relationships: headless: \"headless:http\" crons: cleanup: spec: '*/30 * * * *' cmd: rm screenshots/* web: commands: start: \"nodejs index.js\" mounts: \"/screenshots\": \"shared:files/screenshots\" disk: 512 The configuration uses nodejs 10, since it is required to use the Config Reader library with Puppeteer. It defines the mount screenshots which will be the writable directory to save the screenshots the application takes. In order to prevent screenshots/ from filling up as people use it, a cron job is also defined that removes its contents every 30 minutes. .platform/routes.yaml Lastly, set up a basic routes configuration file, using the name of the application nodejs \"https://{default}/\": id: main type: upstream upstream: \"nodejs:http\" \"https://www.{default}/\": type: redirect to: \"https://{default}/\" 3. Write the screenshots.js file Create a file in the project directory called screenshots.js with the following contents: const puppeteer = require('puppeteer'); const platformsh = require('platformsh-config'); var exports = module.exports = {}; // Create an async function exports.takeScreenshot = async function (url, screenshotID) { try { // Connect to chrome-headless using pre-formatted puppeteer credentials let config = platformsh.config(); const formattedURL = config.formattedCredentials(\"headless\", \"puppeteer\"); const browser = await puppeteer.connect({browserURL: formattedURL}); // Open a new page to the given url and take the screenshot const page = await browser.newPage(); await page.goto(url); await page.screenshot({ fullPage: true, path: `screenshots/${screenshotID}.png` }); await browser.close(); return browser } catch (e) { return Promise.reject(e); } }; It defines an async function called takeScreenshot as a module export. The Node.js Config Reader retrieves the library’s formatted credentials for Puppeteer to create the formattedURL string. path defines the saved location of the screenshot, while fullPage configures Puppeteer to take a screenshot of the entire page, not just the view seen within a window. Additional parameters for page.screenshot() can be found in the . 4. Define index.js Create the file index.js that defines the ExpressJS application app: const fs = require('fs'); const uuidv4 = require('uuid/v4') const platformsh = require('platformsh-config'); const express = require('express'); // Require screenshot file and its function var screenshot = require(\"./screenshots.js\"); // Build the application var app = express() // Define the index route app.get('/', (req, res) = { res.writeHead(200, {\"Content-Type\": \"text/html\"}); res.write(` \r\n\r\nHeadless Chrome on Platform.sh\r\n\r\n Take a screenshot of a page\r\n\r\n Click submit to generate a png of the https://platform.sh/ , or paste in another URL. `); res.end(``); }) // Define screenshot result route app.get('/result', async function(req, res){ // Create a randomly generated ID number for the current screenshot var screenshotID = uuidv4(); // Take the screenshot await screenshot.takeScreenshot(req.query['screenshotURL'], screenshotID) // Define and download the file const file = `screenshots/${screenshotID}.png`; res.download(file); }); // Create config object to get Platform.sh PORT credentials let config = platformsh.config(); // Start the server. app.listen(config.port, function() { console.log(`Listening on port ${config.port}`) }); In addition to the home route, index.js defines a /results path that calls takeScreenshot() and passes a randomly generated ID that will become part of the name for the generated png file. 5. Define the application’s dependencies Include the application’s dependencies in package.json: { \"name\": \"chrome_headless\", \"version\": \"1.0.0\", \"description\": \"A simple example for taking screenshots with Puppeteer and headless Chrome on Platform.sh\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" \u0026\u0026 exit 1\" }, \"author\": \"Chad Carlson\", \"license\": \"MIT\", \"dependencies\": { \"platformsh-config\": \"^2.0.0\", \"puppeteer\": \"^1.14.0\", \"express\": \"^4.16.4\", \"uuid\": \"^3.3.2\" } } Then create the package-lock.json file by running $ npm install 6. Push to http://Platform.sh Commit the changes and push master to http://Platform.sh $ git add . $ git commit -m \"Create screenshot application.\" $ git push platform master 7. Verify Use the command platform url when the build process has completed to visit the site. Click submit to take a screenshot of the http://Platform.sh website, or copy in another url to test the application. Conclusion Using ExpressJS, Puppeteer, and http://Platform.sh headless Chrome as a service, a simple application can be made that takes screenshots of an inputted url.",
        "text": "Goal To use https://github.com/GoogleChrome/puppeteer and https://developers.google.com/web/updates/2017/04/headless-chrome to create an https://expressjs.com/ application that takes website screenshots on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account The https://docs.platform.sh/gettingstarted/cli.html installed locally Node.js and npm installed locally Problems Using headless Chrome to take screenshots of a website requires properly connecting the chrome-headless service container to the Node library https://github.com/GoogleChrome/puppeteer by passing its credentials using the Node.js libary. Steps The project will ultimately have the following structure: . ├── .platform │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml ├── index.js ├── package.json ├── package-lock.json └── screenshots.js 1. Initialize the project Create an empty project on http://Platform.sh using the CLI. $ platform create Create a new project directory for the application on your local machine called screenshots and cd into it. Initialize the directory as a Git repository and set its remote to the newly created http://Platform.sh project using the outputted project ID. $ git init $ platform project:set-remote 2. Create the http://Platform.sh configuration files .platform/services.yaml Define the chrome-headless container using the supported version outlined in the https://docs.platform.sh/configuration/services/headless-chrome.html . headless: type: chrome-headless:73 .platform.app.yaml Configure the application nodejs: name: nodejs type: nodejs:10 relationships: headless: \"headless:http\" crons: cleanup: spec: '*/30 * * * *' cmd: rm screenshots/* web: commands: start: \"nodejs index.js\" mounts: \"/screenshots\": \"shared:files/screenshots\" disk: 512 The configuration uses nodejs 10, since it is required to use the Config Reader library with Puppeteer. It defines the mount screenshots which will be the writable directory to save the screenshots the application takes. In order to prevent screenshots/ from filling up as people use it, a cron job is also defined that removes its contents every 30 minutes. .platform/routes.yaml Lastly, set up a basic routes configuration file, using the name of the application nodejs \"https://{default}/\": id: main type: upstream upstream: \"nodejs:http\" \"https://www.{default}/\": type: redirect to: \"https://{default}/\" 3. Write the screenshots.js file Create a file in the project directory called screenshots.js with the following contents: const puppeteer = require('puppeteer'); const platformsh = require('platformsh-config'); var exports = module.exports = {}; // Create an async function exports.takeScreenshot = async function (url, screenshotID) { try { // Connect to chrome-headless using pre-formatted puppeteer credentials let config = platformsh.config(); const formattedURL = config.formattedCredentials(\"headless\", \"puppeteer\"); const browser = await puppeteer.connect({browserURL: formattedURL}); // Open a new page to the given url and take the screenshot const page = await browser.newPage(); await page.goto(url); await page.screenshot({ fullPage: true, path: `screenshots/${screenshotID}.png` }); await browser.close(); return browser } catch (e) { return Promise.reject(e); } }; It defines an async function called takeScreenshot as a module export. The Node.js Config Reader retrieves the library’s formatted credentials for Puppeteer to create the formattedURL string. path defines the saved location of the screenshot, while fullPage configures Puppeteer to take a screenshot of the entire page, not just the view seen within a window. Additional parameters for page.screenshot() can be found in the . 4. Define index.js Create the file index.js that defines the ExpressJS application app: const fs = require('fs'); const uuidv4 = require('uuid/v4') const platformsh = require('platformsh-config'); const express = require('express'); // Require screenshot file and its function var screenshot = require(\"./screenshots.js\"); // Build the application var app = express() // Define the index route app.get('/', (req, res) = { res.writeHead(200, {\"Content-Type\": \"text/html\"}); res.write(` \r\n\r\nHeadless Chrome on Platform.sh\r\n\r\n Take a screenshot of a page\r\n\r\n Click submit to generate a png of the https://platform.sh/ , or paste in another URL. `); res.end(``); }) // Define screenshot result route app.get('/result', async function(req, res){ // Create a randomly generated ID number for the current screenshot var screenshotID = uuidv4(); // Take the screenshot await screenshot.takeScreenshot(req.query['screenshotURL'], screenshotID) // Define and download the file const file = `screenshots/${screenshotID}.png`; res.download(file); }); // Create config object to get Platform.sh PORT credentials let config = platformsh.config(); // Start the server. app.listen(config.port, function() { console.log(`Listening on port ${config.port}`) }); In addition to the home route, index.js defines a /results path that calls takeScreenshot() and passes a randomly generated ID that will become part of the name for the generated png file. 5. Define the application’s dependencies Include the application’s dependencies in package.json: { \"name\": \"chrome_headless\", \"version\": \"1.0.0\", \"description\": \"A simple example for taking screenshots with Puppeteer and headless Chrome on Platform.sh\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" \u0026\u0026 exit 1\" }, \"author\": \"Chad Carlson\", \"license\": \"MIT\", \"dependencies\": { \"platformsh-config\": \"^2.0.0\", \"puppeteer\": \"^1.14.0\", \"express\": \"^4.16.4\", \"uuid\": \"^3.3.2\" } } Then create the package-lock.json file by running $ npm install 6. Push to http://Platform.sh Commit the changes and push master to http://Platform.sh $ git add . $ git commit -m \"Create screenshot application.\" $ git push platform master 7. Verify Use the command platform url when the build process has completed to visit the site. Click submit to take a screenshot of the http://Platform.sh website, or copy in another url to test the application. Conclusion Using ExpressJS, Puppeteer, and http://Platform.sh headless Chrome as a service, a simple application can be made that takes screenshots of an inputted url.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-take-screenshots-using-puppeteer-and-headless-chrome/305",
        "relurl": "/t/how-to-take-screenshots-using-puppeteer-and-headless-chrome/305"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "00a1490102fc2574211cb363db019dfcef1494d9",
        "title": "How to modify my Platform.sh project plan and its resources",
        "description": "Goal Change a http://Platform.sh plan/and or modify project settings such as environments, storage, and users. Assumptions Owner status on the project to be changed Problems Requirements change and modifications to your current plan sometimes need to be implemented. Steps 1. Edit a Project Login to https://accounts.platform.sh/user to view the accessible projects. For the projects that you are the Account Owner for, you will see three vertical dots. Click those dots to pull up a drop-down menu and select edit to modify that project’s configuration. 2. Upgrade the plan size From the drop down menu, modify the Project’s current plan size. Changing the plan here will make visible changes to: what is included within the plan the cost of the plan. When finished, click the Update Plan button on the bottom of the page. Note: Production-sized http://Platform.sh subscriptions cannot be downgraded to a Development plan due to potential feature conflicts. If looking to downgrade a plan, https://accounts.platform.sh/platform/support 3. Modify number of environments To modify the number of environments, you can click on the drop down menu and select the number of environments you wish to add. Note: You can increase and decrease the number of environments yourself. 4. Modify storage sizing To modify the storage sizing, you can click on the drop down menu and select additional storage you would like to add (in GB). Note: You can only increase the storage sizing here. If you wish to decrease the amount of storage used, https://accounts.platform.sh/platform/support 5. Number of Users This page also shows the Account Owner the number of Users for the specific project in the same way. The number of users can also be modified https://community.platform.sh/t/how-to-add-or-remove-users-from-projects-using-the-command-line/91 . Conclusion Account owner has the power to modify their plan as well as view the costs associated with the change. For further information, please check our https://platform.sh/pricing .",
        "text": "Goal Change a http://Platform.sh plan/and or modify project settings such as environments, storage, and users. Assumptions Owner status on the project to be changed Problems Requirements change and modifications to your current plan sometimes need to be implemented. Steps 1. Edit a Project Login to https://accounts.platform.sh/user to view the accessible projects. For the projects that you are the Account Owner for, you will see three vertical dots. Click those dots to pull up a drop-down menu and select edit to modify that project’s configuration. 2. Upgrade the plan size From the drop down menu, modify the Project’s current plan size. Changing the plan here will make visible changes to: what is included within the plan the cost of the plan. When finished, click the Update Plan button on the bottom of the page. Note: Production-sized http://Platform.sh subscriptions cannot be downgraded to a Development plan due to potential feature conflicts. If looking to downgrade a plan, https://accounts.platform.sh/platform/support 3. Modify number of environments To modify the number of environments, you can click on the drop down menu and select the number of environments you wish to add. Note: You can increase and decrease the number of environments yourself. 4. Modify storage sizing To modify the storage sizing, you can click on the drop down menu and select additional storage you would like to add (in GB). Note: You can only increase the storage sizing here. If you wish to decrease the amount of storage used, https://accounts.platform.sh/platform/support 5. Number of Users This page also shows the Account Owner the number of Users for the specific project in the same way. The number of users can also be modified https://community.platform.sh/t/how-to-add-or-remove-users-from-projects-using-the-command-line/91 . Conclusion Account owner has the power to modify their plan as well as view the costs associated with the change. For further information, please check our https://platform.sh/pricing .",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-modify-my-platform-sh-project-plan-and-its-resources/130",
        "relurl": "/t/how-to-modify-my-platform-sh-project-plan-and-its-resources/130"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "287b8d60ed96f3001ebc1909cf19145ebfd57536",
        "title": "How to sync database backups to Amazon S3",
        "description": "Goal To transfer regular database backups to Amazon S3. Assumptions You will need: An SSH key configured on your http://Platform.sh account A project with a configured database ( https://docs.platform.sh/configuration/services/postgresql.html in this example) Problems While http://Platform.sh provides managed services that make configuring databases to application very easy, certain projects may require syncing regular backups to a specified location like Amazon S3. Steps The following steps refer to PostgreSQL specifically, though they could be generalized to other databases. Consult each service’s documentation for differences, and modify the credentials accordingly. 1. Create sensitive environment variables for AWS credentials AWS access keys are located in “My Security Credentials” under “Access Keys”. New keys can be created there as well. platform variable:create --level=project --name=AWS_ACCESS_KEY_ID --value= --json=false --sensitive=true --prefix=env --visible-build=true --visible-runtime=true platform variable:create --level=project --name=AWS_SECRET_ACCESS_KEY --value= ",
        "text": "Goal To transfer regular database backups to Amazon S3. Assumptions You will need: An SSH key configured on your http://Platform.sh account A project with a configured database ( https://docs.platform.sh/configuration/services/postgresql.html in this example) Problems While http://Platform.sh provides managed services that make configuring databases to application very easy, certain projects may require syncing regular backups to a specified location like Amazon S3. Steps The following steps refer to PostgreSQL specifically, though they could be generalized to other databases. Consult each service’s documentation for differences, and modify the credentials accordingly. 1. Create sensitive environment variables for AWS credentials AWS access keys are located in “My Security Credentials” under “Access Keys”. New keys can be created there as well. platform variable:create --level=project --name=AWS_ACCESS_KEY_ID --value= --json=false --sensitive=true --prefix=env --visible-build=true --visible-runtime=true platform variable:create --level=project --name=AWS_SECRET_ACCESS_KEY --value= ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-sync-database-backups-to-amazon-s3/293",
        "relurl": "/t/how-to-sync-database-backups-to-amazon-s3/293"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "1f55fddf1826d550f5cf3650fcc55c0806d3a069",
        "title": "How to migrate a PostgreSQL database from Heroku to Platform.sh",
        "description": "How to migrate a PostgreSQL database from Heroku to http://Platform.sh Goal Migrate data stored in a Heroku PostgreSQL resource to a http://Platform.sh service. Assumptions You will need: an active application on http://Platform.sh configured to an empty database an active application on Heroku configured to the target database http://Platform.sh Postgres https://community.platform.sh/t/how-to-access-postgresql-credentials-on-platform-sh/150 A local repository with the http://platform.sh/ project as git remote The https://docs.platform.sh/gettingstarted/cli.html installed locally Problems When migrating an existing PostgreSQL project from Heroku to http://Platform.sh, the corresponding resources must be migrated as well. Heroku and http://Platform.sh use different db_dump formats, as well as different styles of configuration settings. Steps 1. Download database dump from Heroku. Find the Resources tab of the Heroku App Dashboard and click on the name of the desired database add-on, for example Heroku Postgres. From the datastore dashboard, visit the Durability tab, create a manual backup of the data, and download the backup file. For the purposes of this guide, the location of the dump will be assumed to be ~/Downloads/db.dump. 2. Add extensions to http://Platform.sh configuration. Next, issue the following command to filter the list of SQL commands in the dump by the word “extension”, giving all Postgres extensions loaded into the database: $ pg_restore -l ~/Downloads/db.dump | grep -o -P '(? db.list Finally, use this list to generate a new, filtered SQL file from the original dump file: pg_restore --no-owner -L db.list ~/Downloads/db.dump db.sql This creates an intermediary file db.list, which may be removed. 4. Import database to http://Platform.sh note Importing a database snapshot is a destructive operation, which will overwrite data already in your database. Backing up data before completing this step is strongly recommended. The final step is importing the SQL file to the Platform database instance. The simplest way to do so is via the following command in the project directory: platform sql ",
        "text": "How to migrate a PostgreSQL database from Heroku to http://Platform.sh Goal Migrate data stored in a Heroku PostgreSQL resource to a http://Platform.sh service. Assumptions You will need: an active application on http://Platform.sh configured to an empty database an active application on Heroku configured to the target database http://Platform.sh Postgres https://community.platform.sh/t/how-to-access-postgresql-credentials-on-platform-sh/150 A local repository with the http://platform.sh/ project as git remote The https://docs.platform.sh/gettingstarted/cli.html installed locally Problems When migrating an existing PostgreSQL project from Heroku to http://Platform.sh, the corresponding resources must be migrated as well. Heroku and http://Platform.sh use different db_dump formats, as well as different styles of configuration settings. Steps 1. Download database dump from Heroku. Find the Resources tab of the Heroku App Dashboard and click on the name of the desired database add-on, for example Heroku Postgres. From the datastore dashboard, visit the Durability tab, create a manual backup of the data, and download the backup file. For the purposes of this guide, the location of the dump will be assumed to be ~/Downloads/db.dump. 2. Add extensions to http://Platform.sh configuration. Next, issue the following command to filter the list of SQL commands in the dump by the word “extension”, giving all Postgres extensions loaded into the database: $ pg_restore -l ~/Downloads/db.dump | grep -o -P '(? db.list Finally, use this list to generate a new, filtered SQL file from the original dump file: pg_restore --no-owner -L db.list ~/Downloads/db.dump db.sql This creates an intermediary file db.list, which may be removed. 4. Import database to http://Platform.sh note Importing a database snapshot is a destructive operation, which will overwrite data already in your database. Backing up data before completing this step is strongly recommended. The final step is importing the SQL file to the Platform database instance. The simplest way to do so is via the following command in the project directory: platform sql ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-migrate-a-postgresql-database-from-heroku-to-platform-sh/301",
        "relurl": "/t/how-to-migrate-a-postgresql-database-from-heroku-to-platform-sh/301"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "23cab84b964712ec40faf61364793a09aabdad48",
        "title": "How to configure caching for static assets",
        "description": "How to configure caching for static assets Goal Caching is a common and effective way to improve the performance of a website. Both http://Platform.sh and your browser can cache static files with the proper cache header configuration. This guide will explain how to configure those headers. Assumptions This guide applies only to static assets. For responses generated dynamically by the application, the application itself will need to include the appropriate cache headers in the response. Some static assets may be inappropriate to cache. This guide assumes that most or all files are safe to cache. Depending on the use case, it may make sense to cache files for a relatively short period (minutes) or a very long period (weeks). This guide applies equally to static files included with your application as well as those produced by your application or uploaded by a user. Problems If a file is cached and then updated, anywhere the old file is already cached will still keep the old copy until its cache expires. For that reason it is usually better to err on the side of shorter cache lifetimes if unsure. Steps 1. Determine which files to cache Different cache settings may be set for different files, based on either their directory or precise filename. For this example assume the following directory structure: / .platform/ .platform.app.yaml web/ index.php robots.txt uploads/ assets/ generated/ nocache/ Where the document root is web. uploads contains user-uploaded files that should be cached for 10 minutes. assets contains developer-provided files that may be cached for longer, such a 1 day. robots.txt is a miscellaneous file that is safe to cache. generated contains application-generated files, such as CSS or JS files, that may be cached for a long period but need to change quickly when code changes. nocache contains files that change so rapidly that caching them is not useful. 2. Set a default cache lifetime for all static files Locate the web.locations block in .platform.app.yaml for the document root. In this example it would look something like: web: locations: '/': root: 'web' As a sibling of root, add an expires key with a time period. To set an expiration time of 5 minutes, use: web: locations: '/': root: 'web' expires: 5m The time period can be in “s” (seconds), “m” (minutes), “h” (hours), “d” (days), “w” (weeks), “M” (30 day months), or “y” (365 day years). This configuration will automatically set a cache-control header for all static files to cache the file for 5 minutes. If you have multiple paths mapped under web.locations, each one may have its own expires declaration. See the for more details. 3. Configure directory-specific alternate cache lifetimes To change the cache information (or any other configuration) for a specific sub-path within the docroot, use the rules block: web: locations: '/': root: 'web' expires: 5m rules: '^/uploads': expires: 10m '^/assets': expires: 1d '^/nocache': expires: -1 Each rules block is a regular expression that may match the path. In this example: any URL that begins with /uploads will have a cache lifetime of 10 minutes any URL that begins with /assets will have a cache lifetime of 1 day any URL that begins with /nocache will explicitly have caching disabled and any other static files will have a cache lifetime of 5 minutes The rules block also makes it possible to target specific files by name or pattern. 4. Configure cache-busting URLs A common pattern is for certain files to have very long cache lifetimes but need to be cleared on-demand after a code deploy. That’s especially true for aggregated CSS or JS files. The standard technique for that is to include a “cache busting” query on the URL as produced by the application, which since it’s a different URL will not use the previously cached value. There are two steps to configure that pattern. First, set an especially long cache lifetime for the files in question. web: locations: '/': root: 'web' expires: 5m rules: '^/generated/*.(css|js)': expires: 2w That configuration will set a 2 week cache lifetime for any files under the generated directory that end in .css or .js. Second, when your application generates URLs to those files it needs to include a ?cache=somestring query parameter. When the application code changes then somestring will need to change, too. An easy way to do that is to base the string on the PLATFORM_TREE_ID environment variable, which will change if and only if the code in the application changed. For example, in PHP one could do: $somestring = substr(getenv('PLATFORM_TREE_ID'), 0, 6); That will produce a junk string that is the first 6 characters of the tree ID. When new code is deployed, PLATFORM_TREE_ID will change and thus $somestring will change as well. The application will then need to generate URLs that include that value, such as: https://www.example.com/generated/forum.css?cache=$somestring 5. Enable router caching In .platform/routes.yaml, ensure that caching is enabled for the appropriate route: \"https://{default}/\": type: upstream upstream: \"app:http\" cache: enabled: true cookies: ['/^SS?ESS/'] default_ttl: 5m The enabled statement turns on the cache, causing the router to cache any responses that have a proper cache header (both static and application-generated responses). The cookies statement is an array of cookie name regular expressions that should be excluded when calculating the cache. Generally it should be set to whatever your application’s cookie name is. In this example, cookies named SESS or SSESS will be excluded from the cache. If this setting is incorrect then any request with an active session will not be cached, even if it’s safe to do so. The default_ttl statement will set a cache header of 5 minutes on all static responses that do not already have one. In the example above all static files will have a cache header already so it has no effect. See the https://docs.platform.sh/configuration/routes/cache.html for more details. Conclusion All static files will now be cached for the specified period of time in the router and in each visitor’s browser cache. Dynamic application responses that have a cache header set by the application will be cached as well. Dynamic application responses that have no cache-header will not be cached at any level.",
        "text": "How to configure caching for static assets Goal Caching is a common and effective way to improve the performance of a website. Both http://Platform.sh and your browser can cache static files with the proper cache header configuration. This guide will explain how to configure those headers. Assumptions This guide applies only to static assets. For responses generated dynamically by the application, the application itself will need to include the appropriate cache headers in the response. Some static assets may be inappropriate to cache. This guide assumes that most or all files are safe to cache. Depending on the use case, it may make sense to cache files for a relatively short period (minutes) or a very long period (weeks). This guide applies equally to static files included with your application as well as those produced by your application or uploaded by a user. Problems If a file is cached and then updated, anywhere the old file is already cached will still keep the old copy until its cache expires. For that reason it is usually better to err on the side of shorter cache lifetimes if unsure. Steps 1. Determine which files to cache Different cache settings may be set for different files, based on either their directory or precise filename. For this example assume the following directory structure: / .platform/ .platform.app.yaml web/ index.php robots.txt uploads/ assets/ generated/ nocache/ Where the document root is web. uploads contains user-uploaded files that should be cached for 10 minutes. assets contains developer-provided files that may be cached for longer, such a 1 day. robots.txt is a miscellaneous file that is safe to cache. generated contains application-generated files, such as CSS or JS files, that may be cached for a long period but need to change quickly when code changes. nocache contains files that change so rapidly that caching them is not useful. 2. Set a default cache lifetime for all static files Locate the web.locations block in .platform.app.yaml for the document root. In this example it would look something like: web: locations: '/': root: 'web' As a sibling of root, add an expires key with a time period. To set an expiration time of 5 minutes, use: web: locations: '/': root: 'web' expires: 5m The time period can be in “s” (seconds), “m” (minutes), “h” (hours), “d” (days), “w” (weeks), “M” (30 day months), or “y” (365 day years). This configuration will automatically set a cache-control header for all static files to cache the file for 5 minutes. If you have multiple paths mapped under web.locations, each one may have its own expires declaration. See the for more details. 3. Configure directory-specific alternate cache lifetimes To change the cache information (or any other configuration) for a specific sub-path within the docroot, use the rules block: web: locations: '/': root: 'web' expires: 5m rules: '^/uploads': expires: 10m '^/assets': expires: 1d '^/nocache': expires: -1 Each rules block is a regular expression that may match the path. In this example: any URL that begins with /uploads will have a cache lifetime of 10 minutes any URL that begins with /assets will have a cache lifetime of 1 day any URL that begins with /nocache will explicitly have caching disabled and any other static files will have a cache lifetime of 5 minutes The rules block also makes it possible to target specific files by name or pattern. 4. Configure cache-busting URLs A common pattern is for certain files to have very long cache lifetimes but need to be cleared on-demand after a code deploy. That’s especially true for aggregated CSS or JS files. The standard technique for that is to include a “cache busting” query on the URL as produced by the application, which since it’s a different URL will not use the previously cached value. There are two steps to configure that pattern. First, set an especially long cache lifetime for the files in question. web: locations: '/': root: 'web' expires: 5m rules: '^/generated/*.(css|js)': expires: 2w That configuration will set a 2 week cache lifetime for any files under the generated directory that end in .css or .js. Second, when your application generates URLs to those files it needs to include a ?cache=somestring query parameter. When the application code changes then somestring will need to change, too. An easy way to do that is to base the string on the PLATFORM_TREE_ID environment variable, which will change if and only if the code in the application changed. For example, in PHP one could do: $somestring = substr(getenv('PLATFORM_TREE_ID'), 0, 6); That will produce a junk string that is the first 6 characters of the tree ID. When new code is deployed, PLATFORM_TREE_ID will change and thus $somestring will change as well. The application will then need to generate URLs that include that value, such as: https://www.example.com/generated/forum.css?cache=$somestring 5. Enable router caching In .platform/routes.yaml, ensure that caching is enabled for the appropriate route: \"https://{default}/\": type: upstream upstream: \"app:http\" cache: enabled: true cookies: ['/^SS?ESS/'] default_ttl: 5m The enabled statement turns on the cache, causing the router to cache any responses that have a proper cache header (both static and application-generated responses). The cookies statement is an array of cookie name regular expressions that should be excluded when calculating the cache. Generally it should be set to whatever your application’s cookie name is. In this example, cookies named SESS or SSESS will be excluded from the cache. If this setting is incorrect then any request with an active session will not be cached, even if it’s safe to do so. The default_ttl statement will set a cache header of 5 minutes on all static responses that do not already have one. In the example above all static files will have a cache header already so it has no effect. See the https://docs.platform.sh/configuration/routes/cache.html for more details. Conclusion All static files will now be cached for the specified period of time in the router and in each visitor’s browser cache. Dynamic application responses that have a cache header set by the application will be cached as well. Dynamic application responses that have no cache-header will not be cached at any level.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-configure-caching-for-static-assets/187",
        "relurl": "/t/how-to-configure-caching-for-static-assets/187"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "1ef7562eeee0768bb4ec1c923e12237acc8dd180",
        "title": "How do I install Apache Tika with Solr",
        "description": "Goal Install Apache Tika with Solr on http://Platform.sh Assumptions A https://github.com/platformsh/template-drupal8 project on http://Platform.sh. https://docs.platform.sh/configuration/services/solr.html on that project. Problems Apache Tika allows you to extract information from binary files (e.g. PDF files) and make them searchable in Solr. Steps 1. Install search modules using composer Install and configure search_api and search_api_solr: composer require drupal/search_api composer require drupal/search_api_solr More information is available for setting up Solr with Drupal 8 in the https://docs.platform.sh/frameworks/drupal8/solr.html . 2. Install search attachments module Install search_api_attachments using composer composer require drupal/search_api_attachments Search API Attachments enable pointing at the tika jar file to index PDF documents. 3. Install the Tika jar Modify or include a build hook to download the Tika jar file to the project by editing .platform.app.yaml. # .platform.app.yaml hooks: build: | mkdir -p /app/srv/bin cd /app/srv/bin \u0026\u0026 curl -OL http://download.nextag.com/apache/tika/tika-app-1.16.jar The build hook creates the directory /srv/bin and downloads the Tika jar executable into it. An https://github.com/thinktandem/platform-tika/blob/master/.platform.app.yaml is available where the full .platform.app.yaml can be found. Consult the documentation for more information about on http://Platform.sh. 4. Configure the search API attachments Visit /admin/config/search/search_api_attachments in a browser and add the method, java executable, and Tika paths configuration. These paths correspond to the paths entered in the .platform.app.yaml file for the build step. Conclusion Apache Tika is now setup with Solr on http://Platform.sh.",
        "text": "Goal Install Apache Tika with Solr on http://Platform.sh Assumptions A https://github.com/platformsh/template-drupal8 project on http://Platform.sh. https://docs.platform.sh/configuration/services/solr.html on that project. Problems Apache Tika allows you to extract information from binary files (e.g. PDF files) and make them searchable in Solr. Steps 1. Install search modules using composer Install and configure search_api and search_api_solr: composer require drupal/search_api composer require drupal/search_api_solr More information is available for setting up Solr with Drupal 8 in the https://docs.platform.sh/frameworks/drupal8/solr.html . 2. Install search attachments module Install search_api_attachments using composer composer require drupal/search_api_attachments Search API Attachments enable pointing at the tika jar file to index PDF documents. 3. Install the Tika jar Modify or include a build hook to download the Tika jar file to the project by editing .platform.app.yaml. # .platform.app.yaml hooks: build: | mkdir -p /app/srv/bin cd /app/srv/bin \u0026\u0026 curl -OL http://download.nextag.com/apache/tika/tika-app-1.16.jar The build hook creates the directory /srv/bin and downloads the Tika jar executable into it. An https://github.com/thinktandem/platform-tika/blob/master/.platform.app.yaml is available where the full .platform.app.yaml can be found. Consult the documentation for more information about on http://Platform.sh. 4. Configure the search API attachments Visit /admin/config/search/search_api_attachments in a browser and add the method, java executable, and Tika paths configuration. These paths correspond to the paths entered in the .platform.app.yaml file for the build step. Conclusion Apache Tika is now setup with Solr on http://Platform.sh.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-install-apache-tika-with-solr/290",
        "relurl": "/t/how-do-i-install-apache-tika-with-solr/290"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "2f4e8d8496af1c4d12403bc0a97cb1d791d9a966",
        "title": "How to develop locally on Platform.sh with a tethered connection",
        "description": "Goal Run a project on a local web server that connects to an application’s services with an SSH tunnel. Assumptions This guide uses the https://docs.platform.sh/gettingstarted/cli.html to connect to a project’s services, and assumes that it and Git are already installed. The guide also assumes that an SSH key is configured on the project account. Problems The local machine must be configured with the appropriate PHP extensions. From a local project, an SSH tunnel is required to connect to http://Platform.sh services and expose those relationships in a local environment variable. Steps 1. Check pcntl and posix Tunneling to the project’s services using the CLI requires the PHP extensions pcntl and posix. Check to make sure they are present, and install if the following output is not received: $ php -m | grep -E 'posix|pcntl' pcntl posix 2. Download local project copy List the projects on the account to find the project-ID: $ platform projects Your projects are: +--------------+------------------------------------------+-------------------------------------------------+ | ID | Title | URL | +--------------+------------------------------------------+-------------------------------------------------+ | | Platform Project | https://eu.platform.sh//#/projects/ | +--------------+------------------------------------------+-------------------------------------------------+ Download the project. $ platform get Directory [repo]: repo Environment [master]: dev Downloading project Platform Project () Cloning into 'my-project/repo'... remote: counting objects: 461, done. Receiving objects: 100% (461/461), 1.41 MiB | 249.00 KiB/s, done. Resolving deltas: 100% (225/225), done. The project Platform Project () was successfully downloaded to: repo 3. Open an SSH tunnel to the project. $ platform tunnel:open SSH tunnel opened on port 30000 to relationship: elasticsearch SSH tunnel opened on port 30001 to relationship: rabbitmq SSH tunnel opened on port 30002 to relationship: postgresql Check to make sure that the tunnels are visible. $ platform tunnel:list +-------+--------------+-------------+-----+---------------+ | Port | Project | Environment | App | Relationship | +-------+--------------+-------------+-----+---------------+ | 30000 | | dev | app | elasticsearch | | 30001 | | dev | app | rabbitmq | | 30002 | | dev | app | postgresql | +-------+--------------+-------------+-----+---------------+ If the tunnels are not visible here, check /Users//.platformsh/tunnels.log to see if the https://docs.platform.sh/development/ssh.html was configured incorrectly. 4. Export PLATFORM_RELATIONSHIPS Create a JSON encoded PLATFORM_RELATIONSHIPS environment variable to mimic the relationships array that will be visible to the application. export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" 5. Build Build the application. If a service is required locally but missing, the CLI will prompt for its installation. $ cd repo $ platform build 6. Run the application Now the project has been built and can be run locally. For a Django application: $ python manage.py runserver 7. Close the tunnel. $ platform tunnel:close Close tunnel to relationship elasticsearch on -dev--app? [Y/n] y Closed tunnel to elasticsearch on -dev--app Close tunnel to relationship rabbitmq on -dev--app? [Y/n] y Closed tunnel to rabbitmq on -dev--app Close tunnel to relationship postgresql on -dev--app? [Y/n] y Closed tunnel to postgresql on -dev--app Conclusion Once PHP extensions are properly configured and SSH tunnels are opened to a http://Platform.sh project’s services, an application can be built for local development. The CLI is used here to connect to PostgreSQL, ElasticSearch and RabbitMQ, but the commands are not specific to those https://docs.platform.sh/configuration/services.html or to the language used in the application. Additional information regarding local development best practices can be found in the https://docs.platform.sh/gettingstarted/local.html .",
        "text": "Goal Run a project on a local web server that connects to an application’s services with an SSH tunnel. Assumptions This guide uses the https://docs.platform.sh/gettingstarted/cli.html to connect to a project’s services, and assumes that it and Git are already installed. The guide also assumes that an SSH key is configured on the project account. Problems The local machine must be configured with the appropriate PHP extensions. From a local project, an SSH tunnel is required to connect to http://Platform.sh services and expose those relationships in a local environment variable. Steps 1. Check pcntl and posix Tunneling to the project’s services using the CLI requires the PHP extensions pcntl and posix. Check to make sure they are present, and install if the following output is not received: $ php -m | grep -E 'posix|pcntl' pcntl posix 2. Download local project copy List the projects on the account to find the project-ID: $ platform projects Your projects are: +--------------+------------------------------------------+-------------------------------------------------+ | ID | Title | URL | +--------------+------------------------------------------+-------------------------------------------------+ | | Platform Project | https://eu.platform.sh//#/projects/ | +--------------+------------------------------------------+-------------------------------------------------+ Download the project. $ platform get Directory [repo]: repo Environment [master]: dev Downloading project Platform Project () Cloning into 'my-project/repo'... remote: counting objects: 461, done. Receiving objects: 100% (461/461), 1.41 MiB | 249.00 KiB/s, done. Resolving deltas: 100% (225/225), done. The project Platform Project () was successfully downloaded to: repo 3. Open an SSH tunnel to the project. $ platform tunnel:open SSH tunnel opened on port 30000 to relationship: elasticsearch SSH tunnel opened on port 30001 to relationship: rabbitmq SSH tunnel opened on port 30002 to relationship: postgresql Check to make sure that the tunnels are visible. $ platform tunnel:list +-------+--------------+-------------+-----+---------------+ | Port | Project | Environment | App | Relationship | +-------+--------------+-------------+-----+---------------+ | 30000 | | dev | app | elasticsearch | | 30001 | | dev | app | rabbitmq | | 30002 | | dev | app | postgresql | +-------+--------------+-------------+-----+---------------+ If the tunnels are not visible here, check /Users//.platformsh/tunnels.log to see if the https://docs.platform.sh/development/ssh.html was configured incorrectly. 4. Export PLATFORM_RELATIONSHIPS Create a JSON encoded PLATFORM_RELATIONSHIPS environment variable to mimic the relationships array that will be visible to the application. export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" 5. Build Build the application. If a service is required locally but missing, the CLI will prompt for its installation. $ cd repo $ platform build 6. Run the application Now the project has been built and can be run locally. For a Django application: $ python manage.py runserver 7. Close the tunnel. $ platform tunnel:close Close tunnel to relationship elasticsearch on -dev--app? [Y/n] y Closed tunnel to elasticsearch on -dev--app Close tunnel to relationship rabbitmq on -dev--app? [Y/n] y Closed tunnel to rabbitmq on -dev--app Close tunnel to relationship postgresql on -dev--app? [Y/n] y Closed tunnel to postgresql on -dev--app Conclusion Once PHP extensions are properly configured and SSH tunnels are opened to a http://Platform.sh project’s services, an application can be built for local development. The CLI is used here to connect to PostgreSQL, ElasticSearch and RabbitMQ, but the commands are not specific to those https://docs.platform.sh/configuration/services.html or to the language used in the application. Additional information regarding local development best practices can be found in the https://docs.platform.sh/gettingstarted/local.html .",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69",
        "relurl": "/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "755ed483076fb2073f5c0dfea720e7d5c55743ca",
        "title": "How to create and restore snapshots using the CLI",
        "description": "Goal Restore a https://platform.sh live environment from a created snapshot. Assumptions This guide requires: An application running on http://Platform.sh A local repository with the http://Platform.sh project as git remote The https://docs.platform.sh/gettingstarted/cli.html installed locally An https://docs.platform.sh/development/ssh.html configured on the project account admin role granted for the project This guide uses a https://github.com/platformsh/template-python3 for a http://Platform.sh application using Flask as well as MySQL and Redis for its services. The template runs a test on both MySQL and Redis and returns a status dictionary to the live page. Problems http://Platform.sh recommends that snapshots are created of the live environment before merging, or when the storage space of services is increased. For one reason or another, it may become necessary to restore an active environment to a previous working state using snapshots. Creation and restoration of snapshots can be executed from the UI and using the CLI. Additional information can be found in the https://pr-1044-6dxt2aq-t2llqeifuhpzg.eu.platform.sh/administration/snapshot-and-restore.html Steps 1. Create a snapshot The master environment is functioning as desired: $ curl https://master-7rqtwti- ..platformsh.site/ {\"mysql\":{\"return\":null,\"status\":\"OK\"},\"redis\":{\"return\":null,\"status\":\"OK\"}} From master create a snapshot using the http://Platform.sh CLI if one has not already been created. $ platform snapshot:create Creating a snapshot of master Waiting for the snapshot to complete... Waiting for the activity khlktlgy4r5uc (User created a backup of Master): Backing up master Backup name is [============================] 12 secs (complete) A snapshot of environment master has been created Snapshot name: 2. Changes were made and merged to Master In another branch dev, some changes were made. For the Python 3 template example, the Redis test was changed from 70 r.set(key_name, \"bar\") to 70 r.set(key_name, \"BEAR\") which will result in an Exception for the failed test. It was not caught in time, and dev was merged into master. $ platform environment:merge Now the live site is failing: $ curl https://master-7rqtwti- ..platformsh.site/ {\"mysql\":{\"return\":null,\"status\":\"OK\"},\"redis\":{\"error\":[\"Traceback (most recent call last):\\n\",\" File \\\"server.py\\\", line 30, in wrap_test\\n result = callback(*args, **kwargs)\\n\",\" File \\\"server.py\\\", line 71, in test_redis\\n assert value == r.get(key_name)\\n\",\"AssertionError\\n\"], \"status\":\"ERROR\"}} 3. Retrieve Snapshot Name Actions must take place from the branch the snapshot was taken from, and will not be available from dev. $ git checkout dev Switched to branch 'dev' $ platform snapshots No snapshots found List the saved snapshots to retrieve the snapshot name. $ git checkout master Switched to branch 'master' Your branch is up to date with 'platform/master'. $ platform snapshots Snapshots on the project ( ), environment Master (master): +---------------------------+----------------------------+----------+----------+---------+ | Created | Snapshot name | Progress | State | Result | +---------------------------+----------------------------+----------+----------+---------+ | 2019-03-11T15:16:16-04:00 | | 100% | complete | success | +---------------------------+----------------------------+----------+----------+---------+ 4. Restore the snapshot The snapshot can be restored to the original environment: $ platform snapshot:restore Are you sure you want to restore the snapshot from 2019-03-11T15:16:16-04:00 to environment Master (master)? [Y/n] Y Restoring snapshot to Master (master) Waiting for the restore to complete... Waiting for the activity tb7sf5gd3ivfw (User restored environment master from backup ): Provisioning certificates Environment certificates - certificate 7455422: expiring on 2019-06-09 17:53:35+00:00, covering master-7rqtwti- ..platformsh.site [============================] 35 secs (complete) The snapshot was successfully restored It can also be restored to another active branch. In this case, the target is the branch feature-x: platform snapshot:restore --target=feature-x 5. Verify the restoration was successful Check that the snapshot has been restored to the environment. $ curl https://master-7rqtwti- ..platformsh.site/ {\"mysql\":{\"return\":null,\"status\":\"OK\"},\"redis\":{\"return\":null,\"status\":\"OK\"}} $ $ curl https://feature-x-c2qo5ma- ..platformsh.site/ {\"mysql\":{\"return\":null,\"status\":\"OK\"},\"redis\":{\"return\":null,\"status\":\"OK\"}} Conclusion Once a snapshot is created, undesired changes can be reverted on an active branch using the http://Platform.sh CLI.",
        "text": "Goal Restore a https://platform.sh live environment from a created snapshot. Assumptions This guide requires: An application running on http://Platform.sh A local repository with the http://Platform.sh project as git remote The https://docs.platform.sh/gettingstarted/cli.html installed locally An https://docs.platform.sh/development/ssh.html configured on the project account admin role granted for the project This guide uses a https://github.com/platformsh/template-python3 for a http://Platform.sh application using Flask as well as MySQL and Redis for its services. The template runs a test on both MySQL and Redis and returns a status dictionary to the live page. Problems http://Platform.sh recommends that snapshots are created of the live environment before merging, or when the storage space of services is increased. For one reason or another, it may become necessary to restore an active environment to a previous working state using snapshots. Creation and restoration of snapshots can be executed from the UI and using the CLI. Additional information can be found in the https://pr-1044-6dxt2aq-t2llqeifuhpzg.eu.platform.sh/administration/snapshot-and-restore.html Steps 1. Create a snapshot The master environment is functioning as desired: $ curl https://master-7rqtwti- ..platformsh.site/ {\"mysql\":{\"return\":null,\"status\":\"OK\"},\"redis\":{\"return\":null,\"status\":\"OK\"}} From master create a snapshot using the http://Platform.sh CLI if one has not already been created. $ platform snapshot:create Creating a snapshot of master Waiting for the snapshot to complete... Waiting for the activity khlktlgy4r5uc (User created a backup of Master): Backing up master Backup name is [============================] 12 secs (complete) A snapshot of environment master has been created Snapshot name: 2. Changes were made and merged to Master In another branch dev, some changes were made. For the Python 3 template example, the Redis test was changed from 70 r.set(key_name, \"bar\") to 70 r.set(key_name, \"BEAR\") which will result in an Exception for the failed test. It was not caught in time, and dev was merged into master. $ platform environment:merge Now the live site is failing: $ curl https://master-7rqtwti- ..platformsh.site/ {\"mysql\":{\"return\":null,\"status\":\"OK\"},\"redis\":{\"error\":[\"Traceback (most recent call last):\\n\",\" File \\\"server.py\\\", line 30, in wrap_test\\n result = callback(*args, **kwargs)\\n\",\" File \\\"server.py\\\", line 71, in test_redis\\n assert value == r.get(key_name)\\n\",\"AssertionError\\n\"], \"status\":\"ERROR\"}} 3. Retrieve Snapshot Name Actions must take place from the branch the snapshot was taken from, and will not be available from dev. $ git checkout dev Switched to branch 'dev' $ platform snapshots No snapshots found List the saved snapshots to retrieve the snapshot name. $ git checkout master Switched to branch 'master' Your branch is up to date with 'platform/master'. $ platform snapshots Snapshots on the project ( ), environment Master (master): +---------------------------+----------------------------+----------+----------+---------+ | Created | Snapshot name | Progress | State | Result | +---------------------------+----------------------------+----------+----------+---------+ | 2019-03-11T15:16:16-04:00 | | 100% | complete | success | +---------------------------+----------------------------+----------+----------+---------+ 4. Restore the snapshot The snapshot can be restored to the original environment: $ platform snapshot:restore Are you sure you want to restore the snapshot from 2019-03-11T15:16:16-04:00 to environment Master (master)? [Y/n] Y Restoring snapshot to Master (master) Waiting for the restore to complete... Waiting for the activity tb7sf5gd3ivfw (User restored environment master from backup ): Provisioning certificates Environment certificates - certificate 7455422: expiring on 2019-06-09 17:53:35+00:00, covering master-7rqtwti- ..platformsh.site [============================] 35 secs (complete) The snapshot was successfully restored It can also be restored to another active branch. In this case, the target is the branch feature-x: platform snapshot:restore --target=feature-x 5. Verify the restoration was successful Check that the snapshot has been restored to the environment. $ curl https://master-7rqtwti- ..platformsh.site/ {\"mysql\":{\"return\":null,\"status\":\"OK\"},\"redis\":{\"return\":null,\"status\":\"OK\"}} $ $ curl https://feature-x-c2qo5ma- ..platformsh.site/ {\"mysql\":{\"return\":null,\"status\":\"OK\"},\"redis\":{\"return\":null,\"status\":\"OK\"}} Conclusion Once a snapshot is created, undesired changes can be reverted on an active branch using the http://Platform.sh CLI.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-create-and-restore-snapshots-using-the-cli/100",
        "relurl": "/t/how-to-create-and-restore-snapshots-using-the-cli/100"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "5cd45449528e68f5bda9c3ab122acb073c01abc3",
        "title": "How to interactively debug Python applications on Platform.sh",
        "description": "Goal Interactively debug python applications running on http://Platform.sh. Assumptions An active Python application running on http://Platform.sh A configured on the project account Familiarity with the https://docs.python.org/3/library/pdb.html Optional: a http://Platform.sh Django https://github.com/platformsh/template-django2 Problems Effectively debugging web applications isn’t trivial, especially when an HTTP request goes through multiple layers until it reaches the web app, which is usually the case in cloud platforms. Debugging in a local environment is easier, but the bug might only trigger when interacting with data present on production. Note: The debugging procedure should only be performed on a non-production environment. So, if necessary clone the production environment, with the data too if need be, depending on the preconditions that trigger the bug you’re investigating. The existence of bugs is sometimes manifested by an unhandled exception being raised and bubbling up to the top of your application, or in other cases, there are no exceptions, but the actions that a request handler is supposed to perform do not occur the way you’d expect. Let’s explore the different set of debugging tools we can use and learn when to use what. Steps (dropping in a REPL shell) Sometimes identifying a bug is possible by just looking at the stack trace of the exception or getting the values of some variables at some stack in the backtrace. This is exactly what the http://werkzeug.pocoo.org/docs/0.14/debug/ WSGI wrapper from the werkzeug library does. To use it: 1. Add the new werkzeug dependency. $ pipenv install werkzeug 2. Wrap and expose the WSGI application Add the script wsgi-debug.py to the project directory. It wraps our WSGI application with DebuggedApplication and exposes it. import werkzeug.debug # For the template, it is assumed wsgi.py is in project/myapp/, # edit to match your project accordingly. import myapp.wsgi application = werkzeug.debug.DebuggedApplication( myapp.wsgi.application, evalex=True, ) 3. Setup the debug script Add the script gunicorn-debug.sh that stops the “app” service in the environment and spawns a new gunicorn instance that’s pointed to the wrapped WSGI application. #!/usr/bin/env sh et -ex # Stop the application. sv stop app # When this script exits, start it again. trap \"exit 0\" INT trap \"sv start app\" EXIT # Run a gunicorn and point it towards the wrapped WSGI application. # It's important to run in sync mode, and a single worker, because werkzeug # DebuggedApplication doesn't like forking. # Set a high enough timeout that gives us time to debug. gunicorn \\ --worker-class sync \\ --workers 1 \\ --timeout 3600 \\ --bind unix:$SOCKET \\ wsgi-debug:application And make it executable. $ chmod +x gunicorn-debug 4. Allow exceptions to propagate Tell Django to let exceptions bubble up, by setting these in your myapp/settings.py file. DEBUG = False DEBUG_PROPAGATE_EXCEPTIONS = True 5. Commit and push these changes. 6. Run the script After the deployment succeeds, ssh into the environment and run the gunicorn-debug script. $ ./gunicorn-debug Now issue the request that makes it raise the unhandled exception, and lo and behold, a beautiful debugging interface appears. The stack trace of the exception can be seen when a line of code is hovered over, and a “shell” button appears at the right that will spawn a Python shell at that line where variables (or anything else can be run, for that matter) can be printed. Note: A PIN code is required to enable the shell, which is printed in the console of the gunicorn-debug script. This is for security reasons, to prevent unauthorized access in case it is accidentally enabled on production. Steps (Debug the application code step by step with pdb.) The previous approach will help to understand why an exception is raised so that it can be fixed, but if the bug occurs at an intermediate stage before the exception is raised, this will not be enough. The werkzeug debugging interface only captures the latest state of the app and the values of each variable (i.e., it cannot go back in time). To gain extra insight into what code path is entered when the request is handled, a pdb trace point can be set at some point in the code, and then interactively execute it line by line and inspect variables as you go. See the https://docs.python.org/3/library/pdb.html for more information. 1. Add a pdb trace While still running the application with our gunicorn-debug script, add a pdb trace (import pdb; pdb.set_trace()) at some point in the code, which can even be inside a conditional. For example, to test this methodology, the following can be included in a Django application’s views.py: if request.GET.get('pdb', False) == '1': import pdb; pdb.set_trace() 2. Use the PDB shell Issue a request that makes it reach the pdb.set_trace() and then code execution will pause and the PDB shell will appear in the terminal where the gunicorn-debug script is running. From there, pdb commands can be issued, for example, to execute code line by line, to print variables, or run any arbitrary code desired. Conclusion Debugging complex bugs that are only reproduced with the data that are present on production is non trivial, but thanks to development environments data cloning capabilities, and the dynamic nature of Python, we have great tools at our disposal to properly investigate these kind of bugs.",
        "text": "Goal Interactively debug python applications running on http://Platform.sh. Assumptions An active Python application running on http://Platform.sh A configured on the project account Familiarity with the https://docs.python.org/3/library/pdb.html Optional: a http://Platform.sh Django https://github.com/platformsh/template-django2 Problems Effectively debugging web applications isn’t trivial, especially when an HTTP request goes through multiple layers until it reaches the web app, which is usually the case in cloud platforms. Debugging in a local environment is easier, but the bug might only trigger when interacting with data present on production. Note: The debugging procedure should only be performed on a non-production environment. So, if necessary clone the production environment, with the data too if need be, depending on the preconditions that trigger the bug you’re investigating. The existence of bugs is sometimes manifested by an unhandled exception being raised and bubbling up to the top of your application, or in other cases, there are no exceptions, but the actions that a request handler is supposed to perform do not occur the way you’d expect. Let’s explore the different set of debugging tools we can use and learn when to use what. Steps (dropping in a REPL shell) Sometimes identifying a bug is possible by just looking at the stack trace of the exception or getting the values of some variables at some stack in the backtrace. This is exactly what the http://werkzeug.pocoo.org/docs/0.14/debug/ WSGI wrapper from the werkzeug library does. To use it: 1. Add the new werkzeug dependency. $ pipenv install werkzeug 2. Wrap and expose the WSGI application Add the script wsgi-debug.py to the project directory. It wraps our WSGI application with DebuggedApplication and exposes it. import werkzeug.debug # For the template, it is assumed wsgi.py is in project/myapp/, # edit to match your project accordingly. import myapp.wsgi application = werkzeug.debug.DebuggedApplication( myapp.wsgi.application, evalex=True, ) 3. Setup the debug script Add the script gunicorn-debug.sh that stops the “app” service in the environment and spawns a new gunicorn instance that’s pointed to the wrapped WSGI application. #!/usr/bin/env sh et -ex # Stop the application. sv stop app # When this script exits, start it again. trap \"exit 0\" INT trap \"sv start app\" EXIT # Run a gunicorn and point it towards the wrapped WSGI application. # It's important to run in sync mode, and a single worker, because werkzeug # DebuggedApplication doesn't like forking. # Set a high enough timeout that gives us time to debug. gunicorn \\ --worker-class sync \\ --workers 1 \\ --timeout 3600 \\ --bind unix:$SOCKET \\ wsgi-debug:application And make it executable. $ chmod +x gunicorn-debug 4. Allow exceptions to propagate Tell Django to let exceptions bubble up, by setting these in your myapp/settings.py file. DEBUG = False DEBUG_PROPAGATE_EXCEPTIONS = True 5. Commit and push these changes. 6. Run the script After the deployment succeeds, ssh into the environment and run the gunicorn-debug script. $ ./gunicorn-debug Now issue the request that makes it raise the unhandled exception, and lo and behold, a beautiful debugging interface appears. The stack trace of the exception can be seen when a line of code is hovered over, and a “shell” button appears at the right that will spawn a Python shell at that line where variables (or anything else can be run, for that matter) can be printed. Note: A PIN code is required to enable the shell, which is printed in the console of the gunicorn-debug script. This is for security reasons, to prevent unauthorized access in case it is accidentally enabled on production. Steps (Debug the application code step by step with pdb.) The previous approach will help to understand why an exception is raised so that it can be fixed, but if the bug occurs at an intermediate stage before the exception is raised, this will not be enough. The werkzeug debugging interface only captures the latest state of the app and the values of each variable (i.e., it cannot go back in time). To gain extra insight into what code path is entered when the request is handled, a pdb trace point can be set at some point in the code, and then interactively execute it line by line and inspect variables as you go. See the https://docs.python.org/3/library/pdb.html for more information. 1. Add a pdb trace While still running the application with our gunicorn-debug script, add a pdb trace (import pdb; pdb.set_trace()) at some point in the code, which can even be inside a conditional. For example, to test this methodology, the following can be included in a Django application’s views.py: if request.GET.get('pdb', False) == '1': import pdb; pdb.set_trace() 2. Use the PDB shell Issue a request that makes it reach the pdb.set_trace() and then code execution will pause and the PDB shell will appear in the terminal where the gunicorn-debug script is running. From there, pdb commands can be issued, for example, to execute code line by line, to print variables, or run any arbitrary code desired. Conclusion Debugging complex bugs that are only reproduced with the data that are present on production is non trivial, but thanks to development environments data cloning capabilities, and the dynamic nature of Python, we have great tools at our disposal to properly investigate these kind of bugs.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-interactively-debug-python-applications-on-platform-sh/170",
        "relurl": "/t/how-to-interactively-debug-python-applications-on-platform-sh/170"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "5624537d7fb8d641a1054fcd30396c335bf6d0b4",
        "title": "How to derive metrics from access log via ngxtop",
        "description": "Goal To derive various metrics from the Nginx access log via https://github.com/lebinh/ngxtop/. Assumptions To complete this, you will need: The https://docs.platform.sh/gettingstarted/cli.html tool installed The https://pip.pypa.io/en/stable/installing/ tool installed Problems Sometimes, you may find the need to derive metrics from the access log files and identify bad traffic. Using raw Linux commands to perform the analysis may not be efficient enough. Steps 1. Check ngxtop installation Make sure ngxtop is installed locally via the below command: pip install --user ngxtop 2. Get web traffic overview To gather an overview of recent web traffic, use below command: platform log access -p -e master --lines 102400 | ngxtop --no-follow Summary: | count | avg_bytes_sent | 2xx | 3xx | 4xx | 5xx | |---------+------------------+-------+-------+-------+-------| | 41553 | 6342.284 | 27380 | 6474 | 100 | 7599 | Detailed: | request_path | count | avg_bytes_sent | 2xx | 3xx | 4xx | 5xx | |-----------------------------------------------+---------+------------------+-------+-------+-------+-------| | /scripts/user-widget/dist/user-widget.min.js | 1190 | 29123.529 | 962 | 228 | 0 | 0 | | /gitbook/gitbook-plugin-codetabs/codetabs.js | 1185 | 186.957 | 961 | 224 | 0 | 0 | | /gitbook/style.css | 1185 | 8088.093 | 964 | 221 | 0 | 0 | | /gitbook/gitbook-plugin-edit-link/plugin.js | 1184 | 340.759 | 961 | 223 | 0 | 0 | | /gitbook/gitbook-plugin-gtm/plugin.js | 1184 | 199.188 | 960 | 224 | 0 | 0 | | /gitbook/gitbook-plugin-atoc/atoc.js | 1183 | 323.057 | 961 | 222 | 0 | 0 | | /gitbook/gitbook-plugin-highlight/website.css | 1181 | 2065.165 | 961 | 220 | 0 | 0 | | /gitbook/gitbook-plugin-reveal/reveal.js | 1181 | 229.190 | 958 | 223 | 0 | 0 | | /gitbook/gitbook.js | 1181 | 23890.862 | 959 | 222 | 0 | 0 | | /gitbook/theme.js | 1179 | 25528.468 | 957 | 222 | 0 | 0 | 3. View top visitors It is also possible to find the top visitor IP address and HTTP User Agent: $ platform log access -p -e master --lines 102400 | ngxtop --no-follow top remote_addr http_user_agent top remote_addr | remote_addr | count | |----------------+---------| | 92.60.188.221 | 6675 | | 179.33.202.58 | 586 | | 94.143.189.241 | 547 | | 81.200.189.9 | 461 | | 35.193.89.58 | 444 | | 82.255.18.24 | 313 | | 78.246.179.170 | 305 | | 151.16.42.51 | 302 | | 89.188.6.127 | 292 | | 84.43.189.112 | 285 | top http_user_agent | http_user_agent | count | |---------------------------------------------------------------------------------------------------------------------------+---------| | Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.21 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.21 | 6623 | | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36 | 4195 | | Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36 | 3563 | | Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36 | 2242 | | Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:65.0) Gecko/20100101 Firefox/65.0 | 1799 | | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36 | 1358 | | Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:65.0) Gecko/20100101 Firefox/65.0 | 1189 | | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36 | 1136 | | Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:65.0) Gecko/20100101 Firefox/65.0 | 1012 | | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36 | 973 | 4. Filter by IP address We could further filter the visits coming from IP address 92.60.188.221: $ platform log access -p -e master --lines 102400 | ngxtop --no-follow -i 'remote_addr == \"92.60.188.221\"' top request status http_user_agent top request | request | count | |--------------------------------------------------+---------| | GET /development/ HTTP/1.1 | 6 | | POST //index.php/api/xmlrpc HTTP/1.1 | 5 | | POST //xmlrpc HTTP/1.1 | 5 | | POST //xmlrpc.php HTTP/1.1 | 5 | | POST /gitbook/gitbook-plugin-edit-link/ HTTP/1.1 | 5 | | POST /gitbook/gitbook-plugin-gtm/ HTTP/1.1 | 5 | | GET /development/logs.html HTTP/1.1 | 4 | | GET /gettingstarted/tools.html HTTP/1.1 | 4 | | GET /styles/styles.css HTTP/1.1 | 4 | | POST //soap.php HTTP/1.1 | 4 | top status | status | count | |----------+---------| | 502 | 6338 | | 200 | 223 | | 403 | 69 | | 304 | 25 | | 405 | 20 | top http_user_agent | http_user_agent | count | |-------------------------------------------------------------------------------------------------------------+---------| | Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.21 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.21 | 6623 | | Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0 | 51 | | Googlebot/2.1 (+http://www.googlebot.com/bot.html) | 1 | From the above output, 92.60.188.221 is generating a lot of bad traffic and should be . Conclusion With the help of ngxtop and Platform CLI, it is easy to perform simple analysis on the Nginx access log and identify bad traffic.",
        "text": "Goal To derive various metrics from the Nginx access log via https://github.com/lebinh/ngxtop/. Assumptions To complete this, you will need: The https://docs.platform.sh/gettingstarted/cli.html tool installed The https://pip.pypa.io/en/stable/installing/ tool installed Problems Sometimes, you may find the need to derive metrics from the access log files and identify bad traffic. Using raw Linux commands to perform the analysis may not be efficient enough. Steps 1. Check ngxtop installation Make sure ngxtop is installed locally via the below command: pip install --user ngxtop 2. Get web traffic overview To gather an overview of recent web traffic, use below command: platform log access -p -e master --lines 102400 | ngxtop --no-follow Summary: | count | avg_bytes_sent | 2xx | 3xx | 4xx | 5xx | |---------+------------------+-------+-------+-------+-------| | 41553 | 6342.284 | 27380 | 6474 | 100 | 7599 | Detailed: | request_path | count | avg_bytes_sent | 2xx | 3xx | 4xx | 5xx | |-----------------------------------------------+---------+------------------+-------+-------+-------+-------| | /scripts/user-widget/dist/user-widget.min.js | 1190 | 29123.529 | 962 | 228 | 0 | 0 | | /gitbook/gitbook-plugin-codetabs/codetabs.js | 1185 | 186.957 | 961 | 224 | 0 | 0 | | /gitbook/style.css | 1185 | 8088.093 | 964 | 221 | 0 | 0 | | /gitbook/gitbook-plugin-edit-link/plugin.js | 1184 | 340.759 | 961 | 223 | 0 | 0 | | /gitbook/gitbook-plugin-gtm/plugin.js | 1184 | 199.188 | 960 | 224 | 0 | 0 | | /gitbook/gitbook-plugin-atoc/atoc.js | 1183 | 323.057 | 961 | 222 | 0 | 0 | | /gitbook/gitbook-plugin-highlight/website.css | 1181 | 2065.165 | 961 | 220 | 0 | 0 | | /gitbook/gitbook-plugin-reveal/reveal.js | 1181 | 229.190 | 958 | 223 | 0 | 0 | | /gitbook/gitbook.js | 1181 | 23890.862 | 959 | 222 | 0 | 0 | | /gitbook/theme.js | 1179 | 25528.468 | 957 | 222 | 0 | 0 | 3. View top visitors It is also possible to find the top visitor IP address and HTTP User Agent: $ platform log access -p -e master --lines 102400 | ngxtop --no-follow top remote_addr http_user_agent top remote_addr | remote_addr | count | |----------------+---------| | 92.60.188.221 | 6675 | | 179.33.202.58 | 586 | | 94.143.189.241 | 547 | | 81.200.189.9 | 461 | | 35.193.89.58 | 444 | | 82.255.18.24 | 313 | | 78.246.179.170 | 305 | | 151.16.42.51 | 302 | | 89.188.6.127 | 292 | | 84.43.189.112 | 285 | top http_user_agent | http_user_agent | count | |---------------------------------------------------------------------------------------------------------------------------+---------| | Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.21 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.21 | 6623 | | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36 | 4195 | | Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36 | 3563 | | Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36 | 2242 | | Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:65.0) Gecko/20100101 Firefox/65.0 | 1799 | | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36 | 1358 | | Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:65.0) Gecko/20100101 Firefox/65.0 | 1189 | | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36 | 1136 | | Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:65.0) Gecko/20100101 Firefox/65.0 | 1012 | | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36 | 973 | 4. Filter by IP address We could further filter the visits coming from IP address 92.60.188.221: $ platform log access -p -e master --lines 102400 | ngxtop --no-follow -i 'remote_addr == \"92.60.188.221\"' top request status http_user_agent top request | request | count | |--------------------------------------------------+---------| | GET /development/ HTTP/1.1 | 6 | | POST //index.php/api/xmlrpc HTTP/1.1 | 5 | | POST //xmlrpc HTTP/1.1 | 5 | | POST //xmlrpc.php HTTP/1.1 | 5 | | POST /gitbook/gitbook-plugin-edit-link/ HTTP/1.1 | 5 | | POST /gitbook/gitbook-plugin-gtm/ HTTP/1.1 | 5 | | GET /development/logs.html HTTP/1.1 | 4 | | GET /gettingstarted/tools.html HTTP/1.1 | 4 | | GET /styles/styles.css HTTP/1.1 | 4 | | POST //soap.php HTTP/1.1 | 4 | top status | status | count | |----------+---------| | 502 | 6338 | | 200 | 223 | | 403 | 69 | | 304 | 25 | | 405 | 20 | top http_user_agent | http_user_agent | count | |-------------------------------------------------------------------------------------------------------------+---------| | Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.21 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.21 | 6623 | | Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0 | 51 | | Googlebot/2.1 (+http://www.googlebot.com/bot.html) | 1 | From the above output, 92.60.188.221 is generating a lot of bad traffic and should be . Conclusion With the help of ngxtop and Platform CLI, it is easy to perform simple analysis on the Nginx access log and identify bad traffic.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-derive-metrics-from-access-log-via-ngxtop/122",
        "relurl": "/t/how-to-derive-metrics-from-access-log-via-ngxtop/122"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "594a3e03bdd259677ae1b35b7946e6d1ae3ebb4e",
        "title": "How to forward Platform.sh logs to Logz.io",
        "description": "Goal To forward http://Platform.sh logs to http://Logz.io. Assumptions An active http://platform.sh/ https://github.com/platformsh/platformsh-cli tool installed locally https://docs.platform.sh/development/ssh.html configured with the project account An active http://Logz.io account Problems Forwarding http://Platform.sh to a third-party indexer such as http://Logz.io is useful, especially when integrating log monitoring across other projects, including those that are not hosted on http://Platform.sh. http://Logz.io uses Filebeat to create configuration, auth and forwarding files when shipping logs, which must be taken into account on http://Platform.sh’s read-only file system. http://Logz.io recommends using Filebeat to forward logs to it, however there are a number of other options outlined in the “Log Forwarding” section on the http://Logz.io dashboard. The following How-to is written to ship logs from a Python application, but the steps do not require that. More information regarding accessing http://Platform.sh logs can be found in the https://docs.platform.sh/development/logs.html . Steps 1. Configure routes and services The project locally will ultimately have the following structure: ├── .platform │ ├── local │ │ ├── .gitignore │ │ ├── README.txt │ │ └── project.yaml │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml ├── Pipfile ├── Pipfile.lock ├── README.md ├── config │ └── filebeat │ └── scripts │ ├── config.sh │ └── install.sh ├── filebeat.yml └── server.py The project is a simple “Hello, world!” application using Flask. Routes must be configured in the .platform/routes.yaml: # .platform/routes.yaml https://{default}/: type: upstream upstream: \"app:http\" as well as its services in .platform/services.yaml, which is defined here with a single MySQL/MariaDB service: # .platform/services.yaml mysql: type: \"mysql:10.2\" disk: 1024 2. Set up the http://Platform.sh Application configuration Modify .platform.app.yaml to define writable mounts for https://www.elastic.co/products/beats/filebeat: # .platform.app.yaml mounts: '/.filebeat': source: local source_path: filebeat https://docs.platform.sh/configuration/app/storage.html are utilized in this configuration because after Filebeat is installed, it will attempt to write files to the project so that logs can be forwarded to http://Logz.io. Since http://Platform.sh is designed to provide a read-only file system, mounts can be used for this purpose. Modify the build hook: # .platform.app.yaml hooks: build: | if [ ! -z $LOGZ_CONFIG ]; then ./config/filebeat/scripts/install.sh fi pipenv install --system --deploy This section runs an install script (defined below) that installs Filebeat if it does not detect a project variable that we will define after the installation has completed. Modify the deploy hook: deploy: | if [ ! \"$(ls -A filebeat)\" ]; then ./config/filebeat/scripts/config.sh fi ./filebeat/filebeat run --once The deploy hook runs a configuration script on the installation if the filebeat directory is empty. The final command runs Filebeat until the logs are up to date with those previously shipped and then forwards them to http://Logz.io on every deployment. 3. Install Filebeat In config/filebeat/scripts/ modify a script install.sh: # config/filebeat/scripts/install.sh #!/usr/bin/env bash TEMP_SPLUNK_HOME=config/splunk/build # Install Splunk Universal Forwarder #!/usr/bin/env bash TEMP_BEAT_HOME=config/filebeat/build [ ! -d $TEMP_BEAT_HOME ] \u0026\u0026 mkdir -p $TEMP_BEAT_HOME cd $TEMP_BEAT_HOME # Install Filebeat curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.7.0-linux-x86_64.tar.gz tar xzvf filebeat-6.7.0-linux-x86_64.tar.gz rm filebeat-6.7.0-linux-x86_64.tar.gz # Download the certificate wget https://raw.githubusercontent.com/logzio/public-certificates/master/COMODORSADomainValidationSecureServerCA.crt mkdir -p filebeat-6.7.0-linux-x86_64/pki/tls/certs cp COMODORSADomainValidationSecureServerCA.crt filebeat-6.7.0-linux-x86_64/pki/tls/certs/ The script will define a temporary location within the config/filebeat subdirectory to install Filebeat. During the build phase mounts are not yet available, so this becomes a necessary step before the mounts are fully defined. The initial configuration requires that a certificate is downloaded along with the installation and placed in the appropriate directory. Details regarding this step are available on the Filebeat steps of the Log Forwarding section of the dashboard. Make sure to replace the installation link with the most recent version listed on the Filebeat downloads page. 4. Move installation to mount point Write a config.sh script within config/filebeat/scripts that will be used to control the initial configuration: # config/filebeat/scripts/config.sh #!/usr/bin/env bash # Move filebeat to mount with write access cd $PLATFORM_HOME cp -v -r config/filebeat/build/filebeat-6.7.0-linux-x86_64/* filebeat mkdir filebeat/registry This script is run in the deploy hook when the mounts defined in .platform.app.yaml are now available, so Filebeat can be moved from the temporary installation directory config/filebeat/build/ to app/filebeat. 5. Finish the configuration The main configuration file for Filebeat is filebeat.yml, which should be placed in the application directory outside of filebeat. On http://Logz.io within the configuration steps for Filebeat, the Wizard will construct this file based on the desired inputs that are given in its fields. An example for forwarding all file in /var/log that end with .log # filebeat.yml ############################# Filebeat ##################################### filebeat.inputs: - type: log paths: - /var/log/*.log fields: logzio_codec: plain token: type: Platform.sh fields_under_root: true encoding: utf-8 ignore_older: 3h registry_file: /filebeat/registry ############################# Output ########################################## output: logstash: hosts: [\"listener.logz.io:5015\"] ssl: certificate_authorities: [\"/app/filebeat/pki/tls/certs/COMODORSADomainValidationSecureServerCA.crt\"] filebeat.yml configures both the forwarder’s inputs and its output location using Logstash to http://Logz.io. 6. Push to http://Platform.sh Commit the changes and push to the http://Platform.sh remote: $ git push platform Filebeat will be installed during the build hook and configured to http://Logz.io during the deploy hook. Each time Filebeat is run at the end of each deployment, it will detect changes to the log files within /var/log and ship those differences to http://Logz.io. 7. Define a project variable To ensure that the installation in the build hook does not run on every build, to the project that will be visible during the build process that will keep this from happening. $ platform variable:create --level project --name LOGZ_CONFIG --value 'true' This step can be automated by https://community.platform.sh/t/how-to-use-the-platform-sh-cli-from-the-web-container/126 itself, although it will force a redeploy of the application when the variable is added. Conclusion http://Platform.sh is set up to provide a read-only file system for application containers so that consistent builds and deployments are ensured. Utilizing writable mounts however provides a simple way of setting up Filebeat to ship http://Platform.sh logs to http://Logz.io. Note: if at any time you would like to remove Splunk from your application configuration, make sure to manually remove the associated files from each mount. Simply removing the mount will not eradicate the files on it and they will exist on disk until . Remember to remove the project variable created as well if keeping the build hook described above. ",
        "text": "Goal To forward http://Platform.sh logs to http://Logz.io. Assumptions An active http://platform.sh/ https://github.com/platformsh/platformsh-cli tool installed locally https://docs.platform.sh/development/ssh.html configured with the project account An active http://Logz.io account Problems Forwarding http://Platform.sh to a third-party indexer such as http://Logz.io is useful, especially when integrating log monitoring across other projects, including those that are not hosted on http://Platform.sh. http://Logz.io uses Filebeat to create configuration, auth and forwarding files when shipping logs, which must be taken into account on http://Platform.sh’s read-only file system. http://Logz.io recommends using Filebeat to forward logs to it, however there are a number of other options outlined in the “Log Forwarding” section on the http://Logz.io dashboard. The following How-to is written to ship logs from a Python application, but the steps do not require that. More information regarding accessing http://Platform.sh logs can be found in the https://docs.platform.sh/development/logs.html . Steps 1. Configure routes and services The project locally will ultimately have the following structure: ├── .platform │ ├── local │ │ ├── .gitignore │ │ ├── README.txt │ │ └── project.yaml │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml ├── Pipfile ├── Pipfile.lock ├── README.md ├── config │ └── filebeat │ └── scripts │ ├── config.sh │ └── install.sh ├── filebeat.yml └── server.py The project is a simple “Hello, world!” application using Flask. Routes must be configured in the .platform/routes.yaml: # .platform/routes.yaml https://{default}/: type: upstream upstream: \"app:http\" as well as its services in .platform/services.yaml, which is defined here with a single MySQL/MariaDB service: # .platform/services.yaml mysql: type: \"mysql:10.2\" disk: 1024 2. Set up the http://Platform.sh Application configuration Modify .platform.app.yaml to define writable mounts for https://www.elastic.co/products/beats/filebeat: # .platform.app.yaml mounts: '/.filebeat': source: local source_path: filebeat https://docs.platform.sh/configuration/app/storage.html are utilized in this configuration because after Filebeat is installed, it will attempt to write files to the project so that logs can be forwarded to http://Logz.io. Since http://Platform.sh is designed to provide a read-only file system, mounts can be used for this purpose. Modify the build hook: # .platform.app.yaml hooks: build: | if [ ! -z $LOGZ_CONFIG ]; then ./config/filebeat/scripts/install.sh fi pipenv install --system --deploy This section runs an install script (defined below) that installs Filebeat if it does not detect a project variable that we will define after the installation has completed. Modify the deploy hook: deploy: | if [ ! \"$(ls -A filebeat)\" ]; then ./config/filebeat/scripts/config.sh fi ./filebeat/filebeat run --once The deploy hook runs a configuration script on the installation if the filebeat directory is empty. The final command runs Filebeat until the logs are up to date with those previously shipped and then forwards them to http://Logz.io on every deployment. 3. Install Filebeat In config/filebeat/scripts/ modify a script install.sh: # config/filebeat/scripts/install.sh #!/usr/bin/env bash TEMP_SPLUNK_HOME=config/splunk/build # Install Splunk Universal Forwarder #!/usr/bin/env bash TEMP_BEAT_HOME=config/filebeat/build [ ! -d $TEMP_BEAT_HOME ] \u0026\u0026 mkdir -p $TEMP_BEAT_HOME cd $TEMP_BEAT_HOME # Install Filebeat curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.7.0-linux-x86_64.tar.gz tar xzvf filebeat-6.7.0-linux-x86_64.tar.gz rm filebeat-6.7.0-linux-x86_64.tar.gz # Download the certificate wget https://raw.githubusercontent.com/logzio/public-certificates/master/COMODORSADomainValidationSecureServerCA.crt mkdir -p filebeat-6.7.0-linux-x86_64/pki/tls/certs cp COMODORSADomainValidationSecureServerCA.crt filebeat-6.7.0-linux-x86_64/pki/tls/certs/ The script will define a temporary location within the config/filebeat subdirectory to install Filebeat. During the build phase mounts are not yet available, so this becomes a necessary step before the mounts are fully defined. The initial configuration requires that a certificate is downloaded along with the installation and placed in the appropriate directory. Details regarding this step are available on the Filebeat steps of the Log Forwarding section of the dashboard. Make sure to replace the installation link with the most recent version listed on the Filebeat downloads page. 4. Move installation to mount point Write a config.sh script within config/filebeat/scripts that will be used to control the initial configuration: # config/filebeat/scripts/config.sh #!/usr/bin/env bash # Move filebeat to mount with write access cd $PLATFORM_HOME cp -v -r config/filebeat/build/filebeat-6.7.0-linux-x86_64/* filebeat mkdir filebeat/registry This script is run in the deploy hook when the mounts defined in .platform.app.yaml are now available, so Filebeat can be moved from the temporary installation directory config/filebeat/build/ to app/filebeat. 5. Finish the configuration The main configuration file for Filebeat is filebeat.yml, which should be placed in the application directory outside of filebeat. On http://Logz.io within the configuration steps for Filebeat, the Wizard will construct this file based on the desired inputs that are given in its fields. An example for forwarding all file in /var/log that end with .log # filebeat.yml ############################# Filebeat ##################################### filebeat.inputs: - type: log paths: - /var/log/*.log fields: logzio_codec: plain token: type: Platform.sh fields_under_root: true encoding: utf-8 ignore_older: 3h registry_file: /filebeat/registry ############################# Output ########################################## output: logstash: hosts: [\"listener.logz.io:5015\"] ssl: certificate_authorities: [\"/app/filebeat/pki/tls/certs/COMODORSADomainValidationSecureServerCA.crt\"] filebeat.yml configures both the forwarder’s inputs and its output location using Logstash to http://Logz.io. 6. Push to http://Platform.sh Commit the changes and push to the http://Platform.sh remote: $ git push platform Filebeat will be installed during the build hook and configured to http://Logz.io during the deploy hook. Each time Filebeat is run at the end of each deployment, it will detect changes to the log files within /var/log and ship those differences to http://Logz.io. 7. Define a project variable To ensure that the installation in the build hook does not run on every build, to the project that will be visible during the build process that will keep this from happening. $ platform variable:create --level project --name LOGZ_CONFIG --value 'true' This step can be automated by https://community.platform.sh/t/how-to-use-the-platform-sh-cli-from-the-web-container/126 itself, although it will force a redeploy of the application when the variable is added. Conclusion http://Platform.sh is set up to provide a read-only file system for application containers so that consistent builds and deployments are ensured. Utilizing writable mounts however provides a simple way of setting up Filebeat to ship http://Platform.sh logs to http://Logz.io. Note: if at any time you would like to remove Splunk from your application configuration, make sure to manually remove the associated files from each mount. Simply removing the mount will not eradicate the files on it and they will exist on disk until . Remember to remove the project variable created as well if keeping the build hook described above. ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-forward-platform-sh-logs-to-logz-io/197",
        "relurl": "/t/how-to-forward-platform-sh-logs-to-logz-io/197"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "778e3414dd1ec2d4702da4545a1df0a7a1d5ea96",
        "title": "How to forward Platform.sh logs to a Splunk Indexer",
        "description": "Goal To forward http://Platform.sh logs into a Splunk indexer. Assumptions An active http://platform.sh/ project https://github.com/platformsh/platformsh-cli tool installed locally https://docs.platform.sh/development/ssh.html configured with the project account A Splunk instance installed on the indexing server that is configured to receive logs on a specified port (default 9997) and a configured admin login. Problems Forwarding http://Platform.sh to a third-party indexer such as Splunk is useful, especially when integrating log monitoring across other projects, including those that are not hosted on http://Platform.sh. Splunk creates configuration, auth and forwarding files when shipping logs, which must be taken into account on http://Platform.sh’s read-only file system. Splunk log forwarding is, in its simplest configuration, dependent on configuring two components: a Receiver/Indexer and a Forwarder. The Forwarder will be installed on a writable mount on a http://Platform.sh project and is responsible for collecting and shipping the desired log files, whereas the Indexer receives those files and provides a dashboard for indexing, searching, and analyzing them. Splunk actions can be executed either through that dashboard or through its CLI. The following How-to is written to ship logs from a Python application, but the steps do not require that. More information regarding accessing http://Platform.sh logs can be found in the https://docs.platform.sh/development/logs.html . Steps (Splunk Indexer) This How-to is based on installing Splunk Enterprise on a local machine, but an Indexer can be created using another Splunk product that is set-up remotely. Consult the https://docs.splunk.com/Documentation to determine the correct Indexer installation. A Splunk receiver has to be enabled to receive forwarded logs on a specified port. By default, the dedicated port for Splunk indexer’s is 9997, and it can be enabled to listen on that port either through the dashboard or using the CLI. Consult the Splunk documentation for full details on how to do that: https://docs.splunk.com/Documentation/Forwarder/7.2.5/Forwarder/Enableareceiver . Steps (Splunk Universal Forwarder) 1. Configure routes and services The project locally will ultimately have the following structure: . ├── .platform │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml ├── Pipfile ├── Pipfile.lock ├── README.md ├── config │ └── splunk │ ├── scripts │ │ ├── config.sh │ │ ├── install.sh │ │ └── uninstall.sh │ └── seeds │ ├── inputs.conf │ ├── outputs.conf │ ├── splunk-launch.conf │ └── user.conf └── server.py The project is a simple “Hello, world!” application using Flask. Routes must be configured in the .platform/routes.yaml: # .platform/routes.yaml https://{default}/: type: upstream upstream: \"app:http\" as well as its services in .platform/services.yaml, which is defined here with a single MySQL/MariaDB service: # .platform/services.yaml mysql: type: \"mysql:10.2\" disk: 1024 2. Set up the http://Platform.sh Application configuration Modify .platform.app.yaml to define writable mounts for the https://docs.splunk.com/Documentation/Forwarder/7.2.5/Forwarder/Abouttheuniversalforwarder : # .platform.app.yaml mounts: 'splunk': source: local source_path: splunk '/.splunk': source: local source_path: splauths https://docs.platform.sh/configuration/app/storage.html are utilized in this configuration because after the Splunk Universal Forwarder is installed, Splunk will still attempt to write files to the project so that logs can be forwarded to the indexer. Since http://Platform.sh is designed to provide a read-only file system, mounts can be used for this purpose. Splunk writes modified log files and authorization files when a connection is made to an indexer, and those files will be written in the directories splunk and .splunk respectively. Modify the build hook: # .platform.app.yaml hooks: build: | if [ ! -z $SPLUNK_CONFIG ]; then ./config/splunk/scripts/install.sh fi pipenv install --system --deploy This section runs an install script (defined below) that installs the Forwarder if it does not detect a project variable that we will define after the installation has completed. Modify the deploy hook: deploy: | if [ ! \"$(ls -A splunk)\" ]; then ./config/splunk/scripts/config.sh fi ./splunk/splunkforwarder/bin/splunk restart The deploy hook checks to see if the splunk directory is empty, and runs a configuration script on the Forwarder if it is. The final command actually restarts the Forwarder on every deploy and is used to ship the logs to the Indexer. 3. Installing the Splunk Universal Forwarder In config/splunk/scripts/ modify a script install.sh: # config/splunk/scripts/install.sh #!/usr/bin/env bash TEMP_SPLUNK_HOME=config/splunk/build # Install Splunk Universal Forwarder [ ! -d $TEMP_SPLUNK_HOME ] \u0026\u0026 mkdir -p $TEMP_SPLUNK_HOME cd $TEMP_SPLUNK_HOME wget -O splunkforwarder-7.2.5.1-962d9a8e1586-Linux-x86_64.tgz 'https://www.splunk.com/bin/splunk/DownloadActivityServlet?architecture=x86_64\u0026platform=linux\u0026version=7.2.5.1\u0026product=universalforwarder\u0026filename=splunkforwarder-7.2.5.1-962d9a8e1586-Linux-x86_64.tgz\u0026wget=true' tar xvzf splunkforwarder-7.2.5.1-962d9a8e1586-Linux-x86_64.tgz rm splunkforwarder-7.2.5.1-962d9a8e1586-Linux-x86_64.tgz The script will define a temporary location within the config/splunk subdirectory to install the Splunk forwarder. During the build phase mounts are not yet available, so this becomes a necessary step before the mounts are fully defined. Make sure to replace the installation link with the most recent version listed on https://www.splunk.com/en_us/download/universal-forwarder.html . 4. Configuring the Splunk Universal Forwarder Write a config.sh script within config/splunk/scripts that will be used to control the initial configuration: # config/splunk/scripts/config.sh #!/usr/bin/env bash cd $PLATFORM_APP_DIR TEMP_SPLUNK_HOME=config/splunk/build/* SPLUNK_HOME=$PLATFORM_APP_DIR/splunk/splunkforwarder # Copy temp build to writable storage cp -v -r $TEMP_SPLUNK_HOME splunk # Migrate used-seed.conf to the forwarder cp -v config/splunk/seeds/user.conf $SPLUNK_HOME/etc/system/local/user-seed.conf # Start Splunk for the first time, accepting license ./splunk/splunkforwarder/bin/splunk start --accept-license # Update outputs.conf with receiver address seed cp -v $PLATFORM_APP_DIR/config/splunk/seeds/outputs.conf $PLATFORM_APP_DIR/splunk/splunkforwarder/etc/system/local/outputs.conf # Update inputs.conf with monitor inputs seed cp -v $PLATFORM_APP_DIR/config/splunk/seeds/inputs.conf $PLATFORM_APP_DIR/splunk/splunkforwarder/etc/system/local/inputs.conf This script is run in the deploy hook when the mounts defined in .platform.app.yaml are now available, so the forwarder can be moved from the temporary installation directory config/splunk/build/ to app/splunk. After that, the installation is set-up with a collection of configuration seed files located in config/splunk/seeds that are also moved to the mounted installation. The first user.conf is a user seed that sets up an admin user for the forwarder. There is already one set up upon install, but unless another admin password is set up, the CLI will not be accessible remotely. # config/splunk/seeds/user.conf [user_info] USERNAME = admin PASSWORD = testpass Once the user seed is moved to the final mounted installation location, the admin credentials can be passed to the Splunk CLI to accept its license and start it for the first time. Next, an outputs.conf seeds the outputs configuration for the forwarder, which identifies the IP address and listening port for the receiver. # config/splunk/seeds/outputs.conf [tcpout] defaultGroup=default [tcpout:default] server= :9997 [tcpout-server:// :9997] Then an inputs.conf seeds the inputs for the forwarder. In this case, the entire var/logs http://Platform.sh log directory is added to the forwarder’s list of inputs. # config/splunk/seeds/inputs.conf [monitor://var/log/] disabled = false Like the admin user seed, outputs.conf and inputs.conf are moved to the final installation directory splunk. 5. Push to http://Platform.sh Commit the changes and push to the http://Platform.sh remote: $ git push platform The Forwarder will be installed during the build hook and configured to the indexer during the deploy hook. At the end of the deploy hook the Splunk forwarder will be restarted during each deployment. Each time it is restarted, it will detect changes to the log files within /var/log and ship those differences to the indexer. 6. Final Configuration The installation is finished, and now that the application has successfully deployed and the indexer is listening on the configured port, logs will be sent there on every deployment. To ensure that the forwarder installation in the build hook does not run on every build, to the project that will be visible during the build process that will keep this from happening. $ platform variable:create --level project --name SPLUNK_CONFIG --value 'true' This step can be automated by https://community.platform.sh/t/how-to-use-the-platform-sh-cli-from-the-web-container/126 itself, although it will force a redeploy of the application when the variable is added. SSH into the environment to modify the admin credentials or add new users using the https://docs.splunk.com/Documentation/Splunk/latest/Admin/CLIadmincommands . Conclusion http://Platform.sh is set up to provide a read-only file system for application containers so that consistent builds and deployments are ensured. Utilizing writable mounts however provides a simple way of setting up a Splunk Universal Forwarder to ship http://Platform.sh logs with an Indexer. Note: if at any time you would like to remove Splunk from your application configuration, make sure to manually remove the associated files from each mount. Simply removing the mount will not eradicate the files on it and they will exist on disk until . Remember to remove the project variable created as well if keeping the build hook described above. ",
        "text": "Goal To forward http://Platform.sh logs into a Splunk indexer. Assumptions An active http://platform.sh/ project https://github.com/platformsh/platformsh-cli tool installed locally https://docs.platform.sh/development/ssh.html configured with the project account A Splunk instance installed on the indexing server that is configured to receive logs on a specified port (default 9997) and a configured admin login. Problems Forwarding http://Platform.sh to a third-party indexer such as Splunk is useful, especially when integrating log monitoring across other projects, including those that are not hosted on http://Platform.sh. Splunk creates configuration, auth and forwarding files when shipping logs, which must be taken into account on http://Platform.sh’s read-only file system. Splunk log forwarding is, in its simplest configuration, dependent on configuring two components: a Receiver/Indexer and a Forwarder. The Forwarder will be installed on a writable mount on a http://Platform.sh project and is responsible for collecting and shipping the desired log files, whereas the Indexer receives those files and provides a dashboard for indexing, searching, and analyzing them. Splunk actions can be executed either through that dashboard or through its CLI. The following How-to is written to ship logs from a Python application, but the steps do not require that. More information regarding accessing http://Platform.sh logs can be found in the https://docs.platform.sh/development/logs.html . Steps (Splunk Indexer) This How-to is based on installing Splunk Enterprise on a local machine, but an Indexer can be created using another Splunk product that is set-up remotely. Consult the https://docs.splunk.com/Documentation to determine the correct Indexer installation. A Splunk receiver has to be enabled to receive forwarded logs on a specified port. By default, the dedicated port for Splunk indexer’s is 9997, and it can be enabled to listen on that port either through the dashboard or using the CLI. Consult the Splunk documentation for full details on how to do that: https://docs.splunk.com/Documentation/Forwarder/7.2.5/Forwarder/Enableareceiver . Steps (Splunk Universal Forwarder) 1. Configure routes and services The project locally will ultimately have the following structure: . ├── .platform │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml ├── Pipfile ├── Pipfile.lock ├── README.md ├── config │ └── splunk │ ├── scripts │ │ ├── config.sh │ │ ├── install.sh │ │ └── uninstall.sh │ └── seeds │ ├── inputs.conf │ ├── outputs.conf │ ├── splunk-launch.conf │ └── user.conf └── server.py The project is a simple “Hello, world!” application using Flask. Routes must be configured in the .platform/routes.yaml: # .platform/routes.yaml https://{default}/: type: upstream upstream: \"app:http\" as well as its services in .platform/services.yaml, which is defined here with a single MySQL/MariaDB service: # .platform/services.yaml mysql: type: \"mysql:10.2\" disk: 1024 2. Set up the http://Platform.sh Application configuration Modify .platform.app.yaml to define writable mounts for the https://docs.splunk.com/Documentation/Forwarder/7.2.5/Forwarder/Abouttheuniversalforwarder : # .platform.app.yaml mounts: 'splunk': source: local source_path: splunk '/.splunk': source: local source_path: splauths https://docs.platform.sh/configuration/app/storage.html are utilized in this configuration because after the Splunk Universal Forwarder is installed, Splunk will still attempt to write files to the project so that logs can be forwarded to the indexer. Since http://Platform.sh is designed to provide a read-only file system, mounts can be used for this purpose. Splunk writes modified log files and authorization files when a connection is made to an indexer, and those files will be written in the directories splunk and .splunk respectively. Modify the build hook: # .platform.app.yaml hooks: build: | if [ ! -z $SPLUNK_CONFIG ]; then ./config/splunk/scripts/install.sh fi pipenv install --system --deploy This section runs an install script (defined below) that installs the Forwarder if it does not detect a project variable that we will define after the installation has completed. Modify the deploy hook: deploy: | if [ ! \"$(ls -A splunk)\" ]; then ./config/splunk/scripts/config.sh fi ./splunk/splunkforwarder/bin/splunk restart The deploy hook checks to see if the splunk directory is empty, and runs a configuration script on the Forwarder if it is. The final command actually restarts the Forwarder on every deploy and is used to ship the logs to the Indexer. 3. Installing the Splunk Universal Forwarder In config/splunk/scripts/ modify a script install.sh: # config/splunk/scripts/install.sh #!/usr/bin/env bash TEMP_SPLUNK_HOME=config/splunk/build # Install Splunk Universal Forwarder [ ! -d $TEMP_SPLUNK_HOME ] \u0026\u0026 mkdir -p $TEMP_SPLUNK_HOME cd $TEMP_SPLUNK_HOME wget -O splunkforwarder-7.2.5.1-962d9a8e1586-Linux-x86_64.tgz 'https://www.splunk.com/bin/splunk/DownloadActivityServlet?architecture=x86_64\u0026platform=linux\u0026version=7.2.5.1\u0026product=universalforwarder\u0026filename=splunkforwarder-7.2.5.1-962d9a8e1586-Linux-x86_64.tgz\u0026wget=true' tar xvzf splunkforwarder-7.2.5.1-962d9a8e1586-Linux-x86_64.tgz rm splunkforwarder-7.2.5.1-962d9a8e1586-Linux-x86_64.tgz The script will define a temporary location within the config/splunk subdirectory to install the Splunk forwarder. During the build phase mounts are not yet available, so this becomes a necessary step before the mounts are fully defined. Make sure to replace the installation link with the most recent version listed on https://www.splunk.com/en_us/download/universal-forwarder.html . 4. Configuring the Splunk Universal Forwarder Write a config.sh script within config/splunk/scripts that will be used to control the initial configuration: # config/splunk/scripts/config.sh #!/usr/bin/env bash cd $PLATFORM_APP_DIR TEMP_SPLUNK_HOME=config/splunk/build/* SPLUNK_HOME=$PLATFORM_APP_DIR/splunk/splunkforwarder # Copy temp build to writable storage cp -v -r $TEMP_SPLUNK_HOME splunk # Migrate used-seed.conf to the forwarder cp -v config/splunk/seeds/user.conf $SPLUNK_HOME/etc/system/local/user-seed.conf # Start Splunk for the first time, accepting license ./splunk/splunkforwarder/bin/splunk start --accept-license # Update outputs.conf with receiver address seed cp -v $PLATFORM_APP_DIR/config/splunk/seeds/outputs.conf $PLATFORM_APP_DIR/splunk/splunkforwarder/etc/system/local/outputs.conf # Update inputs.conf with monitor inputs seed cp -v $PLATFORM_APP_DIR/config/splunk/seeds/inputs.conf $PLATFORM_APP_DIR/splunk/splunkforwarder/etc/system/local/inputs.conf This script is run in the deploy hook when the mounts defined in .platform.app.yaml are now available, so the forwarder can be moved from the temporary installation directory config/splunk/build/ to app/splunk. After that, the installation is set-up with a collection of configuration seed files located in config/splunk/seeds that are also moved to the mounted installation. The first user.conf is a user seed that sets up an admin user for the forwarder. There is already one set up upon install, but unless another admin password is set up, the CLI will not be accessible remotely. # config/splunk/seeds/user.conf [user_info] USERNAME = admin PASSWORD = testpass Once the user seed is moved to the final mounted installation location, the admin credentials can be passed to the Splunk CLI to accept its license and start it for the first time. Next, an outputs.conf seeds the outputs configuration for the forwarder, which identifies the IP address and listening port for the receiver. # config/splunk/seeds/outputs.conf [tcpout] defaultGroup=default [tcpout:default] server= :9997 [tcpout-server:// :9997] Then an inputs.conf seeds the inputs for the forwarder. In this case, the entire var/logs http://Platform.sh log directory is added to the forwarder’s list of inputs. # config/splunk/seeds/inputs.conf [monitor://var/log/] disabled = false Like the admin user seed, outputs.conf and inputs.conf are moved to the final installation directory splunk. 5. Push to http://Platform.sh Commit the changes and push to the http://Platform.sh remote: $ git push platform The Forwarder will be installed during the build hook and configured to the indexer during the deploy hook. At the end of the deploy hook the Splunk forwarder will be restarted during each deployment. Each time it is restarted, it will detect changes to the log files within /var/log and ship those differences to the indexer. 6. Final Configuration The installation is finished, and now that the application has successfully deployed and the indexer is listening on the configured port, logs will be sent there on every deployment. To ensure that the forwarder installation in the build hook does not run on every build, to the project that will be visible during the build process that will keep this from happening. $ platform variable:create --level project --name SPLUNK_CONFIG --value 'true' This step can be automated by https://community.platform.sh/t/how-to-use-the-platform-sh-cli-from-the-web-container/126 itself, although it will force a redeploy of the application when the variable is added. SSH into the environment to modify the admin credentials or add new users using the https://docs.splunk.com/Documentation/Splunk/latest/Admin/CLIadmincommands . Conclusion http://Platform.sh is set up to provide a read-only file system for application containers so that consistent builds and deployments are ensured. Utilizing writable mounts however provides a simple way of setting up a Splunk Universal Forwarder to ship http://Platform.sh logs with an Indexer. Note: if at any time you would like to remove Splunk from your application configuration, make sure to manually remove the associated files from each mount. Simply removing the mount will not eradicate the files on it and they will exist on disk until . Remember to remove the project variable created as well if keeping the build hook described above. ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-forward-platform-sh-logs-to-a-splunk-indexer/196",
        "relurl": "/t/how-to-forward-platform-sh-logs-to-a-splunk-indexer/196"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "42abc4ced5b254bc3aa42cdffc3e6596076a5fff",
        "title": "How to run R Shiny on Platform.sh",
        "description": "Goal To run R Shiny on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account An empty http://Platform.sh project Problems http://Platform.sh offers many different language runtimes by default, but some applications require different environments, such as data science applications which use R Shiny. By installing the Miniconda Python package manager, it is possible to run R applications, such as R Shiny, on http://Platform.sh. Steps 1. Set git remote to http://Platform.sh project $ git init $ platform project:set-remote 2. Create an installation script file The http://Platform.sh build hook runs Dash, not Bash, so the installation script is executed separately in install_r.sh: #!/bin/bash # Download the latest Miniconda3 release and name the file `conda.sh` curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o conda.sh # Run the downloaded `conda.sh` script using bash # — Use the `-b` flag to run the installer in batch mode # — Use the `-p` flag to specify where the package manager will actually be installed bash conda.sh -b -p $PLATFORM_APP_DIR/conda # Source the following `conda.sh` file to put the `conda` command in our path for the duration of this script . /app/conda/etc/profile.d/conda.sh # Add the above command to `.bash_profile` so that it is available during SSH sessions echo \". /app/conda/etc/profile.d/conda.sh\" ~/.bash_profile echo \"conda activate\" ~/.bash_profile # Enter the base conda environment and make sure it is up to date conda activate base conda update -n base -c defaults conda # Install R # `r-essentials` is a bundle of 80 common R packages, including `r-shiny`. # Remove `r-essentials` from the line below to only have a barebones install. # The `-n` flag gives the environment its name. conda create -n r-env r-essentials r-base # Activate the freshly created environment conda activate r-env # Install additional packages if desired # conda install r-rbokeh IMPORTANT: When using conda to install R packages, you will need to add r- before the regular CRAN or MRAN name. For instance, if you want to install rbokeh, you will need to use conda install r-rbokeh. 3. Add a start script for running scripts with R start_command.sh: #!/bin/bash # Source the `conda.sh` file to make this particular shell session able to run the `conda` command . /app/conda/etc/profile.d/conda.sh # Activate the conda environment that was created in the build hook conda activate r-env # Run the Shiny web app Rscript start_shinyapp.R 4. Add a Shiny app in the shinyapp directory In the file shinyapp/app.R: library(shiny) # Define UI for application that draws a histogram ui ",
        "text": "Goal To run R Shiny on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account An empty http://Platform.sh project Problems http://Platform.sh offers many different language runtimes by default, but some applications require different environments, such as data science applications which use R Shiny. By installing the Miniconda Python package manager, it is possible to run R applications, such as R Shiny, on http://Platform.sh. Steps 1. Set git remote to http://Platform.sh project $ git init $ platform project:set-remote 2. Create an installation script file The http://Platform.sh build hook runs Dash, not Bash, so the installation script is executed separately in install_r.sh: #!/bin/bash # Download the latest Miniconda3 release and name the file `conda.sh` curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o conda.sh # Run the downloaded `conda.sh` script using bash # — Use the `-b` flag to run the installer in batch mode # — Use the `-p` flag to specify where the package manager will actually be installed bash conda.sh -b -p $PLATFORM_APP_DIR/conda # Source the following `conda.sh` file to put the `conda` command in our path for the duration of this script . /app/conda/etc/profile.d/conda.sh # Add the above command to `.bash_profile` so that it is available during SSH sessions echo \". /app/conda/etc/profile.d/conda.sh\" ~/.bash_profile echo \"conda activate\" ~/.bash_profile # Enter the base conda environment and make sure it is up to date conda activate base conda update -n base -c defaults conda # Install R # `r-essentials` is a bundle of 80 common R packages, including `r-shiny`. # Remove `r-essentials` from the line below to only have a barebones install. # The `-n` flag gives the environment its name. conda create -n r-env r-essentials r-base # Activate the freshly created environment conda activate r-env # Install additional packages if desired # conda install r-rbokeh IMPORTANT: When using conda to install R packages, you will need to add r- before the regular CRAN or MRAN name. For instance, if you want to install rbokeh, you will need to use conda install r-rbokeh. 3. Add a start script for running scripts with R start_command.sh: #!/bin/bash # Source the `conda.sh` file to make this particular shell session able to run the `conda` command . /app/conda/etc/profile.d/conda.sh # Activate the conda environment that was created in the build hook conda activate r-env # Run the Shiny web app Rscript start_shinyapp.R 4. Add a Shiny app in the shinyapp directory In the file shinyapp/app.R: library(shiny) # Define UI for application that draws a histogram ui ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-run-r-shiny-on-platform-sh/231",
        "relurl": "/t/how-to-run-r-shiny-on-platform-sh/231"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "8a79fae9027c0fce5c6d3fa505ba95e7ea79cca4",
        "title": "How to run an Anaconda/Miniconda Python stack on Platform.sh",
        "description": "Goal To run Anaconda (the full data science Python stack) or Miniconda (just the Python package manager) on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account An empty http://Platform.sh project Problems http://Platform.sh offers several different versions of Python by default, but some applications require different environments, such as data science applications running on the Anaconda stack or packages installed with Miniconda. Running the Anaconda/Miniconda installer in the build hook of a http://Platform.sh application allows for the activation of conda virtual environments for script execution. Steps 1. Set git remote to http://Platform.sh project $ git init $ platform project:set-remote 2. Create an installation script file The http://Platform.sh build hook runs Dash, not Bash, so the installation script is executed separately in install_conda.sh: #!/bin/bash # Download an Anaconda3 release and name the file `conda.sh` curl https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh -o conda.sh # If you wish to install Miniconda3 instead, comment out the line above and uncomment the line below: # curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o conda.sh # Run the downloaded `conda.sh` script using bash # — Use the `-b` flag to run the installer in batch mode # — Use the `-p` flag to specify where the package manager will actually be installed bash conda.sh -b -p $PLATFORM_APP_DIR/conda # Source the following `conda.sh` file to put the `conda` command in our path for the duration of this script . /app/conda/etc/profile.d/conda.sh # Add the above command to `.bash_profile` so that it is available during SSH sessions echo \". /app/conda/etc/profile.d/conda.sh\" ~/.bash_profile echo \"conda activate\" ~/.bash_profile # Enter the base conda environment and prepare to configure it conda activate base # Update conda itself conda update -n base -c defaults conda # OPTIONAL: Print out debugging information in the build hook conda info 3. Add a start script for running scripts with Anaconda start_command.sh: #!/bin/bash # Source the `conda.sh` file to make this particular shell session able to run the `conda` command . /app/conda/etc/profile.d/conda.sh # Activate the conda environment that was created in the build hook conda activate base # Put any commands needed to run a web app here Make sure to put any commands needed to run a web app at the end of this file. 4. Add .platform.app.yaml Create the .platform.app.yaml https://docs.platform.sh/configuration/app-containers.html : # The name of this app. Must be unique within a project. name: app # The runtime the application uses. type: \"python:3.7\" # The hooks executed at various points in the lifecycle of the application. hooks: build: | set -e # The build hook uses `dash` rather than `bash`. # Thus, the installation occurs in separate script that runs with `bash`. bash install_conda.sh deploy: | set -e # Make the start script executable chmod +x start_command.sh # The size of the persistent disk of the application (in MB). disk: 1024 # The configuration of app when it is exposed to the web. web: commands: start: bash ./start_command.sh upstream: socket_family: tcp protocol: http locations: \"/\": passthru: true 5. Define routes ./.platform/routes.yaml https://{default}/: type: upstream upstream: app:http 6. Add empty services ./.platform/services.yaml # empty 7. Add, commit, and push these files to your http://Platform.sh project git add . git commit -m \"Adding configuration to install conda environment\" git push platform master Conclusion By executing the Anaconda or Miniconda installation Bash scripts in the build hook, a project is able to install an Conda-based execution environment on http://Platform.sh.",
        "text": "Goal To run Anaconda (the full data science Python stack) or Miniconda (just the Python package manager) on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account An empty http://Platform.sh project Problems http://Platform.sh offers several different versions of Python by default, but some applications require different environments, such as data science applications running on the Anaconda stack or packages installed with Miniconda. Running the Anaconda/Miniconda installer in the build hook of a http://Platform.sh application allows for the activation of conda virtual environments for script execution. Steps 1. Set git remote to http://Platform.sh project $ git init $ platform project:set-remote 2. Create an installation script file The http://Platform.sh build hook runs Dash, not Bash, so the installation script is executed separately in install_conda.sh: #!/bin/bash # Download an Anaconda3 release and name the file `conda.sh` curl https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh -o conda.sh # If you wish to install Miniconda3 instead, comment out the line above and uncomment the line below: # curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o conda.sh # Run the downloaded `conda.sh` script using bash # — Use the `-b` flag to run the installer in batch mode # — Use the `-p` flag to specify where the package manager will actually be installed bash conda.sh -b -p $PLATFORM_APP_DIR/conda # Source the following `conda.sh` file to put the `conda` command in our path for the duration of this script . /app/conda/etc/profile.d/conda.sh # Add the above command to `.bash_profile` so that it is available during SSH sessions echo \". /app/conda/etc/profile.d/conda.sh\" ~/.bash_profile echo \"conda activate\" ~/.bash_profile # Enter the base conda environment and prepare to configure it conda activate base # Update conda itself conda update -n base -c defaults conda # OPTIONAL: Print out debugging information in the build hook conda info 3. Add a start script for running scripts with Anaconda start_command.sh: #!/bin/bash # Source the `conda.sh` file to make this particular shell session able to run the `conda` command . /app/conda/etc/profile.d/conda.sh # Activate the conda environment that was created in the build hook conda activate base # Put any commands needed to run a web app here Make sure to put any commands needed to run a web app at the end of this file. 4. Add .platform.app.yaml Create the .platform.app.yaml https://docs.platform.sh/configuration/app-containers.html : # The name of this app. Must be unique within a project. name: app # The runtime the application uses. type: \"python:3.7\" # The hooks executed at various points in the lifecycle of the application. hooks: build: | set -e # The build hook uses `dash` rather than `bash`. # Thus, the installation occurs in separate script that runs with `bash`. bash install_conda.sh deploy: | set -e # Make the start script executable chmod +x start_command.sh # The size of the persistent disk of the application (in MB). disk: 1024 # The configuration of app when it is exposed to the web. web: commands: start: bash ./start_command.sh upstream: socket_family: tcp protocol: http locations: \"/\": passthru: true 5. Define routes ./.platform/routes.yaml https://{default}/: type: upstream upstream: app:http 6. Add empty services ./.platform/services.yaml # empty 7. Add, commit, and push these files to your http://Platform.sh project git add . git commit -m \"Adding configuration to install conda environment\" git push platform master Conclusion By executing the Anaconda or Miniconda installation Bash scripts in the build hook, a project is able to install an Conda-based execution environment on http://Platform.sh.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-run-an-anaconda-miniconda-python-stack-on-platform-sh/230",
        "relurl": "/t/how-to-run-an-anaconda-miniconda-python-stack-on-platform-sh/230"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "feffb81cfee90608577cd3f2a7fa88d6e8399e67",
        "title": "How to add a webhook to one or more projects",
        "description": "Goal Add a webhook to a single project, or to all of your http://Platform.sh projects. Assumptions One or more active projects on http://Platform.sh The https://docs.platform.sh/gettingstarted/cli.html installed locally Problems You can use webhooks to integrate http://Platform.sh into your automation chain to trigger actions somewhere else. They can be configured to fire on all events, or specific events that happen on specific environments. More information about https://docs.platform.sh/administration/integrations/webhooks.html is available in the public documentation. Steps (Adding a webhook to a single project) 1. Get the project ID If the local repository already has the remote project set, navigate to that directory. Otherwise, retrieve the project ID using platform project:list 2. Set up the webhook Add the webhook for the desired URL that can receive posted JSON: $ platform integration:add --type=webhook --project= --url=A-URL-THAT-CAN-RECEIVE-THE-POSTED-JSON Then specify the desired webhook properties: Events to report (--events) A list of events to report, e.g. environment.push Default: * Enter comma-separated values (or leave this blank) environment.push States to report (--states) A list of states to report, e.g. pending, in_progress, complete Default: complete Enter comma-separated values (or leave this blank) complete Included environments (--environments) The environment IDs to include Default: * Enter comma-separated values (or leave this blank) * Excluded environments (--excluded-environments) The environment IDs to exclude Enter comma-separated values (or leave this blank) Created integration wuw76ebyhb5ni (type: webhook) +----------------------+-------------------------------------------------------+ | Property | Value | +----------------------+-------------------------------------------------------+ | id | wuw76ebyhb5ni | | type | webhook | | events | - environment.push | | environments | - '*' | | excluded_environment | { } | | s | | | states | - complete | | url | | +----------------------+-------------------------------------------------------+ You can also, evidently pass all the arguments on the command line, so this step itself can be automated. Steps (Adding a webhook to all projects) 1. Add the webhooks in a single command $ platform multi -p$(platform projects --my --columns id --pipe | paste -sd \",\" -) \"integration:add --type webhook --url https://example.com/receiver --events * --states complete --environments * --excluded-environments ''\" The CLI command can be broken down into the following parts: platform multi runs CLI command on multiple projects platform projects --my --columns id --pipe outputs the list of projects with only their IDs separated by newline paste -sd \",\" - joins that array of project IDs with a comma integration:add --type webhook --url https://example.com/receiver --events * --states complete --environments * --excluded-environments '' adds a webhook for all events and all environments 2. Bonus - add the integration to all projects that do not have one configured. And what if you only want to add the integration if it does not already exist? platform multi -p$(platform multi -p$(platform projects --my --columns id --pipe | paste -sd \",\" -) \"integration:list --format=csv --no-header\" 2\u00261 | grep -B1 \"No integrations found\" |grep -o \"\\(([[:alnum:]]*)\\)$\" | cut -c2-14| paste -sd \",\" -) \"integration:add --type webhook --url https://example.com --events * --states complete --environments * --excluded-environments ''\" But then again, bash one liners… you’d be better off at this point using the API directly rather then relying on my bash-foo. Conclusions Using the http://Platform.sh CLI, webhook integrations can easily be added to single projects or even to every project. Bash is cool. ",
        "text": "Goal Add a webhook to a single project, or to all of your http://Platform.sh projects. Assumptions One or more active projects on http://Platform.sh The https://docs.platform.sh/gettingstarted/cli.html installed locally Problems You can use webhooks to integrate http://Platform.sh into your automation chain to trigger actions somewhere else. They can be configured to fire on all events, or specific events that happen on specific environments. More information about https://docs.platform.sh/administration/integrations/webhooks.html is available in the public documentation. Steps (Adding a webhook to a single project) 1. Get the project ID If the local repository already has the remote project set, navigate to that directory. Otherwise, retrieve the project ID using platform project:list 2. Set up the webhook Add the webhook for the desired URL that can receive posted JSON: $ platform integration:add --type=webhook --project= --url=A-URL-THAT-CAN-RECEIVE-THE-POSTED-JSON Then specify the desired webhook properties: Events to report (--events) A list of events to report, e.g. environment.push Default: * Enter comma-separated values (or leave this blank) environment.push States to report (--states) A list of states to report, e.g. pending, in_progress, complete Default: complete Enter comma-separated values (or leave this blank) complete Included environments (--environments) The environment IDs to include Default: * Enter comma-separated values (or leave this blank) * Excluded environments (--excluded-environments) The environment IDs to exclude Enter comma-separated values (or leave this blank) Created integration wuw76ebyhb5ni (type: webhook) +----------------------+-------------------------------------------------------+ | Property | Value | +----------------------+-------------------------------------------------------+ | id | wuw76ebyhb5ni | | type | webhook | | events | - environment.push | | environments | - '*' | | excluded_environment | { } | | s | | | states | - complete | | url | | +----------------------+-------------------------------------------------------+ You can also, evidently pass all the arguments on the command line, so this step itself can be automated. Steps (Adding a webhook to all projects) 1. Add the webhooks in a single command $ platform multi -p$(platform projects --my --columns id --pipe | paste -sd \",\" -) \"integration:add --type webhook --url https://example.com/receiver --events * --states complete --environments * --excluded-environments ''\" The CLI command can be broken down into the following parts: platform multi runs CLI command on multiple projects platform projects --my --columns id --pipe outputs the list of projects with only their IDs separated by newline paste -sd \",\" - joins that array of project IDs with a comma integration:add --type webhook --url https://example.com/receiver --events * --states complete --environments * --excluded-environments '' adds a webhook for all events and all environments 2. Bonus - add the integration to all projects that do not have one configured. And what if you only want to add the integration if it does not already exist? platform multi -p$(platform multi -p$(platform projects --my --columns id --pipe | paste -sd \",\" -) \"integration:list --format=csv --no-header\" 2\u00261 | grep -B1 \"No integrations found\" |grep -o \"\\(([[:alnum:]]*)\\)$\" | cut -c2-14| paste -sd \",\" -) \"integration:add --type webhook --url https://example.com --events * --states complete --environments * --excluded-environments ''\" But then again, bash one liners… you’d be better off at this point using the API directly rather then relying on my bash-foo. Conclusions Using the http://Platform.sh CLI, webhook integrations can easily be added to single projects or even to every project. Bash is cool. ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-add-a-webhook-to-one-or-more-projects/139",
        "relurl": "/t/how-to-add-a-webhook-to-one-or-more-projects/139"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "209745a6abc4402c4bbfb1084a222bb6a1f36b29",
        "title": "How to use Redis for caching a Drupal 8 site",
        "description": "Goal Drupal 8 functions best with a non-database cache, and particularly well with Redis. This guide shows how to set up a Drupal 8 site to use Redis caching. Assumptions You already have a working Drupal 8 site on http://Platform.sh, built with Composer. The project’s plan size has room for an additional Redis service. A Standard plan should have room for a Drupal 8 site with Redis and one search server, although if the site has a lot of authenticated traffic it may be a good idea to increase it to a Medium plan to allow for more resources for each container. Add Redis first, then benchmark the site to determine if that’s necessary. Problems Drupal 8 contains a bug that causes it to fail if the cache backend changes between installation (which always uses the database) and the first page load (which uses the configured cache). Therefore Redis caching cannot be configured out of the box. It can and should be enabled immediately after installation, however. Steps 1. Add a Redis service Add the following to the .platform/services.yaml file to create a Redis service in the application: rediscache: type: redis:5.0 That will create a service named rediscache, of type redis, specifically version 5.0. 2. Expose the Redis service to your application In the .platform.app.yaml file under the relationships section, add the following: relationships: redis: \"rediscache:redis\" 3. Add the Redis PHP extension In the .platform.app.yaml file, add the following right after the type block: # Additional extensions runtime: extensions: - redis 4. Add the Drupal module Add the Drupal https://www.drupal.org/project/redis module. If using Composer (recommended), a single command will add it: composer require drupal/redis Then commit the resulting changes to the composer.json and composer.lock files. The Redis module may be enabled after the setup below is completed, but should not be enabled until it is. 5. Configure Drupal Place the following at the end of settings.platformsh.php. Note the inline comments, which allow for further customization. Also review the README.txt file that comes with the redis module, as it has a great deal more information on possible configuration options. The example below is intended as a “most common case”, so may need adjustment to be optimal for a particular site. note If you do not already have the http://Platform.sh Config Reader library installed and referenced at the top of the file, you will need to install it with composer require platformsh/config-reader and then add the following code before the block below: $platformsh = new \\Platformsh\\ConfigReader\\Config(); if (!$platformsh-inRuntime()) { return; } // Set redis configuration. if ($platformsh-hasRelationship('redis') \u0026\u0026 !drupal_installation_attempted() \u0026\u0026 extension_loaded('redis')) { $redis = $platformsh-credentials('redis'); // Set Redis as the default backend for any cache bin not otherwise specified. $settings['cache']['default'] = 'cache.backend.redis'; $settings['redis.connection']['host'] = $redis['host']; $settings['redis.connection']['port'] = $redis['port']; // Apply changes to the container configuration to better leverage Redis. // This includes using Redis for the lock and flood control systems, as well // as the cache tag checksum. Alternatively, copy the contents of that file // to your project-specific services.yml file, modify as appropriate, and // remove this line. $settings['container_yamls'][] = 'modules/contrib/redis/example.services.yml'; // Allow the services to work before the Redis module itself is enabled. $settings['container_yamls'][] = 'modules/contrib/redis/redis.services.yml'; // Manually add the classloader path, this is required for the container cache bin definition below // and allows to use it without the redis module being enabled. $class_loader-addPsr4('Drupal\\\\redis\\\\', 'modules/contrib/redis/src'); // Use redis for container cache. // The container cache is used to load the container definition itself, and // thus any configuration stored in the container itself is not available // yet. These lines force the container cache to use Redis rather than the // default SQL cache. $settings['bootstrap_container_definition'] = [ 'parameters' = [], 'services' = [ 'redis.factory' = [ 'class' = 'Drupal\\redis\\ClientFactory', ], 'cache.backend.redis' = [ 'class' = 'Drupal\\redis\\Cache\\CacheBackendFactory', 'arguments' = ['@redis.factory', '@cache_tags_provider.container', '@serialization.phpserialize'], ], 'cache.container' = [ 'class' = '\\Drupal\\redis\\Cache\\PhpRedis', 'factory' = ['@cache.backend.redis', 'get'], 'arguments' = ['container'], ], 'cache_tags_provider.container' = [ 'class' = 'Drupal\\redis\\Cache\\RedisCacheTagsChecksum', 'arguments' = ['@redis.factory'], ], 'serialization.phpserialize' = [ 'class' = 'Drupal\\Component\\Serialization\\PhpSerialize', ], ], ]; } The example.services.yml file noted above will also use Redis for the lock and flood control systems. 6. Deploy and Verify Redis Commit the changes to settings.platformsh.php, composer.*, .platform.app.yaml, and .platform/services.yaml. Then git push. Once the deploy is finished, run: platform ssh \"redis-cli -h redis.internal info\" That will show the Redis status information. Toward the top of the output is the memory allocation. If that amount changes as the site is used it means Drupal is correctly connecting to Redis and writing cache values to it. 7. Clear SQL cache tables Once Redis is confirmed running and in use, clear out the remaining, vestigial values in the SQL database cache as they are no longer valid. To do that, connect to the database: platform sql And run TRUNCATE cache_* for every table that begins with cache_, except for cache_form. Despite its name cache_form is not part of the cache system proper and thus should not be moved out of SQL. To see all the cache tables, run: show tables like 'cache_%'; That will list tables like cache_bootstrap, cache_config, cache_container, etc. Then run: TRUNCATE cache_bootstrap; TRUNCATE cache_config; TRUNCATE cache_container; And so on. Conclusion Using Redis for Drupal caching can improve performance, and more importantly free up valuable database disk space. That helps avoid out-of-disk lockups on the database that can impact site availability.",
        "text": "Goal Drupal 8 functions best with a non-database cache, and particularly well with Redis. This guide shows how to set up a Drupal 8 site to use Redis caching. Assumptions You already have a working Drupal 8 site on http://Platform.sh, built with Composer. The project’s plan size has room for an additional Redis service. A Standard plan should have room for a Drupal 8 site with Redis and one search server, although if the site has a lot of authenticated traffic it may be a good idea to increase it to a Medium plan to allow for more resources for each container. Add Redis first, then benchmark the site to determine if that’s necessary. Problems Drupal 8 contains a bug that causes it to fail if the cache backend changes between installation (which always uses the database) and the first page load (which uses the configured cache). Therefore Redis caching cannot be configured out of the box. It can and should be enabled immediately after installation, however. Steps 1. Add a Redis service Add the following to the .platform/services.yaml file to create a Redis service in the application: rediscache: type: redis:5.0 That will create a service named rediscache, of type redis, specifically version 5.0. 2. Expose the Redis service to your application In the .platform.app.yaml file under the relationships section, add the following: relationships: redis: \"rediscache:redis\" 3. Add the Redis PHP extension In the .platform.app.yaml file, add the following right after the type block: # Additional extensions runtime: extensions: - redis 4. Add the Drupal module Add the Drupal https://www.drupal.org/project/redis module. If using Composer (recommended), a single command will add it: composer require drupal/redis Then commit the resulting changes to the composer.json and composer.lock files. The Redis module may be enabled after the setup below is completed, but should not be enabled until it is. 5. Configure Drupal Place the following at the end of settings.platformsh.php. Note the inline comments, which allow for further customization. Also review the README.txt file that comes with the redis module, as it has a great deal more information on possible configuration options. The example below is intended as a “most common case”, so may need adjustment to be optimal for a particular site. note If you do not already have the http://Platform.sh Config Reader library installed and referenced at the top of the file, you will need to install it with composer require platformsh/config-reader and then add the following code before the block below: $platformsh = new \\Platformsh\\ConfigReader\\Config(); if (!$platformsh-inRuntime()) { return; } // Set redis configuration. if ($platformsh-hasRelationship('redis') \u0026\u0026 !drupal_installation_attempted() \u0026\u0026 extension_loaded('redis')) { $redis = $platformsh-credentials('redis'); // Set Redis as the default backend for any cache bin not otherwise specified. $settings['cache']['default'] = 'cache.backend.redis'; $settings['redis.connection']['host'] = $redis['host']; $settings['redis.connection']['port'] = $redis['port']; // Apply changes to the container configuration to better leverage Redis. // This includes using Redis for the lock and flood control systems, as well // as the cache tag checksum. Alternatively, copy the contents of that file // to your project-specific services.yml file, modify as appropriate, and // remove this line. $settings['container_yamls'][] = 'modules/contrib/redis/example.services.yml'; // Allow the services to work before the Redis module itself is enabled. $settings['container_yamls'][] = 'modules/contrib/redis/redis.services.yml'; // Manually add the classloader path, this is required for the container cache bin definition below // and allows to use it without the redis module being enabled. $class_loader-addPsr4('Drupal\\\\redis\\\\', 'modules/contrib/redis/src'); // Use redis for container cache. // The container cache is used to load the container definition itself, and // thus any configuration stored in the container itself is not available // yet. These lines force the container cache to use Redis rather than the // default SQL cache. $settings['bootstrap_container_definition'] = [ 'parameters' = [], 'services' = [ 'redis.factory' = [ 'class' = 'Drupal\\redis\\ClientFactory', ], 'cache.backend.redis' = [ 'class' = 'Drupal\\redis\\Cache\\CacheBackendFactory', 'arguments' = ['@redis.factory', '@cache_tags_provider.container', '@serialization.phpserialize'], ], 'cache.container' = [ 'class' = '\\Drupal\\redis\\Cache\\PhpRedis', 'factory' = ['@cache.backend.redis', 'get'], 'arguments' = ['container'], ], 'cache_tags_provider.container' = [ 'class' = 'Drupal\\redis\\Cache\\RedisCacheTagsChecksum', 'arguments' = ['@redis.factory'], ], 'serialization.phpserialize' = [ 'class' = 'Drupal\\Component\\Serialization\\PhpSerialize', ], ], ]; } The example.services.yml file noted above will also use Redis for the lock and flood control systems. 6. Deploy and Verify Redis Commit the changes to settings.platformsh.php, composer.*, .platform.app.yaml, and .platform/services.yaml. Then git push. Once the deploy is finished, run: platform ssh \"redis-cli -h redis.internal info\" That will show the Redis status information. Toward the top of the output is the memory allocation. If that amount changes as the site is used it means Drupal is correctly connecting to Redis and writing cache values to it. 7. Clear SQL cache tables Once Redis is confirmed running and in use, clear out the remaining, vestigial values in the SQL database cache as they are no longer valid. To do that, connect to the database: platform sql And run TRUNCATE cache_* for every table that begins with cache_, except for cache_form. Despite its name cache_form is not part of the cache system proper and thus should not be moved out of SQL. To see all the cache tables, run: show tables like 'cache_%'; That will list tables like cache_bootstrap, cache_config, cache_container, etc. Then run: TRUNCATE cache_bootstrap; TRUNCATE cache_config; TRUNCATE cache_container; And so on. Conclusion Using Redis for Drupal caching can improve performance, and more importantly free up valuable database disk space. That helps avoid out-of-disk lockups on the database that can impact site availability.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-use-redis-for-caching-a-drupal-8-site/190",
        "relurl": "/t/how-to-use-redis-for-caching-a-drupal-8-site/190"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "bdcb1b935788f4691effe5aab2bbd62bd3ec054c",
        "title": "How to connect to services on Platform.sh from your local system",
        "description": "Goal Access Platform.sh-hosted services from a local computer, over a secure SSH tunnel. Assumptions The http://Platform.sh CLI is installed locally. Whatever additional software is needed (local web server, local mysql client, etc.) is already installed locally. Problems By default the http://Platform.sh CLI will use the current environment. However, when using a 3rd party integration from GitHub or GitLab the desired environment will likely be a PR environment, not the environment that the user is working on directly. That will require extra commands as shown below. Steps 1. Open an SSH tunnel Run the following command locally: platform tunnel:open That will open an SSH tunnel to the current environment and map all services defined on the application to local ports. Accessing those ports locally with the appropriate tool (mysql command line, Redis CLI client, etc.) will connect to the service on the environment. 2. Export relationships Run the following command locally: export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" That will create a PLATFORM_RELATIONSHIPS environment variable locally that provides user/password/host/port credentials to connect to the current environment. A local application that checks that environment variable in order to connect to services on http://Platform.sh will work just the same as if it were on http://Platform.sh. (It will not, however, make any other environment variables available.) It is also possible to combine both commands into a single line, like so: platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" 3. Disconnect from the tunnel When done, clean up the open connections by running: platform tunnel:close Alternative: Connect to a Pull Request environment Both commands above will, by default, use the current branch name/environment. However, the environment created by a Pull Request from GitHub, Bitbucket, or GitLab will be different and thus the above commands will not connect to them. To access a Pull Request environment modify both commands to specify the environment name. For example, a GitHub Pull Request #42 will have an environment name on http://Platform.sh of pr-42, so run: platform tunnel:open -e pr-42 \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode -e pr-42)\" The -e pr-42 (or equivalently --environment pr-42) tells the CLI to connect to the pr-42 environment rather than the environment that corresponds to the branch currently checked out. Conclusion With access to the remote services, customers can take whatever action they need. That includes running code locally against a remote service, accessing the service using a management tool appropriate for that service, or any other task. Just remember the caveat about Pull Request environments.",
        "text": "Goal Access Platform.sh-hosted services from a local computer, over a secure SSH tunnel. Assumptions The http://Platform.sh CLI is installed locally. Whatever additional software is needed (local web server, local mysql client, etc.) is already installed locally. Problems By default the http://Platform.sh CLI will use the current environment. However, when using a 3rd party integration from GitHub or GitLab the desired environment will likely be a PR environment, not the environment that the user is working on directly. That will require extra commands as shown below. Steps 1. Open an SSH tunnel Run the following command locally: platform tunnel:open That will open an SSH tunnel to the current environment and map all services defined on the application to local ports. Accessing those ports locally with the appropriate tool (mysql command line, Redis CLI client, etc.) will connect to the service on the environment. 2. Export relationships Run the following command locally: export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" That will create a PLATFORM_RELATIONSHIPS environment variable locally that provides user/password/host/port credentials to connect to the current environment. A local application that checks that environment variable in order to connect to services on http://Platform.sh will work just the same as if it were on http://Platform.sh. (It will not, however, make any other environment variables available.) It is also possible to combine both commands into a single line, like so: platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" 3. Disconnect from the tunnel When done, clean up the open connections by running: platform tunnel:close Alternative: Connect to a Pull Request environment Both commands above will, by default, use the current branch name/environment. However, the environment created by a Pull Request from GitHub, Bitbucket, or GitLab will be different and thus the above commands will not connect to them. To access a Pull Request environment modify both commands to specify the environment name. For example, a GitHub Pull Request #42 will have an environment name on http://Platform.sh of pr-42, so run: platform tunnel:open -e pr-42 \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode -e pr-42)\" The -e pr-42 (or equivalently --environment pr-42) tells the CLI to connect to the pr-42 environment rather than the environment that corresponds to the branch currently checked out. Conclusion With access to the remote services, customers can take whatever action they need. That includes running code locally against a remote service, accessing the service using a management tool appropriate for that service, or any other task. Just remember the caveat about Pull Request environments.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-connect-to-services-on-platform-sh-from-your-local-system/189",
        "relurl": "/t/how-to-connect-to-services-on-platform-sh-from-your-local-system/189"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "73b639a7e8c09ce98e784efb356e034a85fb009b",
        "title": "How to upgrade Wordpress core and dependencies with Composer",
        "description": "Goal Update WordPress to the latest version using Composer. Assumptions A WordPress installation on http://Platform.sh that uses Composer, for example one that uses the https://github.com/platformsh/template-wordpress . The https://github.com/platformsh/platformsh-cli installed locally An https://docs.platform.sh/development/ssh.html configured on the project account https://getcomposer.org/ installed locally Problems Keeping WordPress up to date is essential for security as well as for bug fixes and new features. Updating WordPress with one command lowers the burden of maintenance. Since WordPress doesn’t provide native Composer support, the template provided by http://Platform.sh depends on a https://packagist.org/packages/johnpbloch/wordpress . Steps 1. Make a branch for updating WordPress: $ platform checkout wordpress-update 2. Update composer.json Check that composer.json is set to track the latest WordPress version. It should have something like ^5.1 listed for wordpress-core. \"require\": { \"php\": \"=5.3.2\", \"johnpbloch/wordpress-core-installer\": \"^1.0\", \"johnpbloch/wordpress-core\": \"^5.1\" }, 3. Run composer update. composer update --with-dependencies Loading composer repositories with package information Updating dependencies (including require-dev) Package operations: 2 installs, 0 updates, 0 removals - Installing johnpbloch/wordpress-core-installer (1.0.2): Loading from cache - Installing johnpbloch/wordpress-core (5.1.1): Downloading (100%) Writing lock file Generating autoload files 4. Check changes Run git status to see which files have changed: git status On branch wordpress-update Your branch is up to date with 'platform/wordpress-update'. Changes not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git checkout -- ...\" to discard changes in working directory) modified: composer.json modified: composer.lock no changes added to commit (use \"git add\" and/or \"git commit -a\") 5. Push changes Now the project can be built properly by http://Platform.sh to provide the latest WordPress version. Add, commit, and push. git add composer.json git add composer.lock git commit -m \"Updating WordPress\" git push platform wordpress-update 6. Update the Wordpress database Update the WordPress database using the platform tool: platform ssh 'wp core update-db' Alternately, log into the admin area of the site an you will be prompted by WordPress to update the database. Note that you will have to re-run the database update when you merge your code changes into other branches, e.g. into master when you deploy these changes to your live site. Conclusion Relying on a package manager such as Composer greatly simplifies the maintenance and security of WordPress sites.",
        "text": "Goal Update WordPress to the latest version using Composer. Assumptions A WordPress installation on http://Platform.sh that uses Composer, for example one that uses the https://github.com/platformsh/template-wordpress . The https://github.com/platformsh/platformsh-cli installed locally An https://docs.platform.sh/development/ssh.html configured on the project account https://getcomposer.org/ installed locally Problems Keeping WordPress up to date is essential for security as well as for bug fixes and new features. Updating WordPress with one command lowers the burden of maintenance. Since WordPress doesn’t provide native Composer support, the template provided by http://Platform.sh depends on a https://packagist.org/packages/johnpbloch/wordpress . Steps 1. Make a branch for updating WordPress: $ platform checkout wordpress-update 2. Update composer.json Check that composer.json is set to track the latest WordPress version. It should have something like ^5.1 listed for wordpress-core. \"require\": { \"php\": \"=5.3.2\", \"johnpbloch/wordpress-core-installer\": \"^1.0\", \"johnpbloch/wordpress-core\": \"^5.1\" }, 3. Run composer update. composer update --with-dependencies Loading composer repositories with package information Updating dependencies (including require-dev) Package operations: 2 installs, 0 updates, 0 removals - Installing johnpbloch/wordpress-core-installer (1.0.2): Loading from cache - Installing johnpbloch/wordpress-core (5.1.1): Downloading (100%) Writing lock file Generating autoload files 4. Check changes Run git status to see which files have changed: git status On branch wordpress-update Your branch is up to date with 'platform/wordpress-update'. Changes not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git checkout -- ...\" to discard changes in working directory) modified: composer.json modified: composer.lock no changes added to commit (use \"git add\" and/or \"git commit -a\") 5. Push changes Now the project can be built properly by http://Platform.sh to provide the latest WordPress version. Add, commit, and push. git add composer.json git add composer.lock git commit -m \"Updating WordPress\" git push platform wordpress-update 6. Update the Wordpress database Update the WordPress database using the platform tool: platform ssh 'wp core update-db' Alternately, log into the admin area of the site an you will be prompted by WordPress to update the database. Note that you will have to re-run the database update when you merge your code changes into other branches, e.g. into master when you deploy these changes to your live site. Conclusion Relying on a package manager such as Composer greatly simplifies the maintenance and security of WordPress sites.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-upgrade-wordpress-core-and-dependencies-with-composer/173",
        "relurl": "/t/how-to-upgrade-wordpress-core-and-dependencies-with-composer/173"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "ffea9b5d9efac32b3aba638c0b455790ee83540a",
        "title": "How to migrate database changes between environments",
        "description": "Goal Import changes to a database in a child environment to its parent using the http://Platform.sh CLI. Assumptions This guide assumes: an active application on configured to a database A local repository with the http://platform.sh/ project as git remote The https://docs.platform.sh/gettingstarted/cli.html installed locally An https://docs.platform.sh/development/ssh.html configured on the project account admin role granted for the project Problems Changes to a database in master will be visible to its child environments when they are , however changes made to a development environment - such as adding an admin user (alan) to a PostgreSQL database - do not run in the opposite direction and will not be available in master. In a Django application, admin user alan is visible from the dev branch. $ platform db:sql psql (11.1 (Debian 11.1-3.pgdg90+1), server 9.6.11) Type \"help\" for help. main= SELECT * FROM auth_user; id | password | last_login | is_superuser | username | first_name | last_name | email | is_staff | is_active | date_joined ----+---------------------------+-------------------------------+--------------+----------+------------+-----------+-------+----------+-----------+------------------------------- 2 | | 2019-03-12 15:52:38.379839+00 | t | chad | | | | t | t | 2019-03-07 16:02:26.583956+00 6 | | 2019-03-12 15:48:02+00 | t | alan | | | | t | t | 2019-03-12 15:47:19+00 (2 rows) alan is not visible in master, however. $ platform db:sql psql (11.1 (Debian 11.1-3.pgdg90+1), server 9.6.11) Type \"help\" for help. main= SELECT * FROM auth_user; id | password | last_login | is_superuser | username | first_name | last_name | email | is_staff | is_active | date_joined ----+---------------------------+-------------------------------+--------------+----------+------------+-----------+-------+----------+-----------+------------------------------- 2 | | 2019-03-11 14:40:56.401026+00 | t | chad | | | | t | t | 2019-03-07 16:02:26.583956+00 (1 row) Note: Importing a database into an active environment is a destructive operation. This guide assumes that the only change made between master and dev is the addition of a new admin user in dev. Synchronize dev if there is data in master you do not want to be overwritten. $ git checkout dev $ platform environment:synchronize http://Platform.sh also strongly recommends that you take a snapshot of the target environment before executing. $ git checkout master $ platform snapshot:create Migrating changes between environments can be done in two ways: Restoring a snapshot to the target environment Manually dumping and importing the database to the target environment Steps (Restoring a snapshot to the target environment) 1. Create a snapshot Create a snapshot of the** dev **environment that contains the database changes $ git checkout dev Switched to branch 'dev' $ platform snapshot:create Creating a snapshot of dev Waiting for the snapshot to complete... Waiting for the activity m3qerwnkyblje (User created a backup of dev): Backing up dev Backup name is [============================] 14 secs (complete) A snapshot of environment dev has been created Snapshot name: 2. Restore the snapshot from dev to master While still checked out as dev and using the from above: $ platform snapshot:restore --target=master Additional information about creating and restoring snapshots can be found in the https://docs.platform.sh/administration/snapshot-and-restore.html and in https://community.platform.sh/t/how-to-create-and-restore-snapshots-using-the-cli/100 . Steps (Manually dump to the target environment) 1. Dump the database For this example, the relationship database was defined in .platform.app.yaml with: relationships: database: \"postgresqldb:postgresql\" From dev dump the PostrgreSQL database according to its relationship name (database). The --relationship tag is useful if there are multiple database relationships present, but if only one is used platform db:dump will work without it. See the for more information. $ git checkout dev $ platform db:dump --relationship database Creating SQL dump file: / --dev-54ta5gq--postgresqldb--main--dump.sql 2. Pipe the SQL dump to the target environment ```bash $ git checkout master $ platform sql --relationship database -e master --dev-54ta5gq--postgresqldb--main--dump.sql ``` Verify Verify that the new admin user alan is now visible from master. $ platform db:sql psql (11.1 (Debian 11.1-3.pgdg90+1), server 9.6.11) Type \"help\" for help. main= SELECT * FROM auth_user; id | password | last_login | is_superuser | username | first_name | last_name | email | is_staff | is_active | date_joined ----+---------------------------+-------------------------------+--------------+----------+------------+-----------+-------+----------+-----------+------------------------------- 2 | | 2019-03-12 15:52:38.379839+00 | t | chad | | | | t | t | 2019-03-07 16:02:26.583956+00 6 | | 2019-03-12 15:48:02+00 | t | alan | | | | t | t | 2019-03-12 15:47:19+00 (2 rows) Conclusion Child environments inherit service visibility from their parents, but to ensure that development changes to not affect a production environment, updates and synchronizations do not occur in the opposite direction. If a change is made in a development environment that is desired on master, the Platform CLI can be used to migrate those changes by either creating and restoring a snapshot to the target environment or by manually dumping a database piping the dump file into the target environment.",
        "text": "Goal Import changes to a database in a child environment to its parent using the http://Platform.sh CLI. Assumptions This guide assumes: an active application on configured to a database A local repository with the http://platform.sh/ project as git remote The https://docs.platform.sh/gettingstarted/cli.html installed locally An https://docs.platform.sh/development/ssh.html configured on the project account admin role granted for the project Problems Changes to a database in master will be visible to its child environments when they are , however changes made to a development environment - such as adding an admin user (alan) to a PostgreSQL database - do not run in the opposite direction and will not be available in master. In a Django application, admin user alan is visible from the dev branch. $ platform db:sql psql (11.1 (Debian 11.1-3.pgdg90+1), server 9.6.11) Type \"help\" for help. main= SELECT * FROM auth_user; id | password | last_login | is_superuser | username | first_name | last_name | email | is_staff | is_active | date_joined ----+---------------------------+-------------------------------+--------------+----------+------------+-----------+-------+----------+-----------+------------------------------- 2 | | 2019-03-12 15:52:38.379839+00 | t | chad | | | | t | t | 2019-03-07 16:02:26.583956+00 6 | | 2019-03-12 15:48:02+00 | t | alan | | | | t | t | 2019-03-12 15:47:19+00 (2 rows) alan is not visible in master, however. $ platform db:sql psql (11.1 (Debian 11.1-3.pgdg90+1), server 9.6.11) Type \"help\" for help. main= SELECT * FROM auth_user; id | password | last_login | is_superuser | username | first_name | last_name | email | is_staff | is_active | date_joined ----+---------------------------+-------------------------------+--------------+----------+------------+-----------+-------+----------+-----------+------------------------------- 2 | | 2019-03-11 14:40:56.401026+00 | t | chad | | | | t | t | 2019-03-07 16:02:26.583956+00 (1 row) Note: Importing a database into an active environment is a destructive operation. This guide assumes that the only change made between master and dev is the addition of a new admin user in dev. Synchronize dev if there is data in master you do not want to be overwritten. $ git checkout dev $ platform environment:synchronize http://Platform.sh also strongly recommends that you take a snapshot of the target environment before executing. $ git checkout master $ platform snapshot:create Migrating changes between environments can be done in two ways: Restoring a snapshot to the target environment Manually dumping and importing the database to the target environment Steps (Restoring a snapshot to the target environment) 1. Create a snapshot Create a snapshot of the** dev **environment that contains the database changes $ git checkout dev Switched to branch 'dev' $ platform snapshot:create Creating a snapshot of dev Waiting for the snapshot to complete... Waiting for the activity m3qerwnkyblje (User created a backup of dev): Backing up dev Backup name is [============================] 14 secs (complete) A snapshot of environment dev has been created Snapshot name: 2. Restore the snapshot from dev to master While still checked out as dev and using the from above: $ platform snapshot:restore --target=master Additional information about creating and restoring snapshots can be found in the https://docs.platform.sh/administration/snapshot-and-restore.html and in https://community.platform.sh/t/how-to-create-and-restore-snapshots-using-the-cli/100 . Steps (Manually dump to the target environment) 1. Dump the database For this example, the relationship database was defined in .platform.app.yaml with: relationships: database: \"postgresqldb:postgresql\" From dev dump the PostrgreSQL database according to its relationship name (database). The --relationship tag is useful if there are multiple database relationships present, but if only one is used platform db:dump will work without it. See the for more information. $ git checkout dev $ platform db:dump --relationship database Creating SQL dump file: / --dev-54ta5gq--postgresqldb--main--dump.sql 2. Pipe the SQL dump to the target environment ```bash $ git checkout master $ platform sql --relationship database -e master --dev-54ta5gq--postgresqldb--main--dump.sql ``` Verify Verify that the new admin user alan is now visible from master. $ platform db:sql psql (11.1 (Debian 11.1-3.pgdg90+1), server 9.6.11) Type \"help\" for help. main= SELECT * FROM auth_user; id | password | last_login | is_superuser | username | first_name | last_name | email | is_staff | is_active | date_joined ----+---------------------------+-------------------------------+--------------+----------+------------+-----------+-------+----------+-----------+------------------------------- 2 | | 2019-03-12 15:52:38.379839+00 | t | chad | | | | t | t | 2019-03-07 16:02:26.583956+00 6 | | 2019-03-12 15:48:02+00 | t | alan | | | | t | t | 2019-03-12 15:47:19+00 (2 rows) Conclusion Child environments inherit service visibility from their parents, but to ensure that development changes to not affect a production environment, updates and synchronizations do not occur in the opposite direction. If a change is made in a development environment that is desired on master, the Platform CLI can be used to migrate those changes by either creating and restoring a snapshot to the target environment or by manually dumping a database piping the dump file into the target environment.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-migrate-database-changes-between-environments/124",
        "relurl": "/t/how-to-migrate-database-changes-between-environments/124"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "812e035a76790794540e6b99bfbc47786a72f317",
        "title": "How to perform a database dump via Platform.sh CLI",
        "description": "Goal Perform a database dump using the platform https://github.com/platformsh/platformsh-cli . One or more active http://platform.sh/ projects The https://github.com/platformsh/platformsh-cli installed locally An https://docs.platform.sh/development/ssh.html configured on the project account Known project IDs. (Use platform project:list) Problems Sometimes it is necessary to do a database dump of a project. This can be especially useful when doing some testing, planning a migration or for recovery purposes. Platform CLI can be used to backup the database with a single command. Steps In general, the following command will be used to perform a database dump using the Platform CLI: Usage: platform db:dump [-f|--file FILE] [-t|--timestamp] [--stdout] [-p|--project PROJECT] [--host HOST] [-e|--environment ENVIRONMENT] [-A|--app APP] 1. Single Database For a single configured database, use the command below: $ platform db:dump Creating SQL dump file:/Users/my_user/platformsh-enterprise/hiera/mysqldb--main--dump.sql 2. Multiple Databases Running the previous command with multiple databases, choosing a relationship will be prompted by the CLI: $ platform db:dump Enter a number to choose a relationship: [0] postgresql (main@postgresql.internal) [1] database (user@database.internal) Dumping the MySQL database (database, in this case) can be chosen by selecting 1. database can be chosen directly ahead of time with the CLI with the --relationship flag: $ platform db:dump --relationship database Further options can be specified, as per the example below: $ platform db:dump --relationship database -p project_id -e environment_name -A app To find the platform relationship details, SSS into the environment and run: $ echo $PLATFORM_RELATIONSHIPS | base64 --decode | json_pp 3. Compression By default the dump file will be uncompressed. To compress it, use the --gzip ( -z ) option: platform db:dump --gzip 4. Troubleshooting Error output: Permission denied (publickey). This error can sometimes show up, after running the db dump command. If this happens, try the following: the user is trying to access. The SSH key may be improperly configured. Check with platform ssh-key:list and platform ssh-key:add if necessary. Remove the user from the project, then add them back and redeploy the environment. Conclusion Database dumps can be easily done by using http://platform.sh/ CLI.",
        "text": "Goal Perform a database dump using the platform https://github.com/platformsh/platformsh-cli . One or more active http://platform.sh/ projects The https://github.com/platformsh/platformsh-cli installed locally An https://docs.platform.sh/development/ssh.html configured on the project account Known project IDs. (Use platform project:list) Problems Sometimes it is necessary to do a database dump of a project. This can be especially useful when doing some testing, planning a migration or for recovery purposes. Platform CLI can be used to backup the database with a single command. Steps In general, the following command will be used to perform a database dump using the Platform CLI: Usage: platform db:dump [-f|--file FILE] [-t|--timestamp] [--stdout] [-p|--project PROJECT] [--host HOST] [-e|--environment ENVIRONMENT] [-A|--app APP] 1. Single Database For a single configured database, use the command below: $ platform db:dump Creating SQL dump file:/Users/my_user/platformsh-enterprise/hiera/mysqldb--main--dump.sql 2. Multiple Databases Running the previous command with multiple databases, choosing a relationship will be prompted by the CLI: $ platform db:dump Enter a number to choose a relationship: [0] postgresql (main@postgresql.internal) [1] database (user@database.internal) Dumping the MySQL database (database, in this case) can be chosen by selecting 1. database can be chosen directly ahead of time with the CLI with the --relationship flag: $ platform db:dump --relationship database Further options can be specified, as per the example below: $ platform db:dump --relationship database -p project_id -e environment_name -A app To find the platform relationship details, SSS into the environment and run: $ echo $PLATFORM_RELATIONSHIPS | base64 --decode | json_pp 3. Compression By default the dump file will be uncompressed. To compress it, use the --gzip ( -z ) option: platform db:dump --gzip 4. Troubleshooting Error output: Permission denied (publickey). This error can sometimes show up, after running the db dump command. If this happens, try the following: the user is trying to access. The SSH key may be improperly configured. Check with platform ssh-key:list and platform ssh-key:add if necessary. Remove the user from the project, then add them back and redeploy the environment. Conclusion Database dumps can be easily done by using http://platform.sh/ CLI.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-perform-a-database-dump-via-platform-sh-cli/143",
        "relurl": "/t/how-to-perform-a-database-dump-via-platform-sh-cli/143"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "41ae8b2d857766a5e6cbaba24005719563ee4c21",
        "title": "How to make internal requests between two applications of a project",
        "description": "Goal This guide shows how make an internal request to a different application in the same project. The goal is to display a message sent by a backend API on our frontend. Assumptions To complete this, you will need: An empty http://Platform.sh project The https://docs.platform.sh/gettingstarted/cli.html tool installed Your ssh key loaded in your https://linux.die.net/man/1/ssh-agent and configured in the https://accounts.platform.sh/user Steps In order to implement this, setup two different applications in your project: frontend and backend. Please refer to the . PHP will be used for both applications. The steps below will create the following project structure: multiapp/ .platform/ routes.yaml services.yaml backend/ .platform.app.yaml index.php frontend/ .platform.app.yaml index.php 1. Setup the base project Create an empty directory, initialize git and add the http://Platform.sh remote: mkdir multiapp cd !$ git init . platform project:set https://.platform.sh/projects/ The git remotes should now be: git remote -v platform @git..platform.sh: .git (fetch) platform @git..platform.sh: .git (push) Create the two required http://Platform.sh configuration files: touch ./.platform/routes.yaml touch ./.platform/services.yaml 2. Create the backend application Create a subfolder (mkdir backend) and add the following files: ./backend/index.php 'I come from the backend' ]); ./backend/.platform.app.yaml name: backend type: \"php:7.3\" disk: 256 web: locations: '/': root: '' passthru: '/index.php' Edit ./.platform/routes.yaml and add the configuration for the backend application: \"https://backend.{default}/\": type: upstream upstream: \"backend:http\" Any request going to https://backend.{default}/ will be dispatched to the backend application. Commit the changes and deploy on the http://Platform.sh environment: git add . git commit -m \"Add backend app\" git push platform master The backend application can be accessed at: https://backend.master-7rqtwti- ..platformsh.site/ Verify that the application returns the message: curl https://backend.master-7rqtwti- ..platformsh.site/ | json_pp { \"message\" : \"I come from the backend\" } 3. Create the frontend application Create the frontend folder: mkdir frontend Add the configuration file ./frontend/.platform.app.yaml: name: frontend type: \"php:7.3\" disk: 256 web: locations: '/': root: '' passthru: '/index.php' ./frontend/index.php \r\n\r\nI am the frontend application\r\n\r\n Add the following configuration in ./routes.yaml: \"https://{default}/\": type: upstream upstream: \"frontend:http\" The configuration will make any incoming request to the default domain sent to our frontend application. Commit and deploy the applications: git add . git commit -m \"Add frontend application\" git push platform master The frontend is now working: curl https://master-7rqtwti- ..platformsh.site/ \r\n\r\nI am the frontend application\r\n\r\n 4. Making an internal request The public route to the backend could be used but using an internal request is way more efficient and secure. The first step is to add the relationship. Like any other data service, this will be added in ./frontend/.platform.app.yaml relationships: backend: 'backend:http' The key of the relationship will be the one used in the $PLATFORM_RELATIONSHIPS array. The first parameter is the name of our second application and http is used as the protocol there. Read more on our relationships system in . Commit and deploy the configuration change: git add . git commit -m \"Add relationship\" git push platform master Test the relationship by connecting to the frontend application: platform ssh Enter a number to choose an app: [0] frontend [1] backend \r\n\r\n 0 The relationships are exposed through the $PLATFORM_RELATIONSHIPS environment variable: web@ :~$ base64 -d :~$ curl http://backend.internal {\"message\":\"I come from the backend\"} 5. Load the message from the frontend Replace ./frontend/index.php with the following content: \r\n\r\n'.$message.'\r\n\r\n'; Our https://github.com/platformsh/platformsh-client-php implements a lot of other features automatically in a project to avoid fetching configuration settings manually. And redeploy your project: git add . git commit -m \"Add backend call on frontend\" git push platform master The message from the backend is now displayed on our frontend application: curl https://master-7rqtwti- . .platformsh.site/ \r\n\r\nI come from the backend\r\n\r\n Conclusion Making internal requests on different applications inside the same project is possible by leveraging the relationships mechanism implemented by http://Platform.sh.",
        "text": "Goal This guide shows how make an internal request to a different application in the same project. The goal is to display a message sent by a backend API on our frontend. Assumptions To complete this, you will need: An empty http://Platform.sh project The https://docs.platform.sh/gettingstarted/cli.html tool installed Your ssh key loaded in your https://linux.die.net/man/1/ssh-agent and configured in the https://accounts.platform.sh/user Steps In order to implement this, setup two different applications in your project: frontend and backend. Please refer to the . PHP will be used for both applications. The steps below will create the following project structure: multiapp/ .platform/ routes.yaml services.yaml backend/ .platform.app.yaml index.php frontend/ .platform.app.yaml index.php 1. Setup the base project Create an empty directory, initialize git and add the http://Platform.sh remote: mkdir multiapp cd !$ git init . platform project:set https://.platform.sh/projects/ The git remotes should now be: git remote -v platform @git..platform.sh: .git (fetch) platform @git..platform.sh: .git (push) Create the two required http://Platform.sh configuration files: touch ./.platform/routes.yaml touch ./.platform/services.yaml 2. Create the backend application Create a subfolder (mkdir backend) and add the following files: ./backend/index.php 'I come from the backend' ]); ./backend/.platform.app.yaml name: backend type: \"php:7.3\" disk: 256 web: locations: '/': root: '' passthru: '/index.php' Edit ./.platform/routes.yaml and add the configuration for the backend application: \"https://backend.{default}/\": type: upstream upstream: \"backend:http\" Any request going to https://backend.{default}/ will be dispatched to the backend application. Commit the changes and deploy on the http://Platform.sh environment: git add . git commit -m \"Add backend app\" git push platform master The backend application can be accessed at: https://backend.master-7rqtwti- ..platformsh.site/ Verify that the application returns the message: curl https://backend.master-7rqtwti- ..platformsh.site/ | json_pp { \"message\" : \"I come from the backend\" } 3. Create the frontend application Create the frontend folder: mkdir frontend Add the configuration file ./frontend/.platform.app.yaml: name: frontend type: \"php:7.3\" disk: 256 web: locations: '/': root: '' passthru: '/index.php' ./frontend/index.php \r\n\r\nI am the frontend application\r\n\r\n Add the following configuration in ./routes.yaml: \"https://{default}/\": type: upstream upstream: \"frontend:http\" The configuration will make any incoming request to the default domain sent to our frontend application. Commit and deploy the applications: git add . git commit -m \"Add frontend application\" git push platform master The frontend is now working: curl https://master-7rqtwti- ..platformsh.site/ \r\n\r\nI am the frontend application\r\n\r\n 4. Making an internal request The public route to the backend could be used but using an internal request is way more efficient and secure. The first step is to add the relationship. Like any other data service, this will be added in ./frontend/.platform.app.yaml relationships: backend: 'backend:http' The key of the relationship will be the one used in the $PLATFORM_RELATIONSHIPS array. The first parameter is the name of our second application and http is used as the protocol there. Read more on our relationships system in . Commit and deploy the configuration change: git add . git commit -m \"Add relationship\" git push platform master Test the relationship by connecting to the frontend application: platform ssh Enter a number to choose an app: [0] frontend [1] backend \r\n\r\n 0 The relationships are exposed through the $PLATFORM_RELATIONSHIPS environment variable: web@ :~$ base64 -d :~$ curl http://backend.internal {\"message\":\"I come from the backend\"} 5. Load the message from the frontend Replace ./frontend/index.php with the following content: \r\n\r\n'.$message.'\r\n\r\n'; Our https://github.com/platformsh/platformsh-client-php implements a lot of other features automatically in a project to avoid fetching configuration settings manually. And redeploy your project: git add . git commit -m \"Add backend call on frontend\" git push platform master The message from the backend is now displayed on our frontend application: curl https://master-7rqtwti- . .platformsh.site/ \r\n\r\nI come from the backend\r\n\r\n Conclusion Making internal requests on different applications inside the same project is possible by leveraging the relationships mechanism implemented by http://Platform.sh.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-make-internal-requests-between-two-applications-of-a-project/77",
        "relurl": "/t/how-to-make-internal-requests-between-two-applications-of-a-project/77"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "fc16388107f31957e32ec97afeb7b7990ff0f5f8",
        "title": "How to add or remove users from projects using the command line",
        "description": "Goal Add or remove a user to one or more http://Platform.sh projects using the platform https://github.com/platformsh/platformsh-cli . Assumptions You have one or more http://Platform.sh projects and you have installed the platform https://github.com/platformsh/platformsh-cli . You will need to know the project IDs. You will need the email address for each user you wish to add or remove. Problems Sometimes it is necessary to automate user access to projects, especially when managing fleets of sites. This is a case for using the CLI. Steps 1. Single projects Add user user1@example.com with the project-wide role of viewer. Confirm the change with Y. platform user:add -p -r viewer user1@example.com Adding the user user1@example.com to ( ): Project role: viewer Adding users can result in additional charges. Are you sure you want to add this user? [Y/n] Y Add user user1@example.com to the staging branch as a contributor: platform user:add -p -r staging:contributor user1@example.com Adding the user user1@example.com to ( ): Project role: viewer Role on staging: contributor Adding users can result in additional charges. Are you sure you want to add this user? [Y/n] Y 2. Multiple projects Remove a user ( mailto:user1@example.com) from multiple projects: platform multi --projects , \\ 'user:delete user1@example.com' This is the output: Running command 'user:delete user1@example.com' on 2 projects. * Project: ( ) Are you sure you want to delete the user user1@example.com? [Y/n] y User user1@example.com deleted * Project: ( ) Are you sure you want to delete the user user1@example.com? [Y/n] y User user1@example.com deleted Conclusion User management, including adding, deleting, and role assignment, can be done from the command line using the http://Platform.sh CLI. Applying the platform multi command allows you to make changes on multiple projects at once.",
        "text": "Goal Add or remove a user to one or more http://Platform.sh projects using the platform https://github.com/platformsh/platformsh-cli . Assumptions You have one or more http://Platform.sh projects and you have installed the platform https://github.com/platformsh/platformsh-cli . You will need to know the project IDs. You will need the email address for each user you wish to add or remove. Problems Sometimes it is necessary to automate user access to projects, especially when managing fleets of sites. This is a case for using the CLI. Steps 1. Single projects Add user user1@example.com with the project-wide role of viewer. Confirm the change with Y. platform user:add -p -r viewer user1@example.com Adding the user user1@example.com to ( ): Project role: viewer Adding users can result in additional charges. Are you sure you want to add this user? [Y/n] Y Add user user1@example.com to the staging branch as a contributor: platform user:add -p -r staging:contributor user1@example.com Adding the user user1@example.com to ( ): Project role: viewer Role on staging: contributor Adding users can result in additional charges. Are you sure you want to add this user? [Y/n] Y 2. Multiple projects Remove a user ( mailto:user1@example.com) from multiple projects: platform multi --projects , \\ 'user:delete user1@example.com' This is the output: Running command 'user:delete user1@example.com' on 2 projects. * Project: ( ) Are you sure you want to delete the user user1@example.com? [Y/n] y User user1@example.com deleted * Project: ( ) Are you sure you want to delete the user user1@example.com? [Y/n] y User user1@example.com deleted Conclusion User management, including adding, deleting, and role assignment, can be done from the command line using the http://Platform.sh CLI. Applying the platform multi command allows you to make changes on multiple projects at once.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-add-or-remove-users-from-projects-using-the-command-line/91",
        "relurl": "/t/how-to-add-or-remove-users-from-projects-using-the-command-line/91"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e9b1d243a15e36ca213c3db2cf271e0c4119537e",
        "title": "How to configure low-disk health notifications on Platform.sh",
        "description": "Goal Set up health notifications for low-disks on project containers. These will notify when free disk space drops below 20% and below 10%, and will issue an all-clear notification when the free disk space returns above 20%. Assumptions You have a project on You are an admin on the project You have the https://docs.platform.sh/gettingstarted/cli.html tool installed Problems Container storage can silently fill due to any number of reasons. This will often cause an application to experience erratic errors or cease to function entirely. Steps 1. Email Notifications Add the integration for email using the Platform CLI: platform integration:add --type health.email --from-address you@example.com --recipients them@example.com --recipients others@example.com Note that the --from-address can be any email address you like (including one of the recipients), and you can define one or more recipients by optionally adding more recipient flags. 2. Slack Notifications Create a custom https://api.slack.com/bot-users for your Slack group, configure it for the appropriate channels, and note the API Token. Add the integration for slack using the Platform CLI: platform integration:add --type health.slack --token YOUR_API_TOKEN --channel '#channelname' The bot will then post the notification to the channel specified by the --channel flag. 3. PagerDuty Notifications Create a PagerDuty https://support.pagerduty.com/docs/services-and-integrations using the Events API v2. Copy the Integration Key to use as the --routing-key flag in step (2). Add the integration for PagerDuty using the Platform CLI: platform integration:add --type health.pagerduty --routing-key YOUR_ROUTING_KEY 4. What to do when you get a low-disk notification There are several different options for resolving low-disk situations. For a database, depending on the application set up, you may be able to truncate a table in MySQL that is performing a logging or caching function. For an application container, you can use du -sh to work your way through your filesystem to find if there are any particular files that are taking a lot of space that can be removed. Alternatively, if the space used is necessary and will continue to grow, you can change the size of your disk to resolve the low-disk situation. For the application disk, you can change this with the disk: key in your .platform.app.yaml file. For any other services, their disk sizes can be adjusted from the relevant disk: key in your .platform/services.yaml file. Conclusion Using low-disk health notification with your environment can help you catch outages before they happen, and can be a red flag if they do lead to an outage.",
        "text": "Goal Set up health notifications for low-disks on project containers. These will notify when free disk space drops below 20% and below 10%, and will issue an all-clear notification when the free disk space returns above 20%. Assumptions You have a project on You are an admin on the project You have the https://docs.platform.sh/gettingstarted/cli.html tool installed Problems Container storage can silently fill due to any number of reasons. This will often cause an application to experience erratic errors or cease to function entirely. Steps 1. Email Notifications Add the integration for email using the Platform CLI: platform integration:add --type health.email --from-address you@example.com --recipients them@example.com --recipients others@example.com Note that the --from-address can be any email address you like (including one of the recipients), and you can define one or more recipients by optionally adding more recipient flags. 2. Slack Notifications Create a custom https://api.slack.com/bot-users for your Slack group, configure it for the appropriate channels, and note the API Token. Add the integration for slack using the Platform CLI: platform integration:add --type health.slack --token YOUR_API_TOKEN --channel '#channelname' The bot will then post the notification to the channel specified by the --channel flag. 3. PagerDuty Notifications Create a PagerDuty https://support.pagerduty.com/docs/services-and-integrations using the Events API v2. Copy the Integration Key to use as the --routing-key flag in step (2). Add the integration for PagerDuty using the Platform CLI: platform integration:add --type health.pagerduty --routing-key YOUR_ROUTING_KEY 4. What to do when you get a low-disk notification There are several different options for resolving low-disk situations. For a database, depending on the application set up, you may be able to truncate a table in MySQL that is performing a logging or caching function. For an application container, you can use du -sh to work your way through your filesystem to find if there are any particular files that are taking a lot of space that can be removed. Alternatively, if the space used is necessary and will continue to grow, you can change the size of your disk to resolve the low-disk situation. For the application disk, you can change this with the disk: key in your .platform.app.yaml file. For any other services, their disk sizes can be adjusted from the relevant disk: key in your .platform/services.yaml file. Conclusion Using low-disk health notification with your environment can help you catch outages before they happen, and can be a red flag if they do lead to an outage.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-configure-low-disk-health-notifications-on-platform-sh/121",
        "relurl": "/t/how-to-configure-low-disk-health-notifications-on-platform-sh/121"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "d0496e7c498d3c3a55511ca6fdf7022cfc5fed48",
        "title": "How to interactively debug Node.js applications on Platform.sh",
        "description": "Goal Interactively debug nodejs applications running on http://Platform.sh, Assumptions A Node.js application that is running on http://Platform.sh An issue that requires interactive debugging, possibly because it can’t be reproduced locally The http://platform.sh CLI Problems Effectively debugging web applications isn’t trivial, especially when an HTTP request goes through multiple layers until it reaches the web app, which is usually the case in cloud platforms. Debugging in a local environment is easier, but the bug might only trigger when interacting with the data that are present on production. Steps The debugging procedure should only be performed on a non-production environment. So, if this is a production issue, first clone the production environment. http://Platform.sh creates byte-for-byte clones of an environment including the data so there basically is a reproducibility guarantee. 1. Clone production Using the CLI: platform branch debug-weird-production-issue master This will create a new environment debug-weird-production-issue, based on the master branch which is the production environment. It will also checkout the branch locally. 2. Restart the application daemon in debug mode SSH into this environment with the command: platform ssh By default it will connect the current environment branch, in this case debug-weird-production-issue Stop the running process with the command: sv stop app Run the application in debug mode: node inspect server.js Note: server.js is the entry point to the application as specified in the platform.app.yaml in the web.start key, so modify this command for the entry point used. It will output something similar to: Debugger listening on ws://127.0.0.1:9229/663e3d85-31b0-4f2c-aeb4-94e2fae886cf Copy the path at the end of the outputted URL (i.e., 663e3d85-31b0-4f2c-aeb4-94e2fae886cf) 3. Forward the debugger port locally In a second terminal window, create an SSH tunnel that forwards the 9229 port: ssh -N -L9229:127.0.0.1:9229 $(platform ssh --pipe) 4. Profit! Chrome Dev tools can be used Open the Chrome web browser and visit the url: chrome-devtools://devtools/bundled/js_app.html?experiments=true\u0026v8only=true\u0026ws=127.0.0.1:9229/{path} Replace {path} with the path that was copied earlier. For example: chrome-devtools://devtools/bundled/js_app.html?experiments=true\u0026v8only=true\u0026ws=127.0.0.1:9229/663e3d85-31b0-4f2c-aeb4-94e2fae886cf Conclusion Debugging complex bugs that are only reproduced with production data present is non-trivial. Using a clone of the production data, plus the dynamic nature of Node.js and the fabulous Chrome Dev tools, any bug can be properly investigated.",
        "text": "Goal Interactively debug nodejs applications running on http://Platform.sh, Assumptions A Node.js application that is running on http://Platform.sh An issue that requires interactive debugging, possibly because it can’t be reproduced locally The http://platform.sh CLI Problems Effectively debugging web applications isn’t trivial, especially when an HTTP request goes through multiple layers until it reaches the web app, which is usually the case in cloud platforms. Debugging in a local environment is easier, but the bug might only trigger when interacting with the data that are present on production. Steps The debugging procedure should only be performed on a non-production environment. So, if this is a production issue, first clone the production environment. http://Platform.sh creates byte-for-byte clones of an environment including the data so there basically is a reproducibility guarantee. 1. Clone production Using the CLI: platform branch debug-weird-production-issue master This will create a new environment debug-weird-production-issue, based on the master branch which is the production environment. It will also checkout the branch locally. 2. Restart the application daemon in debug mode SSH into this environment with the command: platform ssh By default it will connect the current environment branch, in this case debug-weird-production-issue Stop the running process with the command: sv stop app Run the application in debug mode: node inspect server.js Note: server.js is the entry point to the application as specified in the platform.app.yaml in the web.start key, so modify this command for the entry point used. It will output something similar to: Debugger listening on ws://127.0.0.1:9229/663e3d85-31b0-4f2c-aeb4-94e2fae886cf Copy the path at the end of the outputted URL (i.e., 663e3d85-31b0-4f2c-aeb4-94e2fae886cf) 3. Forward the debugger port locally In a second terminal window, create an SSH tunnel that forwards the 9229 port: ssh -N -L9229:127.0.0.1:9229 $(platform ssh --pipe) 4. Profit! Chrome Dev tools can be used Open the Chrome web browser and visit the url: chrome-devtools://devtools/bundled/js_app.html?experiments=true\u0026v8only=true\u0026ws=127.0.0.1:9229/{path} Replace {path} with the path that was copied earlier. For example: chrome-devtools://devtools/bundled/js_app.html?experiments=true\u0026v8only=true\u0026ws=127.0.0.1:9229/663e3d85-31b0-4f2c-aeb4-94e2fae886cf Conclusion Debugging complex bugs that are only reproduced with production data present is non-trivial. Using a clone of the production data, plus the dynamic nature of Node.js and the fabulous Chrome Dev tools, any bug can be properly investigated.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-interactively-debug-node-js-applications-on-platform-sh/172",
        "relurl": "/t/how-to-interactively-debug-node-js-applications-on-platform-sh/172"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "c8ceabe4e192df4fbf57f192d02833813e3da88b",
        "title": "How to see your Nginx configuration file",
        "description": "Goal The of .platform.app.yaml is the developer’s chance to configure the Nginx server for the application. Sometimes it is helpful to see exactly what is in the Nginx configuration file as a result of the values in web. This guide shows how to read the Nginx config file using the http://Platform.sh CLI tool. Assumptions This guide assumes: an active application on http://Platform.sh A local repository with the http://platform.sh/ project as git remote The https://docs.platform.sh/gettingstarted/cli.html installed locally Problems Reading the Nginx config file is a matter of being able to make an SSH connection to your environment and then knowing where the file is. Steps The nginx.conf file can be read with this command: platform ssh 'less /etc/nginx/nginx.conf' Conclusion Nginx configuration is abstracted into the web directive of .platform.app.yaml, but you can still see how that gets expressed by looking at the resultant file directly.",
        "text": "Goal The of .platform.app.yaml is the developer’s chance to configure the Nginx server for the application. Sometimes it is helpful to see exactly what is in the Nginx configuration file as a result of the values in web. This guide shows how to read the Nginx config file using the http://Platform.sh CLI tool. Assumptions This guide assumes: an active application on http://Platform.sh A local repository with the http://platform.sh/ project as git remote The https://docs.platform.sh/gettingstarted/cli.html installed locally Problems Reading the Nginx config file is a matter of being able to make an SSH connection to your environment and then knowing where the file is. Steps The nginx.conf file can be read with this command: platform ssh 'less /etc/nginx/nginx.conf' Conclusion Nginx configuration is abstracted into the web directive of .platform.app.yaml, but you can still see how that gets expressed by looking at the resultant file directly.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-see-your-nginx-configuration-file/168",
        "relurl": "/t/how-to-see-your-nginx-configuration-file/168"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "900cbf71b33dfebbb8ef5e857eaee4c6322b518e",
        "title": "How to utilize the CLI to modify already existing Git integrations",
        "description": "Goal To learn how to use the CLI for getting more information on a Git integration, also how to modify certain configuration options. Assumptions Access to a project hosted on http://platform.sh/ Your project account has administrator rights Knowledge on using the Problems None in particular. This How to is about raising awareness to the usefulness of the CLI tool. Steps 1. Check what “integration” commands are available platform list | grep integration integration integration:add Add an integration to the project integration:delete Delete an integration from a project integration:get View details of an integration integration:list (integrations) View a list of project integration(s) integration:update Update an integration integration:validate Validate an existing integration 2. List the integrations added to your project platform integration:list +------------------+--------+-----------------------------------------------------------------------------------------------------+ | ID | Type | Summary | +------------------+--------+-----------------------------------------------------------------------------------------------------+ | | github | Repository: ... | | | | Hook URL: https://.platform.sh/api/projects/ /integrations/ /hook | +------------------+--------+-----------------------------------------------------------------------------------------------------+ View integration details with: platform integration:get [id] Add a new integration with: platform integration:add Delete an integration with: platform integration:delete [id] 3. Get the integration details platform integration:get +---------------------------------+-------------------------------------------------------------------------------------------+ | Property | Value | +---------------------------------+-------------------------------------------------------------------------------------------+ | id | | | type | github | | token | ****** | | repository | ... | | fetch_branches | true | | prune_branches | true | | build_pull_requests | true | | build_pull_requests_post_merge | false | | pull_requests_clone_parent_data | true | | hook_url | https://.platform.sh/api/projects/ /integrations/ /hook | +---------------------------------+-------------------------------------------------------------------------------------------+ 4. Check the options and select which ones to modify platform integration:update --help Command: integration:update Description: Update an integration Usage: platform integration:update [--type TYPE] [--token TOKEN] [--key KEY] [--secret SECRET] [--base-url BASE-URL] [--gitlab-project GITLAB-PROJECT] [--repository REPOSITORY] [--build-merge-requests BUILD-MERGE-REQUESTS] [--build-pu Arguments: id The ID of the integration to update Options: --type=TYPE The integration type ('bitbucket', 'github', 'gitlab', 'hipchat', 'webhook', 'health.email', 'health.pagerduty', 'health.slack') --token=TOKEN An OAuth token for the integration --key=KEY A Bitbucket OAuth consumer key --secret=SECRET A Bitbucket OAuth consumer secret --base-url=BASE-URL The base URL of the GitLab installation --gitlab-project=GITLAB-PROJECT The GitLab project (e.g. 'namespace/repo') --repository=REPOSITORY The repository to track (e.g. 'user/repo') --build-merge-requests=BUILD-MERGE-REQUESTS GitLab: build merge requests as environments [default: true] --build-pull-requests=BUILD-PULL-REQUESTS Build every pull request as an environment [default: true] --build-pull-requests-post-merge=BUILD-PULL-REQUESTS-POST-MERGE Build pull requests based on their post-merge state [default: false] --merge-requests-clone-parent-data=MERGE-REQUESTS-CLONE-PARENT-DATA GitLab: clone data for merge requests [default: true] --pull-requests-clone-parent-data=PULL-REQUESTS-CLONE-PARENT-DATA Clone the parent environment's data for pull requests [default: true] --resync-pull-requests=RESYNC-PULL-REQUESTS Re-sync pull request environment data on every build [default: false] --fetch-branches=FETCH-BRANCHES Fetch all branches from the remote (as inactive environments) [default: true] --prune-branches=PRUNE-BRANCHES Delete branches that do not exist on the remote [default: true] --room=ROOM HipChat room ID --url=URL Generic webhook: a URL to receive JSON data --events=EVENTS A list of events to report, e.g. environment.push [default: [\"*\"]] (multiple values allowed) --states=STATES A list of states to report, e.g. pending, in_progress, complete [default: [\"complete\"]] (multiple values allowed) --environments=ENVIRONMENTS The environment IDs to include [default: [\"*\"]] (multiple values allowed) --excluded-environments=EXCLUDED-ENVIRONMENTS The environment IDs to exclude (multiple values allowed) --from-address=FROM-ADDRESS The From address for alert emails [default: \"noreply@platform.sh\"] --recipients=RECIPIENTS The recipient email address(es) (multiple values allowed) --channel=CHANNEL The Slack channel --routing-key=ROUTING-KEY The PagerDuty routing key -p, --project=PROJECT The project ID or URL --host=HOST The project's API hostname -W, --no-wait Do not wait for the operation to complete --wait Wait for the operation to complete (default) -h, --help Display this help message -q, --quiet Do not output any message -V, --version Display this application version -y, --yes Answer \"yes\" to all prompts; disable interaction -n, --no Answer \"no\" to all prompts -v|vv|vvv, --verbose Increase the verbosity of messages Examples: Switch on the \"fetch branches\" option for a specific integration: platform integration:update ZXhhbXBsZSB --fetch-branches 1 Conclusion The CLI is a very handy tool for listing information about integrations, also for adding new ones (or modifying those already existing).",
        "text": "Goal To learn how to use the CLI for getting more information on a Git integration, also how to modify certain configuration options. Assumptions Access to a project hosted on http://platform.sh/ Your project account has administrator rights Knowledge on using the Problems None in particular. This How to is about raising awareness to the usefulness of the CLI tool. Steps 1. Check what “integration” commands are available platform list | grep integration integration integration:add Add an integration to the project integration:delete Delete an integration from a project integration:get View details of an integration integration:list (integrations) View a list of project integration(s) integration:update Update an integration integration:validate Validate an existing integration 2. List the integrations added to your project platform integration:list +------------------+--------+-----------------------------------------------------------------------------------------------------+ | ID | Type | Summary | +------------------+--------+-----------------------------------------------------------------------------------------------------+ | | github | Repository: ... | | | | Hook URL: https://.platform.sh/api/projects/ /integrations/ /hook | +------------------+--------+-----------------------------------------------------------------------------------------------------+ View integration details with: platform integration:get [id] Add a new integration with: platform integration:add Delete an integration with: platform integration:delete [id] 3. Get the integration details platform integration:get +---------------------------------+-------------------------------------------------------------------------------------------+ | Property | Value | +---------------------------------+-------------------------------------------------------------------------------------------+ | id | | | type | github | | token | ****** | | repository | ... | | fetch_branches | true | | prune_branches | true | | build_pull_requests | true | | build_pull_requests_post_merge | false | | pull_requests_clone_parent_data | true | | hook_url | https://.platform.sh/api/projects/ /integrations/ /hook | +---------------------------------+-------------------------------------------------------------------------------------------+ 4. Check the options and select which ones to modify platform integration:update --help Command: integration:update Description: Update an integration Usage: platform integration:update [--type TYPE] [--token TOKEN] [--key KEY] [--secret SECRET] [--base-url BASE-URL] [--gitlab-project GITLAB-PROJECT] [--repository REPOSITORY] [--build-merge-requests BUILD-MERGE-REQUESTS] [--build-pu Arguments: id The ID of the integration to update Options: --type=TYPE The integration type ('bitbucket', 'github', 'gitlab', 'hipchat', 'webhook', 'health.email', 'health.pagerduty', 'health.slack') --token=TOKEN An OAuth token for the integration --key=KEY A Bitbucket OAuth consumer key --secret=SECRET A Bitbucket OAuth consumer secret --base-url=BASE-URL The base URL of the GitLab installation --gitlab-project=GITLAB-PROJECT The GitLab project (e.g. 'namespace/repo') --repository=REPOSITORY The repository to track (e.g. 'user/repo') --build-merge-requests=BUILD-MERGE-REQUESTS GitLab: build merge requests as environments [default: true] --build-pull-requests=BUILD-PULL-REQUESTS Build every pull request as an environment [default: true] --build-pull-requests-post-merge=BUILD-PULL-REQUESTS-POST-MERGE Build pull requests based on their post-merge state [default: false] --merge-requests-clone-parent-data=MERGE-REQUESTS-CLONE-PARENT-DATA GitLab: clone data for merge requests [default: true] --pull-requests-clone-parent-data=PULL-REQUESTS-CLONE-PARENT-DATA Clone the parent environment's data for pull requests [default: true] --resync-pull-requests=RESYNC-PULL-REQUESTS Re-sync pull request environment data on every build [default: false] --fetch-branches=FETCH-BRANCHES Fetch all branches from the remote (as inactive environments) [default: true] --prune-branches=PRUNE-BRANCHES Delete branches that do not exist on the remote [default: true] --room=ROOM HipChat room ID --url=URL Generic webhook: a URL to receive JSON data --events=EVENTS A list of events to report, e.g. environment.push [default: [\"*\"]] (multiple values allowed) --states=STATES A list of states to report, e.g. pending, in_progress, complete [default: [\"complete\"]] (multiple values allowed) --environments=ENVIRONMENTS The environment IDs to include [default: [\"*\"]] (multiple values allowed) --excluded-environments=EXCLUDED-ENVIRONMENTS The environment IDs to exclude (multiple values allowed) --from-address=FROM-ADDRESS The From address for alert emails [default: \"noreply@platform.sh\"] --recipients=RECIPIENTS The recipient email address(es) (multiple values allowed) --channel=CHANNEL The Slack channel --routing-key=ROUTING-KEY The PagerDuty routing key -p, --project=PROJECT The project ID or URL --host=HOST The project's API hostname -W, --no-wait Do not wait for the operation to complete --wait Wait for the operation to complete (default) -h, --help Display this help message -q, --quiet Do not output any message -V, --version Display this application version -y, --yes Answer \"yes\" to all prompts; disable interaction -n, --no Answer \"no\" to all prompts -v|vv|vvv, --verbose Increase the verbosity of messages Examples: Switch on the \"fetch branches\" option for a specific integration: platform integration:update ZXhhbXBsZSB --fetch-branches 1 Conclusion The CLI is a very handy tool for listing information about integrations, also for adding new ones (or modifying those already existing).",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-utilize-the-cli-to-modify-already-existing-git-integrations/144",
        "relurl": "/t/how-to-utilize-the-cli-to-modify-already-existing-git-integrations/144"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "bb81b905b1bbdd5aec2b1e607c01ecbbd93de02b",
        "title": "How to run Minio on Platform.sh",
        "description": "Goal https://minio.io/ offers cloud file storage with an S3 compatible API. This guide shows how to deploy it on http://platform.sh/. Assumptions To complete this, you will need: An empty http://platform.sh/ project The https://docs.platform.sh/gettingstarted/cli.html tool installed Your ssh key loaded in your https://linux.die.net/man/1/ssh-agent and configured in the https://accounts.platform.sh/user Steps 1. Create the application directory Create an empty directory and cd into it. mkdir minio cd minio Initialize git and set the platform Git remote: git init platform project:set-remote 2. Configure the application Minio will be configured to store its configuration and files in . Edit .platform/app.yaml to have the following contents: name: app # Any type will work here type: \"nodejs:10\" hooks: build: | set -e wget https://dl.minio.io/server/minio/release/linux-amd64/minio chmod +x minio mounts: 'minio_data': source: local source_path: minio_data_dir 'minio_config': source: local source_path: minio_config_dir web: upstream: socket_family: tcp protocol: http commands: start: | ./minio server /app/minio_data --address localhost:$PORT --config-dir /app/minio_config --certs-dir /app/minio_config/certs variables: env: MINIO_ACCESS_KEY: access MINIO_SECRET_KEY: changeme disk: 2048 Change the MINIO_ACCESS_KEY and MINIO_SECRET_KEY values as required. Modify the disk value based on storage requirements, it should be at least 2048. Define a route in .platform/routes.yaml : \"https://{default}/\": type: upstream upstream: \"app:http\" Add an empty .platform/services.yaml file: touch .platform/services.yaml 3. Add, commit and push: git add . git commit -m \"Minio configured\" git push platform master 4. Test by visiting the URL of your project: platform url The Minio web UI page will be displayed and can be logged in with the MINIO_ACCESS_KEY and MINIO_SECRET_KEY values specified in .platform.app.yaml previously. https://community.platform.sh/uploads/default/e917eef889ae00fcf9be73d28609aad9c14edacc Conclusion The Minio server and web UI are running and ready to handle file requests.",
        "text": "Goal https://minio.io/ offers cloud file storage with an S3 compatible API. This guide shows how to deploy it on http://platform.sh/. Assumptions To complete this, you will need: An empty http://platform.sh/ project The https://docs.platform.sh/gettingstarted/cli.html tool installed Your ssh key loaded in your https://linux.die.net/man/1/ssh-agent and configured in the https://accounts.platform.sh/user Steps 1. Create the application directory Create an empty directory and cd into it. mkdir minio cd minio Initialize git and set the platform Git remote: git init platform project:set-remote 2. Configure the application Minio will be configured to store its configuration and files in . Edit .platform/app.yaml to have the following contents: name: app # Any type will work here type: \"nodejs:10\" hooks: build: | set -e wget https://dl.minio.io/server/minio/release/linux-amd64/minio chmod +x minio mounts: 'minio_data': source: local source_path: minio_data_dir 'minio_config': source: local source_path: minio_config_dir web: upstream: socket_family: tcp protocol: http commands: start: | ./minio server /app/minio_data --address localhost:$PORT --config-dir /app/minio_config --certs-dir /app/minio_config/certs variables: env: MINIO_ACCESS_KEY: access MINIO_SECRET_KEY: changeme disk: 2048 Change the MINIO_ACCESS_KEY and MINIO_SECRET_KEY values as required. Modify the disk value based on storage requirements, it should be at least 2048. Define a route in .platform/routes.yaml : \"https://{default}/\": type: upstream upstream: \"app:http\" Add an empty .platform/services.yaml file: touch .platform/services.yaml 3. Add, commit and push: git add . git commit -m \"Minio configured\" git push platform master 4. Test by visiting the URL of your project: platform url The Minio web UI page will be displayed and can be logged in with the MINIO_ACCESS_KEY and MINIO_SECRET_KEY values specified in .platform.app.yaml previously. https://community.platform.sh/uploads/default/e917eef889ae00fcf9be73d28609aad9c14edacc Conclusion The Minio server and web UI are running and ready to handle file requests.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-run-minio-on-platform-sh/128",
        "relurl": "/t/how-to-run-minio-on-platform-sh/128"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "6cea8d5953bb07afa63e44b5ed9fe79c9d7610a2",
        "title": "How to rely on a PKI (Public Key Infrastructure) on Platform.sh",
        "description": "Goal Using https://en.wikipedia.org/wiki/Public_key_infrastructure on your http://Platform.sh project for authentication. Background PKI is used to: centralize the authentication process automatically provide new accounts for authentication avoid a dependency on a centralized authentication server which might not be available at all times automatically expire accounts for authentication after a given period (unless they refresh) PKI starts with the root certificate authority who keeps a private key and a certificate. The certificate is public, and can be used to verify users’ certificates. When a new user (automated or not) needs a new certificate, they: generate a private key generate a Certificate Signing Request (CSR) that is sent to the root certificate authority The root certificate authority receives the CSR, validates that the user is valid, and returns a signed certificate to the user. The user can then use this signed certificate to authenticate. When a server wants to authenticate a certificate, it uses the root certificate authority’s certificate to verify the signature in the user’s certificate. This verification can be done on http://Platform.sh, which is what this How-To will show. Assumptions You will need: A local git repository that has the http://Platform.sh project as a git remote A working application setup on the http://Platform.sh project openssl Steps 1. Create a PKI Create the certificate authority private key and certificate Create the private key that the certificate authority must keep private. $ openssl genrsa -out /tmp/rootCA.key 4096 Generating RSA private key, 4096 bit long modulus .....++ .................++ $ The key will look similar to this: $ head -n2 /tmp/rootCA.key -----BEGIN RSA PRIVATE KEY----- MIIJJwIBAAKCAgEAyxCraEAvZUnGZ1OVAynDVCjp0qGZvI0nHPwHBxYH5eJ96pB6 $ Based on this, create the public certificate: $ openssl req -x509 -new -nodes -key /tmp/rootCA.key -sha256 -days 365 -out /tmp/rootCA.crt Pass the previously-created key. openssl will ask a few questions, they’re not important for the purpose of this How-To. The public certificate will look similar to this: $ head -n2 /tmp/rootCA.crt -----BEGIN CERTIFICATE----- MIIFXTCCA0WgAwIBAgIJAPs0rURM8vWtMA0GCSqGSIb3DQEBCwUAMEUxCzAJBgNV $ Create the first user’s private key and CSR Now that the certificate authority is setup, create a user’s private key and CSR. This creates the private key: $ openssl genrsa -out /tmp/user.key 4096 Generating RSA private key, 4096 bit long modulus ...........................++ .++ $ And this creates the CSR: $ openssl req -new -key /tmp/user.key -out /tmp/user.csr openssl will ask again some questions that are not important for the purpose of this How-To. Create the first user’s public certificate Based on the explanation above, use the root certificate authority’s private key to sign the CSR, which creates a new public certificate for the user. $ openssl x509 -req -in /tmp/user.csr -CA /tmp/rootCA.crt -CAkey /tmp/rootCA.key \\ -CAcreateserial -out /tmp/user.crt -days 30 -sha256 This creates a public certificate that the user can use, valid for 30 days. Note that only the user can use this certificate to authenticate, because it needs the private key (“user.key” above). Et voila! We now have a PKI with one user. 2. Authenticate users of the PKI on http://Platform.sh We’re now getting to the easy part of this How-To: we’re going to authenticate our user on http://Platform.sh using . For this test, we’re going to need 3 things: The root certificate authority’s public certificate (rootCA.crt above) The user’s private key (user.key above) The user’s public certificate (user.crt above) The first step is configuring your http://Platform.sh project to authenticate users by providing the root certificate authority’s public certificate, as such in the .platform/routes.yaml: https://{default}/: type: upstream upstream: app:http tls: client_authentication: \"require\" client_certificate_authorities: - !include type: string path: rootCA.crt Put the rootCA.crt file in the .platform/ folder. Commit and push these changes: $ git add .platform/routes.yaml .platform/rootCA.crt $ git commit -m \"Enable TLS client certificates authentication\" $ git push platform master And that’s it! Test the changes: $ curl https://my-project.com curl: (35) error:14094412:SSL routines:ssl3_read_bytes:sslv3 alert bad certificate $ curl --key user.key --cert user.crt https://my-project.com It's working! Conclusion Setting up a http://Platform.sh project to authenticate users using PKI requires only a few lines of code added to the .platform/routes.yaml file.",
        "text": "Goal Using https://en.wikipedia.org/wiki/Public_key_infrastructure on your http://Platform.sh project for authentication. Background PKI is used to: centralize the authentication process automatically provide new accounts for authentication avoid a dependency on a centralized authentication server which might not be available at all times automatically expire accounts for authentication after a given period (unless they refresh) PKI starts with the root certificate authority who keeps a private key and a certificate. The certificate is public, and can be used to verify users’ certificates. When a new user (automated or not) needs a new certificate, they: generate a private key generate a Certificate Signing Request (CSR) that is sent to the root certificate authority The root certificate authority receives the CSR, validates that the user is valid, and returns a signed certificate to the user. The user can then use this signed certificate to authenticate. When a server wants to authenticate a certificate, it uses the root certificate authority’s certificate to verify the signature in the user’s certificate. This verification can be done on http://Platform.sh, which is what this How-To will show. Assumptions You will need: A local git repository that has the http://Platform.sh project as a git remote A working application setup on the http://Platform.sh project openssl Steps 1. Create a PKI Create the certificate authority private key and certificate Create the private key that the certificate authority must keep private. $ openssl genrsa -out /tmp/rootCA.key 4096 Generating RSA private key, 4096 bit long modulus .....++ .................++ $ The key will look similar to this: $ head -n2 /tmp/rootCA.key -----BEGIN RSA PRIVATE KEY----- MIIJJwIBAAKCAgEAyxCraEAvZUnGZ1OVAynDVCjp0qGZvI0nHPwHBxYH5eJ96pB6 $ Based on this, create the public certificate: $ openssl req -x509 -new -nodes -key /tmp/rootCA.key -sha256 -days 365 -out /tmp/rootCA.crt Pass the previously-created key. openssl will ask a few questions, they’re not important for the purpose of this How-To. The public certificate will look similar to this: $ head -n2 /tmp/rootCA.crt -----BEGIN CERTIFICATE----- MIIFXTCCA0WgAwIBAgIJAPs0rURM8vWtMA0GCSqGSIb3DQEBCwUAMEUxCzAJBgNV $ Create the first user’s private key and CSR Now that the certificate authority is setup, create a user’s private key and CSR. This creates the private key: $ openssl genrsa -out /tmp/user.key 4096 Generating RSA private key, 4096 bit long modulus ...........................++ .++ $ And this creates the CSR: $ openssl req -new -key /tmp/user.key -out /tmp/user.csr openssl will ask again some questions that are not important for the purpose of this How-To. Create the first user’s public certificate Based on the explanation above, use the root certificate authority’s private key to sign the CSR, which creates a new public certificate for the user. $ openssl x509 -req -in /tmp/user.csr -CA /tmp/rootCA.crt -CAkey /tmp/rootCA.key \\ -CAcreateserial -out /tmp/user.crt -days 30 -sha256 This creates a public certificate that the user can use, valid for 30 days. Note that only the user can use this certificate to authenticate, because it needs the private key (“user.key” above). Et voila! We now have a PKI with one user. 2. Authenticate users of the PKI on http://Platform.sh We’re now getting to the easy part of this How-To: we’re going to authenticate our user on http://Platform.sh using . For this test, we’re going to need 3 things: The root certificate authority’s public certificate (rootCA.crt above) The user’s private key (user.key above) The user’s public certificate (user.crt above) The first step is configuring your http://Platform.sh project to authenticate users by providing the root certificate authority’s public certificate, as such in the .platform/routes.yaml: https://{default}/: type: upstream upstream: app:http tls: client_authentication: \"require\" client_certificate_authorities: - !include type: string path: rootCA.crt Put the rootCA.crt file in the .platform/ folder. Commit and push these changes: $ git add .platform/routes.yaml .platform/rootCA.crt $ git commit -m \"Enable TLS client certificates authentication\" $ git push platform master And that’s it! Test the changes: $ curl https://my-project.com curl: (35) error:14094412:SSL routines:ssl3_read_bytes:sslv3 alert bad certificate $ curl --key user.key --cert user.crt https://my-project.com It's working! Conclusion Setting up a http://Platform.sh project to authenticate users using PKI requires only a few lines of code added to the .platform/routes.yaml file.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-rely-on-a-pki-public-key-infrastructure-on-platform-sh/80",
        "relurl": "/t/how-to-rely-on-a-pki-public-key-infrastructure-on-platform-sh/80"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "8cbb0fa5fc43892359e5bfea9eae975b7f58277f",
        "title": "How to run a Rust web application on Platform.sh",
        "description": "Goal This guide shows how to deploy a simple Rust web app on http://Platform.sh. Assumptions To complete this, you will need: An empty http://Platform.sh project The https://docs.platform.sh/gettingstarted/cli.html tool installed Your ssh key loaded in your https://linux.die.net/man/1/ssh-agent and configured in the https://accounts.platform.sh/user The Rust environment installed Problems http://Platform.sh does not provide a dedicated Rust runtime container. It will thus be necessary to install Rust as the project is built. Steps 1. Create a Rust application Create a new Rust project: cargo new platform_howto \u0026\u0026 cd platform_howto Set the platform remote: platform project:set-remote Add the following under [dependencies] in Cargo.toml: warp = \"0.1\" clap = \"2.32.0\" Replace src/main.rs with: #[macro_use] extern crate clap; use clap::{Arg, App}; use warp::Filter; fn main() { let args = App::new(\"HOWTO\") .version(\"1.0\") .author(\"Platform.sh \") .about(\"How to run a Rust app on Platform.sh\") .arg(Arg::with_name(\"port\") .short(\"p\") .long(\"port\") .value_name(\"PORT\") .help(\"Sets a custom port\") .takes_value(true) .required(true)) .get_matches(); // Get port from command-line arguments let port = value_t!(args, \"port\", u16).unwrap_or_else(|e| e.exit()); // Match any request and return hello world! let routes = warp::any().map(|| \"Hello, World!\"); warp::serve(routes) .run(([127, 0, 0, 1], port)); } 2. Configure the application to run on http://Platform.sh: Add the following to .platform.app.yaml: name: app # Any type will work here type: \"php:7.3\" hooks: build: | set -e curl https://sh.rustup.rs rustup.sh sh rustup.sh -y --default-toolchain stable rm rustup.sh . \"$HOME/.cargo/env\" cargo build --release web: upstream: socket_family: tcp protocol: http commands: start: | target/release/platform_howto --port $PORT disk: 1024 Define a route in .platform/routes.yaml: \"https://{default}/\": type: upstream upstream: \"app:http\" Add an empty .platform/services.yaml file: touch .platform/services.yaml 3. Add, commit and push: git add . git commit -m \"Rust on Platform.sh\" git push platform master 4. Test by visiting the URL of your project: platform url Which should open a browser tab showing Hello, World! Conclusion Even without a dedicated runtime, running a Rust application on http://Platform.sh can be done by leveraging the build hook and the PORT variable.",
        "text": "Goal This guide shows how to deploy a simple Rust web app on http://Platform.sh. Assumptions To complete this, you will need: An empty http://Platform.sh project The https://docs.platform.sh/gettingstarted/cli.html tool installed Your ssh key loaded in your https://linux.die.net/man/1/ssh-agent and configured in the https://accounts.platform.sh/user The Rust environment installed Problems http://Platform.sh does not provide a dedicated Rust runtime container. It will thus be necessary to install Rust as the project is built. Steps 1. Create a Rust application Create a new Rust project: cargo new platform_howto \u0026\u0026 cd platform_howto Set the platform remote: platform project:set-remote Add the following under [dependencies] in Cargo.toml: warp = \"0.1\" clap = \"2.32.0\" Replace src/main.rs with: #[macro_use] extern crate clap; use clap::{Arg, App}; use warp::Filter; fn main() { let args = App::new(\"HOWTO\") .version(\"1.0\") .author(\"Platform.sh \") .about(\"How to run a Rust app on Platform.sh\") .arg(Arg::with_name(\"port\") .short(\"p\") .long(\"port\") .value_name(\"PORT\") .help(\"Sets a custom port\") .takes_value(true) .required(true)) .get_matches(); // Get port from command-line arguments let port = value_t!(args, \"port\", u16).unwrap_or_else(|e| e.exit()); // Match any request and return hello world! let routes = warp::any().map(|| \"Hello, World!\"); warp::serve(routes) .run(([127, 0, 0, 1], port)); } 2. Configure the application to run on http://Platform.sh: Add the following to .platform.app.yaml: name: app # Any type will work here type: \"php:7.3\" hooks: build: | set -e curl https://sh.rustup.rs rustup.sh sh rustup.sh -y --default-toolchain stable rm rustup.sh . \"$HOME/.cargo/env\" cargo build --release web: upstream: socket_family: tcp protocol: http commands: start: | target/release/platform_howto --port $PORT disk: 1024 Define a route in .platform/routes.yaml: \"https://{default}/\": type: upstream upstream: \"app:http\" Add an empty .platform/services.yaml file: touch .platform/services.yaml 3. Add, commit and push: git add . git commit -m \"Rust on Platform.sh\" git push platform master 4. Test by visiting the URL of your project: platform url Which should open a browser tab showing Hello, World! Conclusion Even without a dedicated runtime, running a Rust application on http://Platform.sh can be done by leveraging the build hook and the PORT variable.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-run-a-rust-web-application-on-platform-sh/119",
        "relurl": "/t/how-to-run-a-rust-web-application-on-platform-sh/119"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "a42d531316a10112c87ac90bce3b63c84b5afca6",
        "title": "How to configure HTTP Strict Transport Security (HSTS) on your project",
        "description": "Goal Enable HSTS on your http://Platform.sh project. Read more about HSTS on https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security. Assumptions You will need: A local git repository that has the http://platform.sh/ project as a git remote A working application setup on the http://Platform.sh project This how-to works whether you use the auto-generated domains on platformsh.site or your own custom domain. Steps 1. Check https is enabled The ./.platform/routes.yaml should contain only the https directive: https://{default}/: type: upstream upstream: app:http 2. Add the strict_transport_security configuration item to your route file: ./.platform/routes.yaml https://{default}/: type: upstream upstream: app:http tls: strict_transport_security: enabled: true include_subdomains: true preload: true Please refer to the for details about the different parameters. 3. Commit and deploy the change git add .platform/routes.yaml git commit -m \"Enable HSTS\" git push platform master 4. Check HSTS is enabled Check the response headers being sent by the server. This can checked with the browser console or curl: $ curl -I https://master-7rqtwti- ..platformsh.site HTTP/2 200 ... strict-transport-security: max-age=31536000; includeSubDomains; preload ... Any request to the http endpoint will be upgraded to https: $ curl -I http://master-7rqtwti- ..platformsh.site HTTP/1.1 301 Moved Permanently ... Location: https://master-7rqtwti- ..platformsh.site/ Strict-Transport-Security: max-age=0 ... Conclusion As http://Platform.sh configuration already includes the strict_transport_security parameter, enabling HSTS was a simple configuration change without the need to customize the response directly in your application.",
        "text": "Goal Enable HSTS on your http://Platform.sh project. Read more about HSTS on https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security. Assumptions You will need: A local git repository that has the http://platform.sh/ project as a git remote A working application setup on the http://Platform.sh project This how-to works whether you use the auto-generated domains on platformsh.site or your own custom domain. Steps 1. Check https is enabled The ./.platform/routes.yaml should contain only the https directive: https://{default}/: type: upstream upstream: app:http 2. Add the strict_transport_security configuration item to your route file: ./.platform/routes.yaml https://{default}/: type: upstream upstream: app:http tls: strict_transport_security: enabled: true include_subdomains: true preload: true Please refer to the for details about the different parameters. 3. Commit and deploy the change git add .platform/routes.yaml git commit -m \"Enable HSTS\" git push platform master 4. Check HSTS is enabled Check the response headers being sent by the server. This can checked with the browser console or curl: $ curl -I https://master-7rqtwti- ..platformsh.site HTTP/2 200 ... strict-transport-security: max-age=31536000; includeSubDomains; preload ... Any request to the http endpoint will be upgraded to https: $ curl -I http://master-7rqtwti- ..platformsh.site HTTP/1.1 301 Moved Permanently ... Location: https://master-7rqtwti- ..platformsh.site/ Strict-Transport-Security: max-age=0 ... Conclusion As http://Platform.sh configuration already includes the strict_transport_security parameter, enabling HSTS was a simple configuration change without the need to customize the response directly in your application.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-configure-http-strict-transport-security-hsts-on-your-project/70",
        "relurl": "/t/how-to-configure-http-strict-transport-security-hsts-on-your-project/70"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "fc3faea1b6b875e3332761ea52b3ff3fbbf53117",
        "title": "How to upload and download files to your application (SFTP/RSYNC)",
        "description": "Goal This guide shows how to upload and download files to your application using sftp and rsync. Assumptions To complete this, you will need: A working application setup on a http://Platform.sh project The https://docs.platform.sh/gettingstarted/cli.html tool installed Your ssh key loaded in your https://linux.die.net/man/1/ssh-agent and configured in the https://accounts.platform.sh/user rsync installed on your system or scp installed on your system or an sftp GUI client like https://filezilla-project.org/ Problems http://Platform.sh provides but it can be handy to use native tools instead. Steps Add a mount point to your application By default, all files deployed to http://Platform.sh through git are read-only. Only the folders defined as mounts are writeable. Open ./.platform.app.yaml and add a mount: mounts: 'web/uploads': source: local source_path: uploads Please refer to the if needed. Push your configuration change: git add .platform.app.yaml git commit -m \"Add mount\" git push platform master Check the mount point is writeable platform ssh web@ :~$ touch web/uploads/test.txt The command above shouldn’t trigger any error. Get the environment ssh endpoint Both rsync and sftp protocols use ssh as the transfer layer. Each http://Platform.sh environment inside a project has its own ssh endpoint. If you have the http://Platform.sh CLI installed, you can run platform ssh --pipe to get the endpoint. If not, log into your project dashboard and click on the SSH button for the environment you want to get the endpoint: Our endpoint is -master-7rqtwti--app@ssh..platform.sh Note: All the next commands will be using platform ssh --pipe as the endpoint. Feel free to replace with the full connection string if needed. Create test files Let’s create some files to play with: mkdir -p web/uploads touch web/uploads/test{0001..0010}.txt rsync Upload a file or a directory To upload a directory, make sure you don’t specify the source folder (uploads in this case) in the destination. Take a look at man rsync to view all available options. rsync -avz web/uploads \"$(platform ssh --pipe)\":web/ The log should list all files that were uploaded: sending incremental file list uploads/ uploads/test0001.txt ... uploads/test0010.txt sent 603 bytes received 210 bytes 325.20 bytes/sec total size is 0 speedup is 0.00 We could have synced only the files with the following command: rsync -avz web/uploads/* rsync -avz web/uploads \"$(platform ssh --pipe)\":web/uploads/ As rsync is transferring only the differences between the source and destination, relaunching the same command will end up in an empty transfer. Download a file or a directory Remove your local files: rm web/uploads/* We can now download them from http://Platform.sh: rsync -avz \"$(platform ssh --pipe)\":web/uploads web/ You can see in the output that all files are transferred back to our host. scp Remove the files on http://Platform.sh if you have followed the rsync steps: platform ssh \"rm web/uploads/*\" Upload a file or a directory To upload a file, specify the full path: scp web/uploads/test0001.txt \"$(platform ssh --pipe)\":web/uploads To upload a directory, add the -r argument: scp -r web/uploads \"$(platform ssh --pipe)\":web/ The output should list the 10 files being transferred. Note that scp is not using incremental transfer. All 10 files are being transferred even if they already exist at the destination. Download a file or a directory Remove a local file first: rm web/uploads/test0001.txt To download a file: scp \"$(platform ssh --pipe)\":web/uploads/test0001.txt web/uploads/ To download a directory: scp -r \"$(platform ssh --pipe)\":web/uploads web/ sftp Remove the files on http://Platform.sh if you have followed the previous steps: platform ssh \"rm web/uploads/*\" Configure your client Add a new bookmark to your client with the following configuration (based on our endpoint): hostname: ssh.eu-3.platform.sh port: 22 protocol: SFTP user: -master-7rqtwti--app key/identity file: `/path/to/your/key Filezilla needs to have a .pem key file to list it. Connect to the site. You should be presented with the content of the root folder of your app. Upload a file or a directory Drag and drop files and folders from your computer to the mount: Download a file or a directory Drag and drop files and folders from the mount to your computer. Easy enough. Conclusion Transferring and managing files on your http://Platform.sh environments can be done using native tools that use ssh, scp, rsync, or sftp.",
        "text": "Goal This guide shows how to upload and download files to your application using sftp and rsync. Assumptions To complete this, you will need: A working application setup on a http://Platform.sh project The https://docs.platform.sh/gettingstarted/cli.html tool installed Your ssh key loaded in your https://linux.die.net/man/1/ssh-agent and configured in the https://accounts.platform.sh/user rsync installed on your system or scp installed on your system or an sftp GUI client like https://filezilla-project.org/ Problems http://Platform.sh provides but it can be handy to use native tools instead. Steps Add a mount point to your application By default, all files deployed to http://Platform.sh through git are read-only. Only the folders defined as mounts are writeable. Open ./.platform.app.yaml and add a mount: mounts: 'web/uploads': source: local source_path: uploads Please refer to the if needed. Push your configuration change: git add .platform.app.yaml git commit -m \"Add mount\" git push platform master Check the mount point is writeable platform ssh web@ :~$ touch web/uploads/test.txt The command above shouldn’t trigger any error. Get the environment ssh endpoint Both rsync and sftp protocols use ssh as the transfer layer. Each http://Platform.sh environment inside a project has its own ssh endpoint. If you have the http://Platform.sh CLI installed, you can run platform ssh --pipe to get the endpoint. If not, log into your project dashboard and click on the SSH button for the environment you want to get the endpoint: Our endpoint is -master-7rqtwti--app@ssh..platform.sh Note: All the next commands will be using platform ssh --pipe as the endpoint. Feel free to replace with the full connection string if needed. Create test files Let’s create some files to play with: mkdir -p web/uploads touch web/uploads/test{0001..0010}.txt rsync Upload a file or a directory To upload a directory, make sure you don’t specify the source folder (uploads in this case) in the destination. Take a look at man rsync to view all available options. rsync -avz web/uploads \"$(platform ssh --pipe)\":web/ The log should list all files that were uploaded: sending incremental file list uploads/ uploads/test0001.txt ... uploads/test0010.txt sent 603 bytes received 210 bytes 325.20 bytes/sec total size is 0 speedup is 0.00 We could have synced only the files with the following command: rsync -avz web/uploads/* rsync -avz web/uploads \"$(platform ssh --pipe)\":web/uploads/ As rsync is transferring only the differences between the source and destination, relaunching the same command will end up in an empty transfer. Download a file or a directory Remove your local files: rm web/uploads/* We can now download them from http://Platform.sh: rsync -avz \"$(platform ssh --pipe)\":web/uploads web/ You can see in the output that all files are transferred back to our host. scp Remove the files on http://Platform.sh if you have followed the rsync steps: platform ssh \"rm web/uploads/*\" Upload a file or a directory To upload a file, specify the full path: scp web/uploads/test0001.txt \"$(platform ssh --pipe)\":web/uploads To upload a directory, add the -r argument: scp -r web/uploads \"$(platform ssh --pipe)\":web/ The output should list the 10 files being transferred. Note that scp is not using incremental transfer. All 10 files are being transferred even if they already exist at the destination. Download a file or a directory Remove a local file first: rm web/uploads/test0001.txt To download a file: scp \"$(platform ssh --pipe)\":web/uploads/test0001.txt web/uploads/ To download a directory: scp -r \"$(platform ssh --pipe)\":web/uploads web/ sftp Remove the files on http://Platform.sh if you have followed the previous steps: platform ssh \"rm web/uploads/*\" Configure your client Add a new bookmark to your client with the following configuration (based on our endpoint): hostname: ssh.eu-3.platform.sh port: 22 protocol: SFTP user: -master-7rqtwti--app key/identity file: `/path/to/your/key Filezilla needs to have a .pem key file to list it. Connect to the site. You should be presented with the content of the root folder of your app. Upload a file or a directory Drag and drop files and folders from your computer to the mount: Download a file or a directory Drag and drop files and folders from the mount to your computer. Easy enough. Conclusion Transferring and managing files on your http://Platform.sh environments can be done using native tools that use ssh, scp, rsync, or sftp.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-upload-and-download-files-to-your-application-sftp-rsync/72",
        "relurl": "/t/how-to-upload-and-download-files-to-your-application-sftp-rsync/72"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "9781993d4b038bb106ce45c6dc4384b2d24376c3",
        "title": "Is it possible to kill a build or deploy?",
        "description": "Sometimes I push a commit and the build/deploy either takes a very long time, or it becomes permanently stuck. Am I able to do something to kill the process?",
        "text": "Sometimes I push a commit and the build/deploy either takes a very long time, or it becomes permanently stuck. Am I able to do something to kill the process?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-it-possible-to-kill-a-build-or-deploy/156",
        "relurl": "/t/is-it-possible-to-kill-a-build-or-deploy/156"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "0b1a00d9d63732a668c531cb1adb20cf109b810e",
        "title": "Using a custom certificate and LetsEncrypt certificates on the same project?",
        "description": "Is it possible to use a custom SSL certificate just for a specific domain, and use the autogenerated one for all other domains on the same project?",
        "text": "Is it possible to use a custom SSL certificate just for a specific domain, and use the autogenerated one for all other domains on the same project?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/using-a-custom-certificate-and-letsencrypt-certificates-on-the-same-project/361",
        "relurl": "/t/using-a-custom-certificate-and-letsencrypt-certificates-on-the-same-project/361"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "f01b22d27b2aacc6390703982cb9f05032d1c025",
        "title": "What does \"WARNING: [pool web] server reached max_children setting (2), consider raising it\" mean?",
        "description": "I keep seeing this in my site logs. Does this mean my site is down? How do I raise the max_children setting?",
        "text": "I keep seeing this in my site logs. Does this mean my site is down? How do I raise the max_children setting?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/what-does-warning-pool-web-server-reached-max-children-setting-2-consider-raising-it-mean/240",
        "relurl": "/t/what-does-warning-pool-web-server-reached-max-children-setting-2-consider-raising-it-mean/240"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "25319943a084d07d2f9f057e343d1d98677b06e7",
        "title": "Can I use Platform.sh for a different language?",
        "description": "We can use PHP, NodeJS, Go, and more. But my favorite language is not there. How do I add a new language (e.g. Erlang, Elixir, …) ?",
        "text": "We can use PHP, NodeJS, Go, and more. But my favorite language is not there. How do I add a new language (e.g. Erlang, Elixir, …) ?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-i-use-platform-sh-for-a-different-language/237",
        "relurl": "/t/can-i-use-platform-sh-for-a-different-language/237"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "78fb7e1961e72278eea0aab0f79464f825b8359c",
        "title": "How can I forward my logs to Splunk or Logz.io?",
        "description": "I’d like to integrate my log monitoring across my organization, including applications not hosted on http://Platform.sh.",
        "text": "I’d like to integrate my log monitoring across my organization, including applications not hosted on http://Platform.sh.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-forward-my-logs-to-splunk-or-logz-io/166",
        "relurl": "/t/how-can-i-forward-my-logs-to-splunk-or-logz-io/166"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "c45b8bac2e17d4aa2e45a5aa5ad10c8d77969e38",
        "title": "My plan has 10GB of storage. How can I see precisely how much I am using?",
        "description": "I know that some is used by my services, and some is used by the disk directive in .platform.app.yaml. Is there a way to see how much is being used from the 10GB total that my project has?",
        "text": "I know that some is used by my services, and some is used by the disk directive in .platform.app.yaml. Is there a way to see how much is being used from the 10GB total that my project has?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/my-plan-has-10gb-of-storage-how-can-i-see-precisely-how-much-i-am-using/158",
        "relurl": "/t/my-plan-has-10gb-of-storage-how-can-i-see-precisely-how-much-i-am-using/158"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "43d18bd84360b0a684103f2476382ab3c31bf3bd",
        "title": "Using platform.sh environments for continuous integration / functional tests",
        "description": "What would the pros / cons of using platform to run integration / functional tests, instead of a typical Continuous Integration service? And is using platform to run functional tests possible / a bad idea? My thinking is this: Unit tests, static analysis, code linting etc. are easy to run in isolation, so that should remain in a typical CI environment Functional tests require a functioning environment, which http://platform.sh can automatically provide Sometimes you want to run your functional tests against production data, which is also something http://platform.sh can automatically provide An example implementation could be: On build, detect if this is an environment you want to run tests in (eg. non-production, pull request environment etc.) Install any tools you need to run functional tests (eg. a headless browser and testing framework) Run the tests – fail the build if the tests fail Send notification of the passed/failed build (I think the standard github pull request integration would report this back, but I’m not sure – an email or slack notification could be sent) ",
        "text": "What would the pros / cons of using platform to run integration / functional tests, instead of a typical Continuous Integration service? And is using platform to run functional tests possible / a bad idea? My thinking is this: Unit tests, static analysis, code linting etc. are easy to run in isolation, so that should remain in a typical CI environment Functional tests require a functioning environment, which http://platform.sh can automatically provide Sometimes you want to run your functional tests against production data, which is also something http://platform.sh can automatically provide An example implementation could be: On build, detect if this is an environment you want to run tests in (eg. non-production, pull request environment etc.) Install any tools you need to run functional tests (eg. a headless browser and testing framework) Run the tests – fail the build if the tests fail Send notification of the passed/failed build (I think the standard github pull request integration would report this back, but I’m not sure – an email or slack notification could be sent) ",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/using-platform-sh-environments-for-continuous-integration-functional-tests/386",
        "relurl": "/t/using-platform-sh-environments-for-continuous-integration-functional-tests/386"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "d67481940670bd5c8011651a62b3da836f4565dc",
        "title": "Does Platform.sh support subdomains and Let's Encrypt for them?",
        "description": "I’m looking to add 500+ subdomains to my application on Platform Professional. How do I set them up in my routes.yaml file? Is there a limit to the number of subdomains I can add there? How do I set up certificates for them?",
        "text": "I’m looking to add 500+ subdomains to my application on Platform Professional. How do I set them up in my routes.yaml file? Is there a limit to the number of subdomains I can add there? How do I set up certificates for them?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/does-platform-sh-support-subdomains-and-lets-encrypt-for-them/200",
        "relurl": "/t/does-platform-sh-support-subdomains-and-lets-encrypt-for-them/200"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "1c9c3fab92af90cf13415f50e3a91d5f2b6f4a75",
        "title": "How do I access my .git directory from within my app?",
        "description": "I want to be able to access the commit hash and timestamp of the current state of my project, but I don’t see a .git repository when I SSH in to my application. How can I access the information stored in .git?",
        "text": "I want to be able to access the commit hash and timestamp of the current state of my project, but I don’t see a .git repository when I SSH in to my application. How can I access the information stored in .git?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-access-my-git-directory-from-within-my-app/349",
        "relurl": "/t/how-do-i-access-my-git-directory-from-within-my-app/349"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "c4cf6453d9b8e12a9e4b82c9aa16ab4af5f0b7d0",
        "title": "How to use FTP to upload files?",
        "description": "I want to upload a large amount of files to my production environment, how could I use FTP to upload them?",
        "text": "I want to upload a large amount of files to my production environment, how could I use FTP to upload them?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-use-ftp-to-upload-files/253",
        "relurl": "/t/how-to-use-ftp-to-upload-files/253"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "82879a07c0643378facde81719cbb0e23175a1b1",
        "title": "Transferring backups from P.sh to S3",
        "description": "I think this would make a good how to guide. Asking here first. I understand I can install s3cmd or aws-cli. Is there any examples kicking around?",
        "text": "I think this would make a good how to guide. Asking here first. I understand I can install s3cmd or aws-cli. Is there any examples kicking around?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/transferring-backups-from-p-sh-to-s3/215",
        "relurl": "/t/transferring-backups-from-p-sh-to-s3/215"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "72226d555365347488ad8026b1d97f7e53757e4c",
        "title": "Log location? dblog or syslog?",
        "description": "I’m not sure where to find Drupal logs with syslog on, and I couldn’t find any info as to whether platform recommends/cares Re: syslog vs dblog.",
        "text": "I’m not sure where to find Drupal logs with syslog on, and I couldn’t find any info as to whether platform recommends/cares Re: syslog vs dblog.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/log-location-dblog-or-syslog/210",
        "relurl": "/t/log-location-dblog-or-syslog/210"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "0cb313ca8f4e6e71030c2a7ba72af23cb25185d5",
        "title": "My project has exceeded its cpu limit. What do I do?",
        "description": "I received the below error when trying to deploy: E: Error: Resources exceeding plan limit; cpu: 1.00 0.96; try removing a service, or increase your plan size What does that error mean? It appears to indicate that my project has exceeded its cpu plan limit. How would I increase this and how do I know what my cpu limit is? Thank you!",
        "text": "I received the below error when trying to deploy: E: Error: Resources exceeding plan limit; cpu: 1.00 0.96; try removing a service, or increase your plan size What does that error mean? It appears to indicate that my project has exceeded its cpu plan limit. How would I increase this and how do I know what my cpu limit is? Thank you!",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/my-project-has-exceeded-its-cpu-limit-what-do-i-do/288",
        "relurl": "/t/my-project-has-exceeded-its-cpu-limit-what-do-i-do/288"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "aa206bfd673bc198f9cc9fda8af6ae7777788ff2",
        "title": "How can I move a Wordpress site to Platform.sh?",
        "description": "I have an existing installation of Wordpress with my old hoster, how can I move everything to http://Platform.sh?",
        "text": "I have an existing installation of Wordpress with my old hoster, how can I move everything to http://Platform.sh?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-move-a-wordpress-site-to-platform-sh/271",
        "relurl": "/t/how-can-i-move-a-wordpress-site-to-platform-sh/271"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "0383d4c645cf4b981c810c7eea5ff5a4ca57c4c2",
        "title": "How can I access to an environment variable in Javascript?",
        "description": "I would like to insert my Hotjar Identifier only on certain environments. How can I do that?",
        "text": "I would like to insert my Hotjar Identifier only on certain environments. How can I do that?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-access-to-an-environment-variable-in-javascript/251",
        "relurl": "/t/how-can-i-access-to-an-environment-variable-in-javascript/251"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "405123789db83e4cfd9bb75a83db066eb3ad347d",
        "title": "My project looks to be out of storage. How do I add more?",
        "description": "I received the following error when git pushing to my environment: E: Error: Resources exceeding plan limit; disk: 31192.00 30720.00; try removing a service, or add more storage to your plan It suggests that I add more storage to my plan. How do I do that? Also, what does it mean by removing a service?",
        "text": "I received the following error when git pushing to my environment: E: Error: Resources exceeding plan limit; disk: 31192.00 30720.00; try removing a service, or add more storage to your plan It suggests that I add more storage to my plan. How do I do that? Also, what does it mean by removing a service?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/my-project-looks-to-be-out-of-storage-how-do-i-add-more/261",
        "relurl": "/t/my-project-looks-to-be-out-of-storage-how-do-i-add-more/261"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "5719e1b1e849c206cea3d80ad951638545e83c5f",
        "title": "Is it possible to run a docker container on Platform.sh?",
        "description": "I have my own Docker container prepared. Is there any way to pull it from a registry and run it on http://Platform.sh?",
        "text": "I have my own Docker container prepared. Is there any way to pull it from a registry and run it on http://Platform.sh?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-it-possible-to-run-a-docker-container-on-platform-sh/330",
        "relurl": "/t/is-it-possible-to-run-a-docker-container-on-platform-sh/330"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "a118416f22355bd25338f7dd9653bc7c475df797",
        "title": "How do I fix `fatal: the remote end hung up unexpectedly` while pushing from a Bitbucket pipeline to Platform.sh?",
        "description": "While pushing to Platform git repository from our Bitbucket runner, the pipeline fails with the following error: fatal: the remote end hung up unexpectedly . How can I fix that?",
        "text": "While pushing to Platform git repository from our Bitbucket runner, the pipeline fails with the following error: fatal: the remote end hung up unexpectedly . How can I fix that?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-fix-fatal-the-remote-end-hung-up-unexpectedly-while-pushing-from-a-bitbucket-pipeline-to-platform-sh/492",
        "relurl": "/t/how-do-i-fix-fatal-the-remote-end-hung-up-unexpectedly-while-pushing-from-a-bitbucket-pipeline-to-platform-sh/492"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "7cc5dffcb3cdd9d558319befa8862b7fa5ba8528",
        "title": "How can I refresh my projects list in Platform CLI?",
        "description": "I have created several new projects and my Platform CLI is not refreshing the list of available projects.",
        "text": "I have created several new projects and my Platform CLI is not refreshing the list of available projects.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-refresh-my-projects-list-in-platform-cli/269",
        "relurl": "/t/how-can-i-refresh-my-projects-list-in-platform-cli/269"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "d8bd1641cda2d83292a69ff528fd23243708a6ea",
        "title": "Can I get a dedicated FTP/SFTP account for access to a specific directory?",
        "description": "Can I create an SFTP account for one of my team that only allows access to one folder, not to the whole site?",
        "text": "Can I create an SFTP account for one of my team that only allows access to one folder, not to the whole site?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-i-get-a-dedicated-ftp-sftp-account-for-access-to-a-specific-directory/452",
        "relurl": "/t/can-i-get-a-dedicated-ftp-sftp-account-for-access-to-a-specific-directory/452"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "da301bd57372ab0c5a746c043709b87c9ce5c124",
        "title": "How to build and deploy Angular Universal app?",
        "description": "Hello , We are using latest release of Angular Universal 9 with ivy (next-gen compatibility compiler) but we have an unexpected problem when we tried to build and deploy the app. We launch Ivy compilation in postinstall in sync mode script in order to avoid out of memory error, but the build doesn’t works… How to deploy our application quickly without memory error ? We used “L” plan with 6Gb of Ram, it’s 3x more than our older PaaS system. I provide most information about it, below : Build Hook yarn install yarn run build:ssr:dev The command build:ssr:dev launch webpack compilation : ng run website:build:dev \u0026\u0026 ng run website:server:dev The website will be compiled to the dist directory but this is currently not working : Build output error log $ ng run website:build:dev \u0026\u0026 ng run website:server:dev Generating ES5 bundles for differential loading... W: An unhandled exception occurred: Call retries were exceeded W: See \"/tmp/ng-1pShkt/angular-errors.log\" for further details. W: error Command failed with exit code 127. info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command. The file /tmp/ng-1pShkt/angular-errors.log is not available, so I can’t debug log. How to see Angular build error log ? How to build Angular application ? (Your documentation is only for AngularJS and it’s an old version, completely differente of Angular “next gen”). Thanks for your help ",
        "text": "Hello , We are using latest release of Angular Universal 9 with ivy (next-gen compatibility compiler) but we have an unexpected problem when we tried to build and deploy the app. We launch Ivy compilation in postinstall in sync mode script in order to avoid out of memory error, but the build doesn’t works… How to deploy our application quickly without memory error ? We used “L” plan with 6Gb of Ram, it’s 3x more than our older PaaS system. I provide most information about it, below : Build Hook yarn install yarn run build:ssr:dev The command build:ssr:dev launch webpack compilation : ng run website:build:dev \u0026\u0026 ng run website:server:dev The website will be compiled to the dist directory but this is currently not working : Build output error log $ ng run website:build:dev \u0026\u0026 ng run website:server:dev Generating ES5 bundles for differential loading... W: An unhandled exception occurred: Call retries were exceeded W: See \"/tmp/ng-1pShkt/angular-errors.log\" for further details. W: error Command failed with exit code 127. info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command. The file /tmp/ng-1pShkt/angular-errors.log is not available, so I can’t debug log. How to see Angular build error log ? How to build Angular application ? (Your documentation is only for AngularJS and it’s an old version, completely differente of Angular “next gen”). Thanks for your help ",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-build-and-deploy-angular-universal-app/526",
        "relurl": "/t/how-to-build-and-deploy-angular-universal-app/526"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "a9bb537a932ccac59b2fae03ef549d71b9336db2",
        "title": "Where do I submit bug reports?",
        "description": "If I come across a problem working with http://Platform.sh that seems like a bug, how can I bring it to your attention?",
        "text": "If I come across a problem working with http://Platform.sh that seems like a bug, how can I bring it to your attention?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/where-do-i-submit-bug-reports/286",
        "relurl": "/t/where-do-i-submit-bug-reports/286"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "df848e54a7de22f1eb4599c1655dacd042f85b68",
        "title": "What is the different between Platform.sh and Terraform?",
        "description": "What is the different between http://Platform.sh and Terraform?",
        "text": "What is the different between http://Platform.sh and Terraform?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/what-is-the-different-between-platform-sh-and-terraform/342",
        "relurl": "/t/what-is-the-different-between-platform-sh-and-terraform/342"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "65c2083de024d35b48628e16a2d6dba639e7b68f",
        "title": "What is the difference between a rebuild and a redeploy?",
        "description": "And, does the platform redeploy command also trigger a rebuild?",
        "text": "And, does the platform redeploy command also trigger a rebuild?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/what-is-the-difference-between-a-rebuild-and-a-redeploy/329",
        "relurl": "/t/what-is-the-difference-between-a-rebuild-and-a-redeploy/329"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "88877e4adc233bfb082f21c73f305ee349f13335",
        "title": "I need to connect to a VPN endpoint. What are my options?",
        "description": "I need two types of connections to a server only available through a VPN: One is a real-time http call The other is a cron every night that uploads a file to a FTP server. Is there a way to handle that on http://Platform.sh ?",
        "text": "I need two types of connections to a server only available through a VPN: One is a real-time http call The other is a cron every night that uploads a file to a FTP server. Is there a way to handle that on http://Platform.sh ?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/i-need-to-connect-to-a-vpn-endpoint-what-are-my-options/236",
        "relurl": "/t/i-need-to-connect-to-a-vpn-endpoint-what-are-my-options/236"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "3d7f1cd4cb06f504ee2be52b758dc5878d5ca5ee",
        "title": "How can I connect an external tool to my database?",
        "description": "How would I connect an external tool such as dbeaver / datagrip / … to my database ?",
        "text": "How would I connect an external tool such as dbeaver / datagrip / … to my database ?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-connect-an-external-tool-to-my-database/222",
        "relurl": "/t/how-can-i-connect-an-external-tool-to-my-database/222"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "2f48bf1762fa4bfe846cda2fae003701189a0a89",
        "title": "How can I make custom 404 pages?",
        "description": "I’d like to serve my own HTML for 404 (and other HTTP codes).",
        "text": "I’d like to serve my own HTML for 404 (and other HTTP codes).",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-make-custom-404-pages/198",
        "relurl": "/t/how-can-i-make-custom-404-pages/198"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "7e16ae0fd41457675399c236d947c8068bfb3f10",
        "title": "What is the right way to run multiple processes from my start command?",
        "description": "I’ve read that I shouldn’t background a start process, as this will cause the supervisor process to start a second copy, but is it possible to run multiple processes from the start command? What is the right way to do this?",
        "text": "I’ve read that I shouldn’t background a start process, as this will cause the supervisor process to start a second copy, but is it possible to run multiple processes from the start command? What is the right way to do this?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/what-is-the-right-way-to-run-multiple-processes-from-my-start-command/224",
        "relurl": "/t/what-is-the-right-way-to-run-multiple-processes-from-my-start-command/224"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "38098a5d0e136946b96fbb9986147e3c2ff06cef",
        "title": "Can I point my apex (naked) domain to Platform.sh if I'm using Amazon Route 53 to manage DNS?",
        "description": "Route 53 isn’t listed here… ",
        "text": "Route 53 isn’t listed here… ",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-i-point-my-apex-naked-domain-to-platform-sh-if-im-using-amazon-route-53-to-manage-dns/310",
        "relurl": "/t/can-i-point-my-apex-naked-domain-to-platform-sh-if-im-using-amazon-route-53-to-manage-dns/310"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "ef615bcf478549da087594d0990db5535ef14f1d",
        "title": "(How) Can I change the \"Code base template\" after purchasing a plan?",
        "description": "When starting a new project, I’m not sure which template I should be using, yet I have to commit to choosing one before purchasing my plan. Can I try out different templates later under the same plan? How would I do that?",
        "text": "When starting a new project, I’m not sure which template I should be using, yet I have to commit to choosing one before purchasing my plan. Can I try out different templates later under the same plan? How would I do that?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-change-the-code-base-template-after-purchasing-a-plan/241",
        "relurl": "/t/how-can-i-change-the-code-base-template-after-purchasing-a-plan/241"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "82c7c3758aa276d11a649ec3f755e52d5c337fea",
        "title": "Why is the .platform.app.yaml file separate from the .platform folder?",
        "description": "Why do we have: .platform.* files and also a .platform folder? I mean, why not everything in the same folder?",
        "text": "Why do we have: .platform.* files and also a .platform folder? I mean, why not everything in the same folder?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/why-is-the-platform-app-yaml-file-separate-from-the-platform-folder/281",
        "relurl": "/t/why-is-the-platform-app-yaml-file-separate-from-the-platform-folder/281"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "661d042742c2093406e89ba3537ab618697ae240",
        "title": "Is there a way to do the equivalent of \"screen\" on platform.sh?",
        "description": "I need to import a lot of data in my database, it’s a very long process. What can I use since screen or tmux seem to be absent?",
        "text": "I need to import a lot of data in my database, it’s a very long process. What can I use since screen or tmux seem to be absent?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-there-a-way-to-do-the-equivalent-of-screen-on-platform-sh/252",
        "relurl": "/t/is-there-a-way-to-do-the-equivalent-of-screen-on-platform-sh/252"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "3b03b2e3d54d1c63e3ccf7f0338d46a5dad869e4",
        "title": "How can I remove the \"sent via platform.sh\" in my emails",
        "description": "When sending email from platformsh the recipient gets a ‘via platformsh’ appended. I understand platformsh uses sendgrid. How should i configure my domain so I don’t get the ‘via’ platformsh message?",
        "text": "When sending email from platformsh the recipient gets a ‘via platformsh’ appended. I understand platformsh uses sendgrid. How should i configure my domain so I don’t get the ‘via’ platformsh message?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-remove-the-sent-via-platform-sh-in-my-emails/263",
        "relurl": "/t/how-can-i-remove-the-sent-via-platform-sh-in-my-emails/263"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "b8d2d0995cc2e89f16860501f1bbee7dea12b087",
        "title": "How can I receive a notification if one of my services (eg. MariaDB) dies?",
        "description": "I’d like to get an alert if a service dies or is unreachable.",
        "text": "I’d like to get an alert if a service dies or is unreachable.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-receive-a-notification-if-one-of-my-services-eg-mariadb-dies/164",
        "relurl": "/t/how-can-i-receive-a-notification-if-one-of-my-services-eg-mariadb-dies/164"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e69213f35a82db31e0c798f8dbbf567ef01acc4e",
        "title": "How can I expose non-80/443 TCP port to the Internet?",
        "description": "My mobile application needs to talk to a service running on Platform via non-80/443 TCP port. How could I expose that port?",
        "text": "My mobile application needs to talk to a service running on Platform via non-80/443 TCP port. How could I expose that port?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-expose-non-80-443-tcp-port-to-the-internet/256",
        "relurl": "/t/how-can-i-expose-non-80-443-tcp-port-to-the-internet/256"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "74b7348745133495cb8d456b058a272167c0e156",
        "title": "How do I fix `fatal: the remote end hung up unexpectedly` while pushing from a GitLab pipeline to Platform.sh?",
        "description": "While pushing to Platform git repository from our GitLab runner, the pipeline fails with the following error: fatal: the remote end hung up unexpectedly. How can I fix that?",
        "text": "While pushing to Platform git repository from our GitLab runner, the pipeline fails with the following error: fatal: the remote end hung up unexpectedly. How can I fix that?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-fix-fatal-the-remote-end-hung-up-unexpectedly-while-pushing-from-a-gitlab-pipeline-to-platform-sh/491",
        "relurl": "/t/how-do-i-fix-fatal-the-remote-end-hung-up-unexpectedly-while-pushing-from-a-gitlab-pipeline-to-platform-sh/491"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "96b4db735287085d0211042db7a16bc72fd7fca5",
        "title": "How are mounts in .platform.app.yaml sized?",
        "description": "Is everything OK as long as the total size of the mounts is less than the value in the disk directive? ",
        "text": "Is everything OK as long as the total size of the mounts is less than the value in the disk directive? ",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-are-mounts-in-platform-app-yaml-sized/157",
        "relurl": "/t/how-are-mounts-in-platform-app-yaml-sized/157"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "9dc93dc80b9c6dab43079e136e018355a37faff5",
        "title": "Why should I use Platform.sh over AWS?",
        "description": "What are the benefits of using http://Platform.sh instead?",
        "text": "What are the benefits of using http://Platform.sh instead?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/why-should-i-use-platform-sh-over-aws/314",
        "relurl": "/t/why-should-i-use-platform-sh-over-aws/314"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "b9a1f0d901c6b6e5bc240f275728588d869d2407",
        "title": "Where could I submit feature request?",
        "description": "I have some features in mind that would like to see on http://Platform.sh, where could I submit the feature request?",
        "text": "I have some features in mind that would like to see on http://Platform.sh, where could I submit the feature request?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/where-could-i-submit-feature-request/257",
        "relurl": "/t/where-could-i-submit-feature-request/257"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "1dd456534b116946a2f4e5c08ec4bc634e91c33f",
        "title": "What is Git? How to use it?",
        "description": "What is Git and how could I use it to upload my website?",
        "text": "What is Git and how could I use it to upload my website?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/what-is-git-how-to-use-it/254",
        "relurl": "/t/what-is-git-how-to-use-it/254"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "060b37643a19335e1f31453d82b701e3ff02d654",
        "title": "Is there a public product roadmap?",
        "description": "Do you have a public product roadmap?",
        "text": "Do you have a public product roadmap?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-there-a-public-product-roadmap/258",
        "relurl": "/t/is-there-a-public-product-roadmap/258"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "4a1a5a0e3a0079b0eb55763c9e0dc392c293ba02",
        "title": "How much storage can I add to my project?",
        "description": "How much do I get by default, and how much can I add?",
        "text": "How much do I get by default, and how much can I add?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-much-storage-can-i-add-to-my-project/228",
        "relurl": "/t/how-much-storage-can-i-add-to-my-project/228"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "d328ee08f5f65726b008ef0a06045d3fead0f110",
        "title": "How can I see the status of a bug?",
        "description": "I opened a ticket a while ago for a problem and received the reply that my problem is a known bug and will be fixed in the future. How can I track the status of my bug ? will I be informed when it’s fixed somehow ?",
        "text": "I opened a ticket a while ago for a problem and received the reply that my problem is a known bug and will be fixed in the future. How can I track the status of my bug ? will I be informed when it’s fixed somehow ?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-see-the-status-of-a-bug/260",
        "relurl": "/t/how-can-i-see-the-status-of-a-bug/260"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "16ad6d527398bd989cc45b9a12d59c8e927e2674",
        "title": "Is there a way to fail a build when one of the build hooks throws an error?",
        "description": "In my case I want to use npm clean-install instead of npm install to ensure that my package-lock and my package.json have not diverged.",
        "text": "In my case I want to use npm clean-install instead of npm install to ensure that my package-lock and my package.json have not diverged.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-there-a-way-to-fail-a-build-when-one-of-the-build-hooks-throws-an-error/366",
        "relurl": "/t/is-there-a-way-to-fail-a-build-when-one-of-the-build-hooks-throws-an-error/366"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "cb6205bb5e44b2d51b9fef2baccf94bc825db67c",
        "title": "Is my built code counted against my global plan storage?",
        "description": "If my plan has 5GB storage, and my built code takes up 500K, does that get taken off of my available storage (5GB)?",
        "text": "If my plan has 5GB storage, and my built code takes up 500K, does that get taken off of my available storage (5GB)?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-my-built-code-counted-against-my-global-plan-storage/163",
        "relurl": "/t/is-my-built-code-counted-against-my-global-plan-storage/163"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "bbf5e031a9b9509037da12cc27e589fa3d34445f",
        "title": "What is the routes.yaml max file size?",
        "description": "Is it a number of routes? Characters? File size?",
        "text": "Is it a number of routes? Characters? File size?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/what-is-the-routes-yaml-max-file-size/185",
        "relurl": "/t/what-is-the-routes-yaml-max-file-size/185"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "d6fe6c4e416d64bc77662aa45cb656c386ad57df",
        "title": "Can I SSH into my website?",
        "description": "Can I SSH into my website? How?",
        "text": "Can I SSH into my website? How?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-i-ssh-into-my-website/225",
        "relurl": "/t/can-i-ssh-into-my-website/225"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "35e665e5efb1489c0d07cb1637df11b054233e1f",
        "title": "Can I just have my whole project read / write and serve my PHP website from here?",
        "description": "Hello, I feel that having everything read-only by default prevents me from working fast enough - can I just make a mount to / and rsync then serve my PHP website from here, so I can open an editor and make my changes directly there? Thanks!",
        "text": "Hello, I feel that having everything read-only by default prevents me from working fast enough - can I just make a mount to / and rsync then serve my PHP website from here, so I can open an editor and make my changes directly there? Thanks!",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-i-just-have-my-whole-project-read-write-and-serve-my-php-website-from-here/245",
        "relurl": "/t/can-i-just-have-my-whole-project-read-write-and-serve-my-php-website-from-here/245"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "57fe61a78b87fb2b672f70f12efde2fbee8318fd",
        "title": "More production environments on the project",
        "description": "Is it possible to have different production environments on the same project? Or only master branch can go live?",
        "text": "Is it possible to have different production environments on the same project? Or only master branch can go live?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/more-production-environments-on-the-project/250",
        "relurl": "/t/more-production-environments-on-the-project/250"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "c1f6700ce2c7ca461e446401f23e7d27e1bd6e44",
        "title": "What does it mean when my environment has a \"dirty\" status?",
        "description": "As opposed to “active”?",
        "text": "As opposed to “active”?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/what-does-it-mean-when-my-environment-has-a-dirty-status/175",
        "relurl": "/t/what-does-it-mean-when-my-environment-has-a-dirty-status/175"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "3dc43992994769ad3d5838dd0387d1ed167509dd",
        "title": "Do you support other version control system?",
        "description": "Do you support SVN, CVS, Perforce, etc to upload my website?",
        "text": "Do you support SVN, CVS, Perforce, etc to upload my website?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/do-you-support-other-version-control-system/255",
        "relurl": "/t/do-you-support-other-version-control-system/255"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "b72e5949b7a15fee2210099c07d1778079beab3a",
        "title": "How do the queues in my project work? Is it only possible to have one thing building/deploying at a time?",
        "description": "For example, if a developer pushes something on the stage branch, and it takes 10 minutes to build, can I deploy a hotfix on master in that time?",
        "text": "For example, if a developer pushes something on the stage branch, and it takes 10 minutes to build, can I deploy a hotfix on master in that time?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-the-queues-in-my-project-work-is-it-only-possible-to-have-one-thing-building-deploying-at-a-time/165",
        "relurl": "/t/how-do-the-queues-in-my-project-work-is-it-only-possible-to-have-one-thing-building-deploying-at-a-time/165"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "6521f992150f9375174e64232d2a45ab8ad8fbde",
        "title": "Can I have root permissions?",
        "description": "Can I have root permissions on my http://Platform.sh projects ?",
        "text": "Can I have root permissions on my http://Platform.sh projects ?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-i-have-root-permissions/248",
        "relurl": "/t/can-i-have-root-permissions/248"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "5afb0e8de5fdd067d15381b4c48aa657be47f824",
        "title": "Can I get FTP access to change my html pages?",
        "description": "My developers are asking for an FTP access to update the html pages of my website. Can I get one?",
        "text": "My developers are asking for an FTP access to update the html pages of my website. Can I get one?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-i-get-ftp-access-to-change-my-html-pages/246",
        "relurl": "/t/can-i-get-ftp-access-to-change-my-html-pages/246"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "42cd2b08b4b1d2328c2c55ec3456f99778479a2e",
        "title": "How can I use nutch to index my site?",
        "description": "I’d like to use nutch to index my site and insert the index into Solr. How can I do this on Platform?",
        "text": "I’d like to use nutch to index my site and insert the index into Solr. How can I do this on Platform?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-use-nutch-to-index-my-site/244",
        "relurl": "/t/how-can-i-use-nutch-to-index-my-site/244"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "828a553b5da1e4d147a9ec5817c2369bd0681d7b",
        "title": "What is a best way to sync files from `develop` to `master`?",
        "description": "I need to move my files from the develop environment into master.",
        "text": "I need to move my files from the develop environment into master.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/what-is-a-best-way-to-sync-files-from-develop-to-master/354",
        "relurl": "/t/what-is-a-best-way-to-sync-files-from-develop-to-master/354"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "4e1814ccb1fac31223f8cd3a8dcadf326ab80d45",
        "title": "For Ruby on Rails, what's the best way to delete the database records?",
        "description": "What’s the best way to wipe my DB? I have a live ruby on rails site. I would like to purge or delete the data from the master branch Posgres database. I’d like to keep the tables intact, and only delete the rows from each. This is so that I can start my site development anew. My thinking is to change to the master branch in command line then following the reference below I would proceed with the command $ rake db:reset db:migrate Which will reset the database and reload the current schema. https://medium.com/forest-admin/rails-migrations-tricks-guide-code-cheatsheet-included-dca935354f22 Is this best way as its my first in production site no account or customers to worry about?",
        "text": "What’s the best way to wipe my DB? I have a live ruby on rails site. I would like to purge or delete the data from the master branch Posgres database. I’d like to keep the tables intact, and only delete the rows from each. This is so that I can start my site development anew. My thinking is to change to the master branch in command line then following the reference below I would proceed with the command $ rake db:reset db:migrate Which will reset the database and reload the current schema. https://medium.com/forest-admin/rails-migrations-tricks-guide-code-cheatsheet-included-dca935354f22 Is this best way as its my first in production site no account or customers to worry about?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/for-ruby-on-rails-whats-the-best-way-to-delete-the-database-records/455",
        "relurl": "/t/for-ruby-on-rails-whats-the-best-way-to-delete-the-database-records/455"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "c07b222cdb375fb8ec4bd381c55e9f3e75a2ac3f",
        "title": "How can I identify MySQL slow queries on the PaaS",
        "description": "I have identified PHP scripts that are abnormally long to execute in some cases. (Thanks to the php.access.log) I’m trying to find which MySQL request is taking so much time (up to 200 sec) and under which conditions. Is there a way to enable the mysql_slow_query log?",
        "text": "I have identified PHP scripts that are abnormally long to execute in some cases. (Thanks to the php.access.log) I’m trying to find which MySQL request is taking so much time (up to 200 sec) and under which conditions. Is there a way to enable the mysql_slow_query log?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-identify-mysql-slow-queries-on-the-paas/321",
        "relurl": "/t/how-can-i-identify-mysql-slow-queries-on-the-paas/321"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "78ebe12c97edfae6479a687ccf610f2b92e6756e",
        "title": "Odoo 13 + python",
        "description": "Has anybody got the latest version of Odoo running on platform successfully ? Cheers Dan",
        "text": "Has anybody got the latest version of Odoo running on platform successfully ? Cheers Dan",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/odoo-13-python/447",
        "relurl": "/t/odoo-13-python/447"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "d769c0cdcff5416728aeb1b809a14a08b8a8c538",
        "title": "What's the correct route conf for a websocket?",
        "description": "Hi, I’m trying to run a django/channels/daphne websocket. I believe I’ve got the backend set up correctly - at least, it works locally. I’m trying to connect from the front end via JS: var voteSocket = new WebSocket('wss://' + window.location.host + '/sync/votes/'); …which gives me a 404, presumably because the URI isn’t declared in routes.yaml If I add that route though, I get a deploy error: routes.key: 'wss://{default}/sync/votes/' scheme is not one of http, https And if I change the frontend JS to use a https URL, the browser complains that: The URL’s scheme must be either ‘ws’ or ‘wss’. ‘https’ is not allowed. So… how do I resolve that conflict? How do I declare a route that can be used as a websocket?",
        "text": "Hi, I’m trying to run a django/channels/daphne websocket. I believe I’ve got the backend set up correctly - at least, it works locally. I’m trying to connect from the front end via JS: var voteSocket = new WebSocket('wss://' + window.location.host + '/sync/votes/'); …which gives me a 404, presumably because the URI isn’t declared in routes.yaml If I add that route though, I get a deploy error: routes.key: 'wss://{default}/sync/votes/' scheme is not one of http, https And if I change the frontend JS to use a https URL, the browser complains that: The URL’s scheme must be either ‘ws’ or ‘wss’. ‘https’ is not allowed. So… how do I resolve that conflict? How do I declare a route that can be used as a websocket?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/whats-the-correct-route-conf-for-a-websocket/472",
        "relurl": "/t/whats-the-correct-route-conf-for-a-websocket/472"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "d49128c3f89e349f660fef8bdd071fe6681420b2",
        "title": "Do redirects in `routes.yaml` file also show up in `PLATFORM_ROUTES`?",
        "description": "Is this even possible if I have many (thousands) of redirects since they would be stored in an environment variable?",
        "text": "Is this even possible if I have many (thousands) of redirects since they would be stored in an environment variable?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/do-redirects-in-routes-yaml-file-also-show-up-in-platform-routes/308",
        "relurl": "/t/do-redirects-in-routes-yaml-file-also-show-up-in-platform-routes/308"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "2e8c7026eb62b735fef709855b09ef282ed7e9d7",
        "title": "What databases can I use in my project?",
        "description": "What database servers can I use in my project ? do you offer NoSQL DBs too ?",
        "text": "What database servers can I use in my project ? do you offer NoSQL DBs too ?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/what-databases-can-i-use-in-my-project/219",
        "relurl": "/t/what-databases-can-i-use-in-my-project/219"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "4f2bf0e1c2737e2f6065a4305b0e1fa47c3b65de",
        "title": "How much disk space is available during the build hook for application code?",
        "description": "I’m getting out of disk errors when running npm install.",
        "text": "I’m getting out of disk errors when running npm install.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-much-disk-space-is-available-during-the-build-hook-for-application-code/181",
        "relurl": "/t/how-much-disk-space-is-available-during-the-build-hook-for-application-code/181"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e2e75f7287eb926221c24d865614f6ec832e5e9e",
        "title": "How do I change my project name?",
        "description": "I have created a project, but I made a typo. Can I change the project name? Or do I have to delete and recreate the project completely?",
        "text": "I have created a project, but I made a typo. Can I change the project name? Or do I have to delete and recreate the project completely?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-change-my-project-name/242",
        "relurl": "/t/how-do-i-change-my-project-name/242"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "1b2e3d97805226c7aefadb8065b8f8484cf12dc2",
        "title": "How can I force a PDF download with Platform.sh?",
        "description": "I’d like to force a PDF download from my static site, rather than have that PDF open in a browser. With Apache, I’d do it like so: ForceType application/octet-stream Header set Content-Disposition attachment How do I do that with http://Platform.sh?",
        "text": "I’d like to force a PDF download from my static site, rather than have that PDF open in a browser. With Apache, I’d do it like so: ForceType application/octet-stream Header set Content-Disposition attachment How do I do that with http://Platform.sh?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-force-a-pdf-download-with-platform-sh/334",
        "relurl": "/t/how-can-i-force-a-pdf-download-with-platform-sh/334"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "9152de4909dc6ba5a2a0da17cbead906628daf3f",
        "title": "Why is my site slow?",
        "description": "It was fast before and works fine in my development environment.",
        "text": "It was fast before and works fine in my development environment.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/why-is-my-site-slow/238",
        "relurl": "/t/why-is-my-site-slow/238"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "5b8074847b47559a6748d83a431a267e6ef12422",
        "title": "How to git push without rebuilding?",
        "description": "How can I push a commit without rebuilding the app? It takes long time. No config change just css.",
        "text": "How can I push a commit without rebuilding the app? It takes long time. No config change just css.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-git-push-without-rebuilding/516",
        "relurl": "/t/how-to-git-push-without-rebuilding/516"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "7b2d198f5f3a83d1affbceec3d93a12ee9cb3ba6",
        "title": "Is it possible to increase Redis `maxmemory` configuration option?",
        "description": "I tried bumping up cluster size to L and adding a configuration: { maxmemory: 134217728 } to redis service but I did not manage to change the default 28MB limit.",
        "text": "I tried bumping up cluster size to L and adding a configuration: { maxmemory: 134217728 } to redis service but I did not manage to change the default 28MB limit.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-it-possible-to-increase-redis-maxmemory-configuration-option/350",
        "relurl": "/t/is-it-possible-to-increase-redis-maxmemory-configuration-option/350"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "a01179f56da655d929782eda5ebe04a28460981a",
        "title": "When does a variable requires a JSON formatting?",
        "description": "When providing environment variables, you have the option to provide JSON formatted values. What would be the deciding factor to create a JSON formatted variable? In my case, I’m looking to provide an empty string (\"\") as --value for my variable.",
        "text": "When providing environment variables, you have the option to provide JSON formatted values. What would be the deciding factor to create a JSON formatted variable? In my case, I’m looking to provide an empty string (\"\") as --value for my variable.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/when-does-a-variable-requires-a-json-formatting/193",
        "relurl": "/t/when-does-a-variable-requires-a-json-formatting/193"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "c0f6ed10075641cce9c786373e650156efeb3f00",
        "title": "How can I connect to my database from my local workstation?",
        "description": "I need to import data into my database from a local file. How can I do that from my local workstation?",
        "text": "I need to import data into my database from a local file. How can I do that from my local workstation?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-connect-to-my-database-from-my-local-workstation/207",
        "relurl": "/t/how-can-i-connect-to-my-database-from-my-local-workstation/207"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "ceefe1223c35ff42d119fef874ec37d701dd0dfd",
        "title": "If I add a service, can I still restore a snapshot from before that service existed?",
        "description": "Adding a new service (eg. a new MongoDB) changes the persistent data model of my project. Does that render it incompatible with backups/snapshots made before?",
        "text": "Adding a new service (eg. a new MongoDB) changes the persistent data model of my project. Does that render it incompatible with backups/snapshots made before?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/if-i-add-a-service-can-i-still-restore-a-snapshot-from-before-that-service-existed/298",
        "relurl": "/t/if-i-add-a-service-can-i-still-restore-a-snapshot-from-before-that-service-existed/298"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "4318eaf2ef85e51fbdc2ae5d9bf9bbb10fe58858",
        "title": "How to get ssh connection details from the commandline?",
        "description": "I know I can find ssh connection info , but I’d prefer not to switch out of my terminal to get this each time, or I want to automate a connection from a process of my own. I don’t recognise the IDs in that connection string, do they change?",
        "text": "I know I can find ssh connection info , but I’d prefer not to switch out of my terminal to get this each time, or I want to automate a connection from a process of my own. I don’t recognise the IDs in that connection string, do they change?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-get-ssh-connection-details-from-the-commandline/284",
        "relurl": "/t/how-to-get-ssh-connection-details-from-the-commandline/284"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "10c8c7536211b18d648871f17942514d3d4939ea",
        "title": "(Solved) Pyodbc install fails",
        "description": "Currently working on setting up a django app that needs a mssql database connection. The mssql library (posted below) operates off of Pyodbc. Pyodbc has a requirement of unixodbc. Unfortunately, Im having issues with getting this requirement installed. https://github.com/ESSolutions/django-mssql-backend https://github.com/ESSolutions/django-mssql-backend https://github.com/mkleehammer/pyodbc https://github.com/mkleehammer/pyodbc http://www.unixodbc.org/ Currently trying to following to get the dependency installed. export LD_LIBRARY_PATH=$LIBDIR:/app/unixODBC-2.3.7 wget http://www.unixodbc.org/unixODBC-2.3.7.tar.gz gunzip unixODBC*.tar.gz tar xvf unixODBC*.tar cd unixODBC* mkdir /app/unixODBC-2.3.7/etc ./configure --prefix=/app/unixODBC-2.3.7 --sysconfdir=/app/unixODBC-2.3.7 make make install The last error Im getting is below. make[2]: Nothing to be done for 'install-exec-am'. /bin/mkdir -p '/app/unixODBC-2.3.7/include' /usr/bin/install -c -m 644 odbcinst.h odbcinstext.h sql.h sqlext.h sqltypes.h sqlucode.h sqlspi.h autotest.h uodbc_stats.h uodbc_extras.h '/app/unixODBC-2.3.7/include' W: /usr/bin/install: 'odbcinst.h' and '/app/unixODBC-2.3.7/include/odbcinst.h' are the same file W: /usr/bin/install: 'odbcinstext.h' and '/app/unixODBC-2.3.7/include/odbcinstext.h' are the same file W: /usr/bin/install: 'sql.h' and '/app/unixODBC-2.3.7/include/sql.h' are the same file W: /usr/bin/install: 'sqlext.h' and '/app/unixODBC-2.3.7/include/sqlext.h' are the same file W: /usr/bin/install: 'sqltypes.h' and '/app/unixODBC-2.3.7/include/sqltypes.h' are the same file W: /usr/bin/install: 'sqlucode.h' and '/app/unixODBC-2.3.7/include/sqlucode.h' are the same file W: /usr/bin/install: 'sqlspi.h' and '/app/unixODBC-2.3.7/include/sqlspi.h' are the same file W: /usr/bin/install: 'autotest.h' and '/app/unixODBC-2.3.7/include/autotest.h' are the same file W: /usr/bin/install: 'uodbc_stats.h' and '/app/unixODBC-2.3.7/include/uodbc_stats.h' are the same file W: /usr/bin/install: 'uodbc_extras.h' and '/app/unixODBC-2.3.7/include/uodbc_extras.h' are the same file W: make[2]: *** [install-includeHEADERS] Error 1 Makefile:409: recipe for target 'install-includeHEADERS' failed make[2]: Leaving directory '/app/unixODBC-2.3.7/include' W: make[1]: *** [install-am] Error 2 Makefile:525: recipe for target 'install-am' failed make[1]: Leaving directory '/app/unixODBC-2.3.7/include' W: make: *** [install-recursive] Error 1 Makefile:548: recipe for target 'install-recursive' failed Any help with getting this dependency installed would be much appreciated.",
        "text": "Currently working on setting up a django app that needs a mssql database connection. The mssql library (posted below) operates off of Pyodbc. Pyodbc has a requirement of unixodbc. Unfortunately, Im having issues with getting this requirement installed. https://github.com/ESSolutions/django-mssql-backend https://github.com/ESSolutions/django-mssql-backend https://github.com/mkleehammer/pyodbc https://github.com/mkleehammer/pyodbc http://www.unixodbc.org/ Currently trying to following to get the dependency installed. export LD_LIBRARY_PATH=$LIBDIR:/app/unixODBC-2.3.7 wget http://www.unixodbc.org/unixODBC-2.3.7.tar.gz gunzip unixODBC*.tar.gz tar xvf unixODBC*.tar cd unixODBC* mkdir /app/unixODBC-2.3.7/etc ./configure --prefix=/app/unixODBC-2.3.7 --sysconfdir=/app/unixODBC-2.3.7 make make install The last error Im getting is below. make[2]: Nothing to be done for 'install-exec-am'. /bin/mkdir -p '/app/unixODBC-2.3.7/include' /usr/bin/install -c -m 644 odbcinst.h odbcinstext.h sql.h sqlext.h sqltypes.h sqlucode.h sqlspi.h autotest.h uodbc_stats.h uodbc_extras.h '/app/unixODBC-2.3.7/include' W: /usr/bin/install: 'odbcinst.h' and '/app/unixODBC-2.3.7/include/odbcinst.h' are the same file W: /usr/bin/install: 'odbcinstext.h' and '/app/unixODBC-2.3.7/include/odbcinstext.h' are the same file W: /usr/bin/install: 'sql.h' and '/app/unixODBC-2.3.7/include/sql.h' are the same file W: /usr/bin/install: 'sqlext.h' and '/app/unixODBC-2.3.7/include/sqlext.h' are the same file W: /usr/bin/install: 'sqltypes.h' and '/app/unixODBC-2.3.7/include/sqltypes.h' are the same file W: /usr/bin/install: 'sqlucode.h' and '/app/unixODBC-2.3.7/include/sqlucode.h' are the same file W: /usr/bin/install: 'sqlspi.h' and '/app/unixODBC-2.3.7/include/sqlspi.h' are the same file W: /usr/bin/install: 'autotest.h' and '/app/unixODBC-2.3.7/include/autotest.h' are the same file W: /usr/bin/install: 'uodbc_stats.h' and '/app/unixODBC-2.3.7/include/uodbc_stats.h' are the same file W: /usr/bin/install: 'uodbc_extras.h' and '/app/unixODBC-2.3.7/include/uodbc_extras.h' are the same file W: make[2]: *** [install-includeHEADERS] Error 1 Makefile:409: recipe for target 'install-includeHEADERS' failed make[2]: Leaving directory '/app/unixODBC-2.3.7/include' W: make[1]: *** [install-am] Error 2 Makefile:525: recipe for target 'install-am' failed make[1]: Leaving directory '/app/unixODBC-2.3.7/include' W: make: *** [install-recursive] Error 1 Makefile:548: recipe for target 'install-recursive' failed Any help with getting this dependency installed would be much appreciated.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/solved-pyodbc-install-fails/450",
        "relurl": "/t/solved-pyodbc-install-fails/450"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "50e55567fb501d05f60d895e44e520c2d2d20ebf",
        "title": "Can I serve custom error pages (4xx, 5xx)?",
        "description": "I’d like to serve a custom error page when the application returns a 5xx error rather than the standard http://Platform.sh error page. Can I do this?",
        "text": "I’d like to serve a custom error page when the application returns a 5xx error rather than the standard http://Platform.sh error page. Can I do this?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-i-serve-custom-error-pages-4xx-5xx/287",
        "relurl": "/t/can-i-serve-custom-error-pages-4xx-5xx/287"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "2feeec7c5d948f99388deddf1861814a002afb3a",
        "title": "How do I share files between multiple applications?",
        "description": "I have a multi-app project, and a mounted directory of files that I would like all applications to have access to. How do I accomplish this? Project structure: files/ application1/ .platform.app.yaml .. application2/ .platform.app.yaml .. .. ",
        "text": "I have a multi-app project, and a mounted directory of files that I would like all applications to have access to. How do I accomplish this? Project structure: files/ application1/ .platform.app.yaml .. application2/ .platform.app.yaml .. .. ",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-share-files-between-multiple-applications/304",
        "relurl": "/t/how-do-i-share-files-between-multiple-applications/304"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e044f84e01ee4fb54e93cb4184b05291219e5114",
        "title": "MongoDB configuration at Platform.sh",
        "description": "How the replica set works at http://Platform.sh? How to do a query on a MongoDB database using a UI client such as MongoDB Compass?",
        "text": "How the replica set works at http://Platform.sh? How to do a query on a MongoDB database using a UI client such as MongoDB Compass?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/mongodb-configuration-at-platform-sh/341",
        "relurl": "/t/mongodb-configuration-at-platform-sh/341"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "62a6a8b59dcaeb265016c9e4dca124c20e1748b9",
        "title": "Use the same root domain across multiple projects",
        "description": "We have two very similar projects and they’ll be served onto the same root domain, project1.mysite.com and project2.mysite.com. Since we’re using the public IPs of our cluster to connect DNS to http://P.sh, I do know you can only add a root domain once to the entire cluster (eu-2 in my case). In normal circumstances, you’ll add mysite.com as (default) domain and using routes.yaml to handle your routing. But what if we provide a full domain, rather then a root domain to our domains? platform domain:add project.mysite.com rather then platform domain:add mysite.com. Is this possible of does it have any drawbacks in using this approach?",
        "text": "We have two very similar projects and they’ll be served onto the same root domain, project1.mysite.com and project2.mysite.com. Since we’re using the public IPs of our cluster to connect DNS to http://P.sh, I do know you can only add a root domain once to the entire cluster (eu-2 in my case). In normal circumstances, you’ll add mysite.com as (default) domain and using routes.yaml to handle your routing. But what if we provide a full domain, rather then a root domain to our domains? platform domain:add project.mysite.com rather then platform domain:add mysite.com. Is this possible of does it have any drawbacks in using this approach?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/use-the-same-root-domain-across-multiple-projects/213",
        "relurl": "/t/use-the-same-root-domain-across-multiple-projects/213"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "6c3ed69901e599c563adbf6221aaed221efe7174",
        "title": "How do I monitor my application outgoing emails?",
        "description": "Hi Platform, My client application monitor physical hardware device and monitor users by mail. Therefore, emails are critical to the application. We need to ensure that email services are constantly working as expected and that emails are delivered correctly to users. How can I achieve that?",
        "text": "Hi Platform, My client application monitor physical hardware device and monitor users by mail. Therefore, emails are critical to the application. We need to ensure that email services are constantly working as expected and that emails are delivered correctly to users. How can I achieve that?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-monitor-my-application-outgoing-emails/243",
        "relurl": "/t/how-do-i-monitor-my-application-outgoing-emails/243"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e3bf6923188b2cde797850ec0eb982ee14f5f585",
        "title": "How can I access the live site using the original Platform-provided URLs?",
        "description": "If I “go live”, I lose the ability to go directly to the *.platformsh.site url. Can I go back to using that at any point?",
        "text": "If I “go live”, I lose the ability to go directly to the *.platformsh.site url. Can I go back to using that at any point?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-access-the-live-site-using-the-original-platform-provided-urls/279",
        "relurl": "/t/how-can-i-access-the-live-site-using-the-original-platform-provided-urls/279"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "7f5f3f80b98a86b4b897b8b6e49535db7b02b29c",
        "title": "How can I get information on the various Platform.sh regions?",
        "description": "For example, which ones are available, where are they, what IaaS infrastructure do they run on?",
        "text": "For example, which ones are available, where are they, what IaaS infrastructure do they run on?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-get-information-on-the-various-platform-sh-regions/331",
        "relurl": "/t/how-can-i-get-information-on-the-various-platform-sh-regions/331"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "3006582e1c36cbf81bf5a8f702e7254d760ad99c",
        "title": "The environment \"master\" is not currently active",
        "description": "I created a new project and tried to use the Platform CLI to SSH into my project’s environment via platform ssh and received the following warning message. Could not find applications: the environment \"master\" is not currently active. ",
        "text": "I created a new project and tried to use the Platform CLI to SSH into my project’s environment via platform ssh and received the following warning message. Could not find applications: the environment \"master\" is not currently active. ",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/the-environment-master-is-not-currently-active/268",
        "relurl": "/t/the-environment-master-is-not-currently-active/268"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "dc14f96a968e98d6df58d1d44cecad5abf257a04",
        "title": "Can these old TLS ciphers be deactivated?",
        "description": "When testing the encryption ciphers served by http://Platform.sh with my security scanner, some of them appear to be older/deprecated (like TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA and TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256). Can these be disabled ?",
        "text": "When testing the encryption ciphers served by http://Platform.sh with my security scanner, some of them appear to be older/deprecated (like TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA and TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256). Can these be disabled ?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-these-old-tls-ciphers-be-deactivated/463",
        "relurl": "/t/can-these-old-tls-ciphers-be-deactivated/463"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "74e79b60d6984e1ae5b1c930f013971aa1b4b489",
        "title": "Link to communicate multiple applications as web services in a form",
        "description": "Given four applications into a single project that will work as microservices: \"https://{default}/conferences\": id: conference type: upstream upstream: \"conference:http\" \"https://{default}/sessions\": id: session type: upstream upstream: \"session:http\" \"https://{default}/speakers\": id: speaker type: upstream upstream: \"speaker:http\" How can I make HTTP request to each other? E.g: Into conference application, how can I do a request into a speaker? Such as HTTP GET http://somehost/speakers",
        "text": "Given four applications into a single project that will work as microservices: \"https://{default}/conferences\": id: conference type: upstream upstream: \"conference:http\" \"https://{default}/sessions\": id: session type: upstream upstream: \"session:http\" \"https://{default}/speakers\": id: speaker type: upstream upstream: \"speaker:http\" How can I make HTTP request to each other? E.g: Into conference application, how can I do a request into a speaker? Such as HTTP GET http://somehost/speakers",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/link-to-communicate-multiple-applications-as-web-services-in-a-form/338",
        "relurl": "/t/link-to-communicate-multiple-applications-as-web-services-in-a-form/338"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "708e3a2034a4bab3a683acf60cdb829fab363fe3",
        "title": "Given a project that there are five microservices, is there any way to make four private and just one public?",
        "description": "Given four applications into a single project that will work as microservices: \"https://{default}/conferences\": id: conference type: upstream upstream: \"conference:http\" \"https://{default}/sessions\": id: session type: upstream upstream: \"session:http\" \"https://{default}/speakers\": id: speaker type: upstream upstream: \"speaker:http\" \"https://{default}/\": type: upstream upstream: \"app:http\" How can we make some resources as private? I mean, how can I make that all services internally communicate via HTTP, but just the app accesses publicly?",
        "text": "Given four applications into a single project that will work as microservices: \"https://{default}/conferences\": id: conference type: upstream upstream: \"conference:http\" \"https://{default}/sessions\": id: session type: upstream upstream: \"session:http\" \"https://{default}/speakers\": id: speaker type: upstream upstream: \"speaker:http\" \"https://{default}/\": type: upstream upstream: \"app:http\" How can we make some resources as private? I mean, how can I make that all services internally communicate via HTTP, but just the app accesses publicly?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/given-a-project-that-there-are-five-microservices-is-there-any-way-to-make-four-private-and-just-one-public/340",
        "relurl": "/t/given-a-project-that-there-are-five-microservices-is-there-any-way-to-make-four-private-and-just-one-public/340"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "d756f01f593957364ffb8278565639c01a08eb21",
        "title": "Reverse Proxy rules in router",
        "description": "Is it possible to make Reverse Proxy rules in the routes. I would like to Reverse Proxy to a legacy application for some parts which cannot be migrates easily to platform sh As far as I saw I can only add “redirect” rules. In the Netlify documentation I found that there you can configure something that they call “rewrite” rules: https://docs.netlify.com/routing/redirects/rewrites-proxies/ This is exactly the thing that I imagine having in platform sh For other purposes it would be great to have Reverse Proxy rules which can proxy to a WebSocket endpoint: https://www.nginx.com/blog/websocket-nginx/",
        "text": "Is it possible to make Reverse Proxy rules in the routes. I would like to Reverse Proxy to a legacy application for some parts which cannot be migrates easily to platform sh As far as I saw I can only add “redirect” rules. In the Netlify documentation I found that there you can configure something that they call “rewrite” rules: https://docs.netlify.com/routing/redirects/rewrites-proxies/ This is exactly the thing that I imagine having in platform sh For other purposes it would be great to have Reverse Proxy rules which can proxy to a WebSocket endpoint: https://www.nginx.com/blog/websocket-nginx/",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/reverse-proxy-rules-in-router/479",
        "relurl": "/t/reverse-proxy-rules-in-router/479"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "63be71397f0c0151ef64ee8332095165da6c3898",
        "title": "Is it possible to run X-Debug on Platform.sh?",
        "description": "I’d like to do step through debugging on my PHP projects.",
        "text": "I’d like to do step through debugging on my PHP projects.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-it-possible-to-run-x-debug-on-platform-sh/357",
        "relurl": "/t/is-it-possible-to-run-x-debug-on-platform-sh/357"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "f008b28e2686fb3de7e0d16422bceeb09767cb51",
        "title": "Will removing the account that created an integration break the integration?",
        "description": "in regards to integrations, if the project admin that creates the original integration is later removed from the project, will the integration break?",
        "text": "in regards to integrations, if the project admin that creates the original integration is later removed from the project, will the integration break?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/will-removing-the-account-that-created-an-integration-break-the-integration/204",
        "relurl": "/t/will-removing-the-account-that-created-an-integration-break-the-integration/204"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "a056c2d1ded02214e01c5666db347b1935447496",
        "title": "Is it possible to upsize my project plan using the CLI?",
        "description": "I’d like to automate my plan size changes so that I can trigger upsizes and downsizes. Can this be done with the CLI?",
        "text": "I’d like to automate my plan size changes so that I can trigger upsizes and downsizes. Can this be done with the CLI?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-it-possible-to-upsize-my-project-plan-using-the-cli/376",
        "relurl": "/t/is-it-possible-to-upsize-my-project-plan-using-the-cli/376"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "8cf18890ed19584272d6613350eb9a13a2f1c7eb",
        "title": "Is it possible to open direct access to a db service?",
        "description": "I need to allow auth0 to make queries to it in an incremental user migration. I could firewall it to an IP range for security, but I’m not sure how to do this on http://Platform.sh?",
        "text": "I need to allow auth0 to make queries to it in an incremental user migration. I could firewall it to an IP range for security, but I’m not sure how to do this on http://Platform.sh?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-it-possible-to-open-direct-access-to-a-db-service/362",
        "relurl": "/t/is-it-possible-to-open-direct-access-to-a-db-service/362"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "d78376eaeb5cb5fe82ddc78d0404e5d2858fc726",
        "title": "The connection link to several projects",
        "description": "How can I make HTTP request to each other? E.g: From my project application A, how can I do a request into a Project application B? Such as HTTP GET on http://somehost/projectB",
        "text": "How can I make HTTP request to each other? E.g: From my project application A, how can I do a request into a Project application B? Such as HTTP GET on http://somehost/projectB",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/the-connection-link-to-several-projects/339",
        "relurl": "/t/the-connection-link-to-several-projects/339"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "853b9c7b727b5d0a86c7d29559ed51ffe17454a9",
        "title": "Can I set a project variable to run different build scripts among several environments?",
        "description": "I would like a bash environment variable which would be different on environments at build time in order to build my app with it and configure different api endpoints differently.",
        "text": "I would like a bash environment variable which would be different on environments at build time in order to build my app with it and configure different api endpoints differently.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-i-set-a-project-variable-to-run-different-build-scripts-among-several-environments/249",
        "relurl": "/t/can-i-set-a-project-variable-to-run-different-build-scripts-among-several-environments/249"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "9a3063d439b4cf420cc9561d981d23ffcd9eb6b2",
        "title": "Is there a way to mount a network storage on the service instance, but not the app?",
        "description": "For example, to gain access to a backup of the Solr service?",
        "text": "For example, to gain access to a backup of the Solr service?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-there-a-way-to-mount-a-network-storage-on-the-service-instance-but-not-the-app/333",
        "relurl": "/t/is-there-a-way-to-mount-a-network-storage-on-the-service-instance-but-not-the-app/333"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e682e154c7a2352d0848e19be3d1538ea59e1d97",
        "title": "Is there a build dependencies cache on Platform.sh?",
        "description": "Are the contents of pip install, npm install, and yarn install cached from build to build?",
        "text": "Are the contents of pip install, npm install, and yarn install cached from build to build?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-there-a-build-dependencies-cache-on-platform-sh/464",
        "relurl": "/t/is-there-a-build-dependencies-cache-on-platform-sh/464"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "4be75a404886c29257645f420b417a5f8551d0c4",
        "title": "Why are my Let's Encrypt certificates failing to provision on my development environments?",
        "description": "I’ve noticed that occasionally Let’s Encrypt certificates will fail to provision on some of my development environments. The most recent example I have of this is when I’ve been working on the adding-emphemeral-redis branch of my project. They fail to provision on that branch, but not on any of my other environments.",
        "text": "I’ve noticed that occasionally Let’s Encrypt certificates will fail to provision on some of my development environments. The most recent example I have of this is when I’ve been working on the adding-emphemeral-redis branch of my project. They fail to provision on that branch, but not on any of my other environments.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/why-are-my-lets-encrypt-certificates-failing-to-provision-on-my-development-environments/459",
        "relurl": "/t/why-are-my-lets-encrypt-certificates-failing-to-provision-on-my-development-environments/459"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "9e393e9bb652758d6294bc73784c9d16218f1747",
        "title": "Is there a way to change the parent environment?",
        "description": "And, if I do so, will I lose data?",
        "text": "And, if I do so, will I lose data?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-there-a-way-to-change-the-parent-environment/295",
        "relurl": "/t/is-there-a-way-to-change-the-parent-environment/295"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "966eb00a25187a24fec6ba6dad38a3db04ba1bba",
        "title": "Is it possible to set a password on a MariaDB/MySQL service?",
        "description": "I have an application that throws an error if I don’t give it a password for a MySQL endpoint, but the default configuration for MySQL on http://Platform.sh has an empty string in the . How can I manually add a password to this service?",
        "text": "I have an application that throws an error if I don’t give it a password for a MySQL endpoint, but the default configuration for MySQL on http://Platform.sh has an empty string in the . How can I manually add a password to this service?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-it-possible-to-set-a-password-on-a-mariadb-mysql-service/432",
        "relurl": "/t/is-it-possible-to-set-a-password-on-a-mariadb-mysql-service/432"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "c18ad8b96eec6f93a45d936a0dacefe09a7bf80a",
        "title": "How can I reset or empty a service?",
        "description": "I have totally messed up my database’s data and schema. Can I just drop it and start fresh?",
        "text": "I have totally messed up my database’s data and schema. Can I just drop it and start fresh?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-reset-or-empty-a-service/381",
        "relurl": "/t/how-can-i-reset-or-empty-a-service/381"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e5f021bc7a591bf78d60873f00f6eb5d871d6e35",
        "title": "Is it possible to post a test results somewhere?",
        "description": "Is it possible to place some artifacts from the testing somewhere publicly available after build is done? For example scenario: build is completed visual testing job triggered with Diffy ( https://diffy.website) test is completed and would like to post them somewhere I am thinking about a place where test artifacts can be stored. In our case this could be simple description and a link to actual report. Thanks Yuriy",
        "text": "Is it possible to place some artifacts from the testing somewhere publicly available after build is done? For example scenario: build is completed visual testing job triggered with Diffy ( https://diffy.website) test is completed and would like to post them somewhere I am thinking about a place where test artifacts can be stored. In our case this could be simple description and a link to actual report. Thanks Yuriy",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-it-possible-to-post-a-test-results-somewhere/379",
        "relurl": "/t/is-it-possible-to-post-a-test-results-somewhere/379"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "ae8a386c0b0c7ac2376882b444e987a410194d2c",
        "title": "If I have a custom certificate installed on my project and it expires, will there be an automatic switch to a LetsEncrypt cert?",
        "description": "I have a custom . If it expires what happens?",
        "text": "I have a custom . If it expires what happens?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/if-i-have-a-custom-certificate-installed-on-my-project-and-it-expires-will-there-be-an-automatic-switch-to-a-letsencrypt-cert/440",
        "relurl": "/t/if-i-have-a-custom-certificate-installed-on-my-project-and-it-expires-will-there-be-an-automatic-switch-to-a-letsencrypt-cert/440"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "93a5ba1096cdac4fc6212c29be897e958771b1a1",
        "title": "How do I serve some requests from a different PHP front controller?",
        "description": "Most of the requests for my app are served from public/index.php, but I want to serve requests to /api from public/sites/api/api.php. Is this possible on http://platform.sh?",
        "text": "Most of the requests for my app are served from public/index.php, but I want to serve requests to /api from public/sites/api/api.php. Is this possible on http://platform.sh?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-serve-some-requests-from-a-different-php-front-controller/487",
        "relurl": "/t/how-do-i-serve-some-requests-from-a-different-php-front-controller/487"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "0056ab3940df4e9d16a1ef60599ac50d81655a53",
        "title": "For persistent Redis, what persistence methods does Platform.sh employs? RDB, AOF, or both?",
        "description": "https://redis.io/topics/persistence",
        "text": "https://redis.io/topics/persistence",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/for-persistent-redis-what-persistence-methods-does-platform-sh-employs-rdb-aof-or-both/517",
        "relurl": "/t/for-persistent-redis-what-persistence-methods-does-platform-sh-employs-rdb-aof-or-both/517"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "530aec95c5c76f234f3a267d29ff78e433ecf3db",
        "title": "Where is my database storage going to?",
        "description": "Question I’ve followed the following article to check my database storage usage: https://community.platform.sh/t/how-to-determine-database-usage-of-an-environment/180 /c/how-to http://platform.sh/ https://github.com/platformsh/platformsh-cli http://platform.sh/ https://github.com/platformsh/platformsh-cli https://docs.platform.sh/development/ssh.html http://Platform.sh But the size it reports seems wrong to me. I’ve imported a 3GB sized database backup. But when I check platform db:size it is reporting 9GB. Checking database service db... +----------------+-----------------+--------+ | Allocated disk | Estimated usage | % used | +----------------+-----------------+--------+ | 14.6 GiB | 9.4 GiB | ~ 64% | +----------------+-----------------+--------+ Why the big difference? Answer There is some overhead in operating databases so that accounts for some disk space. (e.g. on mysql information_schema/mysql, etc…) If you just measured the file that you import, be aware that the space used in the file is always smaller than when it is unpacked into the database. Databases are faster and more flexible than a flat file. But they create additional data to be able to do this. The platform db:size makes a good estimate on how much is used, but it can deviate by a few percentages. InnoDB does not reclaim removed space. So everything you once inserted and removed, is still there. This is especially true for cache tables (which get a lot of insert/deletes). db:size actually has a trick to help you identify wasted space and clean those up. platform db:size --cleanup -p YOUR_PROJECT_ID -e master Checking database service db... +----------------+-----------------+--------+ | Allocated disk | Estimated usage | % used | +----------------+-----------------+--------+ | 14.6 GiB | 9.4 GiB | ~ 64% | +----------------+-----------------+--------+ Warning This is an estimate of the database's disk usage. It does not represent its real size on disk. You can save space by running the following commands during a maintenance window: ALTER TABLE `main`.`cache_page` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`xmlsitemap` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`cache_block` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`queue` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`cache_advagg_aggregates` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`sessions` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`batch` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`field_revision_field_address_line_3` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`field_data_field_paragraph_background` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`field_data_field_bs_comms` ENGINE=\"InnoDB\"; Warning: Running these may lock up your database for several minutes. Only run these when you know what you're doing. That should clean up some storage, and you can even run those queries in a cron once a week to keep it nice and clean (Sunday early morning for example). Recommendations Move cache tables to a dedicated instance that can take the load off your database. Make sure you have less than 80% disk used on databases. Databases don’t work well with little disk space. ",
        "text": "Question I’ve followed the following article to check my database storage usage: https://community.platform.sh/t/how-to-determine-database-usage-of-an-environment/180 /c/how-to http://platform.sh/ https://github.com/platformsh/platformsh-cli http://platform.sh/ https://github.com/platformsh/platformsh-cli https://docs.platform.sh/development/ssh.html http://Platform.sh But the size it reports seems wrong to me. I’ve imported a 3GB sized database backup. But when I check platform db:size it is reporting 9GB. Checking database service db... +----------------+-----------------+--------+ | Allocated disk | Estimated usage | % used | +----------------+-----------------+--------+ | 14.6 GiB | 9.4 GiB | ~ 64% | +----------------+-----------------+--------+ Why the big difference? Answer There is some overhead in operating databases so that accounts for some disk space. (e.g. on mysql information_schema/mysql, etc…) If you just measured the file that you import, be aware that the space used in the file is always smaller than when it is unpacked into the database. Databases are faster and more flexible than a flat file. But they create additional data to be able to do this. The platform db:size makes a good estimate on how much is used, but it can deviate by a few percentages. InnoDB does not reclaim removed space. So everything you once inserted and removed, is still there. This is especially true for cache tables (which get a lot of insert/deletes). db:size actually has a trick to help you identify wasted space and clean those up. platform db:size --cleanup -p YOUR_PROJECT_ID -e master Checking database service db... +----------------+-----------------+--------+ | Allocated disk | Estimated usage | % used | +----------------+-----------------+--------+ | 14.6 GiB | 9.4 GiB | ~ 64% | +----------------+-----------------+--------+ Warning This is an estimate of the database's disk usage. It does not represent its real size on disk. You can save space by running the following commands during a maintenance window: ALTER TABLE `main`.`cache_page` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`xmlsitemap` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`cache_block` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`queue` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`cache_advagg_aggregates` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`sessions` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`batch` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`field_revision_field_address_line_3` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`field_data_field_paragraph_background` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`field_data_field_bs_comms` ENGINE=\"InnoDB\"; Warning: Running these may lock up your database for several minutes. Only run these when you know what you're doing. That should clean up some storage, and you can even run those queries in a cron once a week to keep it nice and clean (Sunday early morning for example). Recommendations Move cache tables to a dedicated instance that can take the load off your database. Make sure you have less than 80% disk used on databases. Databases don’t work well with little disk space. ",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/where-is-my-database-storage-going-to/467",
        "relurl": "/t/where-is-my-database-storage-going-to/467"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "674deb06dcbe9d66ce0807d74d8f229521656a4f",
        "title": "Expose Elasticsearch over HTTP",
        "description": "Is there a way to expose the Elasticsearch service for READ over HTTP through a route?",
        "text": "Is there a way to expose the Elasticsearch service for READ over HTTP through a route?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/expose-elasticsearch-over-http/510",
        "relurl": "/t/expose-elasticsearch-over-http/510"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "44618c600be830a9a3ad5e89ab322ff53842155a",
        "title": "Can I set a friendly/custom/vanity domain for my development branches on the grid?",
        "description": "I don’t like the look of the develop-fdn5g37-lsk5nfuysnbehv6.us-1.platformsh.site sort of link that I get with my non-master branches, and I’d like something shorter and cuter, or easier to remember for my presentations. Can I set it to be dev.myproject.org instead?",
        "text": "I don’t like the look of the develop-fdn5g37-lsk5nfuysnbehv6.us-1.platformsh.site sort of link that I get with my non-master branches, and I’d like something shorter and cuter, or easier to remember for my presentations. Can I set it to be dev.myproject.org instead?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-i-set-a-friendly-custom-vanity-domain-for-my-development-branches-on-the-grid/559",
        "relurl": "/t/can-i-set-a-friendly-custom-vanity-domain-for-my-development-branches-on-the-grid/559"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "a3d4932b5ff493a1d60bc8c479d419b48e977734",
        "title": "Connecting to MongoDB with GUI tool like Robo 3T or Mongo Compass",
        "description": "Hey, Im wondering how I can connect via a GUI tool like Robo 3T or Mongo Compass to have a look in the database. Im able to tunnel into the database and use the mongo cli like so: % platform tunnel:open % mongo mongodb://main:main@localhost:30000/main ^ this works % mongodump --port 30000 -u main -p main --authenticationDatabase main --db main 2020-04-26T13:02:47.152+1200 writing main.startrek to 2020-04-26T13:02:47.302+1200 done dumping main.startrek (108 documents) ^ this also works but if I try to connect via a GUI tool it doesn’t, I’m unsure what I’m doing wrong because the tunnel is working because I can dump the database through it, as well as query it through mongo cli, just not through a GUI tool. https://community.platform.sh/uploads/default/01ec804d8f51ca22920fed5797a6c8116006a6de ",
        "text": "Hey, Im wondering how I can connect via a GUI tool like Robo 3T or Mongo Compass to have a look in the database. Im able to tunnel into the database and use the mongo cli like so: % platform tunnel:open % mongo mongodb://main:main@localhost:30000/main ^ this works % mongodump --port 30000 -u main -p main --authenticationDatabase main --db main 2020-04-26T13:02:47.152+1200 writing main.startrek to 2020-04-26T13:02:47.302+1200 done dumping main.startrek (108 documents) ^ this also works but if I try to connect via a GUI tool it doesn’t, I’m unsure what I’m doing wrong because the tunnel is working because I can dump the database through it, as well as query it through mongo cli, just not through a GUI tool. https://community.platform.sh/uploads/default/01ec804d8f51ca22920fed5797a6c8116006a6de ",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/connecting-to-mongodb-with-gui-tool-like-robo-3t-or-mongo-compass/544",
        "relurl": "/t/connecting-to-mongodb-with-gui-tool-like-robo-3t-or-mongo-compass/544"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "2d63b8924785554f1a6fb137f0ebb640799288f4",
        "title": "How to share code between multi-apps from a monorepo",
        "description": "I have several services in a monorepo that use a shared library. I want to be able to install the shared library when running tests and deploying. Here’s the layout .platform .platform/routes.yaml quote-api composer.json events composer.json .platform/routes.yaml \"https://quote-api.{default}/\": type: upstream upstream: \"quote-api:http\" quote-api/composer.json { \"require\": { \"paqman/events\": \"*\" }, \"repositories\": [ { \"type\": \"path\", \"url\": \"../events\" } ] } But when http://platform.sh is building each service, it says “Moving the application to the output directory” which I assume moves it to some sort of isolated area so now I get the error message: Source path \"../events\" is not found for package paqman/events Note: I do not want to use a package manager as I want to get the exact shared code with the commit (releasing a new package for a branch etc. would be difficult)",
        "text": "I have several services in a monorepo that use a shared library. I want to be able to install the shared library when running tests and deploying. Here’s the layout .platform .platform/routes.yaml quote-api composer.json events composer.json .platform/routes.yaml \"https://quote-api.{default}/\": type: upstream upstream: \"quote-api:http\" quote-api/composer.json { \"require\": { \"paqman/events\": \"*\" }, \"repositories\": [ { \"type\": \"path\", \"url\": \"../events\" } ] } But when http://platform.sh is building each service, it says “Moving the application to the output directory” which I assume moves it to some sort of isolated area so now I get the error message: Source path \"../events\" is not found for package paqman/events Note: I do not want to use a package manager as I want to get the exact shared code with the commit (releasing a new package for a branch etc. would be difficult)",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-share-code-between-multi-apps-from-a-monorepo/550",
        "relurl": "/t/how-to-share-code-between-multi-apps-from-a-monorepo/550"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "2d14e8df39b813f184e046f46c8a76fb836fb57f",
        "title": "How do I use MongoDB with my Java app?",
        "description": "Java is one of the most popular programming languages in the MongoDB Community. For new users, it’s essential to provide an overview of how to work with the MongoDB Java driver at http://Platform.sh. Java has several frameworks and combinations to work with MongoDB, but it’s important to provide an overview of how to work with the MongoDB Java driver and how to use MongoDB as a Java developer. Therefore, https://docs.platform.sh/languages/java.html integration between Java and MongoDB. This integration became easier thanks to the https://github.com/platformsh/config-reader-java . MongoDB database = config.getCredential(\"mongodb\", MongoDB::new); MongoClient mongoClient = database.get(); final MongoDatabase mongoDatabase = mongoClient.getDatabase(database.getDatabase()); MongoCollection collection = mongoDatabase.getCollection(\"scientist\"); Document doc = new Document(\"name\", \"Ada Lovelace\").append(\"city\", \"London\"); collection.insertOne(doc); Document myDoc = collection.find(eq(\"_id\", doc.get(\"_id\"))).first(); However, we have the option to overwrite the configurations only when it is running at http://Platform.sh such as https://community.platform.sh/t/how-to-overwrite-spring-data-mongodb-variable-to-access-platform-sh-services/528 and https://community.platform.sh/t/how-to-overwrite-configuration-to-jakarta-microprofile-to-access-platform-sh-services/520 . Articles https://platform.sh/blog/2019/spring-mvc-and-mongodb-a-match-made-in-platform.sh-heaven/ https://platform.sh/blog/2019/what-is-new-with-jakarta-nosql/ https://platform.sh/blog/2020/what-is-new-with-jakarta-nosql-part-ii/ https://dzone.com/articles/deploy-quarkus-faster-in-the-cloud-with-platformsh-3 More Java combinations We have several articles and source code on this repository: https://github.com/platformsh/java-quick-start https://github.com/platformsh/java-quick-start This repository has several Java combinations it includes Spring, Jakarta EE, MicroProfile, MongoDB, JPA, and so on.",
        "text": "Java is one of the most popular programming languages in the MongoDB Community. For new users, it’s essential to provide an overview of how to work with the MongoDB Java driver at http://Platform.sh. Java has several frameworks and combinations to work with MongoDB, but it’s important to provide an overview of how to work with the MongoDB Java driver and how to use MongoDB as a Java developer. Therefore, https://docs.platform.sh/languages/java.html integration between Java and MongoDB. This integration became easier thanks to the https://github.com/platformsh/config-reader-java . MongoDB database = config.getCredential(\"mongodb\", MongoDB::new); MongoClient mongoClient = database.get(); final MongoDatabase mongoDatabase = mongoClient.getDatabase(database.getDatabase()); MongoCollection collection = mongoDatabase.getCollection(\"scientist\"); Document doc = new Document(\"name\", \"Ada Lovelace\").append(\"city\", \"London\"); collection.insertOne(doc); Document myDoc = collection.find(eq(\"_id\", doc.get(\"_id\"))).first(); However, we have the option to overwrite the configurations only when it is running at http://Platform.sh such as https://community.platform.sh/t/how-to-overwrite-spring-data-mongodb-variable-to-access-platform-sh-services/528 and https://community.platform.sh/t/how-to-overwrite-configuration-to-jakarta-microprofile-to-access-platform-sh-services/520 . Articles https://platform.sh/blog/2019/spring-mvc-and-mongodb-a-match-made-in-platform.sh-heaven/ https://platform.sh/blog/2019/what-is-new-with-jakarta-nosql/ https://platform.sh/blog/2020/what-is-new-with-jakarta-nosql-part-ii/ https://dzone.com/articles/deploy-quarkus-faster-in-the-cloud-with-platformsh-3 More Java combinations We have several articles and source code on this repository: https://github.com/platformsh/java-quick-start https://github.com/platformsh/java-quick-start This repository has several Java combinations it includes Spring, Jakarta EE, MicroProfile, MongoDB, JPA, and so on.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-use-mongodb-with-my-java-app/582",
        "relurl": "/t/how-do-i-use-mongodb-with-my-java-app/582"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "1840f7a2235edbb4b002cf72a382452660db85e9",
        "title": "Spring and Platform.sh Getting started",
        "description": "Deploy Friday: E09 Spring Framework The Spring Framework is an application framework and inversion of control container for the Java platform. Try Spring: https://github.com/platformsh-templates/spring-mvc-maven-mongodb Articles Article Link https://dzone.com/articles/introduction-to-spring-data-mongodb-reactive-and-h https://github.com/platformsh/java-quick-start/tree/master/spring/spring-mongodb-reactive https://dzone.com/articles/introduction-of-spring-webflux-and-how-to-apply-cl https://github.com/platformsh/java-quick-start/tree/master/spring/spring-webflux https://platform.sh/blog/2019/spring-data-redis-in-the-cloud/ https://github.com/platformsh/java-quick-start/tree/master/spring/spring-boot-maven-redis https://platform.sh/blog/2019/simplify-your-script-build-with-gradle/ https://github.com/platformsh-templates/spring-boot-gradle-mysql https://platform.sh/blog/2019/elasticsearch-vs-solr-have-both-with-spring-data-and-platform.sh/ https://github.com/platformsh/java-quick-start/tree/master/spring/spring-mvc-maven-elasticsearch and https://github.com/platformsh/java-quick-start/tree/master/spring/spring-mvc-maven-solr https://platform.sh/blog/2019/spring-mvc-and-mongodb-a-match-made-in-platform.sh-heaven/ https://github.com/platformsh/java-quick-start/tree/master/spring/spring-mvc-maven-mongodb https://platform.sh/blog/2019/java-hello-world-at-platform.sh/ ",
        "text": "Deploy Friday: E09 Spring Framework The Spring Framework is an application framework and inversion of control container for the Java platform. Try Spring: https://github.com/platformsh-templates/spring-mvc-maven-mongodb Articles Article Link https://dzone.com/articles/introduction-to-spring-data-mongodb-reactive-and-h https://github.com/platformsh/java-quick-start/tree/master/spring/spring-mongodb-reactive https://dzone.com/articles/introduction-of-spring-webflux-and-how-to-apply-cl https://github.com/platformsh/java-quick-start/tree/master/spring/spring-webflux https://platform.sh/blog/2019/spring-data-redis-in-the-cloud/ https://github.com/platformsh/java-quick-start/tree/master/spring/spring-boot-maven-redis https://platform.sh/blog/2019/simplify-your-script-build-with-gradle/ https://github.com/platformsh-templates/spring-boot-gradle-mysql https://platform.sh/blog/2019/elasticsearch-vs-solr-have-both-with-spring-data-and-platform.sh/ https://github.com/platformsh/java-quick-start/tree/master/spring/spring-mvc-maven-elasticsearch and https://github.com/platformsh/java-quick-start/tree/master/spring/spring-mvc-maven-solr https://platform.sh/blog/2019/spring-mvc-and-mongodb-a-match-made-in-platform.sh-heaven/ https://github.com/platformsh/java-quick-start/tree/master/spring/spring-mvc-maven-mongodb https://platform.sh/blog/2019/java-hello-world-at-platform.sh/ ",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/spring-and-platform-sh-getting-started/583",
        "relurl": "/t/spring-and-platform-sh-getting-started/583"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "2441b1d1ee6e21d690f144f9dc9e040e15177750",
        "title": "Quarkus and Platform.sh Getting started",
        "description": "Deploy Friday: E06 Quarkus Supersonic Subatomic Java QuarkusIO, the Supersonic Subatomic Java, promises to deliver small artifacts, extremely fast boot time, and lower time-to-first-request. Try Quarkus: https://github.com/platformsh-templates/quarkus Articles Article Links https://dzone.com/articles/deploy-quarkus-faster-in-the-cloud-with-platformsh-3 https://dzone.com/articles/deploy-quarkus-faster-in-the-cloud-with-platformsh-2 https://dzone.com/articles/deploy-quarkus-faster-in-the-cloud-with-platformsh-1 https://github.com/platformsh-examples/quarkus/tree/master/elasticsearch https://dzone.com/articles/deploy-quarkus-faster-in-the-cloud-with-platformsh https://github.com/platformsh-examples/quarkus/tree/master/panache https://dzone.com/articles/quarkus-supersonic-subatomic-java-deploy-faster-in https://github.com/platformsh-examples/quarkus/tree/master/jpa https://dzone.com/articles/quarkus-supersonic-subatomic-java-goes-faster-in-t https://github.com/platformsh-templates/quarkus ",
        "text": "Deploy Friday: E06 Quarkus Supersonic Subatomic Java QuarkusIO, the Supersonic Subatomic Java, promises to deliver small artifacts, extremely fast boot time, and lower time-to-first-request. Try Quarkus: https://github.com/platformsh-templates/quarkus Articles Article Links https://dzone.com/articles/deploy-quarkus-faster-in-the-cloud-with-platformsh-3 https://dzone.com/articles/deploy-quarkus-faster-in-the-cloud-with-platformsh-2 https://dzone.com/articles/deploy-quarkus-faster-in-the-cloud-with-platformsh-1 https://github.com/platformsh-examples/quarkus/tree/master/elasticsearch https://dzone.com/articles/deploy-quarkus-faster-in-the-cloud-with-platformsh https://github.com/platformsh-examples/quarkus/tree/master/panache https://dzone.com/articles/quarkus-supersonic-subatomic-java-deploy-faster-in https://github.com/platformsh-examples/quarkus/tree/master/jpa https://dzone.com/articles/quarkus-supersonic-subatomic-java-goes-faster-in-t https://github.com/platformsh-templates/quarkus ",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/quarkus-and-platform-sh-getting-started/564",
        "relurl": "/t/quarkus-and-platform-sh-getting-started/564"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "09eae73ed02dc174db655f7e50e53d0c5e2b3940",
        "title": "Hokus is an open-source CMS for Hugo - completely free to use",
        "description": "Developers love Hugo because of its astonishing speed. Content editors love Hokus because it brings the same speed benefit while hiding the complexity of editing raw files to edit a website. When you save your content, you can see your updated website in your browser almost instantly. Hokus is a multi-platform desktop application (Windows, macOS, and Linux are supported). Because it runs on your computer, you can use it even when you are offline. Unlike Netlify, Hokus does not assume any infrastructure vendor, meaning that you are free to move your website to another host without losing the user interface you are used to. This tutorial will show how to install Hokus, run a website locally and then move it to the cloud easily with http://Platform.sh. Steps https://github.com/julianoappelklein/hokus/releases https://docs.platform.sh/gettingstarted/own-code/create-project.html https://github.com/platformsh-templates/hugo https://github.com/platformsh-templates/hugo it with https://docs.platform.sh/administration/integrations/github.html Download the project with a git clone from the Git Repository that you wish. Cick in new Site, plus on the left button to create a new project. https://community.platform.sh/uploads/default/41d507beda8230e89a9f0ecbfbab746e49341d90 Now we have a Site configured, let’s use it! (keep in mind that Hokus will assume a default configuration for your website and to add more fields to your forms, or new forms, you need to edit the file hokus.toml). Select the website you just created and select the workspace “source” Click source - click select - then in sidebar. https://community.platform.sh/uploads/default/672fe35c6168598b8817f31321be9d8b9581fa90 On the left, you’ll see both the Post and Main Config option. If your current post is not showing, beware of the folder structure. We also have the option to customize it with the hokus.toml file. On this repository we have the posts under the post structure; therefore, we went in the collections and changed the folder line: folder = \"content/post/\" Now it’s time to run your website locally. ON the right sidebar, click in the “play” icon, and select the default option. Your browser will open showing your website! Click in the Posts button. You have the option to create, edit and delete posts.Open an existing post or create a new one. https://community.platform.sh/uploads/default/6984f91e3f659b7917e81eb8120a3f98c69cebef All the posts work with Markdown, so you can add bullets or pictures with Markdown After editing the post, click on the Save button (bottom right corner). Your changes will update your website in your browser instantly! https://community.platform.sh/uploads/default/f949075cbc28d7222c0aceef32d961de56dfa94a Are you happy with what you have? Let’s move it to the cloud with http://Platform.sh. We need to set the https://docs.platform.sh/configuration/app-containers.html and the https://docs.platform.sh/configuration/routes.html you can see respectively. File: .platform.app.yaml # .platform.app.yaml # The name of this application, which must be unique within a project. name: 'app' # The type key specifies the language and version for your application. type: golang:1.14 # The hooks that will be triggered when the package is deployed. hooks: # Build hooks can modify the application files on disk but not access any services like databases. build: !include type: string path: build.sh # The size of the persistent disk of the application (in MB). disk: 5120 # The configuration of the application when it is exposed to the web. web: locations: '/': # The public directory of the application relative to its root. root: 'public' index: ['index.html'] scripts: false allow: true expires: 1d File `.platform/routes.yaml` \"https://{default}/\": type: upstream upstream: \"app:http\" The last step is to push your changes to a remote repository. git push origin master Done, Hugo, Hokus, and http://Platform.sh at your service! https://community.platform.sh/uploads/default/68458a2ccb5d235315d4399b1cf64a0c3e93355a References https://www.hokuscms.com/ https://github.com/platformsh-examples/hokus-cms ",
        "text": "Developers love Hugo because of its astonishing speed. Content editors love Hokus because it brings the same speed benefit while hiding the complexity of editing raw files to edit a website. When you save your content, you can see your updated website in your browser almost instantly. Hokus is a multi-platform desktop application (Windows, macOS, and Linux are supported). Because it runs on your computer, you can use it even when you are offline. Unlike Netlify, Hokus does not assume any infrastructure vendor, meaning that you are free to move your website to another host without losing the user interface you are used to. This tutorial will show how to install Hokus, run a website locally and then move it to the cloud easily with http://Platform.sh. Steps https://github.com/julianoappelklein/hokus/releases https://docs.platform.sh/gettingstarted/own-code/create-project.html https://github.com/platformsh-templates/hugo https://github.com/platformsh-templates/hugo it with https://docs.platform.sh/administration/integrations/github.html Download the project with a git clone from the Git Repository that you wish. Cick in new Site, plus on the left button to create a new project. https://community.platform.sh/uploads/default/41d507beda8230e89a9f0ecbfbab746e49341d90 Now we have a Site configured, let’s use it! (keep in mind that Hokus will assume a default configuration for your website and to add more fields to your forms, or new forms, you need to edit the file hokus.toml). Select the website you just created and select the workspace “source” Click source - click select - then in sidebar. https://community.platform.sh/uploads/default/672fe35c6168598b8817f31321be9d8b9581fa90 On the left, you’ll see both the Post and Main Config option. If your current post is not showing, beware of the folder structure. We also have the option to customize it with the hokus.toml file. On this repository we have the posts under the post structure; therefore, we went in the collections and changed the folder line: folder = \"content/post/\" Now it’s time to run your website locally. ON the right sidebar, click in the “play” icon, and select the default option. Your browser will open showing your website! Click in the Posts button. You have the option to create, edit and delete posts.Open an existing post or create a new one. https://community.platform.sh/uploads/default/6984f91e3f659b7917e81eb8120a3f98c69cebef All the posts work with Markdown, so you can add bullets or pictures with Markdown After editing the post, click on the Save button (bottom right corner). Your changes will update your website in your browser instantly! https://community.platform.sh/uploads/default/f949075cbc28d7222c0aceef32d961de56dfa94a Are you happy with what you have? Let’s move it to the cloud with http://Platform.sh. We need to set the https://docs.platform.sh/configuration/app-containers.html and the https://docs.platform.sh/configuration/routes.html you can see respectively. File: .platform.app.yaml # .platform.app.yaml # The name of this application, which must be unique within a project. name: 'app' # The type key specifies the language and version for your application. type: golang:1.14 # The hooks that will be triggered when the package is deployed. hooks: # Build hooks can modify the application files on disk but not access any services like databases. build: !include type: string path: build.sh # The size of the persistent disk of the application (in MB). disk: 5120 # The configuration of the application when it is exposed to the web. web: locations: '/': # The public directory of the application relative to its root. root: 'public' index: ['index.html'] scripts: false allow: true expires: 1d File `.platform/routes.yaml` \"https://{default}/\": type: upstream upstream: \"app:http\" The last step is to push your changes to a remote repository. git push origin master Done, Hugo, Hokus, and http://Platform.sh at your service! https://community.platform.sh/uploads/default/68458a2ccb5d235315d4399b1cf64a0c3e93355a References https://www.hokuscms.com/ https://github.com/platformsh-examples/hokus-cms ",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/hokus-is-an-open-source-cms-for-hugo-completely-free-to-use/551",
        "relurl": "/t/hokus-is-an-open-source-cms-for-hugo-completely-free-to-use/551"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "0c14095828f6a0fe91313d100b23ad19455292ec",
        "title": "Multiple Applications (Tomcat)",
        "description": "http://Platform.sh supports building multiple applications per project (for example, RESTful web services with a front-end, or a main website and a blog). All of the applications share a common configuration through the files present in the .platform/ folder at the root of the Git repository. Every application exists in its own subdirectory, and each of them contain its own application configuration via a .platform.app.yaml file, so that your directory structure looks something like this: $ ls -a .git/ .platform/ routes.yaml services.yaml app/ .platform.app.yaml pom.xml [...] app2/ .platform.app.yaml pom.xml [...] pom.xml When you push via Git, http://Platform.sh will build each application separately. Only the application(s) that have been modified will be rebuilt. https://github.com/platformsh-examples/tomcat-multi-app https://docs.platform.sh/configuration/app/multi-app.html ",
        "text": "http://Platform.sh supports building multiple applications per project (for example, RESTful web services with a front-end, or a main website and a blog). All of the applications share a common configuration through the files present in the .platform/ folder at the root of the Git repository. Every application exists in its own subdirectory, and each of them contain its own application configuration via a .platform.app.yaml file, so that your directory structure looks something like this: $ ls -a .git/ .platform/ routes.yaml services.yaml app/ .platform.app.yaml pom.xml [...] app2/ .platform.app.yaml pom.xml [...] pom.xml When you push via Git, http://Platform.sh will build each application separately. Only the application(s) that have been modified will be rebuilt. https://github.com/platformsh-examples/tomcat-multi-app https://docs.platform.sh/configuration/app/multi-app.html ",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/multiple-applications-tomcat/468",
        "relurl": "/t/multiple-applications-tomcat/468"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "dda9e1a0e25a119b8f5617658617061ff05295f6",
        "title": "Tips for running Java on Platform.sh",
        "description": "Goal This tutorial will explain some Java commands and some recommendations to optimize the process of running your Java applications safety on http://Platform.sh. Assumptions You either have a Java application that you want to run on http://Platform.sh, or you already have a Java application already running on http://Platform.sh A text editor of your choice. Steps Once you have your Java application running on http://Platform.sh, there are several steps you should take to optimize the application runtime. Minimum requirement (Memory) The first one is Xmx, which defines the maximum size that the JVM can use. The optimal value can be derived from the application’s generated configuration files and will scale with your container size automatically. To extract the appropriate value on the command line, use $(jq .info.limits.memory /run/config.json). The second parameter is ExitOnOutOfMemoryError. When you enable this option, the JVM exits on the first occurrence of an out-of-memory error. http://Platform.sh will restart the application automatically. These are the basic recommended parameters for running a Java application. So, the command will start with: java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError //The rest of the arguments and the jar file. Garbage collector (Optional) When migrating the application to a cloud environment, it is often essential to analyze the Garbage Collector’s log and behavior. For this, there are two options: Placing the log into the http://Platform.sh app.log file (which captures STDOUT). Creating a log file specifically for the GC. To use the STDOUT log, you can add the parameter -XX: + PrintGCDetails, E.g.: java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails //The rest of the arguments and the jar file. Java supports a number of different garbage collection strategies. Which one is optimal for your application will vary depending on your available memory, Java version, and application profile. Determining which is best for your application is out of scope, but the main options and how to enable them are: Name Command Flag Description Serial Garbage Collector -XX:+UseSerialGC This is the simplest GC implementation, as it basically works with a single thread. Parallel Garbage Collector -XX:+UseParallelGC Unlike Serial Garbage Collector, this uses multiple threads for managing heap space. But it also freezes other application threads while performing GC. CMS Garbage Collector -XX:+USeParNewGC The Concurrent Mark Sweep (CMS) implementation uses multiple garbage collector threads for garbage collection. It’s for applications that prefer shorter garbage collection pauses, and that can afford to share processor resources with the garbage collector while the application is running. G1 Garbage Collector -XX:+UseG1GC Garbage First, G1, is for applications running on multiprocessor machines with large memory space. The default stragtegy on Java 9 and later is G1. The GC strategy to use can be set in the start line with: Serial java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails -XX:+UseSerialGC //The rest of the arguments and the jar file. Parallel Garbage Collector java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails -XX:+UseParallelGC //The rest of the arguments and the jar file. CMS Garbage Collector java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails -XX:+USeParNewGC //The rest of the arguments and the jar file. G1 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails -XX:+UseG1GC //The rest of the arguments and the jar file. Java 8 Optimization Ideally, all applications should run the latest LTS release of the JVM at least. That is currently Java 11. Java 11 has a number of performance improvements, particularly on container-based environments such as http://Platform.sh. However, in many cases, this is not possible. If you are still running on Java 8 there are two additional considerations. The default garbage collector for Java 8 is Parallel GC. In most cases G1 will offer better performance. We recommend enabling it, as above. Furthermore, there is the UseStringDeduplication flag which works to eliminate duplicate Strings within the GC process. That flag can save between 13% to 30% of memory, depending on application. However, this can impact on the pause time of your app. java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails References https://community.platform.sh/t/java-memory-commands/481 https://community.platform.sh/t/how-to-migrate-my-java-application-to-platfrom-sh/529 https://community.platform.sh/t/garbage-collector-log/482 ",
        "text": "Goal This tutorial will explain some Java commands and some recommendations to optimize the process of running your Java applications safety on http://Platform.sh. Assumptions You either have a Java application that you want to run on http://Platform.sh, or you already have a Java application already running on http://Platform.sh A text editor of your choice. Steps Once you have your Java application running on http://Platform.sh, there are several steps you should take to optimize the application runtime. Minimum requirement (Memory) The first one is Xmx, which defines the maximum size that the JVM can use. The optimal value can be derived from the application’s generated configuration files and will scale with your container size automatically. To extract the appropriate value on the command line, use $(jq .info.limits.memory /run/config.json). The second parameter is ExitOnOutOfMemoryError. When you enable this option, the JVM exits on the first occurrence of an out-of-memory error. http://Platform.sh will restart the application automatically. These are the basic recommended parameters for running a Java application. So, the command will start with: java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError //The rest of the arguments and the jar file. Garbage collector (Optional) When migrating the application to a cloud environment, it is often essential to analyze the Garbage Collector’s log and behavior. For this, there are two options: Placing the log into the http://Platform.sh app.log file (which captures STDOUT). Creating a log file specifically for the GC. To use the STDOUT log, you can add the parameter -XX: + PrintGCDetails, E.g.: java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails //The rest of the arguments and the jar file. Java supports a number of different garbage collection strategies. Which one is optimal for your application will vary depending on your available memory, Java version, and application profile. Determining which is best for your application is out of scope, but the main options and how to enable them are: Name Command Flag Description Serial Garbage Collector -XX:+UseSerialGC This is the simplest GC implementation, as it basically works with a single thread. Parallel Garbage Collector -XX:+UseParallelGC Unlike Serial Garbage Collector, this uses multiple threads for managing heap space. But it also freezes other application threads while performing GC. CMS Garbage Collector -XX:+USeParNewGC The Concurrent Mark Sweep (CMS) implementation uses multiple garbage collector threads for garbage collection. It’s for applications that prefer shorter garbage collection pauses, and that can afford to share processor resources with the garbage collector while the application is running. G1 Garbage Collector -XX:+UseG1GC Garbage First, G1, is for applications running on multiprocessor machines with large memory space. The default stragtegy on Java 9 and later is G1. The GC strategy to use can be set in the start line with: Serial java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails -XX:+UseSerialGC //The rest of the arguments and the jar file. Parallel Garbage Collector java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails -XX:+UseParallelGC //The rest of the arguments and the jar file. CMS Garbage Collector java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails -XX:+USeParNewGC //The rest of the arguments and the jar file. G1 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails -XX:+UseG1GC //The rest of the arguments and the jar file. Java 8 Optimization Ideally, all applications should run the latest LTS release of the JVM at least. That is currently Java 11. Java 11 has a number of performance improvements, particularly on container-based environments such as http://Platform.sh. However, in many cases, this is not possible. If you are still running on Java 8 there are two additional considerations. The default garbage collector for Java 8 is Parallel GC. In most cases G1 will offer better performance. We recommend enabling it, as above. Furthermore, there is the UseStringDeduplication flag which works to eliminate duplicate Strings within the GC process. That flag can save between 13% to 30% of memory, depending on application. However, this can impact on the pause time of your app. java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails References https://community.platform.sh/t/java-memory-commands/481 https://community.platform.sh/t/how-to-migrate-my-java-application-to-platfrom-sh/529 https://community.platform.sh/t/garbage-collector-log/482 ",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/tips-for-running-java-on-platform-sh/531",
        "relurl": "/t/tips-for-running-java-on-platform-sh/531"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "eef2aafc2b60649d7d2d9fc2524852dce8152851",
        "title": "Java Memory commands",
        "description": "Java Heap space is used by the Java runtime to allocate memory to Objects and JRE classes. Whenever we create an object, it’s always created in the Heap space. This post, we’ll explain to how to set the memory size on http://Platform.sh. Garbage Collection runs on the heap memory to free the memory used by objects that don’t have any reference. Any object created in the heap space has global access and can be referenced from anywhere in the application. To set the memory size on JVM on http://Platform.sh we need to append the memory settings with the Java startup parameter. http://Platform.sh has a command that returns the memory that is available with -$(jq .info.limits.memory /run/config.json) that value is on megabytes. As a recommendation, you can use the fine maximum JVM memory. web: commands: start: java -jar -Xmx$(jq .info.limits.memory /run/config.json)m target/microprofile-microbundle.jar --port $PORT Below, more arguments to set the memory: -Xmn size Sets the initial and maximum size (in bytes) of the heap for the young generation (nursery). Append the letter k or K to indicate kilobytes, m or M to indicate megabytes, or g or G to indicate gigabytes. The young generation region of the heap is used for new objects. GC is performed in this region more often than in other regions. If the size for the young generation is too small, then a lot of minor garbage collections are performed. If the size is too large, then only full garbage collections are performed, which can take a long time to complete. Oracle recommends that you keep the size for the young generation greater than 25% and less than 50% of the overall heap size. #-Xms size Sets the initial size (in bytes) of the heap. This value must be a multiple of 1024 and greater than 1 MB. Append the letter k or K to indicate kilobytes, m or M to indicate megabytes, g or G to indicate gigabytes. -Xmx size Specifies the maximum size (in bytes) of the memory allocation pool in bytes. This value must be a multiple of 1024 and greater than 2 MB. Append the letter k or K to indicate kilobytes, m or M to indicate megabytes, and g or G to indicate gigabytes. The default value is chosen at runtime based on system configuration. For server deployments, -Xms and -Xmx are often set to the same value. As mentioned, previously use the $(jq .info.limits.memory /run/config.json) to create the Xmx command. E.g.: java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -Xnoclassgc Disables garbage collection (GC) of classes. This can save some GC time, which shortens interruptions during the application run. When you specify -Xnoclassgc at startup, the class objects in the application are left untouched during GC and are always considered live. -Xss size Sets the thread stack size (in bytes). Append the letter k or K to indicate KB, m or M to indicate MB, and g or G to indicate GB. ExitOnOutOfMemoryError When you enable this option, the JVM exits on the first occurrence of an out-of-memory error. It can be used if you prefer restarting an instance of the JVM rather than handling out of memory errors. Recommended Java memory command at http://Platform.sh java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError ... GC Log activated java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails ... References https://community.platform.sh/t/garbage-collector-log/482 ",
        "text": "Java Heap space is used by the Java runtime to allocate memory to Objects and JRE classes. Whenever we create an object, it’s always created in the Heap space. This post, we’ll explain to how to set the memory size on http://Platform.sh. Garbage Collection runs on the heap memory to free the memory used by objects that don’t have any reference. Any object created in the heap space has global access and can be referenced from anywhere in the application. To set the memory size on JVM on http://Platform.sh we need to append the memory settings with the Java startup parameter. http://Platform.sh has a command that returns the memory that is available with -$(jq .info.limits.memory /run/config.json) that value is on megabytes. As a recommendation, you can use the fine maximum JVM memory. web: commands: start: java -jar -Xmx$(jq .info.limits.memory /run/config.json)m target/microprofile-microbundle.jar --port $PORT Below, more arguments to set the memory: -Xmn size Sets the initial and maximum size (in bytes) of the heap for the young generation (nursery). Append the letter k or K to indicate kilobytes, m or M to indicate megabytes, or g or G to indicate gigabytes. The young generation region of the heap is used for new objects. GC is performed in this region more often than in other regions. If the size for the young generation is too small, then a lot of minor garbage collections are performed. If the size is too large, then only full garbage collections are performed, which can take a long time to complete. Oracle recommends that you keep the size for the young generation greater than 25% and less than 50% of the overall heap size. #-Xms size Sets the initial size (in bytes) of the heap. This value must be a multiple of 1024 and greater than 1 MB. Append the letter k or K to indicate kilobytes, m or M to indicate megabytes, g or G to indicate gigabytes. -Xmx size Specifies the maximum size (in bytes) of the memory allocation pool in bytes. This value must be a multiple of 1024 and greater than 2 MB. Append the letter k or K to indicate kilobytes, m or M to indicate megabytes, and g or G to indicate gigabytes. The default value is chosen at runtime based on system configuration. For server deployments, -Xms and -Xmx are often set to the same value. As mentioned, previously use the $(jq .info.limits.memory /run/config.json) to create the Xmx command. E.g.: java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -Xnoclassgc Disables garbage collection (GC) of classes. This can save some GC time, which shortens interruptions during the application run. When you specify -Xnoclassgc at startup, the class objects in the application are left untouched during GC and are always considered live. -Xss size Sets the thread stack size (in bytes). Append the letter k or K to indicate KB, m or M to indicate MB, and g or G to indicate GB. ExitOnOutOfMemoryError When you enable this option, the JVM exits on the first occurrence of an out-of-memory error. It can be used if you prefer restarting an instance of the JVM rather than handling out of memory errors. Recommended Java memory command at http://Platform.sh java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError ... GC Log activated java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails ... References https://community.platform.sh/t/garbage-collector-log/482 ",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/java-memory-commands/481",
        "relurl": "/t/java-memory-commands/481"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "c1b5b43e9322d5adfe76792220565f0760cafeb2",
        "title": "Garbage Collector Log",
        "description": "Java garbage collection is the process by which Java programs perform automatic memory management. This post, we’ll explain how to add the log in the http://Platform.sh application. How to Generate GC Log File? In order to understand the GC log, you first need to generate one. Passing the following system properties to your JVM would generate GC logs. To set the JVM log on http://Platform.sh we need to append it with the Java startup parameter. E.g., To define the GC log. From your app’s https://docs.platform.sh/configuration/app-containers.html . web: commands: start: java -jar -Xmx2048m -Xlog:gc*=debug:stdout:time,uptimemillis,tid target/microprofile-microbundle.jar --port $PORT When you append the GC log, the next step is to check the information, the default file to the registry is on /tmp/log/app.log. Therefore you can check the GC information from the command below once you’re inside the application container. tail -f /tmp/log/app.log To access the machine through ssh, please check the PSH documentation: https://docs.platform.sh/development/ssh.html -XX:+PrintGC The flag -XX:+PrintGC (or the alias -verbose:gc) activates the “simple” GC logging mode, which prints a line for every young generation GC and every full GC. -XX:+PrintGCDetails If we use -XX:+PrintGCDetails instead of -XX:+PrintGC, we activate the “detailed” GC logging mode which differs depending on the GC algorithm used. We start by taking a look at the output produced by a young generation GC using the Throughput Collector. Unified JVM Logging Java 9 comes with a unified logging architecture ( http://openjdk.java.net/jeps/158 ) that pipes a lot of messages that the JVM generates through the same mechanism, which can be configured with the -Xlog option. E.g.: -Xlog:gc*=debug:stdout:time,uptimemillis,tid -Xloggc By default the GC log is written to stdout. With -Xloggc: we may instead specify an output file. Note that this flag implicitly sets -XX:+PrintGC and -XX:+PrintGCTimeStamps as well. Still, I would recommend setting these flags explicitly if desired, in order to safeguard yourself against unexpected changes in new JVM versions. Migrate to Java 11 The benefits of using the latest LTS version are the increase of support to the containers, therefore, beyond the new APIs and security fixes, there is an increase of performance to run on GC. D Command-Line Options This appendix describes some command-line options that can be useful when diagnosing problems with the Java HotSpot VM. References https://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html https://stackify.com/what-is-java-garbage-collection/ https://dzone.com/articles/understanding-garbage-collection-log See also Java Memory arguments: https://community.platform.sh/t/java-memory-commands/481 ",
        "text": "Java garbage collection is the process by which Java programs perform automatic memory management. This post, we’ll explain how to add the log in the http://Platform.sh application. How to Generate GC Log File? In order to understand the GC log, you first need to generate one. Passing the following system properties to your JVM would generate GC logs. To set the JVM log on http://Platform.sh we need to append it with the Java startup parameter. E.g., To define the GC log. From your app’s https://docs.platform.sh/configuration/app-containers.html . web: commands: start: java -jar -Xmx2048m -Xlog:gc*=debug:stdout:time,uptimemillis,tid target/microprofile-microbundle.jar --port $PORT When you append the GC log, the next step is to check the information, the default file to the registry is on /tmp/log/app.log. Therefore you can check the GC information from the command below once you’re inside the application container. tail -f /tmp/log/app.log To access the machine through ssh, please check the PSH documentation: https://docs.platform.sh/development/ssh.html -XX:+PrintGC The flag -XX:+PrintGC (or the alias -verbose:gc) activates the “simple” GC logging mode, which prints a line for every young generation GC and every full GC. -XX:+PrintGCDetails If we use -XX:+PrintGCDetails instead of -XX:+PrintGC, we activate the “detailed” GC logging mode which differs depending on the GC algorithm used. We start by taking a look at the output produced by a young generation GC using the Throughput Collector. Unified JVM Logging Java 9 comes with a unified logging architecture ( http://openjdk.java.net/jeps/158 ) that pipes a lot of messages that the JVM generates through the same mechanism, which can be configured with the -Xlog option. E.g.: -Xlog:gc*=debug:stdout:time,uptimemillis,tid -Xloggc By default the GC log is written to stdout. With -Xloggc: we may instead specify an output file. Note that this flag implicitly sets -XX:+PrintGC and -XX:+PrintGCTimeStamps as well. Still, I would recommend setting these flags explicitly if desired, in order to safeguard yourself against unexpected changes in new JVM versions. Migrate to Java 11 The benefits of using the latest LTS version are the increase of support to the containers, therefore, beyond the new APIs and security fixes, there is an increase of performance to run on GC. D Command-Line Options This appendix describes some command-line options that can be useful when diagnosing problems with the Java HotSpot VM. References https://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html https://stackify.com/what-is-java-garbage-collection/ https://dzone.com/articles/understanding-garbage-collection-log See also Java Memory arguments: https://community.platform.sh/t/java-memory-commands/481 ",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/garbage-collector-log/482",
        "relurl": "/t/garbage-collector-log/482"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "2d036908792920f9482048b80392348b07fcf747",
        "title": "Java Email at Platform.sh",
        "description": "Electronic mail is a method of exchanging messages between people using electronic devices. In this post, we’ll explain how to send an email message with Java through http://Platform.sh. By default, only the master environment can send emails. For non-master environments, you can configure outgoing emails . Emails from http://Platform.sh are sent via a SendGrid-based SMTP proxy. Each http://Platform.sh project is provisioned as a SendGrid sub-account. These SendGrid sub-accounts are capped at 12k emails per month. Below the sample code that uses https://javaee.github.io/javamail/ : import sh.platform.config.Config; import javax.mail.Message; import javax.mail.MessagingException; import javax.mail.Session; import javax.mail.Transport; import javax.mail.internet.InternetAddress; import javax.mail.internet.MimeMessage; import java.util.Properties; import java.util.logging.Level; import java.util.logging.Logger; public class JavaEmailSender { private static final Logger LOGGER = Logger.getLogger(JavaEmailSender.class.getName()); public void send() { Config config = new Config(); String to = \"\";//change accordingly String from = \"\";//change accordingly String host = config.getSmtpHost(); //or IP address //Get the session object Properties properties = System.getProperties(); properties.setProperty(\"mail.smtp.host\", host); Session session = Session.getDefaultInstance(properties); //compose the message try { MimeMessage message = new MimeMessage(session); message.setFrom(new InternetAddress(from)); message.addRecipient(Message.RecipientType.TO, new InternetAddress(to)); message.setSubject(\"Ping\"); message.setText(\"Hello, this is example of sending email \"); // Send message Transport.send(message); System.out.println(\"message sent successfully....\"); } catch (MessagingException exp) { exp.printStackTrace(); LOGGER.log(Level.SEVERE, \"there is an error to send an message\", exp); } } } There is plenty of additional l documentation about using JavaMail, like this one, that shows how to send email with HTML format and an attachment: https://mkyong.com/java/java-how-to-send-email/ References https://docs.platform.sh/development/email.html https://mkyong.com/java/java-how-to-send-email/ https://javaee.github.io/javamail/ ",
        "text": "Electronic mail is a method of exchanging messages between people using electronic devices. In this post, we’ll explain how to send an email message with Java through http://Platform.sh. By default, only the master environment can send emails. For non-master environments, you can configure outgoing emails . Emails from http://Platform.sh are sent via a SendGrid-based SMTP proxy. Each http://Platform.sh project is provisioned as a SendGrid sub-account. These SendGrid sub-accounts are capped at 12k emails per month. Below the sample code that uses https://javaee.github.io/javamail/ : import sh.platform.config.Config; import javax.mail.Message; import javax.mail.MessagingException; import javax.mail.Session; import javax.mail.Transport; import javax.mail.internet.InternetAddress; import javax.mail.internet.MimeMessage; import java.util.Properties; import java.util.logging.Level; import java.util.logging.Logger; public class JavaEmailSender { private static final Logger LOGGER = Logger.getLogger(JavaEmailSender.class.getName()); public void send() { Config config = new Config(); String to = \"\";//change accordingly String from = \"\";//change accordingly String host = config.getSmtpHost(); //or IP address //Get the session object Properties properties = System.getProperties(); properties.setProperty(\"mail.smtp.host\", host); Session session = Session.getDefaultInstance(properties); //compose the message try { MimeMessage message = new MimeMessage(session); message.setFrom(new InternetAddress(from)); message.addRecipient(Message.RecipientType.TO, new InternetAddress(to)); message.setSubject(\"Ping\"); message.setText(\"Hello, this is example of sending email \"); // Send message Transport.send(message); System.out.println(\"message sent successfully....\"); } catch (MessagingException exp) { exp.printStackTrace(); LOGGER.log(Level.SEVERE, \"there is an error to send an message\", exp); } } } There is plenty of additional l documentation about using JavaMail, like this one, that shows how to send email with HTML format and an attachment: https://mkyong.com/java/java-how-to-send-email/ References https://docs.platform.sh/development/email.html https://mkyong.com/java/java-how-to-send-email/ https://javaee.github.io/javamail/ ",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/java-email-at-platform-sh/483",
        "relurl": "/t/java-email-at-platform-sh/483"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "f354d8595e6b65a7f014a653d644365b7ab5444f",
        "title": "How to kill stuck processes that block your deployments",
        "description": "One of the most common causes for environment builds that get stuck is a runaway/stuck process. To ensure data consistency, the deployment flow tries to terminate running processes gracefully. Sometimes, this does not work and the deployment ends up waiting forever due to a blocking process. Using two simple commands when connected to SSH can help you get things moving without having to wait for our support team to intervene. As an example, let’s assume a cron process is stuck. The demo application contains one blocker.sh script: #!/bin/sh sleep 3600 which is configured as an every 5 minute cron in .platform.app.yaml: blocker: spec: '*/5 * * * *' cmd: '/bin/bash /app/block.sh' Now, this process is blocking our new deployment - the log is stuck at: Redeploying environment master Preparing deployment Closing services router and app First thing to do is see if you can connect with SSH to the environment (this should work most of the time). If the SSH connection is successful, run: ps fuxa. The output will be a list of processes, similar to this one: web@app.0:~$ ps fuxa USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 153 0.9 0.0 197656 33176 ? Sl 15:05 0:00 /usr/bin/python2.7 /etc/platform/commands/notify web 159 0.0 0.0 271388 26620 ? Sl 15:05 0:00 \\_ /usr/bin/python2.7 /etc/platform/commands/notify web 162 0.0 0.0 4280 740 ? S 15:05 0:00 \\_ /bin/dash -c /bin/bash /app/blocker.sh web 165 0.0 0.0 12920 2740 ? S 15:05 0:00 \\_ /bin/bash /app/blocker.sh web 166 0.0 0.0 7580 668 ? S 15:05 0:00 \\_ sleep 3600 root 1 0.0 0.0 15816 1096 ? Ss+ 15:02 0:00 init [2] root 74 0.0 0.0 4204 1132 ? Ss 15:02 0:00 runsvdir -P /etc/service log: ................................................................................................................... root 80 0.0 0.0 4052 704 ? Ss 15:02 0:00 \\_ runsv tideways root 81 0.0 0.0 4052 696 ? Ss 15:02 0:00 \\_ runsv ssh root 90 0.0 0.0 72104 5608 ? S 15:02 0:00 | \\_ /usr/sbin/sshd -D root 167 0.0 0.0 94876 6472 ? Ss 15:06 0:00 | \\_ sshd: web [priv] web 173 0.0 0.0 94876 3668 ? S 15:06 0:00 | \\_ sshd: web@pts/0 web 174 0.0 0.0 21768 3852 pts/0 Ss 15:06 0:00 | \\_ -bash web 190 0.0 0.0 37448 3172 pts/0 R+ 15:06 0:00 | \\_ ps fuxa root 82 0.0 0.0 4052 700 ? Ss 15:02 0:00 \\_ runsv nginx root 115 0.0 0.0 36984 6684 ? S 15:02 0:00 | \\_ nginx: master process /usr/sbin/nginx -g daemon off; error_log /var/log/error.log; -c /etc/nginx/nginx.conf web 121 0.0 0.0 45460 11560 ? S 15:02 0:00 | \\_ nginx: worker process root 83 0.0 0.0 4052 752 ? Ss 15:02 0:00 \\_ runsv newrelic root 84 0.0 0.0 4052 644 ? Ss 15:02 0:00 \\_ runsv idmapd root 87 0.0 0.0 23348 2152 ? S 15:02 0:00 | \\_ /usr/sbin/rpc.idmapd -f -C -p /run/rpc_pipefs root 85 0.0 0.0 4052 700 ? Ss 15:02 0:00 \\_ runsv app web 111 0.0 0.0 359464 30288 ? Ss 15:02 0:00 \\_ php-fpm: master process (/etc/php/7.2-zts/fpm/php-fpm.conf) web 116 0.0 0.0 12932 296 ? S 15:02 0:00 \\_ /bin/bash /etc/platform/start-app web 117 0.0 0.0 7584 656 ? S 15:02 0:00 \\_ tee -a /var/log/app.log You can probably see already our stuck process is here: web 162 0.0 0.0 4280 740 ? S 15:05 0:00 \\_ /bin/dash -c /bin/bash /app/blocker.sh web 165 0.0 0.0 12920 2740 ? S 15:05 0:00 \\_ /bin/bash /app/blocker.sh web 166 0.0 0.0 7580 668 ? S 15:05 0:00 \\_ sleep 3600 The important thing in the above output is the number listed in the second column, after the web user: this is the process ID, a unique identifier for each running process. As we’re interested in stopping the process, it’d be very useful to somehow forcefully stop it. This can be done with the kill -9 command, followed by the list of process IDs you want to stop. Therefore, to stop the cron in our example, we’d need to run: kill -9 162 165 166 Be careful: there might be more processes that block the deployment ! Inspect the process list carefully (all application processes will be under the web user) and repeat the previous command for all of them. Once done, the SSH connection will be terminated and you’ll see the friendly: Message from bot@platform.sh at 15:09:36: This container is being dematerialized. See you on the other side. message. Your previously stuck deployment will now continue. Note: if the SSH connection cannot be established, you will need to open a support ticket.",
        "text": "One of the most common causes for environment builds that get stuck is a runaway/stuck process. To ensure data consistency, the deployment flow tries to terminate running processes gracefully. Sometimes, this does not work and the deployment ends up waiting forever due to a blocking process. Using two simple commands when connected to SSH can help you get things moving without having to wait for our support team to intervene. As an example, let’s assume a cron process is stuck. The demo application contains one blocker.sh script: #!/bin/sh sleep 3600 which is configured as an every 5 minute cron in .platform.app.yaml: blocker: spec: '*/5 * * * *' cmd: '/bin/bash /app/block.sh' Now, this process is blocking our new deployment - the log is stuck at: Redeploying environment master Preparing deployment Closing services router and app First thing to do is see if you can connect with SSH to the environment (this should work most of the time). If the SSH connection is successful, run: ps fuxa. The output will be a list of processes, similar to this one: web@app.0:~$ ps fuxa USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 153 0.9 0.0 197656 33176 ? Sl 15:05 0:00 /usr/bin/python2.7 /etc/platform/commands/notify web 159 0.0 0.0 271388 26620 ? Sl 15:05 0:00 \\_ /usr/bin/python2.7 /etc/platform/commands/notify web 162 0.0 0.0 4280 740 ? S 15:05 0:00 \\_ /bin/dash -c /bin/bash /app/blocker.sh web 165 0.0 0.0 12920 2740 ? S 15:05 0:00 \\_ /bin/bash /app/blocker.sh web 166 0.0 0.0 7580 668 ? S 15:05 0:00 \\_ sleep 3600 root 1 0.0 0.0 15816 1096 ? Ss+ 15:02 0:00 init [2] root 74 0.0 0.0 4204 1132 ? Ss 15:02 0:00 runsvdir -P /etc/service log: ................................................................................................................... root 80 0.0 0.0 4052 704 ? Ss 15:02 0:00 \\_ runsv tideways root 81 0.0 0.0 4052 696 ? Ss 15:02 0:00 \\_ runsv ssh root 90 0.0 0.0 72104 5608 ? S 15:02 0:00 | \\_ /usr/sbin/sshd -D root 167 0.0 0.0 94876 6472 ? Ss 15:06 0:00 | \\_ sshd: web [priv] web 173 0.0 0.0 94876 3668 ? S 15:06 0:00 | \\_ sshd: web@pts/0 web 174 0.0 0.0 21768 3852 pts/0 Ss 15:06 0:00 | \\_ -bash web 190 0.0 0.0 37448 3172 pts/0 R+ 15:06 0:00 | \\_ ps fuxa root 82 0.0 0.0 4052 700 ? Ss 15:02 0:00 \\_ runsv nginx root 115 0.0 0.0 36984 6684 ? S 15:02 0:00 | \\_ nginx: master process /usr/sbin/nginx -g daemon off; error_log /var/log/error.log; -c /etc/nginx/nginx.conf web 121 0.0 0.0 45460 11560 ? S 15:02 0:00 | \\_ nginx: worker process root 83 0.0 0.0 4052 752 ? Ss 15:02 0:00 \\_ runsv newrelic root 84 0.0 0.0 4052 644 ? Ss 15:02 0:00 \\_ runsv idmapd root 87 0.0 0.0 23348 2152 ? S 15:02 0:00 | \\_ /usr/sbin/rpc.idmapd -f -C -p /run/rpc_pipefs root 85 0.0 0.0 4052 700 ? Ss 15:02 0:00 \\_ runsv app web 111 0.0 0.0 359464 30288 ? Ss 15:02 0:00 \\_ php-fpm: master process (/etc/php/7.2-zts/fpm/php-fpm.conf) web 116 0.0 0.0 12932 296 ? S 15:02 0:00 \\_ /bin/bash /etc/platform/start-app web 117 0.0 0.0 7584 656 ? S 15:02 0:00 \\_ tee -a /var/log/app.log You can probably see already our stuck process is here: web 162 0.0 0.0 4280 740 ? S 15:05 0:00 \\_ /bin/dash -c /bin/bash /app/blocker.sh web 165 0.0 0.0 12920 2740 ? S 15:05 0:00 \\_ /bin/bash /app/blocker.sh web 166 0.0 0.0 7580 668 ? S 15:05 0:00 \\_ sleep 3600 The important thing in the above output is the number listed in the second column, after the web user: this is the process ID, a unique identifier for each running process. As we’re interested in stopping the process, it’d be very useful to somehow forcefully stop it. This can be done with the kill -9 command, followed by the list of process IDs you want to stop. Therefore, to stop the cron in our example, we’d need to run: kill -9 162 165 166 Be careful: there might be more processes that block the deployment ! Inspect the process list carefully (all application processes will be under the web user) and repeat the previous command for all of them. Once done, the SSH connection will be terminated and you’ll see the friendly: Message from bot@platform.sh at 15:09:36: This container is being dematerialized. See you on the other side. message. Your previously stuck deployment will now continue. Note: if the SSH connection cannot be established, you will need to open a support ticket.",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-kill-stuck-processes-that-block-your-deployments/473",
        "relurl": "/t/how-to-kill-stuck-processes-that-block-your-deployments/473"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "fecba4a9ca27e045ed28286c41c431a09f85f6da",
        "title": "Consequences of restoring backups after the persistent data model has been changed",
        "description": "Goal The ability to restore environment backups is a powerful tool, but only when backups are taken regularly to reflect the rapid changes that can occur over the course of the development of your application. Failing to do so has some consequences, as older backups that don’t reflect major changes can result in data loss once restored. The goal in this tutorial is to explore some of these consequences, and to illustrate how taking the time to https://community.platform.sh/t/how-to-set-up-automated-environment-tasks/127 can alleviate plenty of headaches down the line. Preparation You will need: A http://Platform.sh account A http://Platform.sh project http://Platform.sh CLI installed locally A local copy of that project Problems http://Platform.sh gives you the ability to quickly create backups of the state of your environments, and then easily restore those backups to those environments should you need to. This is not yet an automatic process, and both creation and restoration must be executed manually by the user. But, of course, people forget to take frequent backups - they’re busy developing. It’s for this reason that we often recommend users https://community.platform.sh/t/how-to-set-up-automated-environment-tasks/127 in order to automate this process. If you have not yet set up automatic backups as described in the above How-to, you may very well find yourself in the following situation. I’ve created a backup close to the start of an environment’s history. I’ve made a lot of changes to that environment (added services, added data to those services, created new mounts, etc.). Something went wrong. Well, look, I have a backup right here. But it’s old. What will happen? This tutorial is meant to explore this exact scenario, with the intended takeaways: the importance of setting up automatic backups on your projects using the How-to above as a guide (so you don’t find yourself in this situation in the first place). some greater understanding of how http://Platform.sh handles your data during backups, restores, and syncs. Steps Creating backups note This tutorial uses a modified version of our https://github.com/platformsh/language-examples project. That project is intended to show how to interact with all of our managed services with each of our runtimes. It’s a great resource, and it even powers most of the examples in our public documentation, but it’s gigantic, and much more than we need for this tutorial. We removed all of the runtimes except for Python, leaving us with only the main and python application container directories. We also removed all of the services save postgresql, but we will be adding a few of them back in the steps below. It is not necessary that you use the same repository if going through this tutorial step by step, just try to match the steps in your own project. 1. Create a new branch Assuming you have already created a project and initialized it with some code, create a new environment platform branch add-mongo master 2. Create a backup Then create a backup of that environment in its current state. platform backup:create Creating a backup of add-mongo Waiting for the activity is5ytp2lbm7p4 (Chad Carlson created a backup of add-mongo): Creating snapshot of add-mongo Created backup bivzemjpeqvohqey2t7fo7vs5m [============================] 15 secs (complete) Activity is5ytp2lbm7p4 succeeded Backup name: bivzemjpeqvohqey2t7fo7vs5m So what happened here? Each service, which includes applications, has its own persistent storage. During a backup, a copy is made for each of them. It’s the collection of these backups that then makes up backup bivzemjpeqvohqey2t7fo7vs5m. Adding a new service 1. Configure a new service First, let’s add a new service to the project, one that did not exist when the backup was created. Modify your services.yaml file to include MongoDB: dbmongo: type: mongodb:3.6 disk: 1024 size: S and your .platform.app.yaml to include the new relationship: relationships: mongodb: 'dbmongo:mongodb' Then commit and push the changes to http://Platform.sh. git add . git commit -m \"Adds mongodb.\" git push platform add-mongo http://Platform.sh will provision MongoDB to your virtual cluster, and expose the following credentials in your PLATFORM_RELATIONSHIPS environment variable: { \"username\": \"main\", \"scheme\": \"mongodb\", \"service\": \"mongodb\", \"ip\": \"169.254.117.167\", \"hostname\": \"ldh423mk2e7o6qto2syljqbg5u.mongodb.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"mongodb.internal\", \"rel\": \"mongodb\", \"path\": \"main\", \"query\": { \"is_master\": true }, \"password\": \"main\", \"type\": \"mongodb:3.6\", \"port\": 27017 } 2. Develop and verify At this point, MongoDB is available for you to develop with. Over the next few days, you may make the following changes as you are developing on the environment: you create a new collection in main called starwars. your application adds a number of documents to that collection. (In the case of our language examples project, the Python app adds a document with the contents {\"name\": \"Rey\", \"occupation\": \"Jedi\"} as a test each time the site is visited) You can verify those documents have been added locally by opening an SSH tunnel to the service (platform:tunnel single -r mongodb) and then connecting to MongoDB via that tunnel and the credentials above: $ mongo --port 30000 -u main -p main --authenticationDatabase main MongoDB shell version v4.0.3 use main switched to db main show collections starwars db.starwars.find() { \"_id\" : ObjectId(\"5e4457d05908440effc53a20\"), \"name\" : \"Rey\", \"occupation\" : \"Jedi\" } { \"_id\" : ObjectId(\"5e4457d35908440effc53a22\"), \"name\" : \"Rey\", \"occupation\" : \"Jedi\" } { \"_id\" : ObjectId(\"5e4457d75908440effc53a24\"), \"name\" : \"Rey\", \"occupation\" : \"Jedi\" } { \"_id\" : ObjectId(\"5e4457d95908440effc53a26\"), \"name\" : \"Rey\", \"occupation\" : \"Jedi\" } { \"_id\" : ObjectId(\"5e44585c5908440effc53a28\"), \"name\" : \"Rey\", \"occupation\" : \"Jedi\" } There’s all the newly added data. 3. Restore the backup Now, if we restore the backup we created at the beginning (before MongoDB was a part of the cluster) using the command platform backup:restore bivzemjpeqvohqey2t7fo7vs5m The restore will do a few things: All persistent data currently in that environment is wiped. Backup bivzemjpeqvohqey2t7fo7vs5m, which contains a backup of each service present when it was taken, is then applied to the containers present in the environment one by one. Because of that first point, and because no backup for MongoDB exists in backup bivzemjpeqvohqey2t7fo7vs5m, all data and code pertaining to MongoDB is erased before anything else happens. If you attempt to open a tunnel and locally connect to MongoDB straight away, the service won’t even be recognized as existing in the project. Run platform redeploy, re-open the tunnel to MongoDB, and then repeat the prior steps to connect to MongoDB: $ mongo --port 30000 -u main -p main --authenticationDatabase main MongoDB shell version v4.0.3 use main switched to db main show collections starwars As you can see, the added collection no longer exists on the service. This is our first example of why it’s important to create backups regularly. If one had been taken some time after MongoDB existed, we would still have been able to keep some of its data. Mounts 1. Configure a new mount Another case where this would be relevant is the addition of new mounts to the project. If you were to create a mount that was not included in backup bivzemjpeqvohqey2t7fo7vs5m, would the files in that mount be erased? You can probably guess already based on the previous example, but let’s find out. You can add a mount to an application by adding the following lines to your .platform.app.yaml file: mounts: 'add-mount': source: local source_path: add-mount Then commit and push to http://Platform.sh: git add . git commit -m \"Add a mount.\" git push platform add-mongo 2. Add data and verify Let’s just create a simple file mkdir mount-data \u0026\u0026 touch mount-data/test.txt echo \"Here's our mounted data on Platform.sh.\" mount-data/test.txt and then upload it to the newly defined mount: platform mount:upload --mount add-mount --source ./mount-data You can then verify that the file was uploaded to the project by runing platform ssh to SSH into the application container. Then run, web@app.0:~$ echo \"$( ",
        "text": "Goal The ability to restore environment backups is a powerful tool, but only when backups are taken regularly to reflect the rapid changes that can occur over the course of the development of your application. Failing to do so has some consequences, as older backups that don’t reflect major changes can result in data loss once restored. The goal in this tutorial is to explore some of these consequences, and to illustrate how taking the time to https://community.platform.sh/t/how-to-set-up-automated-environment-tasks/127 can alleviate plenty of headaches down the line. Preparation You will need: A http://Platform.sh account A http://Platform.sh project http://Platform.sh CLI installed locally A local copy of that project Problems http://Platform.sh gives you the ability to quickly create backups of the state of your environments, and then easily restore those backups to those environments should you need to. This is not yet an automatic process, and both creation and restoration must be executed manually by the user. But, of course, people forget to take frequent backups - they’re busy developing. It’s for this reason that we often recommend users https://community.platform.sh/t/how-to-set-up-automated-environment-tasks/127 in order to automate this process. If you have not yet set up automatic backups as described in the above How-to, you may very well find yourself in the following situation. I’ve created a backup close to the start of an environment’s history. I’ve made a lot of changes to that environment (added services, added data to those services, created new mounts, etc.). Something went wrong. Well, look, I have a backup right here. But it’s old. What will happen? This tutorial is meant to explore this exact scenario, with the intended takeaways: the importance of setting up automatic backups on your projects using the How-to above as a guide (so you don’t find yourself in this situation in the first place). some greater understanding of how http://Platform.sh handles your data during backups, restores, and syncs. Steps Creating backups note This tutorial uses a modified version of our https://github.com/platformsh/language-examples project. That project is intended to show how to interact with all of our managed services with each of our runtimes. It’s a great resource, and it even powers most of the examples in our public documentation, but it’s gigantic, and much more than we need for this tutorial. We removed all of the runtimes except for Python, leaving us with only the main and python application container directories. We also removed all of the services save postgresql, but we will be adding a few of them back in the steps below. It is not necessary that you use the same repository if going through this tutorial step by step, just try to match the steps in your own project. 1. Create a new branch Assuming you have already created a project and initialized it with some code, create a new environment platform branch add-mongo master 2. Create a backup Then create a backup of that environment in its current state. platform backup:create Creating a backup of add-mongo Waiting for the activity is5ytp2lbm7p4 (Chad Carlson created a backup of add-mongo): Creating snapshot of add-mongo Created backup bivzemjpeqvohqey2t7fo7vs5m [============================] 15 secs (complete) Activity is5ytp2lbm7p4 succeeded Backup name: bivzemjpeqvohqey2t7fo7vs5m So what happened here? Each service, which includes applications, has its own persistent storage. During a backup, a copy is made for each of them. It’s the collection of these backups that then makes up backup bivzemjpeqvohqey2t7fo7vs5m. Adding a new service 1. Configure a new service First, let’s add a new service to the project, one that did not exist when the backup was created. Modify your services.yaml file to include MongoDB: dbmongo: type: mongodb:3.6 disk: 1024 size: S and your .platform.app.yaml to include the new relationship: relationships: mongodb: 'dbmongo:mongodb' Then commit and push the changes to http://Platform.sh. git add . git commit -m \"Adds mongodb.\" git push platform add-mongo http://Platform.sh will provision MongoDB to your virtual cluster, and expose the following credentials in your PLATFORM_RELATIONSHIPS environment variable: { \"username\": \"main\", \"scheme\": \"mongodb\", \"service\": \"mongodb\", \"ip\": \"169.254.117.167\", \"hostname\": \"ldh423mk2e7o6qto2syljqbg5u.mongodb.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"mongodb.internal\", \"rel\": \"mongodb\", \"path\": \"main\", \"query\": { \"is_master\": true }, \"password\": \"main\", \"type\": \"mongodb:3.6\", \"port\": 27017 } 2. Develop and verify At this point, MongoDB is available for you to develop with. Over the next few days, you may make the following changes as you are developing on the environment: you create a new collection in main called starwars. your application adds a number of documents to that collection. (In the case of our language examples project, the Python app adds a document with the contents {\"name\": \"Rey\", \"occupation\": \"Jedi\"} as a test each time the site is visited) You can verify those documents have been added locally by opening an SSH tunnel to the service (platform:tunnel single -r mongodb) and then connecting to MongoDB via that tunnel and the credentials above: $ mongo --port 30000 -u main -p main --authenticationDatabase main MongoDB shell version v4.0.3 use main switched to db main show collections starwars db.starwars.find() { \"_id\" : ObjectId(\"5e4457d05908440effc53a20\"), \"name\" : \"Rey\", \"occupation\" : \"Jedi\" } { \"_id\" : ObjectId(\"5e4457d35908440effc53a22\"), \"name\" : \"Rey\", \"occupation\" : \"Jedi\" } { \"_id\" : ObjectId(\"5e4457d75908440effc53a24\"), \"name\" : \"Rey\", \"occupation\" : \"Jedi\" } { \"_id\" : ObjectId(\"5e4457d95908440effc53a26\"), \"name\" : \"Rey\", \"occupation\" : \"Jedi\" } { \"_id\" : ObjectId(\"5e44585c5908440effc53a28\"), \"name\" : \"Rey\", \"occupation\" : \"Jedi\" } There’s all the newly added data. 3. Restore the backup Now, if we restore the backup we created at the beginning (before MongoDB was a part of the cluster) using the command platform backup:restore bivzemjpeqvohqey2t7fo7vs5m The restore will do a few things: All persistent data currently in that environment is wiped. Backup bivzemjpeqvohqey2t7fo7vs5m, which contains a backup of each service present when it was taken, is then applied to the containers present in the environment one by one. Because of that first point, and because no backup for MongoDB exists in backup bivzemjpeqvohqey2t7fo7vs5m, all data and code pertaining to MongoDB is erased before anything else happens. If you attempt to open a tunnel and locally connect to MongoDB straight away, the service won’t even be recognized as existing in the project. Run platform redeploy, re-open the tunnel to MongoDB, and then repeat the prior steps to connect to MongoDB: $ mongo --port 30000 -u main -p main --authenticationDatabase main MongoDB shell version v4.0.3 use main switched to db main show collections starwars As you can see, the added collection no longer exists on the service. This is our first example of why it’s important to create backups regularly. If one had been taken some time after MongoDB existed, we would still have been able to keep some of its data. Mounts 1. Configure a new mount Another case where this would be relevant is the addition of new mounts to the project. If you were to create a mount that was not included in backup bivzemjpeqvohqey2t7fo7vs5m, would the files in that mount be erased? You can probably guess already based on the previous example, but let’s find out. You can add a mount to an application by adding the following lines to your .platform.app.yaml file: mounts: 'add-mount': source: local source_path: add-mount Then commit and push to http://Platform.sh: git add . git commit -m \"Add a mount.\" git push platform add-mongo 2. Add data and verify Let’s just create a simple file mkdir mount-data \u0026\u0026 touch mount-data/test.txt echo \"Here's our mounted data on Platform.sh.\" mount-data/test.txt and then upload it to the newly defined mount: platform mount:upload --mount add-mount --source ./mount-data You can then verify that the file was uploaded to the project by runing platform ssh to SSH into the application container. Then run, web@app.0:~$ echo \"$( ",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/consequences-of-restoring-backups-after-the-persistent-data-model-has-been-changed/461",
        "relurl": "/t/consequences-of-restoring-backups-after-the-persistent-data-model-has-been-changed/461"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "879db006eecc051f69937eecbb477e89999bfe8c",
        "title": "Preventing stuck deployments caused by long-running cron jobs",
        "description": "The default way of deploying to production on http://Platform.sh is to simply push master to the platform remote. However, if a cron job is running, , and the deployment will not finish until the cron job is finished. When this happens, the application goes down. In order to prevent this from happening, you can use a custom deployment script that first makes sure that no Django management commands are running in the production environment. In the example below, we are checking if any Django management commands are running. If so, we abort the deployment. I have tested the script using Python 3.7.5, but it should work on 3.6.0 and later. If you’re not running Python, you can use it as inspiration for porting it to your language of choice. If you do, please consider sharing it with the community, for example as a reply to this post. As a caveat, note that if a cron job starts after the build has started, it might still block the deployment. The probability of this increases if your build takes a long time. #!/usr/bin/env python import sys from os import getenv from subprocess import CompletedProcess, run def git(command: str) - None: run(f\"git {command}\", shell=True) def main() - None: git(\"checkout master\") git(\"pull origin\") print(\"Getting running processes in order to prevent stuck deployment…\") completed_process: CompletedProcess = run( f\"ssh {getenv('PLATFORM_SH_SSH_DESTINATION')} 'ps auxf'\", capture_output=True, shell=True, ) output = str(completed_process.stdout) print(\"Making sure that process list was received…\") if \"/app/.global/bin/gunicorn\" not in output: sys.exit(\"Gunicorn process not found. Are you able to connect using SSH?\") print(\"Process list received. Checking for running Django management commands…\") if \"manage.py\" in output: sys.exit(\"Running Django management command detected. Not safe to continue.\") print( \"No running commands detected. Pushing changes to the production environment…\" ) git(\"push platform\") if __name__ == \"__main__\": main() To use the script: Create a Python file, e.g. deploy.py, in the root of your project and paste the script contents into it. Set the environment variable PLATFORM_SH_SSH_DESTINATION to the user@host destination of your production environment. Check if there are any modifications you need to make. For example, you may be running other kinds of jobs than just Django management commands, so you may need to add other strings in addition to manage.py Run ./deploy.py when you’re ready to deploy. Commit the script so you don’t lose it. ",
        "text": "The default way of deploying to production on http://Platform.sh is to simply push master to the platform remote. However, if a cron job is running, , and the deployment will not finish until the cron job is finished. When this happens, the application goes down. In order to prevent this from happening, you can use a custom deployment script that first makes sure that no Django management commands are running in the production environment. In the example below, we are checking if any Django management commands are running. If so, we abort the deployment. I have tested the script using Python 3.7.5, but it should work on 3.6.0 and later. If you’re not running Python, you can use it as inspiration for porting it to your language of choice. If you do, please consider sharing it with the community, for example as a reply to this post. As a caveat, note that if a cron job starts after the build has started, it might still block the deployment. The probability of this increases if your build takes a long time. #!/usr/bin/env python import sys from os import getenv from subprocess import CompletedProcess, run def git(command: str) - None: run(f\"git {command}\", shell=True) def main() - None: git(\"checkout master\") git(\"pull origin\") print(\"Getting running processes in order to prevent stuck deployment…\") completed_process: CompletedProcess = run( f\"ssh {getenv('PLATFORM_SH_SSH_DESTINATION')} 'ps auxf'\", capture_output=True, shell=True, ) output = str(completed_process.stdout) print(\"Making sure that process list was received…\") if \"/app/.global/bin/gunicorn\" not in output: sys.exit(\"Gunicorn process not found. Are you able to connect using SSH?\") print(\"Process list received. Checking for running Django management commands…\") if \"manage.py\" in output: sys.exit(\"Running Django management command detected. Not safe to continue.\") print( \"No running commands detected. Pushing changes to the production environment…\" ) git(\"push platform\") if __name__ == \"__main__\": main() To use the script: Create a Python file, e.g. deploy.py, in the root of your project and paste the script contents into it. Set the environment variable PLATFORM_SH_SSH_DESTINATION to the user@host destination of your production environment. Check if there are any modifications you need to make. For example, you may be running other kinds of jobs than just Django management commands, so you may need to add other strings in addition to manage.py Run ./deploy.py when you’re ready to deploy. Commit the script so you don’t lose it. ",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/preventing-stuck-deployments-caused-by-long-running-cron-jobs/434",
        "relurl": "/t/preventing-stuck-deployments-caused-by-long-running-cron-jobs/434"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "cc40a421ad7b01871f84ddedf39d7787fd1eac04",
        "title": "Compiling a Drupal site with Tome",
        "description": "Goal To demonstrate how a “compile to static” tool, such as the https://www.drupal.org/project/tome , can be used on http://Platform.sh using multi-application configurations. Preparation This tutorial will start from a basic Drupal 8 Composer site. However, it will be easier to start from an empty Git repository. It can be done from a Drupal 8 template site but that requires moving a number of hidden files and is therefore more prone to error. You will need: A newly created empty site on http://Platform.sh. Git and SSH installed locally. Problems How can you serve two closely related sites off of the same project? How can static site generation work in production, rather than compiling locally or during build? Steps 1. Create an empty working directory On your local system, create an empty directory for your project and initialize it for Git. Eg: mkdir project cd project git init 2. Download Drupal 8 Clone the Drupal 8 http://Platform.sh template into your project, then remove the git repository from it. git clone https://github.com/platformsh-templates/drupal8.git drupal --depth=1 rm -rf drupal/.git That Git repository is what the Drupal 8 template in the new project wizard populates from. The above commands are essentially what the new project wizard does, although it does not use a subdirectory. 2. Move the .platform definition files to the repo root The .platform directory must be in the repository root, so move it there: mv drupal/.platform . The .platform.app.yaml file stays in the drupal directory so that the drupal directory becomes an application container itself. That will let you create another directory for the static site. 3. Add a network-storage service Open .platform/services.yaml in your favorite text editor and add the following: files: type: network-storage:1.0 disk: 512 That creates a new network-storage service named files. Files there can be shared between the two application containers. 4. Update Drupal mounts Open drupal/.platform.app.yaml in your favorite text editor and update the mounts section to be as follows: mounts: '/web/sites/default/files': source: local source_path: files '/tmp': source: local source_path: tmp '/private': source: local source_path: private '/.drush': source: local source_path: drush '/drush-backups': source: local source_path: drush-backups '/.console': source: local source_path: console '/content': source: local source_path: content '/config': source: local source_path: config '/static': source: service service: files source_path: static Most of those are simply converting the normal Drupal mounts to the new-style mount syntax. The most important are the final three paths. Tome will automatically run a config-export every time configuration is exported, so the /config directory must be writeable. It will also write out a JSON version of all content as it is created and edited to /content. The /static directory is where you’ll instruct Tome to write out the static version of the site. Of particular note, the /static mount uses the network storage service, not the local file system. That will make it available to other application containers. Note: Because these directories are writeable, they will not be tracked in Git. That includes configuration exports, which will not be deployable via Git in this setup. 5. Install Drupal Tome module Run: cd drupal composer require drupal/tome to add the Tome module. (It’s actually a suite of modules). You’ll enable it later once the code is deployed. Once that’s done return to the repository root directory: cd .. 6. Set Tome export location Modify the drupal/web/sites/default/settings.php file. Add the following line at the location of your choice: $settings['tome_static_directory'] = '../static/html'; That will set the Tome export directory to the /static/html directory, which is inside the writeable mount you created above. Because Tome completely removes the output directory before each rebuild it needs to be in a subdirectory of the mount. 7. Disable allowed-host checking In the drupal/web/sites/default/settings.platformsh.php file, look for a comment line that reads // Set trusted hosts based on Platform.sh routes. The code block below that configures the hosts that Drupal is allowed to requests for. Normally it is a security feature, however, on http://Platform.sh Professional the domain is already mangled by the router and will always be a “safe” domain. That makes this code block unnecessary. (It is included in the template only for http://Platform.sh Enterprise.) The Tome module’s export functionality, however, is incompatible with this code block as it will need to use the un-prefixed domain for the export. It is safe to simply remove/comment out this entire block. Alternatively, just comment out the last line that sets $settings['trusted_host_patterns']. 8. Add a static site application to serve files Create a new directory called static, and inside it create a .platform.app.yaml file. Give it the following contents: # Note that this is different than what the Drupal site is called. name: 'static' # The type here doesn't really matter since it's just a static site. type: 'php:7.3' # This must be set, but can be set at the minimum value. disk: 16 mounts: 'static': source: service service: files source_path: static web: locations: '/': root: 'static/html' index: ['index.html'] scripts: false allow: true That file is all that is needed to create a second application container. This container will be called static, and will mount the same network storage directory as the app container that holds the Drupal site. It will be empty aside from that mount, which is fine as the mount is where the files will be served from. Note that the web root is the static/html directory, not static itself, as Tome was configured above to write files to static/html. 9. Update routes Open .platform/routes.yaml in your editor. Update the route definitions to be as follows: \"https://drupal.{default}/\": type: upstream upstream: \"app:http\" cache: enabled: true # Base the cache on the session cookie and custom Drupal cookies. Ignore all other cookies. cookies: ['/^SS?ESS/', '/^Drupal.visitor/'] \"https://{default}/\": type: upstream upstream: \"static:http\" (You can also include a www redirect route if you want, but that is unnecessary.) This configuration creates two routes: The drupal. subdomain will be where the Drupal site lives, and it will run as any other Drupal site. The main domain (and/or www. subdomain if desired) will be served by the static application. Once the configuration is complete and you are happy with the result, you will most likely want to come back and add HTTP caching to the static route, including a default_ttl. 10. Commit and push Commit all of the files you just created to Git: git add . git commit -m \"Setting up Tome\" Now add a remote for your empty http://Platform.sh project. The Git URL can be found in your Management Console in the browser. The command will be something along the lines of: git remote add platform bzh2mp6iabike@git.eu-3.platform.sh:bzh2mp6iabike.git And then push all of the code to the platform remote: git push -u platform master If everything is setup correctly it will push and deploy a two-application cluster, containing the Drupal container, static container, and the MariaDB and Redis services used by Drupal itself. 11. Install Drupal and Tome Once deployment is finished it will show you the domain names that are served. Go to the drupal. domain in your browser. Complete the Drupal installation process as normal. Once Drupal is installed, select “Extend” in the menu and enable the “Tome” module. That will also enable several sub-modules. You don’t need to create any content at this point, but if you do it will make the demonstration of Tome’s functionality more interesting. 12. Export the site Go to /admin/config/tome/static/generate in your Drupal site. The generation form will ask you for the domain to generate for; give it the unprefixed domain (which could be a dev domain or your production domain, depending on whether or not you’ve gone live yet). Click “Submit”. Tome will generate a static version of your site as it is seen by an anonymous user. (Which means you must allow anonymous user to see the site.) The process takes anywhere from a few seconds to a few minutes depending on how much content you have. As the site was only just created it should be quite fast. Now go to the unprefixed version of your site (the domain without drupal.) in your favorite browser. You should see the anonymous version of your site, now served entirely as static files. Conclusion In this tutorial, you have seen how to: Create a multi-application project on http://Platform.sh. Use network-storage to share content between sites. Configure an application to serve static files. All of which can be applied to other configurations if needed. Note that in the case of Tome the generation process does delete files first, so it is recommended to configure caching at the router level for the static container to minimize any disruption to the site as it is being regenerated.",
        "text": "Goal To demonstrate how a “compile to static” tool, such as the https://www.drupal.org/project/tome , can be used on http://Platform.sh using multi-application configurations. Preparation This tutorial will start from a basic Drupal 8 Composer site. However, it will be easier to start from an empty Git repository. It can be done from a Drupal 8 template site but that requires moving a number of hidden files and is therefore more prone to error. You will need: A newly created empty site on http://Platform.sh. Git and SSH installed locally. Problems How can you serve two closely related sites off of the same project? How can static site generation work in production, rather than compiling locally or during build? Steps 1. Create an empty working directory On your local system, create an empty directory for your project and initialize it for Git. Eg: mkdir project cd project git init 2. Download Drupal 8 Clone the Drupal 8 http://Platform.sh template into your project, then remove the git repository from it. git clone https://github.com/platformsh-templates/drupal8.git drupal --depth=1 rm -rf drupal/.git That Git repository is what the Drupal 8 template in the new project wizard populates from. The above commands are essentially what the new project wizard does, although it does not use a subdirectory. 2. Move the .platform definition files to the repo root The .platform directory must be in the repository root, so move it there: mv drupal/.platform . The .platform.app.yaml file stays in the drupal directory so that the drupal directory becomes an application container itself. That will let you create another directory for the static site. 3. Add a network-storage service Open .platform/services.yaml in your favorite text editor and add the following: files: type: network-storage:1.0 disk: 512 That creates a new network-storage service named files. Files there can be shared between the two application containers. 4. Update Drupal mounts Open drupal/.platform.app.yaml in your favorite text editor and update the mounts section to be as follows: mounts: '/web/sites/default/files': source: local source_path: files '/tmp': source: local source_path: tmp '/private': source: local source_path: private '/.drush': source: local source_path: drush '/drush-backups': source: local source_path: drush-backups '/.console': source: local source_path: console '/content': source: local source_path: content '/config': source: local source_path: config '/static': source: service service: files source_path: static Most of those are simply converting the normal Drupal mounts to the new-style mount syntax. The most important are the final three paths. Tome will automatically run a config-export every time configuration is exported, so the /config directory must be writeable. It will also write out a JSON version of all content as it is created and edited to /content. The /static directory is where you’ll instruct Tome to write out the static version of the site. Of particular note, the /static mount uses the network storage service, not the local file system. That will make it available to other application containers. Note: Because these directories are writeable, they will not be tracked in Git. That includes configuration exports, which will not be deployable via Git in this setup. 5. Install Drupal Tome module Run: cd drupal composer require drupal/tome to add the Tome module. (It’s actually a suite of modules). You’ll enable it later once the code is deployed. Once that’s done return to the repository root directory: cd .. 6. Set Tome export location Modify the drupal/web/sites/default/settings.php file. Add the following line at the location of your choice: $settings['tome_static_directory'] = '../static/html'; That will set the Tome export directory to the /static/html directory, which is inside the writeable mount you created above. Because Tome completely removes the output directory before each rebuild it needs to be in a subdirectory of the mount. 7. Disable allowed-host checking In the drupal/web/sites/default/settings.platformsh.php file, look for a comment line that reads // Set trusted hosts based on Platform.sh routes. The code block below that configures the hosts that Drupal is allowed to requests for. Normally it is a security feature, however, on http://Platform.sh Professional the domain is already mangled by the router and will always be a “safe” domain. That makes this code block unnecessary. (It is included in the template only for http://Platform.sh Enterprise.) The Tome module’s export functionality, however, is incompatible with this code block as it will need to use the un-prefixed domain for the export. It is safe to simply remove/comment out this entire block. Alternatively, just comment out the last line that sets $settings['trusted_host_patterns']. 8. Add a static site application to serve files Create a new directory called static, and inside it create a .platform.app.yaml file. Give it the following contents: # Note that this is different than what the Drupal site is called. name: 'static' # The type here doesn't really matter since it's just a static site. type: 'php:7.3' # This must be set, but can be set at the minimum value. disk: 16 mounts: 'static': source: service service: files source_path: static web: locations: '/': root: 'static/html' index: ['index.html'] scripts: false allow: true That file is all that is needed to create a second application container. This container will be called static, and will mount the same network storage directory as the app container that holds the Drupal site. It will be empty aside from that mount, which is fine as the mount is where the files will be served from. Note that the web root is the static/html directory, not static itself, as Tome was configured above to write files to static/html. 9. Update routes Open .platform/routes.yaml in your editor. Update the route definitions to be as follows: \"https://drupal.{default}/\": type: upstream upstream: \"app:http\" cache: enabled: true # Base the cache on the session cookie and custom Drupal cookies. Ignore all other cookies. cookies: ['/^SS?ESS/', '/^Drupal.visitor/'] \"https://{default}/\": type: upstream upstream: \"static:http\" (You can also include a www redirect route if you want, but that is unnecessary.) This configuration creates two routes: The drupal. subdomain will be where the Drupal site lives, and it will run as any other Drupal site. The main domain (and/or www. subdomain if desired) will be served by the static application. Once the configuration is complete and you are happy with the result, you will most likely want to come back and add HTTP caching to the static route, including a default_ttl. 10. Commit and push Commit all of the files you just created to Git: git add . git commit -m \"Setting up Tome\" Now add a remote for your empty http://Platform.sh project. The Git URL can be found in your Management Console in the browser. The command will be something along the lines of: git remote add platform bzh2mp6iabike@git.eu-3.platform.sh:bzh2mp6iabike.git And then push all of the code to the platform remote: git push -u platform master If everything is setup correctly it will push and deploy a two-application cluster, containing the Drupal container, static container, and the MariaDB and Redis services used by Drupal itself. 11. Install Drupal and Tome Once deployment is finished it will show you the domain names that are served. Go to the drupal. domain in your browser. Complete the Drupal installation process as normal. Once Drupal is installed, select “Extend” in the menu and enable the “Tome” module. That will also enable several sub-modules. You don’t need to create any content at this point, but if you do it will make the demonstration of Tome’s functionality more interesting. 12. Export the site Go to /admin/config/tome/static/generate in your Drupal site. The generation form will ask you for the domain to generate for; give it the unprefixed domain (which could be a dev domain or your production domain, depending on whether or not you’ve gone live yet). Click “Submit”. Tome will generate a static version of your site as it is seen by an anonymous user. (Which means you must allow anonymous user to see the site.) The process takes anywhere from a few seconds to a few minutes depending on how much content you have. As the site was only just created it should be quite fast. Now go to the unprefixed version of your site (the domain without drupal.) in your favorite browser. You should see the anonymous version of your site, now served entirely as static files. Conclusion In this tutorial, you have seen how to: Create a multi-application project on http://Platform.sh. Use network-storage to share content between sites. Configure an application to serve static files. All of which can be applied to other configurations if needed. Note that in the case of Tome the generation process does delete files first, so it is recommended to configure caching at the router level for the static container to minimize any disruption to the site as it is being regenerated.",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/compiling-a-drupal-site-with-tome/363",
        "relurl": "/t/compiling-a-drupal-site-with-tome/363"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "69e493ce38a4fc00fe39aff1404bba2c741cc215",
        "title": "Installing Drupal and Mautic together in one project",
        "description": "Goal To set up a multi-app site with both Drupal 8 and Mautic. The process is also very similar for any other site that combines two templates. Preparation You will need: A new project with no code. You will need at least a Medium plan to go live, but you can start with a Dev plan for setup. You will need a Medium plan and a domain configured before the Drupal Mautic module can be configured, however. A local working Git client and Composer. Problems Drupal needs a Mautic site URL in order to connect to it. It will do so over HTTP, not through a backend connection. That means the domain name will be different on every branch. Generally you would only care about Mautic tracking on production anyway, so that is not a major issue. Steps 1. Download the Drupal 8 template The http://Platform.sh Drupal 8 template is available on https://github.com/platformsh-templates/drupal8. Clone it to your local computer, then remove the .git directory to reset the repository. # Download Drupal 8 git clone https://github.com/platformsh-templates/drupal8.git d8-mautic # Remove the git repository itself. cd d8-mautic rm -rf .git Resetting the repository is technically optional, but the existing history is useless for you and as you will be moving a lot of files around it’s easier to not deal with Git at this point. 2. Move Drupal to a subdirectory Make a new directory named drupal and move all of the existing files into it, except for the .platform subdirectory. Make sure to include the various other dot files. mkdir drupal mv * drupal mv .platform.app.yaml .env* .editorconfig .gitattributes .gitignore drupal 3. Add Mautic Download the https://github.com/platformsh-templates/mautic into the project directory and remove its .git repository as well. # Download Mautic $ git clone https://github.com/platformsh-templates/mautic.git # Remove the git repository itself. $ cd mautic $ rm -rf .git 4. Rename applications The two applications will need unique names, such as drupal and mautic. (You may use other names if you wish.) Change the name field in each application’s .platform.app.yaml file: # in drupal/.platform.app.yaml name: drupal # in mautic/.platform.app.yaml name: mautic Note: If you have an existing Drupal site you are modifying to include Mautic, do not rename that application. Doing so will result in data loss. 5. Update routes.yaml Update the .platform/routes.yaml file to supply different routes for each application. The exact configuration will vary depending on your preferred domains. For example, the following configuration will serve Drupal from www.YOURSITE.com and Mautic from mautic.YOURSITE.com: \"https://www.{default}/\": type: upstream upstream: \"drupal:http\" cache: enabled: true cookies: ['/^SS?ESS/', '/^Drupal.visitor/'] \"https://{default}/\": type: redirect to: \"https://www.{default}/\" \"https://mautic.{default}/\": type: upstream upstream: \"mautic:http\" cache: enabled: true 6. Merge the services.yaml files At this point, Drupal’s original services.yaml is in .platform/services.yaml and Mautic’s is in mautic/.platform/services.yaml. The second will not be used so its content should be merged into the main one, and then it can be removed. By default, Mautic and Drupal both use MariaDB; Drupal also uses Redis, and Mautic also uses RabbitMQ. If you wish to add additional services you may do so. The same MariaDB service can hold the database for both applications. A typical configuration for running both applications would looks like this: db: type: mariadb:10.4 disk: 2048 configuration: schemas: - drupal - mautic endpoints: drupal: default_schema: drupal privileges: drupal: admin mautic: default_schema: mautic privileges: mautic: admin cache: type: redis:5.0 queuerabbit: type: rabbitmq:3.7 disk: 256 That includes the cache service from Drupal, the queuerabbit service from Mautic, and a single MariaDB 10.4 server to serve both databases. It defines two databases, drupal and mautic, and then creates two separate endpoints of the same name that have full access to their respective databases only. Note: If you are adding Mautic to an existing Drupal site, you must name the Drupal database main and the Drupal endpoint mysql. Doing otherwise will result in data loss. Once that is done, remove the now-unused Mautic .platform directory. rm -rf mautic/.platform 7. Update each application’s relationships definition. In drupal/.platform.app.yaml, change the database relationship to point to the drupal endpoint: # in drupal/.platform.app.yaml relationships: database: 'db:drupal' ## Uncomment this line to enable Redis caching for Drupal. # redis: 'cache:redis' # in mautic/.platform.app.yaml relationships: database: \"db:mautic\" rabbitmqqueue: \"queuerabbit:rabbitmq\" 8. Reduce disk usage OR increase plan size The default templates for Drupal and Mautic, when combined, will ask for a total of about 6 GB of storage. By default plans on http://Platform.sh start with 5 GB. You may either increase the disk usage of your plan to a value higher than 6 GB, OR you can reduce the disk space requested by each application container. For the latter, change the disk key in both .platform.app.yaml files to 1024: disk: 1024 That will give each application 1 GB of disk space. Both applications will share the disk space used for the database (2 GB in the example above). 9. Add the Drupal Mautic module to the project Install the drupal/mautic module in the Drupal instance, using Composer: cd drupal composer require drupal/mautic Because of the way Composer works that will require downloading all dependencies, even though they will not be needed. That’s fine. When it is done go back to the project directory: cd .. 10. Commit and deploy Initialize a new Git repository and commit all of the files you’ve created to it. git init git add . git commit -m \"Add Drupal and Mautic.\" Next, set a Git remote for the empty http://Platform.sh project you have waiting for it, using the http://Platform.sh CLI. You can find the exact command to copy and paste in the project’s setup wizard. Then push your code to the new remote. platform project:set-remote YOUR_PROJECT_ID_HERE git push -u platform master The codebase will push to http://Platform.sh, and both applications will be built and deployed. 11. Install both applications Once the deploy is complete, run through the web installer for both applications. The environment URL for each one will be visible in the CLI output as well as in the web console. Consult the README.md file for each application for steps that should be taken post-install. They are not required for completing the integration but doing so will lead to a better experience for both applications. You may also do any additional configuration desired for both applications either now or afterward. 12. Enable and configure the Drupal module Once logged into Drupal as the site administrator, go to /admin/modules and enable the Mautic module. Once the module is enabled, go to the module’s configuration page at /admin/config/system/mautic. Check “Include Mautic Javascript Code”. For the Mautic URL, enter the domain name of your Mautic site followed by /mtc.js. If you are on a development plan, it will be something similar to: https://mautic.master-t6dnbai-aqatptktdkxmi.us-2.platformsh.site/mtc.js If you already have a domain name configured, it will be whatever the domain name is that Mautic is served from instead. Click “save configuration”. The setup is now complete. Browse to the Drupal home page and inspect the source code to locate the Mautic tracking Javascript code. Conclusion In this tutorial you have learned how to create a multi-application project on http://Platform.sh based on two separate application templates. You’ve also seen how to add a module to Drupal and configure it with Mautic. The same basic process applies to any other two-template project, although the specific names and services will of course vary depending on the applications. Example An example repository for this tutorial can be found https://github.com/platformsh-examples/d8-mautic . Note that it is not maintained and may include out of date versions by the time you read this.",
        "text": "Goal To set up a multi-app site with both Drupal 8 and Mautic. The process is also very similar for any other site that combines two templates. Preparation You will need: A new project with no code. You will need at least a Medium plan to go live, but you can start with a Dev plan for setup. You will need a Medium plan and a domain configured before the Drupal Mautic module can be configured, however. A local working Git client and Composer. Problems Drupal needs a Mautic site URL in order to connect to it. It will do so over HTTP, not through a backend connection. That means the domain name will be different on every branch. Generally you would only care about Mautic tracking on production anyway, so that is not a major issue. Steps 1. Download the Drupal 8 template The http://Platform.sh Drupal 8 template is available on https://github.com/platformsh-templates/drupal8. Clone it to your local computer, then remove the .git directory to reset the repository. # Download Drupal 8 git clone https://github.com/platformsh-templates/drupal8.git d8-mautic # Remove the git repository itself. cd d8-mautic rm -rf .git Resetting the repository is technically optional, but the existing history is useless for you and as you will be moving a lot of files around it’s easier to not deal with Git at this point. 2. Move Drupal to a subdirectory Make a new directory named drupal and move all of the existing files into it, except for the .platform subdirectory. Make sure to include the various other dot files. mkdir drupal mv * drupal mv .platform.app.yaml .env* .editorconfig .gitattributes .gitignore drupal 3. Add Mautic Download the https://github.com/platformsh-templates/mautic into the project directory and remove its .git repository as well. # Download Mautic $ git clone https://github.com/platformsh-templates/mautic.git # Remove the git repository itself. $ cd mautic $ rm -rf .git 4. Rename applications The two applications will need unique names, such as drupal and mautic. (You may use other names if you wish.) Change the name field in each application’s .platform.app.yaml file: # in drupal/.platform.app.yaml name: drupal # in mautic/.platform.app.yaml name: mautic Note: If you have an existing Drupal site you are modifying to include Mautic, do not rename that application. Doing so will result in data loss. 5. Update routes.yaml Update the .platform/routes.yaml file to supply different routes for each application. The exact configuration will vary depending on your preferred domains. For example, the following configuration will serve Drupal from www.YOURSITE.com and Mautic from mautic.YOURSITE.com: \"https://www.{default}/\": type: upstream upstream: \"drupal:http\" cache: enabled: true cookies: ['/^SS?ESS/', '/^Drupal.visitor/'] \"https://{default}/\": type: redirect to: \"https://www.{default}/\" \"https://mautic.{default}/\": type: upstream upstream: \"mautic:http\" cache: enabled: true 6. Merge the services.yaml files At this point, Drupal’s original services.yaml is in .platform/services.yaml and Mautic’s is in mautic/.platform/services.yaml. The second will not be used so its content should be merged into the main one, and then it can be removed. By default, Mautic and Drupal both use MariaDB; Drupal also uses Redis, and Mautic also uses RabbitMQ. If you wish to add additional services you may do so. The same MariaDB service can hold the database for both applications. A typical configuration for running both applications would looks like this: db: type: mariadb:10.4 disk: 2048 configuration: schemas: - drupal - mautic endpoints: drupal: default_schema: drupal privileges: drupal: admin mautic: default_schema: mautic privileges: mautic: admin cache: type: redis:5.0 queuerabbit: type: rabbitmq:3.7 disk: 256 That includes the cache service from Drupal, the queuerabbit service from Mautic, and a single MariaDB 10.4 server to serve both databases. It defines two databases, drupal and mautic, and then creates two separate endpoints of the same name that have full access to their respective databases only. Note: If you are adding Mautic to an existing Drupal site, you must name the Drupal database main and the Drupal endpoint mysql. Doing otherwise will result in data loss. Once that is done, remove the now-unused Mautic .platform directory. rm -rf mautic/.platform 7. Update each application’s relationships definition. In drupal/.platform.app.yaml, change the database relationship to point to the drupal endpoint: # in drupal/.platform.app.yaml relationships: database: 'db:drupal' ## Uncomment this line to enable Redis caching for Drupal. # redis: 'cache:redis' # in mautic/.platform.app.yaml relationships: database: \"db:mautic\" rabbitmqqueue: \"queuerabbit:rabbitmq\" 8. Reduce disk usage OR increase plan size The default templates for Drupal and Mautic, when combined, will ask for a total of about 6 GB of storage. By default plans on http://Platform.sh start with 5 GB. You may either increase the disk usage of your plan to a value higher than 6 GB, OR you can reduce the disk space requested by each application container. For the latter, change the disk key in both .platform.app.yaml files to 1024: disk: 1024 That will give each application 1 GB of disk space. Both applications will share the disk space used for the database (2 GB in the example above). 9. Add the Drupal Mautic module to the project Install the drupal/mautic module in the Drupal instance, using Composer: cd drupal composer require drupal/mautic Because of the way Composer works that will require downloading all dependencies, even though they will not be needed. That’s fine. When it is done go back to the project directory: cd .. 10. Commit and deploy Initialize a new Git repository and commit all of the files you’ve created to it. git init git add . git commit -m \"Add Drupal and Mautic.\" Next, set a Git remote for the empty http://Platform.sh project you have waiting for it, using the http://Platform.sh CLI. You can find the exact command to copy and paste in the project’s setup wizard. Then push your code to the new remote. platform project:set-remote YOUR_PROJECT_ID_HERE git push -u platform master The codebase will push to http://Platform.sh, and both applications will be built and deployed. 11. Install both applications Once the deploy is complete, run through the web installer for both applications. The environment URL for each one will be visible in the CLI output as well as in the web console. Consult the README.md file for each application for steps that should be taken post-install. They are not required for completing the integration but doing so will lead to a better experience for both applications. You may also do any additional configuration desired for both applications either now or afterward. 12. Enable and configure the Drupal module Once logged into Drupal as the site administrator, go to /admin/modules and enable the Mautic module. Once the module is enabled, go to the module’s configuration page at /admin/config/system/mautic. Check “Include Mautic Javascript Code”. For the Mautic URL, enter the domain name of your Mautic site followed by /mtc.js. If you are on a development plan, it will be something similar to: https://mautic.master-t6dnbai-aqatptktdkxmi.us-2.platformsh.site/mtc.js If you already have a domain name configured, it will be whatever the domain name is that Mautic is served from instead. Click “save configuration”. The setup is now complete. Browse to the Drupal home page and inspect the source code to locate the Mautic tracking Javascript code. Conclusion In this tutorial you have learned how to create a multi-application project on http://Platform.sh based on two separate application templates. You’ve also seen how to add a module to Drupal and configure it with Mautic. The same basic process applies to any other two-template project, although the specific names and services will of course vary depending on the applications. Example An example repository for this tutorial can be found https://github.com/platformsh-examples/d8-mautic . Note that it is not maintained and may include out of date versions by the time you read this.",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/installing-drupal-and-mautic-together-in-one-project/412",
        "relurl": "/t/installing-drupal-and-mautic-together-in-one-project/412"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "81ba15cf84e22e2354d0ef5a8272e09e6d43c5fb",
        "title": "About the Activity scripts category",
        "description": "A place to share and get feedback on Activity scripts for http://Platform.sh. Activity scripts let you customize the http://Platform.sh workflow by responding to “activities,” events within the project life-cycle. See the https://docs.platform.sh/administration/integrations/activity-scripts.html for more details.",
        "text": "A place to share and get feedback on Activity scripts for http://Platform.sh. Activity scripts let you customize the http://Platform.sh workflow by responding to “activities,” events within the project life-cycle. See the https://docs.platform.sh/administration/integrations/activity-scripts.html for more details.",
        "section": "Activity scripts",
        "subsections": "Activity scripts",
        "image": "",
        "url": "https://community.platform.sh//t/about-the-activity-scripts-category/561",
        "relurl": "/t/about-the-activity-scripts-category/561"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "283b44e7e222d1c43318e7e265c3c7e29f2b2104",
        "title": "Script that creates a Github Status providing the URL of a new deployed environnement",
        "description": "Hello, Please find the script there to follow its update! https://github.com/Plopix/platformsh-activity-scripts/blob/master/github/env_url.js https://github.com/Plopix/platformsh-activity-scripts/blob/master/github/env_url.js \u0026\u0026 \u0026\u0026 https://github.com/Plopix/platformsh-activity-scripts/blob/master/github/env_url.js Don’t hesitate to give me your feedback. Thanks",
        "text": "Hello, Please find the script there to follow its update! https://github.com/Plopix/platformsh-activity-scripts/blob/master/github/env_url.js https://github.com/Plopix/platformsh-activity-scripts/blob/master/github/env_url.js \u0026\u0026 \u0026\u0026 https://github.com/Plopix/platformsh-activity-scripts/blob/master/github/env_url.js Don’t hesitate to give me your feedback. Thanks",
        "section": "Activity scripts",
        "subsections": "Activity scripts",
        "image": "",
        "url": "https://community.platform.sh//t/script-that-creates-a-github-status-providing-the-url-of-a-new-deployed-environnement/577",
        "relurl": "/t/script-that-creates-a-github-status-providing-the-url-of-a-new-deployed-environnement/577"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "aa6d18602b239e1a1b3833d9eaf18c0b02281eae",
        "title": "Script to check for deprecated or newer available runtimes and services versions",
        "description": "Overview Every time you redeploy an environment, the script will: check the versions of your project’s services and runtimes. compare those versions with the https://docs.platform.sh/registry/images/registry.json . send a Slack notification about the ones which you need to upgrade. https://community.platform.sh/uploads/default/a9bfce0a3251ab3f968bd419c322cdc9948e12c4 Colors Red: 1 or more service is deprecated. Orange: 1 or more service is not at the latest version. Green: All services are at the latest version. Script /** * Returns a key/value object containing all variables relevant for the activity. * * That includes project level variables, plus any variables visible for * the relevant environment for the activity, if any. * * Note that JSON-encoded values will show up as a string, and need to be * decoded with JSON.parse(). */ function variables() { var vars = {}; activity.payload.deployment.variables.forEach(function(variable) { vars[variable.name] = variable.value; }); return vars; } /** * Sends a color-coded formatted message to Slack. * * You must first configure a Platform.sh variable named \"SLACK_URL\". * That is the group and channel to which the message will be sent. * * To control what events it will run on, use the --events switch in * the Platform.sh CLI. * * @param {string} title * The title of the message block to send. * @param {string} message * The message body to send. */ function sendSlackMessage(title, message) { if ((new Date).getDay() === 5) { message += '\\r\\nCongratulations for deploying on a Friday! :calendar:'; } var body = { 'attachments': [{ 'title': title, 'text': message, 'color': color, }], }; var url = variables()['SLACK_URL']; if (!url) { throw new Error('You must define a SLACK_URL project variable.'); } var resp = fetch(url,{ method: 'POST', headers: { 'Content-Type': 'application/json', }, body: JSON.stringify(body), }); if (!resp.ok) { console.log('[LOG] Sending slack message failed: ' + resp.body.text()); } } /** * Compare the services and runtimes versions with the Platform.sh public registry. * * @param {json} services * The services/runtimes that your project is using within the activity.payload.deployment payload. * @param {json} registry * The Platform.sh registry available at: https://docs.platform.sh/registry/images/registry.json */ function compareVersions(services, registry) { var results = Object.keys(services).map(function(serviceName) { var service = services[serviceName]; var s = service.type.split(':'); var type = s[0]; var version = s[1]; var registryService = registry[type]; var name = registryService.name; if(!registryService) { return 'Unsupported '+ type; } var versions = registryService.versions; // Check for supported versions var indexOfSupported = versions.supported.indexOf(version); if(indexOfSupported !== -1) { var resp = 'Your ' + name + ' ' + version + ' is the most recent one.\\n'; if(indexOfSupported ",
        "text": "Overview Every time you redeploy an environment, the script will: check the versions of your project’s services and runtimes. compare those versions with the https://docs.platform.sh/registry/images/registry.json . send a Slack notification about the ones which you need to upgrade. https://community.platform.sh/uploads/default/a9bfce0a3251ab3f968bd419c322cdc9948e12c4 Colors Red: 1 or more service is deprecated. Orange: 1 or more service is not at the latest version. Green: All services are at the latest version. Script /** * Returns a key/value object containing all variables relevant for the activity. * * That includes project level variables, plus any variables visible for * the relevant environment for the activity, if any. * * Note that JSON-encoded values will show up as a string, and need to be * decoded with JSON.parse(). */ function variables() { var vars = {}; activity.payload.deployment.variables.forEach(function(variable) { vars[variable.name] = variable.value; }); return vars; } /** * Sends a color-coded formatted message to Slack. * * You must first configure a Platform.sh variable named \"SLACK_URL\". * That is the group and channel to which the message will be sent. * * To control what events it will run on, use the --events switch in * the Platform.sh CLI. * * @param {string} title * The title of the message block to send. * @param {string} message * The message body to send. */ function sendSlackMessage(title, message) { if ((new Date).getDay() === 5) { message += '\\r\\nCongratulations for deploying on a Friday! :calendar:'; } var body = { 'attachments': [{ 'title': title, 'text': message, 'color': color, }], }; var url = variables()['SLACK_URL']; if (!url) { throw new Error('You must define a SLACK_URL project variable.'); } var resp = fetch(url,{ method: 'POST', headers: { 'Content-Type': 'application/json', }, body: JSON.stringify(body), }); if (!resp.ok) { console.log('[LOG] Sending slack message failed: ' + resp.body.text()); } } /** * Compare the services and runtimes versions with the Platform.sh public registry. * * @param {json} services * The services/runtimes that your project is using within the activity.payload.deployment payload. * @param {json} registry * The Platform.sh registry available at: https://docs.platform.sh/registry/images/registry.json */ function compareVersions(services, registry) { var results = Object.keys(services).map(function(serviceName) { var service = services[serviceName]; var s = service.type.split(':'); var type = s[0]; var version = s[1]; var registryService = registry[type]; var name = registryService.name; if(!registryService) { return 'Unsupported '+ type; } var versions = registryService.versions; // Check for supported versions var indexOfSupported = versions.supported.indexOf(version); if(indexOfSupported !== -1) { var resp = 'Your ' + name + ' ' + version + ' is the most recent one.\\n'; if(indexOfSupported ",
        "section": "Activity scripts",
        "subsections": "Activity scripts",
        "image": "",
        "url": "https://community.platform.sh//t/script-to-check-for-deprecated-or-newer-available-runtimes-and-services-versions/575",
        "relurl": "/t/script-to-check-for-deprecated-or-newer-available-runtimes-and-services-versions/575"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "b04dd04d473aba6383887776cbe077d9caaedfb9",
        "title": "Post activities to a Slack room",
        "description": "Here’s a simple script to post activities to Slack. You’ll need to create a SLACK_URL project variable, which you set to the value Slack gives you when setting up an integration on their end. You can customize the message format by changing the body variable. See https://api.slack.com/messaging/composing/layouts for the various options there. It doesn’t filter the type of activity or the environment. You can do that when configuring the script on http://Platform.sh directly. /** * Sends a color-coded formatted message to Slack. * * You must first configure a Platform.sh variable named \"SLACK_URL\". * That is the group and channel to which the message will be sent. * * To control what events it will run on, use the --events switch in * the Platform.sh CLI. * * @param {string} title * The title of the message block to send. * @param {string} message * The message body to send. */ function sendSlackMessage(title, message) { console.log((new Date).getDay()); if ((new Date).getDay() === 5) { message += \"\\r\\nOn a Friday! :calendar:\"; } var color = activity.result === 'success' ? '#66c000' : '#ff0000'; var body = { 'attachments': [{ \"title\": title, \"text\": message, \"color\": color, }], }; var url = variables()['SLACK_URL']; if (!url) { throw new Error('You must define a SLACK_URL project variable.'); } var resp = fetch(url,{ method: 'POST', headers: { 'Content-Type': 'application/json', }, body: JSON.stringify(body), }); if (!resp.ok) { console.log(\"Sending slack message failed: \" + resp.body.text()); } } function variables() { var vars = {}; activity.payload.deployment.variables.forEach(function(variable) { vars[variable.name] = variable.value; }); return vars; } sendSlackMessage(activity.text, activity.log); ",
        "text": "Here’s a simple script to post activities to Slack. You’ll need to create a SLACK_URL project variable, which you set to the value Slack gives you when setting up an integration on their end. You can customize the message format by changing the body variable. See https://api.slack.com/messaging/composing/layouts for the various options there. It doesn’t filter the type of activity or the environment. You can do that when configuring the script on http://Platform.sh directly. /** * Sends a color-coded formatted message to Slack. * * You must first configure a Platform.sh variable named \"SLACK_URL\". * That is the group and channel to which the message will be sent. * * To control what events it will run on, use the --events switch in * the Platform.sh CLI. * * @param {string} title * The title of the message block to send. * @param {string} message * The message body to send. */ function sendSlackMessage(title, message) { console.log((new Date).getDay()); if ((new Date).getDay() === 5) { message += \"\\r\\nOn a Friday! :calendar:\"; } var color = activity.result === 'success' ? '#66c000' : '#ff0000'; var body = { 'attachments': [{ \"title\": title, \"text\": message, \"color\": color, }], }; var url = variables()['SLACK_URL']; if (!url) { throw new Error('You must define a SLACK_URL project variable.'); } var resp = fetch(url,{ method: 'POST', headers: { 'Content-Type': 'application/json', }, body: JSON.stringify(body), }); if (!resp.ok) { console.log(\"Sending slack message failed: \" + resp.body.text()); } } function variables() { var vars = {}; activity.payload.deployment.variables.forEach(function(variable) { vars[variable.name] = variable.value; }); return vars; } sendSlackMessage(activity.text, activity.log); ",
        "section": "Activity scripts",
        "subsections": "Activity scripts",
        "image": "",
        "url": "https://community.platform.sh//t/post-activities-to-a-slack-room/574",
        "relurl": "/t/post-activities-to-a-slack-room/574"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "52b6b4c30e71bc2be8aaab02a13ff839afee6c72",
        "title": "Get an environment",
        "description": "Retrieve the details of a single existing environment.",
        "text": "Retrieve the details of a single existing environment.",
        "section": "/projects/{projectId}/environments/{environmentId}",
        "subsections": "/projects/{projectId}/environments/{environmentId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment/paths/~1projects~1{projectId}~1environments~1{environmentId}/get",
        "relurl": "/#tag/Environment/paths/~1projects~1{projectId}~1environments~1{environmentId}/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "1cde26bfa59513ba9cda1f623072aa29878f0278",
        "title": "Delete an environment",
        "description": "Delete a specified environment.",
        "text": "Delete a specified environment.",
        "section": "/projects/{projectId}/environments/{environmentId}",
        "subsections": "/projects/{projectId}/environments/{environmentId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment/paths/~1projects~1{projectId}~1environments~1{environmentId}/delete",
        "relurl": "/#tag/Environment/paths/~1projects~1{projectId}~1environments~1{environmentId}/delete"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "f6485ca7346a94f3b262abdf50d434efcf99de9e",
        "title": "Update an environment",
        "description": "Update the details of a single existing environment.",
        "text": "Update the details of a single existing environment.",
        "section": "/projects/{projectId}/environments/{environmentId}",
        "subsections": "/projects/{projectId}/environments/{environmentId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment/paths/~1projects~1{projectId}~1environments~1{environmentId}/patch",
        "relurl": "/#tag/Environment/paths/~1projects~1{projectId}~1environments~1{environmentId}/patch"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "2cb77392270a2aaaf523780a7a97a5c172fd35af",
        "title": "Get environment activity log",
        "description": "Retrieve an environment's activity log. This returns a list of object with records of actions such as:  - Commits being pushed to the repository - A new environment being branched out from the specified environment - A snapshot being created of the specified environment  The object includes a timestamp of when the action occurred (`created_at`), when the action concluded (`updated_at`), the current `state` of the action, the action's completion percentage (`completion_percent`), and other related information in the `payload`.  The contents of the `payload` varies based on the `type` of the activity. For example:  - An `environment.branch` action's `payload` can contain objects representing the `parent` environment and the branching action's `outcome`.  - An `environment.push` action's `payload` can contain objects representing the `environment`, the specific `commits` included in the push, and the `user` who pushed. ",
        "text": "Retrieve an environment's activity log. This returns a list of object with records of actions such as:  - Commits being pushed to the repository - A new environment being branched out from the specified environment - A snapshot being created of the specified environment  The object includes a timestamp of when the action occurred (`created_at`), when the action concluded (`updated_at`), the current `state` of the action, the action's completion percentage (`completion_percent`), and other related information in the `payload`.  The contents of the `payload` varies based on the `type` of the activity. For example:  - An `environment.branch` action's `payload` can contain objects representing the `parent` environment and the branching action's `outcome`.  - An `environment.push` action's `payload` can contain objects representing the `environment`, the specific `commits` included in the push, and the `user` who pushed. ",
        "section": "/projects/{projectId}/environments/{environmentId}/activities",
        "subsections": "/projects/{projectId}/environments/{environmentId}/activities",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment-Activity/paths/~1projects~1{projectId}~1environments~1{environmentId}~1activities/get",
        "relurl": "/#tag/Environment-Activity/paths/~1projects~1{projectId}~1environments~1{environmentId}~1activities/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "64fe5a4211d2ec99c2f4eec92a04f79a73b513e1",
        "title": "Create snapshot of environment",
        "description": "Trigger a new snapshot of an environment to be created. See the [Snapshot and Restore](https://docs.platform.sh/administration/snapshot-and-restore.html) section of the documentation for more information. ",
        "text": "Trigger a new snapshot of an environment to be created. See the [Snapshot and Restore](https://docs.platform.sh/administration/snapshot-and-restore.html) section of the documentation for more information. ",
        "section": "/projects/{projectId}/environments/{environmentId}/backup",
        "subsections": "/projects/{projectId}/environments/{environmentId}/backup",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment-Backups/paths/~1projects~1{projectId}~1environments~1{environmentId}~1backup/post",
        "relurl": "/#tag/Environment-Backups/paths/~1projects~1{projectId}~1environments~1{environmentId}~1backup/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "381c004fcc3c761ba4183054b65e5ea3d98cc150",
        "title": "Get an environment's snapshot list",
        "description": "Retrieve a list of objects representing backups of this environment. ",
        "text": "Retrieve a list of objects representing backups of this environment. ",
        "section": "/projects/{projectId}/environments/{environmentId}/backups",
        "subsections": "/projects/{projectId}/environments/{environmentId}/backups",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment-Backups/paths/~1projects~1{projectId}~1environments~1{environmentId}~1backups/get",
        "relurl": "/#tag/Environment-Backups/paths/~1projects~1{projectId}~1environments~1{environmentId}~1backups/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "52a0cf4d288135f195d5f1f34291c3492728279b",
        "title": "Merge an environment",
        "description": "Merge an environment into its parent. This means that code changes from the branch environment will be merged into the parent branch, and the parent branch will be rebuilt and deployed with the new code changes, retaining the existing data in the parent environment. ",
        "text": "Merge an environment into its parent. This means that code changes from the branch environment will be merged into the parent branch, and the parent branch will be rebuilt and deployed with the new code changes, retaining the existing data in the parent environment. ",
        "section": "/projects/{projectId}/environments/{environmentId}/merge",
        "subsections": "/projects/{projectId}/environments/{environmentId}/merge",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment/paths/~1projects~1{projectId}~1environments~1{environmentId}~1merge/post",
        "relurl": "/#tag/Environment/paths/~1projects~1{projectId}~1environments~1{environmentId}~1merge/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "a80e3e3bd69acf9888ff704fec0bd75eb7fea5ac",
        "title": "Redeploy an environment",
        "description": "Trigger the redeployment sequence of an environment.",
        "text": "Trigger the redeployment sequence of an environment.",
        "section": "/projects/{projectId}/environments/{environmentId}/redeploy",
        "subsections": "/projects/{projectId}/environments/{environmentId}/redeploy",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment/paths/~1projects~1{projectId}~1environments~1{environmentId}~1redeploy/post",
        "relurl": "/#tag/Environment/paths/~1projects~1{projectId}~1environments~1{environmentId}~1redeploy/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "df76bc8c97bb938b14031eea41f8f530e33285f3",
        "title": "",
        "description": "",
        "text": "",
        "section": "/projects/{projectId}/system",
        "subsections": "/projects/{projectId}/system",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/SystemInformation/paths/~1projects~1{projectId}~1system/get",
        "relurl": "/#tag/SystemInformation/paths/~1projects~1{projectId}~1system/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "a8ffce545b5735b084e93ebdf76b9ab08d913039",
        "title": "",
        "description": "",
        "text": "",
        "section": "/projects/{projectId}/system",
        "subsections": "/projects/{projectId}/system",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/SystemInformation/paths/~1projects~1{projectId}~1system/delete",
        "relurl": "/#tag/SystemInformation/paths/~1projects~1{projectId}~1system/delete"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "a27072de824c0cb419e6793168bacb75d92ae84b",
        "title": "",
        "description": "",
        "text": "",
        "section": "/projects/{projectId}/system",
        "subsections": "/projects/{projectId}/system",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/SystemInformation/paths/~1projects~1{projectId}~1system/patch",
        "relurl": "/#tag/SystemInformation/paths/~1projects~1{projectId}~1system/patch"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "f80083efe5dcae075bc754a7616bdcc875adf20e",
        "title": "Get a user address",
        "description": "",
        "text": "",
        "section": "/profiles/{userId}/address",
        "subsections": "/profiles/{userId}/address",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/Drupal\\blimp_user\\Plugin\\ApiResource\\RestfulUserProfiles::viewAddress",
        "relurl": "/#operation/Drupal\\blimp_user\\Plugin\\ApiResource\\RestfulUserProfiles::viewAddress"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "a19aa8bc4cb46834cbd789e71ca3ab60de4d38a1",
        "title": "Update a user address",
        "description": "Update a user address, supplying one or more key/value pairs to to change.",
        "text": "Update a user address, supplying one or more key/value pairs to to change.",
        "section": "/profiles/{userId}/address",
        "subsections": "/profiles/{userId}/address",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/Drupal\\blimp_user\\Plugin\\ApiResource\\RestfulUserProfiles::patchAddress",
        "relurl": "/#operation/Drupal\\blimp_user\\Plugin\\ApiResource\\RestfulUserProfiles::patchAddress"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "46d29fc5c7e19744a1c023761eb1c6fbcbb73bc0",
        "title": "Get a single project ACL entry",
        "description": "Retrieve the details of a user from a project's access control list using the `id` of the entry in the access control list retrieved with the [Get project access control list](#tag/Project-Access%2Fpaths%2F~1projects~1%7BprojectId%7D~1access%2Fget) endpoint. ",
        "text": "Retrieve the details of a user from a project's access control list using the `id` of the entry in the access control list retrieved with the [Get project access control list](#tag/Project-Access%2Fpaths%2F~1projects~1%7BprojectId%7D~1access%2Fget) endpoint. ",
        "section": "/projects/{projectId}/access/{projectAccessId}",
        "subsections": "/projects/{projectId}/access/{projectAccessId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project-Access/paths/~1projects~1{projectId}~1access~1{projectAccessId}/get",
        "relurl": "/#tag/Project-Access/paths/~1projects~1{projectId}~1access~1{projectAccessId}/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "d5f888be4f3c080081e84da23c43b699d8419f9f",
        "title": "Remove a user from a project",
        "description": "Remove a user from a project's access control list using the `id` of the entry in the access control list retrieved with the [Get project access control list](#tag/Project-Access%2Fpaths%2F~1projects~1%7BprojectId%7D~1access%2Fget) endpoint. ",
        "text": "Remove a user from a project's access control list using the `id` of the entry in the access control list retrieved with the [Get project access control list](#tag/Project-Access%2Fpaths%2F~1projects~1%7BprojectId%7D~1access%2Fget) endpoint. ",
        "section": "/projects/{projectId}/access/{projectAccessId}",
        "subsections": "/projects/{projectId}/access/{projectAccessId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project-Access/paths/~1projects~1{projectId}~1access~1{projectAccessId}/delete",
        "relurl": "/#tag/Project-Access/paths/~1projects~1{projectId}~1access~1{projectAccessId}/delete"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "a1c4f4f8981ca17b037664bbd9a4b43ce9809997",
        "title": "Update a project user's role",
        "description": "Change the role of a user from a project's access control list using the `id` of the entry in the access control list retrieved with the [Get project access control list](#tag/Project-Access%2Fpaths%2F~1projects~1%7BprojectId%7D~1access%2Fget) endpoint. ",
        "text": "Change the role of a user from a project's access control list using the `id` of the entry in the access control list retrieved with the [Get project access control list](#tag/Project-Access%2Fpaths%2F~1projects~1%7BprojectId%7D~1access%2Fget) endpoint. ",
        "section": "/projects/{projectId}/access/{projectAccessId}",
        "subsections": "/projects/{projectId}/access/{projectAccessId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project-Access/paths/~1projects~1{projectId}~1access~1{projectAccessId}/patch",
        "relurl": "/#tag/Project-Access/paths/~1projects~1{projectId}~1access~1{projectAccessId}/patch"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "22f05d609cde527be4d80b5a0d3fbc7d61c832e1",
        "title": "Get a single project deployment target",
        "description": "Get a single deployment target configuration of a project. ",
        "text": "Get a single deployment target configuration of a project. ",
        "section": "/projects/{projectId}/deployments/{deploymentTargetConfigurationId}",
        "subsections": "/projects/{projectId}/deployments/{deploymentTargetConfigurationId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Deployment-Target/paths/~1projects~1{projectId}~1deployments~1{deploymentTargetConfigurationId}/get",
        "relurl": "/#tag/Deployment-Target/paths/~1projects~1{projectId}~1deployments~1{deploymentTargetConfigurationId}/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "7007f57ff077128c8b3ce0ee6f44ed456edd7102",
        "title": "Delete a single project deployment target",
        "description": "Delete a single deployment target configuration associated with a specific project. ",
        "text": "Delete a single deployment target configuration associated with a specific project. ",
        "section": "/projects/{projectId}/deployments/{deploymentTargetConfigurationId}",
        "subsections": "/projects/{projectId}/deployments/{deploymentTargetConfigurationId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Deployment-Target/paths/~1projects~1{projectId}~1deployments~1{deploymentTargetConfigurationId}/delete",
        "relurl": "/#tag/Deployment-Target/paths/~1projects~1{projectId}~1deployments~1{deploymentTargetConfigurationId}/delete"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "c4d15b1a2c35b035df1bf692ced9df46fc1077b5",
        "title": "Update a project deployment",
        "description": "",
        "text": "",
        "section": "/projects/{projectId}/deployments/{deploymentTargetConfigurationId}",
        "subsections": "/projects/{projectId}/deployments/{deploymentTargetConfigurationId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Deployment-Target/paths/~1projects~1{projectId}~1deployments~1{deploymentTargetConfigurationId}/patch",
        "relurl": "/#tag/Deployment-Target/paths/~1projects~1{projectId}~1deployments~1{deploymentTargetConfigurationId}/patch"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "d5a8bd605038e2df249ef53b23421283417cdea0",
        "title": "Get an environment variable",
        "description": "Retrieve a single user-defined environment variable.",
        "text": "Retrieve a single user-defined environment variable.",
        "section": "/projects/{projectId}/environments/{environmentId}/variables/{variableId}",
        "subsections": "/projects/{projectId}/environments/{environmentId}/variables/{variableId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment-Variables/paths/~1projects~1{projectId}~1environments~1{environmentId}~1variables~1{variableId}/get",
        "relurl": "/#tag/Environment-Variables/paths/~1projects~1{projectId}~1environments~1{environmentId}~1variables~1{variableId}/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "2e3c2593033b2828173b1b1249b98f5dbb3d9a46",
        "title": "Delete an environment variable",
        "description": "Delete a single user-defined environment variable.",
        "text": "Delete a single user-defined environment variable.",
        "section": "/projects/{projectId}/environments/{environmentId}/variables/{variableId}",
        "subsections": "/projects/{projectId}/environments/{environmentId}/variables/{variableId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment-Variables/paths/~1projects~1{projectId}~1environments~1{environmentId}~1variables~1{variableId}/delete",
        "relurl": "/#tag/Environment-Variables/paths/~1projects~1{projectId}~1environments~1{environmentId}~1variables~1{variableId}/delete"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "4690f56f1e3b62175f6696b9e042ed371e97af55",
        "title": "Update an environment variable",
        "description": "Update a single user-defined environment variable. The `value` can be either a string or a JSON object (default: string), as specified by the `is_json` boolean flag. Additionally, the inheritability of an environment variable can be determined through the `is_inheritable` flag (default: true). See the [Variables](https://docs.platform.sh/development/variables.html#platformsh-environment-variables) section in our documentation for more information. ",
        "text": "Update a single user-defined environment variable. The `value` can be either a string or a JSON object (default: string), as specified by the `is_json` boolean flag. Additionally, the inheritability of an environment variable can be determined through the `is_inheritable` flag (default: true). See the [Variables](https://docs.platform.sh/development/variables.html#platformsh-environment-variables) section in our documentation for more information. ",
        "section": "/projects/{projectId}/environments/{environmentId}/variables/{variableId}",
        "subsections": "/projects/{projectId}/environments/{environmentId}/variables/{variableId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment-Variables/paths/~1projects~1{projectId}~1environments~1{environmentId}~1variables~1{variableId}/patch",
        "relurl": "/#tag/Environment-Variables/paths/~1projects~1{projectId}~1environments~1{environmentId}~1variables~1{variableId}/patch"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "32a8a41ff26285ad8e894094bf49f72ad6b0c8fb",
        "title": "Get list of project variables",
        "description": "Retrieve a list of objects representing the user-defined variables within a project. ",
        "text": "Retrieve a list of objects representing the user-defined variables within a project. ",
        "section": "/projects/{projectId}/variables",
        "subsections": "/projects/{projectId}/variables",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project-Variables/paths/~1projects~1{projectId}~1variables/get",
        "relurl": "/#tag/Project-Variables/paths/~1projects~1{projectId}~1variables/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "12441e9b083b66485247cb8933d5f9c16c312e1e",
        "title": "Add a project variable",
        "description": "Add a variable to a project. The `value` can be either a string or a JSON object (default: string), as specified by the `is_json` boolean flag. See the [Variables](https://docs.platform.sh/development/variables.html#project-variables) section in our documentation for more information. ",
        "text": "Add a variable to a project. The `value` can be either a string or a JSON object (default: string), as specified by the `is_json` boolean flag. See the [Variables](https://docs.platform.sh/development/variables.html#project-variables) section in our documentation for more information. ",
        "section": "/projects/{projectId}/variables",
        "subsections": "/projects/{projectId}/variables",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project-Variables/paths/~1projects~1{projectId}~1variables/post",
        "relurl": "/#tag/Project-Variables/paths/~1projects~1{projectId}~1variables/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "d5505d920baa8915ae29349c942b87cb2b406c25",
        "title": "Get list of available regions",
        "description": "",
        "text": "",
        "section": "/regions",
        "subsections": "/regions",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulEntityBlimpPlatformRegion::index",
        "relurl": "/#operation/RestfulEntityBlimpPlatformRegion::index"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "c383b33572f795bfbeb1c0454b62528027341152",
        "title": "Apply a voucher to a user",
        "description": "Apply a voucher to a user and refresh the currently open order.",
        "text": "Apply a voucher to a user and refresh the currently open order.",
        "section": "/vouchers/apply",
        "subsections": "/vouchers/apply",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/_blimp_vouchers_apply_voucher",
        "relurl": "/#operation/_blimp_vouchers_apply_voucher"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "90e0a8c0fabc21a3ca14db1508150b06ed241a58",
        "title": "Platform.sh Payment Source",
        "description": "",
        "text": "",
        "section": "/payment_source",
        "subsections": "/payment_source",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulEntityStripeSource::viewSource",
        "relurl": "/#operation/RestfulEntityStripeSource::viewSource"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "6a04c3ec4592a7e0bd871a465ccff005f186f1c0",
        "title": "Create a Platform.sh Payment Source",
        "description": "",
        "text": "",
        "section": "/payment_source",
        "subsections": "/payment_source",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulEntityStripeSource::createSource",
        "relurl": "/#operation/RestfulEntityStripeSource::createSource"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "51e9ba397898c9c72291b35db52f08a325b214c4",
        "title": "Delete a Platform.sh Payment Source",
        "description": "",
        "text": "",
        "section": "/payment_source",
        "subsections": "/payment_source",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulEntityStripeSource::deleteSource",
        "relurl": "/#operation/RestfulEntityStripeSource::deleteSource"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "f2d2f28d4bf07605e9d478b265f97e979747c5b6",
        "title": "",
        "description": "",
        "text": "",
        "section": "/projects/{projectId}/activities/{activityId}/cancel",
        "subsections": "/projects/{projectId}/activities/{activityId}/cancel",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Activity/paths/~1projects~1{projectId}~1activities~1{activityId}~1cancel/post",
        "relurl": "/#tag/Activity/paths/~1projects~1{projectId}~1activities~1{activityId}~1cancel/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "6e50b97787dd7bbe7960f040446d8aa60f84c45f",
        "title": "Get list of project domains",
        "description": "Retrieve a list of objects representing the user-specified domains associated with a project. Note that this does *not* return the domains automatically assigned to a project that appear under \"Access site\" on the user interface. ",
        "text": "Retrieve a list of objects representing the user-specified domains associated with a project. Note that this does *not* return the domains automatically assigned to a project that appear under \"Access site\" on the user interface. ",
        "section": "/projects/{projectId}/domains",
        "subsections": "/projects/{projectId}/domains",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Domain-Management/paths/~1projects~1{projectId}~1domains/get",
        "relurl": "/#tag/Domain-Management/paths/~1projects~1{projectId}~1domains/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "1dc05cfe887ae99bd8d2277006d5dfd5dabf628f",
        "title": "Add a project domain",
        "description": "Add a single domain to a project. If the `ssl` field is left blank without an object containing a PEM-encoded SSL certificate, a certificate will [be provisioned for you via Let's Encrypt.](https://docs.platform.sh/configuration/routes/https.html#lets-encrypt) ",
        "text": "Add a single domain to a project. If the `ssl` field is left blank without an object containing a PEM-encoded SSL certificate, a certificate will [be provisioned for you via Let's Encrypt.](https://docs.platform.sh/configuration/routes/https.html#lets-encrypt) ",
        "section": "/projects/{projectId}/domains",
        "subsections": "/projects/{projectId}/domains",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Domain-Management/paths/~1projects~1{projectId}~1domains/post",
        "relurl": "/#tag/Domain-Management/paths/~1projects~1{projectId}~1domains/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "1a1daaa4a1f2e137f217dde9b5d83ddb3e98741a",
        "title": "Synchronize a child environment with its parent",
        "description": "This synchronizes the code and/or data of an environment with that of its parent, then redeploys the environment. Synchronization is only possible if a branch has no unmerged commits and it can be fast-forwarded.  If data synchronization is specified, the data in the environment will be overwritten with that of its parent. ",
        "text": "This synchronizes the code and/or data of an environment with that of its parent, then redeploys the environment. Synchronization is only possible if a branch has no unmerged commits and it can be fast-forwarded.  If data synchronization is specified, the data in the environment will be overwritten with that of its parent. ",
        "section": "/projects/{projectId}/environments/{environmentId}/synchronize",
        "subsections": "/projects/{projectId}/environments/{environmentId}/synchronize",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment/paths/~1projects~1{projectId}~1environments~1{environmentId}~1synchronize/post",
        "relurl": "/#tag/Environment/paths/~1projects~1{projectId}~1environments~1{environmentId}~1synchronize/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "942d4eba94c54bc674bf6e0597d7f2a5c1e5ec7e",
        "title": "Get a project's info",
        "description": "Retrieve the details about an existing project",
        "text": "Retrieve the details about an existing project",
        "section": "/projects/{projectId}",
        "subsections": "/projects/{projectId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project/paths/~1projects~1{projectId}/get",
        "relurl": "/#tag/Project/paths/~1projects~1{projectId}/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "de1b129fb0badc4d35138ada40925c98c6d5ec3b",
        "title": "Delete a project",
        "description": "Delete a project from the platform",
        "text": "Delete a project from the platform",
        "section": "/projects/{projectId}",
        "subsections": "/projects/{projectId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project/paths/~1projects~1{projectId}/delete",
        "relurl": "/#tag/Project/paths/~1projects~1{projectId}/delete"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "918b699080f66dcdfb1bed5f6c74da51c25fbcb1",
        "title": "Update a project's info",
        "description": "Update the details about an existing project",
        "text": "Update the details about an existing project",
        "section": "/projects/{projectId}",
        "subsections": "/projects/{projectId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project/paths/~1projects~1{projectId}/patch",
        "relurl": "/#tag/Project/paths/~1projects~1{projectId}/patch"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "8a5de71e4c73febe72f311f1ccebcd28cf766cb0",
        "title": "Get list of SSL certificates",
        "description": "Retrieve a list of objects representing the SSL certificates associated with a project. ",
        "text": "Retrieve a list of objects representing the SSL certificates associated with a project. ",
        "section": "/projects/{projectId}/certificates",
        "subsections": "/projects/{projectId}/certificates",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Cert-Management/paths/~1projects~1{projectId}~1certificates/get",
        "relurl": "/#tag/Cert-Management/paths/~1projects~1{projectId}~1certificates/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "644200a73dd0584cba371766f0f77bd9b7125ed4",
        "title": "Add an SSL certificate",
        "description": "Add a single SSL certificate to a project. ",
        "text": "Add a single SSL certificate to a project. ",
        "section": "/projects/{projectId}/certificates",
        "subsections": "/projects/{projectId}/certificates",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Cert-Management/paths/~1projects~1{projectId}~1certificates/post",
        "relurl": "/#tag/Cert-Management/paths/~1projects~1{projectId}~1certificates/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "db1c05b5325ba122ed98ecfed230fbe27043bbbe",
        "title": "Initialize a new environment",
        "description": "Initialize and configure a new environment with an existing repository. The payload is the url of a git repository with a profile name:  ``` {     \"repository\": \"git@github.com:platformsh/a-project-template.git@master\",     \"profile\": \"Example Project\",     \"files\": [       {         \"mode\": 0600,         \"path\": \"config.json\",         \"contents\": \"XXXXXXXX\"       }     ] } ``` It can optionally carry additional files that will be committed to the repository, the POSIX file mode to set on each file, and the base64-encoded contents of each file.  This endpoint can also add a second repository URL in the `config` parameter that will be added to the contents of the first. This allows you to put your application in one repository and the Platform.sh YAML configuration files in another. ",
        "text": "Initialize and configure a new environment with an existing repository. The payload is the url of a git repository with a profile name:  ``` {     \"repository\": \"git@github.com:platformsh/a-project-template.git@master\",     \"profile\": \"Example Project\",     \"files\": [       {         \"mode\": 0600,         \"path\": \"config.json\",         \"contents\": \"XXXXXXXX\"       }     ] } ``` It can optionally carry additional files that will be committed to the repository, the POSIX file mode to set on each file, and the base64-encoded contents of each file.  This endpoint can also add a second repository URL in the `config` parameter that will be added to the contents of the first. This allows you to put your application in one repository and the Platform.sh YAML configuration files in another. ",
        "section": "/projects/{projectId}/environments/{environmentId}/initialize",
        "subsections": "/projects/{projectId}/environments/{environmentId}/initialize",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment/paths/~1projects~1{projectId}~1environments~1{environmentId}~1initialize/post",
        "relurl": "/#tag/Environment/paths/~1projects~1{projectId}~1environments~1{environmentId}~1initialize/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "264e0c279951d8b0cee1df55a19cceec983a2471",
        "title": "Retrieve project plan records",
        "description": "",
        "text": "",
        "section": "/records/plan",
        "subsections": "/records/plan",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulDataPlatformRecordPlans::index",
        "relurl": "/#operation/RestfulDataPlatformRecordPlans::index"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "e9665f25b0c4629e1ab1b0590c7ef67e07e28337",
        "title": "Search for subscriptions",
        "description": "",
        "text": "",
        "section": "/subscriptions",
        "subsections": "/subscriptions",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulEntityPlatformSubscription::index",
        "relurl": "/#operation/RestfulEntityPlatformSubscription::index"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "59e0339dd47d6260eb6c0a72bac5aa88eb148e4b",
        "title": "Start a new subscription",
        "description": "",
        "text": "",
        "section": "/subscriptions",
        "subsections": "/subscriptions",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulEntityPlatformSubscription::create",
        "relurl": "/#operation/RestfulEntityPlatformSubscription::create"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "146ef9e9a8204ad1c16b3a7fc38b0d5a12ac44d7",
        "title": "Return a list of available priorities for a license",
        "description": "",
        "text": "",
        "section": "/tickets/priority",
        "subsections": "/tickets/priority",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulDataTickets::priorityList",
        "relurl": "/#operation/RestfulDataTickets::priorityList"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "6e4ac7b7aa9a1402e806b7a0438b33c0a6a4fbd5",
        "title": "Get list of a user's SSH keys",
        "description": "Retrieve all SSH keys associated with a single user, as specified by the user's UUID. ",
        "text": "Retrieve all SSH keys associated with a single user, as specified by the user's UUID. ",
        "section": "/users/{uuid}/ssh_keys",
        "subsections": "/users/{uuid}/ssh_keys",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/_blimp_platform_user_resource_sshkeys",
        "relurl": "/#operation/_blimp_platform_user_resource_sshkeys"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "4e3d6031571e12c45512c46ea0af54c0710787be",
        "title": "Create a Platform.sh Setup Intent",
        "description": "",
        "text": "",
        "section": "/payment_source/intent",
        "subsections": "/payment_source/intent",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulEntityStripeSource::createIntent",
        "relurl": "/#operation/RestfulEntityStripeSource::createIntent"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "163b9c314451fe489002dd161490ccac946431d4",
        "title": "Get list of routes",
        "description": "Retrieve a list of objects containing route definitions for a specific environment. The definitions returned by this endpoint are those present in an environment's `.platform/routes.yaml` file. ",
        "text": "Retrieve a list of objects containing route definitions for a specific environment. The definitions returned by this endpoint are those present in an environment's `.platform/routes.yaml` file. ",
        "section": "/projects/{projectId}/environments/{environmentId}/routes",
        "subsections": "/projects/{projectId}/environments/{environmentId}/routes",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Routing/paths/~1projects~1{projectId}~1environments~1{environmentId}~1routes/get",
        "relurl": "/#tag/Routing/paths/~1projects~1{projectId}~1environments~1{environmentId}~1routes/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "ce699bfdc23ef928bbe4e000917b75a323facd3b",
        "title": "Create a new route",
        "description": "Add a new route to the specified environment. More information about how routes are defined can be found in the [Routes](https://docs.platform.sh/configuration/routes.html) section of the documentation.  This endpoint modifies an environment's `.platform/routes.yaml` file. For routes to propagate to child environments, the child environments must be synchronized with their parent. ",
        "text": "Add a new route to the specified environment. More information about how routes are defined can be found in the [Routes](https://docs.platform.sh/configuration/routes.html) section of the documentation.  This endpoint modifies an environment's `.platform/routes.yaml` file. For routes to propagate to child environments, the child environments must be synchronized with their parent. ",
        "section": "/projects/{projectId}/environments/{environmentId}/routes",
        "subsections": "/projects/{projectId}/environments/{environmentId}/routes",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Routing/paths/~1projects~1{projectId}~1environments~1{environmentId}~1routes/post",
        "relurl": "/#tag/Routing/paths/~1projects~1{projectId}~1environments~1{environmentId}~1routes/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "74bb28525bb69c1405f2fe7f150cb641d3afe55d",
        "title": "Get information about existing an third-party integration",
        "description": "",
        "text": "",
        "section": "/projects/{projectId}/integrations/{integrationId}",
        "subsections": "/projects/{projectId}/integrations/{integrationId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Third-Party-Integrations/paths/~1projects~1{projectId}~1integrations~1{integrationId}/get",
        "relurl": "/#tag/Third-Party-Integrations/paths/~1projects~1{projectId}~1integrations~1{integrationId}/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "32b7eeb9e1326bf9222083eb30090527a3dda280",
        "title": "Delete an existing third-party integration",
        "description": "",
        "text": "",
        "section": "/projects/{projectId}/integrations/{integrationId}",
        "subsections": "/projects/{projectId}/integrations/{integrationId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Third-Party-Integrations/paths/~1projects~1{projectId}~1integrations~1{integrationId}/delete",
        "relurl": "/#tag/Third-Party-Integrations/paths/~1projects~1{projectId}~1integrations~1{integrationId}/delete"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "ca985874c6050fd71950eab6347de4fe2f4dabfd",
        "title": "Update an existing third-party integration",
        "description": "",
        "text": "",
        "section": "/projects/{projectId}/integrations/{integrationId}",
        "subsections": "/projects/{projectId}/integrations/{integrationId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Third-Party-Integrations/paths/~1projects~1{projectId}~1integrations~1{integrationId}/patch",
        "relurl": "/#tag/Third-Party-Integrations/paths/~1projects~1{projectId}~1integrations~1{integrationId}/patch"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "8f9ef2c2a7d430507fbba91998609d2f92d752c9",
        "title": "Estimate existing subscription price",
        "description": "",
        "text": "",
        "section": "/subscriptions/{subscriptionId}/estimate",
        "subsections": "/subscriptions/{subscriptionId}/estimate",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulEntityPlatformSubscription::estimateExisting",
        "relurl": "/#operation/RestfulEntityPlatformSubscription::estimateExisting"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "bd7a1bdd9c7f8a90c357526150f5b2d4da774acb",
        "title": "Get project activity log",
        "description": "Retrieve a project's activity log including logging actions in all environments within a project. This returns a list of objects with records of actions such as:  - Commits being pushed to the repository - A new environment being branched out from the specified environment - A snapshot being created of the specified environment  The object includes a timestamp of when the action occurred (`created_at`), when the action concluded (`updated_at`), the current `state` of the action, the action's completion percentage (`completion_percent`), the `environments` it applies to and other related information in the `payload`.  The contents of the `payload` varies based on the `type` of the activity. For example:  - An `environment.branch` action's `payload` can contain objects representing the environment's `parent` environment and the branching action's `outcome`.  - An `environment.push` action's `payload` can contain objects representing the `environment`, the specific `commits` included in the push, and the `user` who pushed. ",
        "text": "Retrieve a project's activity log including logging actions in all environments within a project. This returns a list of objects with records of actions such as:  - Commits being pushed to the repository - A new environment being branched out from the specified environment - A snapshot being created of the specified environment  The object includes a timestamp of when the action occurred (`created_at`), when the action concluded (`updated_at`), the current `state` of the action, the action's completion percentage (`completion_percent`), the `environments` it applies to and other related information in the `payload`.  The contents of the `payload` varies based on the `type` of the activity. For example:  - An `environment.branch` action's `payload` can contain objects representing the environment's `parent` environment and the branching action's `outcome`.  - An `environment.push` action's `payload` can contain objects representing the `environment`, the specific `commits` included in the push, and the `user` who pushed. ",
        "section": "/projects/{projectId}/activities",
        "subsections": "/projects/{projectId}/activities",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project-Activity/paths/~1projects~1{projectId}~1activities/get",
        "relurl": "/#tag/Project-Activity/paths/~1projects~1{projectId}~1activities/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "887aa51b9de7e6a912736ac5dc3b1fd8269cc064",
        "title": "[ALPHA] Trigger a source operation",
        "description": "This endpoint triggers a source code operation as defined in the `source.operations` key in a project's `.platform.app.yaml` configuration. More information on source code operations is [available in our user documentation](https://docs.platform.sh/configuration/app/source-operations.html). ",
        "text": "This endpoint triggers a source code operation as defined in the `source.operations` key in a project's `.platform.app.yaml` configuration. More information on source code operations is [available in our user documentation](https://docs.platform.sh/configuration/app/source-operations.html). ",
        "section": "/projects/{projectId}/environments/{environmentId}/source_operation",
        "subsections": "/projects/{projectId}/environments/{environmentId}/source_operation",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Source-Operations/paths/~1projects~1{projectId}~1environments~1{environmentId}~1source_operation/post",
        "relurl": "/#tag/Source-Operations/paths/~1projects~1{projectId}~1environments~1{environmentId}~1source_operation/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "3b133ef1880e80f26bc4e1fb094e55823e527b14",
        "title": "",
        "description": "",
        "text": "",
        "section": "/projects/{projectId}/integrations/{integrationId}/activities/{activityId}",
        "subsections": "/projects/{projectId}/integrations/{integrationId}/activities/{activityId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Activity/paths/~1projects~1{projectId}~1integrations~1{integrationId}~1activities~1{activityId}/get",
        "relurl": "/#tag/Activity/paths/~1projects~1{projectId}~1integrations~1{integrationId}~1activities~1{activityId}/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "bd565693e0f3e56a5996d6c0299be90193d83980",
        "title": "",
        "description": "",
        "text": "",
        "section": "/projects/{projectId}/integrations/{integrationId}/activities/{activityId}",
        "subsections": "/projects/{projectId}/integrations/{integrationId}/activities/{activityId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Activity/paths/~1projects~1{projectId}~1integrations~1{integrationId}~1activities~1{activityId}/delete",
        "relurl": "/#tag/Activity/paths/~1projects~1{projectId}~1integrations~1{integrationId}~1activities~1{activityId}/delete"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "4a99ccc8ddad3d5116b42bc41c78c70ebf7892e9",
        "title": "",
        "description": "",
        "text": "",
        "section": "/projects/{projectId}/integrations/{integrationId}/activities/{activityId}",
        "subsections": "/projects/{projectId}/integrations/{integrationId}/activities/{activityId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Activity/paths/~1projects~1{projectId}~1integrations~1{integrationId}~1activities~1{activityId}/patch",
        "relurl": "/#tag/Activity/paths/~1projects~1{projectId}~1integrations~1{integrationId}~1activities~1{activityId}/patch"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "e86c426c0119e31e45ec49903c3dbb2394001de7",
        "title": "Get a single SSH key record",
        "description": "",
        "text": "",
        "section": "/ssh_keys/{key_id}",
        "subsections": "/ssh_keys/{key_id}",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/_blimp_platform_sshkey_resource_retrieve",
        "relurl": "/#operation/_blimp_platform_sshkey_resource_retrieve"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "969701828b3fd16d978993f339dc930cc67365fc",
        "title": "Delete a single SSH key",
        "description": "",
        "text": "",
        "section": "/ssh_keys/{key_id}",
        "subsections": "/ssh_keys/{key_id}",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/_blimp_platform_sshkey_resource_delete",
        "relurl": "/#operation/_blimp_platform_sshkey_resource_delete"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "2a4d090224677647f52692fe0d0ad88662b63c92",
        "title": "Get information about a single subscription",
        "description": "",
        "text": "",
        "section": "/subscriptions/{subscriptionId}",
        "subsections": "/subscriptions/{subscriptionId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulEntityPlatformSubscription::viewEntity",
        "relurl": "/#operation/RestfulEntityPlatformSubscription::viewEntity"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "ad64e8078611f9b7a48a0db004453d939da86173",
        "title": "Delete a subscription",
        "description": "",
        "text": "",
        "section": "/subscriptions/{subscriptionId}",
        "subsections": "/subscriptions/{subscriptionId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulEntityPlatformSubscription::delete",
        "relurl": "/#operation/RestfulEntityPlatformSubscription::delete"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "a307629faa035a49abe0fffd1b493b2a3d99bb91",
        "title": "Update a subscription",
        "description": "Update a subscription, supplying one or more key/value pairs to to change.",
        "text": "Update a subscription, supplying one or more key/value pairs to to change.",
        "section": "/subscriptions/{subscriptionId}",
        "subsections": "/subscriptions/{subscriptionId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulEntityPlatformSubscription::update",
        "relurl": "/#operation/RestfulEntityPlatformSubscription::update"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "7d331f18ba909b571aff91fb2d12c0259352957e",
        "title": "Platform.sh support tickets",
        "description": "",
        "text": "",
        "section": "/tickets",
        "subsections": "/tickets",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulDataTickets::index",
        "relurl": "/#operation/RestfulDataTickets::index"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "22c445e2f88eed613cbe8d0e3bc86b53ebab6a59",
        "title": "Create a new Platform.sh support ticket",
        "description": "",
        "text": "",
        "section": "/tickets",
        "subsections": "/tickets",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulDataTickets::create",
        "relurl": "/#operation/RestfulDataTickets::create"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "0669021d390336cfcc1b21d6e47064a230815cbd",
        "title": "Get a single user profile",
        "description": "",
        "text": "",
        "section": "/profiles/{userId}",
        "subsections": "/profiles/{userId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/Drupal\\blimp_user\\Plugin\\ApiResource\\RestfulUserProfiles::controllersInfoGet",
        "relurl": "/#operation/Drupal\\blimp_user\\Plugin\\ApiResource\\RestfulUserProfiles::controllersInfoGet"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "8d929230e3fdbdfdae868d019305733917bdb808",
        "title": "Update a user profile",
        "description": "Update a user profile, supplying one or more key/value pairs to to change.",
        "text": "Update a user profile, supplying one or more key/value pairs to to change.",
        "section": "/profiles/{userId}",
        "subsections": "/profiles/{userId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/Drupal\\blimp_user\\Plugin\\ApiResource\\RestfulUserProfiles::controllersInfoPatch",
        "relurl": "/#operation/Drupal\\blimp_user\\Plugin\\ApiResource\\RestfulUserProfiles::controllersInfoPatch"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "d2d09936b7c3f25896902e52998caeb03b0ba956",
        "title": "Get current logged-in user info",
        "description": "Return the information about the currently logged-in user (the user associated with the access token). ",
        "text": "Return the information about the currently logged-in user (the user associated with the access token). ",
        "section": "/me",
        "subsections": "/me",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/_blimp_platform_me_resource",
        "relurl": "/#operation/_blimp_platform_me_resource"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "8efaf6c954ba234edc33e258190195956b842157",
        "title": "Get an order",
        "description": "",
        "text": "",
        "section": "/orders/{orderId}",
        "subsections": "/orders/{orderId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Orders/paths/~1orders~1{orderId}/get",
        "relurl": "/#tag/Orders/paths/~1orders~1{orderId}/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "ddebc177bd5cb658acd9519a7f03d374c3a4afb0",
        "title": "Get available plans",
        "description": "Retrieve information about plans and pricing on Platform.sh.",
        "text": "Retrieve information about plans and pricing on Platform.sh.",
        "section": "/plans",
        "subsections": "/plans",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Plans/paths/~1plans/get",
        "relurl": "/#tag/Plans/paths/~1plans/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "08dd2fc5e09913a418a30f94c9560661a3559877",
        "title": "Get a single environment ACL entry",
        "description": "Retrieve the details of a user from an environments's access control list using the `id` of the entry in the access control list retrieved with the [Get environment access control list](#tag/Environment-Access%2Fpaths%2F~1projects~1%7BprojectId%7D~1environments~1%7BenvironmentId%7D~1access%2Fget) endpoint. ",
        "text": "Retrieve the details of a user from an environments's access control list using the `id` of the entry in the access control list retrieved with the [Get environment access control list](#tag/Environment-Access%2Fpaths%2F~1projects~1%7BprojectId%7D~1environments~1%7BenvironmentId%7D~1access%2Fget) endpoint. ",
        "section": "/projects/{projectId}/environments/{environmentId}/access/{environmentAccessId}",
        "subsections": "/projects/{projectId}/environments/{environmentId}/access/{environmentAccessId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment-Access/paths/~1projects~1{projectId}~1environments~1{environmentId}~1access~1{environmentAccessId}/get",
        "relurl": "/#tag/Environment-Access/paths/~1projects~1{projectId}~1environments~1{environmentId}~1access~1{environmentAccessId}/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "d0ebf2975c5688abf4f792e78ae4e93ce615c4c8",
        "title": "Remove a user from an environment",
        "description": "Remove a user from an environments's access control list using the `id` of the entry in the access control list retrieved with the [Get environment access control list](#tag/Environment-Access%2Fpaths%2F~1projects~1%7BprojectId%7D~1environments~1%7BenvironmentId%7D~1access%2Fget) endpoint. ",
        "text": "Remove a user from an environments's access control list using the `id` of the entry in the access control list retrieved with the [Get environment access control list](#tag/Environment-Access%2Fpaths%2F~1projects~1%7BprojectId%7D~1environments~1%7BenvironmentId%7D~1access%2Fget) endpoint. ",
        "section": "/projects/{projectId}/environments/{environmentId}/access/{environmentAccessId}",
        "subsections": "/projects/{projectId}/environments/{environmentId}/access/{environmentAccessId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment-Access/paths/~1projects~1{projectId}~1environments~1{environmentId}~1access~1{environmentAccessId}/delete",
        "relurl": "/#tag/Environment-Access/paths/~1projects~1{projectId}~1environments~1{environmentId}~1access~1{environmentAccessId}/delete"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "91e1041844f075005e3534d1ae7ed882ec36ce26",
        "title": "Update an environment user's role",
        "description": "Update the role of a user from an environments's access control list using the `id` of the entry in the access control list retrieved with the [Get environment access control list](#tag/Environment-Access%2Fpaths%2F~1projects~1%7BprojectId%7D~1environments~1%7BenvironmentId%7D~1access%2Fget) endpoint. ",
        "text": "Update the role of a user from an environments's access control list using the `id` of the entry in the access control list retrieved with the [Get environment access control list](#tag/Environment-Access%2Fpaths%2F~1projects~1%7BprojectId%7D~1environments~1%7BenvironmentId%7D~1access%2Fget) endpoint. ",
        "section": "/projects/{projectId}/environments/{environmentId}/access/{environmentAccessId}",
        "subsections": "/projects/{projectId}/environments/{environmentId}/access/{environmentAccessId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment-Access/paths/~1projects~1{projectId}~1environments~1{environmentId}~1access~1{environmentAccessId}/patch",
        "relurl": "/#tag/Environment-Access/paths/~1projects~1{projectId}~1environments~1{environmentId}~1access~1{environmentAccessId}/patch"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "1e5a3075bba3ac64993eeb9626b16b1b36aa2fa5",
        "title": "Get an environment snapshot's info",
        "description": "Get the details of a specific backup from an environment using the `id` of the entry retrieved by the [Get backups list](#tag/Environment-Backups%2Fpaths%2F~1projects~1%7BprojectId%7D~1environments~1%7BenvironmentId%7D~1backups%2Fget) endpoint. ",
        "text": "Get the details of a specific backup from an environment using the `id` of the entry retrieved by the [Get backups list](#tag/Environment-Backups%2Fpaths%2F~1projects~1%7BprojectId%7D~1environments~1%7BenvironmentId%7D~1backups%2Fget) endpoint. ",
        "section": "/projects/{projectId}/environments/{environmentId}/backups/{backupId}",
        "subsections": "/projects/{projectId}/environments/{environmentId}/backups/{backupId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment-Backups/paths/~1projects~1{projectId}~1environments~1{environmentId}~1backups~1{backupId}/get",
        "relurl": "/#tag/Environment-Backups/paths/~1projects~1{projectId}~1environments~1{environmentId}~1backups~1{backupId}/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "99d4f7b8a4030cc55ff50c9c073ae8117740cbfa",
        "title": "Delete an environment snapshot",
        "description": "Delete a specific backup from an environment using the `id` of the entry retrieved by the [Get backups list](#tag/Environment-Backups%2Fpaths%2F~1projects~1%7BprojectId%7D~1environments~1%7BenvironmentId%7D~1backups%2Fget) endpoint. ",
        "text": "Delete a specific backup from an environment using the `id` of the entry retrieved by the [Get backups list](#tag/Environment-Backups%2Fpaths%2F~1projects~1%7BprojectId%7D~1environments~1%7BenvironmentId%7D~1backups%2Fget) endpoint. ",
        "section": "/projects/{projectId}/environments/{environmentId}/backups/{backupId}",
        "subsections": "/projects/{projectId}/environments/{environmentId}/backups/{backupId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment-Backups/paths/~1projects~1{projectId}~1environments~1{environmentId}~1backups~1{backupId}/delete",
        "relurl": "/#tag/Environment-Backups/paths/~1projects~1{projectId}~1environments~1{environmentId}~1backups~1{backupId}/delete"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "4fefadd9d6c53e70ff46b31ffc17356d39a79d02",
        "title": "Restore an environment snapshot",
        "description": "Restore a specific backup from an environment using the `id` of the entry retrieved by the [Get backups list](#tag/Environment-Backups%2Fpaths%2F~1projects~1%7BprojectId%7D~1environments~1%7BenvironmentId%7D~1backups%2Fget) endpoint. ",
        "text": "Restore a specific backup from an environment using the `id` of the entry retrieved by the [Get backups list](#tag/Environment-Backups%2Fpaths%2F~1projects~1%7BprojectId%7D~1environments~1%7BenvironmentId%7D~1backups%2Fget) endpoint. ",
        "section": "/projects/{projectId}/environments/{environmentId}/backups/{backupId}/restore",
        "subsections": "/projects/{projectId}/environments/{environmentId}/backups/{backupId}/restore",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment-Backups/paths/~1projects~1{projectId}~1environments~1{environmentId}~1backups~1{backupId}~1restore/post",
        "relurl": "/#tag/Environment-Backups/paths/~1projects~1{projectId}~1environments~1{environmentId}~1backups~1{backupId}~1restore/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "9b9c89553bdaff0a81739212cbf351b78f4ea93a",
        "title": "Get an environment's deployment information",
        "description": "Retrieve the read-only configuration of an environment's deployment. The returned information is everything required to recreate a project's current deployment.  More specifically, the objects returned by this endpoint contain the configuration derived from the repository's YAML configuration files: `.platform.app.yaml`, `.platform/services.yaml`, and `.platform/routes.yaml`.  Additionally, any values deriving from environment variables, the domains attached to a project, project access settings, etc. are included here.  This endpoint currently returns a list containing a single deployment configuration with an `id` of `current`. This may be subject to change in the future. ",
        "text": "Retrieve the read-only configuration of an environment's deployment. The returned information is everything required to recreate a project's current deployment.  More specifically, the objects returned by this endpoint contain the configuration derived from the repository's YAML configuration files: `.platform.app.yaml`, `.platform/services.yaml`, and `.platform/routes.yaml`.  Additionally, any values deriving from environment variables, the domains attached to a project, project access settings, etc. are included here.  This endpoint currently returns a list containing a single deployment configuration with an `id` of `current`. This may be subject to change in the future. ",
        "section": "/projects/{projectId}/environments/{environmentId}/deployments",
        "subsections": "/projects/{projectId}/environments/{environmentId}/deployments",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Deployment/paths/~1projects~1{projectId}~1environments~1{environmentId}~1deployments/get",
        "relurl": "/#tag/Deployment/paths/~1projects~1{projectId}~1environments~1{environmentId}~1deployments/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "97e6775adc38521db0a91be34e19f85686ddfb5d",
        "title": "Create a new deployment",
        "description": "",
        "text": "",
        "section": "/projects/{projectId}/environments/{environmentId}/deployments",
        "subsections": "/projects/{projectId}/environments/{environmentId}/deployments",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Internal/paths/~1projects~1{projectId}~1environments~1{environmentId}~1deployments/post",
        "relurl": "/#tag/Internal/paths/~1projects~1{projectId}~1environments~1{environmentId}~1deployments/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "d9142a65057c58bbf44f7f1b93bd98bbac6f1116",
        "title": "Suspend a Platform.sh subscription",
        "description": "",
        "text": "",
        "section": "/subscriptions/{subscriptionId}/suspend",
        "subsections": "/subscriptions/{subscriptionId}/suspend",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulEntityPlatformSubscription::suspendLicense",
        "relurl": "/#operation/RestfulEntityPlatformSubscription::suspendLicense"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "a5d5988d132eea7ca5863f1052ef5f5759c1a4a8",
        "title": "Register a new user",
        "description": "Register a new user on Platform.sh.",
        "text": "Register a new user on Platform.sh.",
        "section": "/users/register",
        "subsections": "/users/register",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/_blimp_platform_users_create_resource",
        "relurl": "/#operation/_blimp_platform_users_create_resource"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "6ef17e3f72482d59d393b7b2e5148a775a678207",
        "title": "Add a new public SSH key to a user",
        "description": "",
        "text": "",
        "section": "/ssh_keys",
        "subsections": "/ssh_keys",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/_blimp_platform_sshkey_resource_create",
        "relurl": "/#operation/_blimp_platform_sshkey_resource_create"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "e68c7582769ae9f5bc7336f01fc3418d18f98feb",
        "title": "Activate an environment",
        "description": "Set the specified environment's status to active",
        "text": "Set the specified environment's status to active",
        "section": "/projects/{projectId}/environments/{environmentId}/activate",
        "subsections": "/projects/{projectId}/environments/{environmentId}/activate",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment/paths/~1projects~1{projectId}~1environments~1{environmentId}~1activate/post",
        "relurl": "/#tag/Environment/paths/~1projects~1{projectId}~1environments~1{environmentId}~1activate/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "4e6fe62bff1c368ac5bb58b0fb14fc57725b4cf2",
        "title": "",
        "description": "",
        "text": "",
        "section": "/projects/{projectId}/integrations/{integrationId}/activities",
        "subsections": "/projects/{projectId}/integrations/{integrationId}/activities",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Activity/paths/~1projects~1{projectId}~1integrations~1{integrationId}~1activities/get",
        "relurl": "/#tag/Activity/paths/~1projects~1{projectId}~1integrations~1{integrationId}~1activities/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "68f41b215e53d3af0bd83f19c029f72874c83d54",
        "title": "",
        "description": "",
        "text": "",
        "section": "/projects/{projectId}/integrations/{integrationId}/activities",
        "subsections": "/projects/{projectId}/integrations/{integrationId}/activities",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Activity/paths/~1projects~1{projectId}~1integrations~1{integrationId}~1activities/post",
        "relurl": "/#tag/Activity/paths/~1projects~1{projectId}~1integrations~1{integrationId}~1activities/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "e927ee413d54e9e9d408ea5956bf85b8ca04ced8",
        "title": "Get a single user",
        "description": "Retrieve an object representing a single user, as specified by UUID. ",
        "text": "Retrieve an object representing a single user, as specified by UUID. ",
        "section": "/users/{uuid}",
        "subsections": "/users/{uuid}",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/_blimp_platform_users_resource_retrieve",
        "relurl": "/#operation/_blimp_platform_users_resource_retrieve"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "bc10e5fc9cde826d25ef274d9a4e004cacf5c4f5",
        "title": "Allowed Platform.sh Payment Source Types.",
        "description": "",
        "text": "",
        "section": "/payment_source/allowed",
        "subsections": "/payment_source/allowed",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulEntityStripeSource::viewAllowedTypes",
        "relurl": "/#operation/RestfulEntityStripeSource::viewAllowedTypes"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "ad47f849406fdc2f03d5054437211dc63b0437c7",
        "title": "Get a project variable",
        "description": "Retrieve a single user-defined project variable.",
        "text": "Retrieve a single user-defined project variable.",
        "section": "/projects/{projectId}/variables/{projectVariableId}",
        "subsections": "/projects/{projectId}/variables/{projectVariableId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project-Variables/paths/~1projects~1{projectId}~1variables~1{projectVariableId}/get",
        "relurl": "/#tag/Project-Variables/paths/~1projects~1{projectId}~1variables~1{projectVariableId}/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "d4ea81cb13acaf03fa3fa1ff0bf7aff90453bc7d",
        "title": "Delete a project variable",
        "description": "Delete a single user-defined project variable.",
        "text": "Delete a single user-defined project variable.",
        "section": "/projects/{projectId}/variables/{projectVariableId}",
        "subsections": "/projects/{projectId}/variables/{projectVariableId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project-Variables/paths/~1projects~1{projectId}~1variables~1{projectVariableId}/delete",
        "relurl": "/#tag/Project-Variables/paths/~1projects~1{projectId}~1variables~1{projectVariableId}/delete"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "dfaaefd377589a7ec0ed634613e63706ce195f62",
        "title": "Update a project variable",
        "description": "Update a single user-defined project variable. The `value` can be either a string or a JSON object (default: string), as specified by the `is_json` boolean flag. See the [Variables](https://docs.platform.sh/development/variables.html#project-variables) section in our documentation for more information. ",
        "text": "Update a single user-defined project variable. The `value` can be either a string or a JSON object (default: string), as specified by the `is_json` boolean flag. See the [Variables](https://docs.platform.sh/development/variables.html#project-variables) section in our documentation for more information. ",
        "section": "/projects/{projectId}/variables/{projectVariableId}",
        "subsections": "/projects/{projectId}/variables/{projectVariableId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project-Variables/paths/~1projects~1{projectId}~1variables~1{projectVariableId}/patch",
        "relurl": "/#tag/Project-Variables/paths/~1projects~1{projectId}~1variables~1{projectVariableId}/patch"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "4ef72d96e3de030a0aefd19dfab1cbec932a1088",
        "title": "Search for users",
        "description": "Search for a user by name or email address.",
        "text": "Search for a user by name or email address.",
        "section": "/users",
        "subsections": "/users",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/_blimp_platform_users_resource",
        "relurl": "/#operation/_blimp_platform_users_resource"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "c01bed173ba6289b0378f5a5bb2814f696562087",
        "title": "Get list of repository refs",
        "description": "Retrieve a list of `refs/*` in the repository backing a project. This endpoint functions similarly to `git show-ref`, with each returned object containing a `ref` field with the ref's name, and an object containing the associated commit ID.  The returned commit ID can be used with the [Get a commit object](#tag/Git-Repo%2Fpaths%2F~1projects~1%7BprojectId%7D~1git~1commits~1%7BrepositoryCommitId%7D%2Fget) endpoint to retrieve information about that specific commit. ",
        "text": "Retrieve a list of `refs/*` in the repository backing a project. This endpoint functions similarly to `git show-ref`, with each returned object containing a `ref` field with the ref's name, and an object containing the associated commit ID.  The returned commit ID can be used with the [Get a commit object](#tag/Git-Repo%2Fpaths%2F~1projects~1%7BprojectId%7D~1git~1commits~1%7BrepositoryCommitId%7D%2Fget) endpoint to retrieve information about that specific commit. ",
        "section": "/projects/{projectId}/git/refs",
        "subsections": "/projects/{projectId}/git/refs",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Repository/paths/~1projects~1{projectId}~1git~1refs/get",
        "relurl": "/#tag/Repository/paths/~1projects~1{projectId}~1git~1refs/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "29142d0dddc469e54445b85022cd8e78ef5d43e7",
        "title": "Get an environment activity log entry",
        "description": "Retrieve a single environment activity entry as specified by an `id` returned by the [Get environment activities list](#tag/Environment-Activity%2Fpaths%2F~1projects~1%7BprojectId%7D~1environments~1%7BenvironmentId%7D~1activities%2Fget) endpoint. See the documentation on that endpoint for details about the information this endpoint can return. ",
        "text": "Retrieve a single environment activity entry as specified by an `id` returned by the [Get environment activities list](#tag/Environment-Activity%2Fpaths%2F~1projects~1%7BprojectId%7D~1environments~1%7BenvironmentId%7D~1activities%2Fget) endpoint. See the documentation on that endpoint for details about the information this endpoint can return. ",
        "section": "/projects/{projectId}/environments/{environmentId}/activities/{activityId}",
        "subsections": "/projects/{projectId}/environments/{environmentId}/activities/{activityId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment-Activity/paths/~1projects~1{projectId}~1environments~1{environmentId}~1activities~1{activityId}/get",
        "relurl": "/#tag/Environment-Activity/paths/~1projects~1{projectId}~1environments~1{environmentId}~1activities~1{activityId}/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "c4f39f2013ded6ebcccdfebdc87e8c476580e9fc",
        "title": "",
        "description": "",
        "text": "",
        "section": "/projects/{projectId}/environments/{environmentId}/activities/{activityId}/cancel",
        "subsections": "/projects/{projectId}/environments/{environmentId}/activities/{activityId}/cancel",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Activity/paths/~1projects~1{projectId}~1environments~1{environmentId}~1activities~1{activityId}~1cancel/post",
        "relurl": "/#tag/Activity/paths/~1projects~1{projectId}~1environments~1{environmentId}~1activities~1{activityId}~1cancel/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "cee593083aebaaaf1fa80e8084d2728c5a3a40f3",
        "title": "",
        "description": "",
        "text": "",
        "section": "/projects/{projectId}/integrations/{integrationId}/activities/{activityId}/cancel",
        "subsections": "/projects/{projectId}/integrations/{integrationId}/activities/{activityId}/cancel",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Activity/paths/~1projects~1{projectId}~1integrations~1{integrationId}~1activities~1{activityId}~1cancel/post",
        "relurl": "/#tag/Activity/paths/~1projects~1{projectId}~1integrations~1{integrationId}~1activities~1{activityId}~1cancel/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "4d3bb762facbdd112680cc61024f2237fdea2383",
        "title": "Get user profiles",
        "description": "",
        "text": "",
        "section": "/profiles",
        "subsections": "/profiles",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/Drupal\\blimp_user\\Plugin\\ApiResource\\RestfulUserProfiles::controllersInfo",
        "relurl": "/#operation/Drupal\\blimp_user\\Plugin\\ApiResource\\RestfulUserProfiles::controllersInfo"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "938b2b7714c4ab50ad547123027edff3e5868d70",
        "title": "",
        "description": "",
        "text": "",
        "section": "/projects/{projectId}/system/restart",
        "subsections": "/projects/{projectId}/system/restart",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/SystemInformation/paths/~1projects~1{projectId}~1system~1restart/post",
        "relurl": "/#tag/SystemInformation/paths/~1projects~1{projectId}~1system~1restart/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "22c3164bf0b3b8d7d6434eac304a4eb6e72fb699",
        "title": "Get an SSL certificate",
        "description": "Retrieve information about a single SSL certificate associated with a project. ",
        "text": "Retrieve information about a single SSL certificate associated with a project. ",
        "section": "/projects/{projectId}/certificates/{certificateId}",
        "subsections": "/projects/{projectId}/certificates/{certificateId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Cert-Management/paths/~1projects~1{projectId}~1certificates~1{certificateId}/get",
        "relurl": "/#tag/Cert-Management/paths/~1projects~1{projectId}~1certificates~1{certificateId}/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "06e6484b27039475d0dca8e71f0cf01110934928",
        "title": "Delete an SSL certificate",
        "description": "Delete a single SSL certificate associated with a project. ",
        "text": "Delete a single SSL certificate associated with a project. ",
        "section": "/projects/{projectId}/certificates/{certificateId}",
        "subsections": "/projects/{projectId}/certificates/{certificateId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Cert-Management/paths/~1projects~1{projectId}~1certificates~1{certificateId}/delete",
        "relurl": "/#tag/Cert-Management/paths/~1projects~1{projectId}~1certificates~1{certificateId}/delete"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "4cf0dcbc01ab77167f1a6c5e7ddc04b763fe434c",
        "title": "Update an SSL certificate",
        "description": "Update a single SSL certificate associated with a project. ",
        "text": "Update a single SSL certificate associated with a project. ",
        "section": "/projects/{projectId}/certificates/{certificateId}",
        "subsections": "/projects/{projectId}/certificates/{certificateId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Cert-Management/paths/~1projects~1{projectId}~1certificates~1{certificateId}/patch",
        "relurl": "/#tag/Cert-Management/paths/~1projects~1{projectId}~1certificates~1{certificateId}/patch"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "c03f132b7fa12982aae361cd0bd1043a705de315",
        "title": "Get a ref object",
        "description": "Retrieve the details of a single `refs` object in the repository backing a project. This endpoint functions similarly to `git show-ref \u003cpattern\u003e`, although the pattern must be a full ref `id`, rather than a matching pattern.  *NOTE: The `{repositoryRefId}` must be properly escaped.* That is, the ref `refs/heads/master` is accessible via `/projects/{projectId}/git/refs/heads%2Fmaster`. ",
        "text": "Retrieve the details of a single `refs` object in the repository backing a project. This endpoint functions similarly to `git show-ref \u003cpattern\u003e`, although the pattern must be a full ref `id`, rather than a matching pattern.  *NOTE: The `{repositoryRefId}` must be properly escaped.* That is, the ref `refs/heads/master` is accessible via `/projects/{projectId}/git/refs/heads%2Fmaster`. ",
        "section": "/projects/{projectId}/git/refs/{repositoryRefId}",
        "subsections": "/projects/{projectId}/git/refs/{repositoryRefId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Repository/paths/~1projects~1{projectId}~1git~1refs~1{repositoryRefId}/get",
        "relurl": "/#tag/Repository/paths/~1projects~1{projectId}~1git~1refs~1{repositoryRefId}/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "b9282ebddfa137c9935d9f7bf315fa3ec5b01b6b",
        "title": "Get list of project settings",
        "description": "Retrieve the global settings for a project.",
        "text": "Retrieve the global settings for a project.",
        "section": "/projects/{projectId}/settings",
        "subsections": "/projects/{projectId}/settings",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project-Settings/paths/~1projects~1{projectId}~1settings/get",
        "relurl": "/#tag/Project-Settings/paths/~1projects~1{projectId}~1settings/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "0685dce8dcb4ed66c49e43152e6c2f91aadc01e6",
        "title": "",
        "description": "",
        "text": "",
        "section": "/projects/{projectId}/settings",
        "subsections": "/projects/{projectId}/settings",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/ProjectSettings/paths/~1projects~1{projectId}~1settings/delete",
        "relurl": "/#tag/ProjectSettings/paths/~1projects~1{projectId}~1settings/delete"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "7e021cf643cf41446b712a3fb9d2e55f6ae1ca4d",
        "title": "Update a project setting",
        "description": "Update one or more project-level settings.",
        "text": "Update one or more project-level settings.",
        "section": "/projects/{projectId}/settings",
        "subsections": "/projects/{projectId}/settings",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project-Settings/paths/~1projects~1{projectId}~1settings/patch",
        "relurl": "/#tag/Project-Settings/paths/~1projects~1{projectId}~1settings/patch"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "a98fda27f99fdb9d551117c399ccd6470c15a239",
        "title": "Branch an environment",
        "description": "Create a new environment as a branch of the current environment. ",
        "text": "Create a new environment as a branch of the current environment. ",
        "section": "/projects/{projectId}/environments/{environmentId}/branch",
        "subsections": "/projects/{projectId}/environments/{environmentId}/branch",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment/paths/~1projects~1{projectId}~1environments~1{environmentId}~1branch/post",
        "relurl": "/#tag/Environment/paths/~1projects~1{projectId}~1environments~1{environmentId}~1branch/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "ccde7055e70fda4bc11b71b6ccacd11e66a2b85d",
        "title": "Get list of projects",
        "description": "Retrieve list of projects associated with account",
        "text": "Retrieve list of projects associated with account",
        "section": "/projects",
        "subsections": "/projects",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project/paths/~1projects/get",
        "relurl": "/#tag/Project/paths/~1projects/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "b75779eab803c9a5618300c08ce2f4f0b79abeb0",
        "title": "Create a new project",
        "description": "Create an empty project on the platform",
        "text": "Create an empty project on the platform",
        "section": "/projects",
        "subsections": "/projects",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project/paths/~1projects/post",
        "relurl": "/#tag/Project/paths/~1projects/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "5143f745b8499cca1bd9f109666410573340ada4",
        "title": "Get a project's access control list",
        "description": "Retrieve a list of objects specifying the users with access to a project and those users' roles. ",
        "text": "Retrieve a list of objects specifying the users with access to a project and those users' roles. ",
        "section": "/projects/{projectId}/access",
        "subsections": "/projects/{projectId}/access",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project-Access/paths/~1projects~1{projectId}~1access/get",
        "relurl": "/#tag/Project-Access/paths/~1projects~1{projectId}~1access/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "dfb5c637663a10c37f0de50b8a120f38246f2dae",
        "title": "Add a user to a project ACL",
        "description": "Add a user to a project's access control list",
        "text": "Add a user to a project's access control list",
        "section": "/projects/{projectId}/access",
        "subsections": "/projects/{projectId}/access",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project-Access/paths/~1projects~1{projectId}~1access/post",
        "relurl": "/#tag/Project-Access/paths/~1projects~1{projectId}~1access/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "c2c0dc94f10e9fbe3cf4097ea9f23f3273a94a66",
        "title": "Get a project's capabilities",
        "description": "Get a list of capabilities on a project, as defined by the billing system. For instance, one special capability that could be defined on a project is large development environments. ",
        "text": "Get a list of capabilities on a project, as defined by the billing system. For instance, one special capability that could be defined on a project is large development environments. ",
        "section": "/projects/{projectId}/capabilities",
        "subsections": "/projects/{projectId}/capabilities",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project/paths/~1projects~1{projectId}~1capabilities/get",
        "relurl": "/#tag/Project/paths/~1projects~1{projectId}~1capabilities/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "f895fa302a03915844d57227e3838de378f03b49",
        "title": "Get project deployment target info",
        "description": "The deployment target information for the project. ",
        "text": "The deployment target information for the project. ",
        "section": "/projects/{projectId}/deployments",
        "subsections": "/projects/{projectId}/deployments",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Deployment-Target/paths/~1projects~1{projectId}~1deployments/get",
        "relurl": "/#tag/Deployment-Target/paths/~1projects~1{projectId}~1deployments/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "b2494bca71f747fc186c835eaa6755b558c2e9a6",
        "title": "Create a project deployment target",
        "description": "Set the deployment target information for a project. ",
        "text": "Set the deployment target information for a project. ",
        "section": "/projects/{projectId}/deployments",
        "subsections": "/projects/{projectId}/deployments",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Deployment-Target/paths/~1projects~1{projectId}~1deployments/post",
        "relurl": "/#tag/Deployment-Target/paths/~1projects~1{projectId}~1deployments/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "714a3c457011c0f6cbbc48beac7803b521143f50",
        "title": "Get a tree object",
        "description": "Retrieve, by hash, the tree state represented by a commit. The returned object's `tree` field contains a list of files and directories present in the tree.  Directories in the tree can be recursively retrieved by this endpoint through their hashes. Files in the tree can be retrieved by the [Get a blob object](#tag/Git-Repo%2Fpaths%2F~1projects~1%7BprojectId%7D~1git~1blobs~1%7BrepositoryBlobId%7D%2Fget) endpoint. ",
        "text": "Retrieve, by hash, the tree state represented by a commit. The returned object's `tree` field contains a list of files and directories present in the tree.  Directories in the tree can be recursively retrieved by this endpoint through their hashes. Files in the tree can be retrieved by the [Get a blob object](#tag/Git-Repo%2Fpaths%2F~1projects~1%7BprojectId%7D~1git~1blobs~1%7BrepositoryBlobId%7D%2Fget) endpoint. ",
        "section": "/projects/{projectId}/git/trees/{repositoryTreeId}",
        "subsections": "/projects/{projectId}/git/trees/{repositoryTreeId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Repository/paths/~1projects~1{projectId}~1git~1trees~1{repositoryTreeId}/get",
        "relurl": "/#tag/Repository/paths/~1projects~1{projectId}~1git~1trees~1{repositoryTreeId}/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "262d5fbdc82c272ce2f99d1fb2edd908ef3cab0d",
        "title": "Estimate new subscription price",
        "description": "",
        "text": "",
        "section": "/subscriptions/estimate",
        "subsections": "/subscriptions/estimate",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulEntityPlatformSubscription::estimate",
        "relurl": "/#operation/RestfulEntityPlatformSubscription::estimate"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "facb4d007f61fd64302934e5e5b2a401f3a001ea",
        "title": "Get a project activity log entry",
        "description": "Retrieve a single activity log entry as specified by an `id` returned by the [Get project activity log](#tag/Project-Activity%2Fpaths%2F~1projects~1%7BprojectId%7D~1activities%2Fget) endpoint. See the documentation on that endpoint for details about the information this endpoint can return. ",
        "text": "Retrieve a single activity log entry as specified by an `id` returned by the [Get project activity log](#tag/Project-Activity%2Fpaths%2F~1projects~1%7BprojectId%7D~1activities%2Fget) endpoint. See the documentation on that endpoint for details about the information this endpoint can return. ",
        "section": "/projects/{projectId}/activities/{activityId}",
        "subsections": "/projects/{projectId}/activities/{activityId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project-Activity/paths/~1projects~1{projectId}~1activities~1{activityId}/get",
        "relurl": "/#tag/Project-Activity/paths/~1projects~1{projectId}~1activities~1{activityId}/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "f3f618b57e2b60744754502eb449a59f1a2242e1",
        "title": "Get list of project environments",
        "description": "Retrieve a list of a project's existing environments and the information associated with each environment. ",
        "text": "Retrieve a list of a project's existing environments and the information associated with each environment. ",
        "section": "/projects/{projectId}/environments",
        "subsections": "/projects/{projectId}/environments",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment/paths/~1projects~1{projectId}~1environments/get",
        "relurl": "/#tag/Environment/paths/~1projects~1{projectId}~1environments/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "1a0536e0af75e784c89aeb8a94141a4a8b07f019",
        "title": "Create a new environment",
        "description": "Create a new environment on a project and define the configuration information associated with the environment. ",
        "text": "Create a new environment on a project and define the configuration information associated with the environment. ",
        "section": "/projects/{projectId}/environments",
        "subsections": "/projects/{projectId}/environments",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment/paths/~1projects~1{projectId}~1environments/post",
        "relurl": "/#tag/Environment/paths/~1projects~1{projectId}~1environments/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "e0dbae70c58c57ee3a17dc5f2c61e435baa49ada",
        "title": "Get an environment's access control list",
        "description": "Retrieve a list of objects specifying the users with access to an environment and those users' roles. ",
        "text": "Retrieve a list of objects specifying the users with access to an environment and those users' roles. ",
        "section": "/projects/{projectId}/environments/{environmentId}/access",
        "subsections": "/projects/{projectId}/environments/{environmentId}/access",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment-Access/paths/~1projects~1{projectId}~1environments~1{environmentId}~1access/get",
        "relurl": "/#tag/Environment-Access/paths/~1projects~1{projectId}~1environments~1{environmentId}~1access/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "79881d680c2de395875f4c25d264683e44cb8770",
        "title": "Add a user to an environment ACL",
        "description": "Add a user to an environment's access control list",
        "text": "Add a user to an environment's access control list",
        "section": "/projects/{projectId}/environments/{environmentId}/access",
        "subsections": "/projects/{projectId}/environments/{environmentId}/access",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment-Access/paths/~1projects~1{projectId}~1environments~1{environmentId}~1access/post",
        "relurl": "/#tag/Environment-Access/paths/~1projects~1{projectId}~1environments~1{environmentId}~1access/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "e784ab0766a3e093f89f5f284dd32e652b117ea1",
        "title": "Get a single environment deployment",
        "description": "",
        "text": "",
        "section": "/projects/{projectId}/environments/{environmentId}/deployments/{deploymentId}",
        "subsections": "/projects/{projectId}/environments/{environmentId}/deployments/{deploymentId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Internal/paths/~1projects~1{projectId}~1environments~1{environmentId}~1deployments~1{deploymentId}/get",
        "relurl": "/#tag/Internal/paths/~1projects~1{projectId}~1environments~1{environmentId}~1deployments~1{deploymentId}/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "1a0f372a3869cc29fdd29d0de9c5bc775f2c6d1d",
        "title": "Get a blob object",
        "description": "Retrieve, by hash, an object representing a blob in the repository backing a project. This endpoint allows direct read-only access to the contents of files in a repo. It returns the file in the `content` field of the response object, encoded according to the format in the `encoding` field, e.g. `base64`. ",
        "text": "Retrieve, by hash, an object representing a blob in the repository backing a project. This endpoint allows direct read-only access to the contents of files in a repo. It returns the file in the `content` field of the response object, encoded according to the format in the `encoding` field, e.g. `base64`. ",
        "section": "/projects/{projectId}/git/blobs/{repositoryBlobId}",
        "subsections": "/projects/{projectId}/git/blobs/{repositoryBlobId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Repository/paths/~1projects~1{projectId}~1git~1blobs~1{repositoryBlobId}/get",
        "relurl": "/#tag/Repository/paths/~1projects~1{projectId}~1git~1blobs~1{repositoryBlobId}/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "bf770c5e266098f3673c09e0395b4fad9b65da90",
        "title": "Get list of existing integrations for a project",
        "description": "",
        "text": "",
        "section": "/projects/{projectId}/integrations",
        "subsections": "/projects/{projectId}/integrations",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Third-Party-Integrations/paths/~1projects~1{projectId}~1integrations/get",
        "relurl": "/#tag/Third-Party-Integrations/paths/~1projects~1{projectId}~1integrations/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "31d3006bc061ff253612d06ad3447602463c4ee3",
        "title": "Integrate project with a third-party service",
        "description": "",
        "text": "",
        "section": "/projects/{projectId}/integrations",
        "subsections": "/projects/{projectId}/integrations",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Third-Party-Integrations/paths/~1projects~1{projectId}~1integrations/post",
        "relurl": "/#tag/Third-Party-Integrations/paths/~1projects~1{projectId}~1integrations/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "dfd03cc29d31d72ba25c8bbb3dfc3eea89f322f9",
        "title": "Reactivate a Platform.sh subscription",
        "description": "",
        "text": "",
        "section": "/subscriptions/{subscriptionId}/reactivate",
        "subsections": "/subscriptions/{subscriptionId}/reactivate",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulEntityPlatformSubscription::reactivateLicense",
        "relurl": "/#operation/RestfulEntityPlatformSubscription::reactivateLicense"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "d89473eaf14bb943c24860fc6978aceb6fdae49b",
        "title": "Locate user projects",
        "description": "This endpoint returns a paginated list of all the projects associated with a given UUID. The returned information includes each project's respective endpoint and management console URLs, as well as information about the project owner. ",
        "text": "This endpoint returns a paginated list of all the projects associated with a given UUID. The returned information includes each project's respective endpoint and management console URLs, as well as information about the project owner. ",
        "section": "/locate",
        "subsections": "/locate",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulEntityPlatformProject::index",
        "relurl": "/#operation/RestfulEntityPlatformProject::index"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "5c86dac8255b8f4c9f27ccbe20b16cb5155fd8f4",
        "title": "Get list of vouchers associated with a user",
        "description": "",
        "text": "",
        "section": "/vouchers",
        "subsections": "/vouchers",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/_blimp_vouchers_index",
        "relurl": "/#operation/_blimp_vouchers_index"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "32e84da73a79ce0b2538aa37534950d2617f2f9b",
        "title": "Deactivate an environment",
        "description": "Destroy all services and data running on this environment so that only the Git branch remains. The environment can be reactivated later at any time; reactivating an environment will sync data from the parent environment and redeploy.  **NOTE: ALL DATA IN THIS ENVIRONMENT WILL BE IRREVOCABLY LOST** ",
        "text": "Destroy all services and data running on this environment so that only the Git branch remains. The environment can be reactivated later at any time; reactivating an environment will sync data from the parent environment and redeploy.  **NOTE: ALL DATA IN THIS ENVIRONMENT WILL BE IRREVOCABLY LOST** ",
        "section": "/projects/{projectId}/environments/{environmentId}/deactivate",
        "subsections": "/projects/{projectId}/environments/{environmentId}/deactivate",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment/paths/~1projects~1{projectId}~1environments~1{environmentId}~1deactivate/post",
        "relurl": "/#tag/Environment/paths/~1projects~1{projectId}~1environments~1{environmentId}~1deactivate/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "4c8d0d58a3fc2f0b6c88e63d568981ac93079a11",
        "title": "Get list of environment variables",
        "description": "Retrieve a list of objects representing the user-defined variables within an environment. ",
        "text": "Retrieve a list of objects representing the user-defined variables within an environment. ",
        "section": "/projects/{projectId}/environments/{environmentId}/variables",
        "subsections": "/projects/{projectId}/environments/{environmentId}/variables",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment-Variables/paths/~1projects~1{projectId}~1environments~1{environmentId}~1variables/get",
        "relurl": "/#tag/Environment-Variables/paths/~1projects~1{projectId}~1environments~1{environmentId}~1variables/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "17fc345e10e697b14bb4872ebc6e59bbc5d2ca8e",
        "title": "Add an environment variable",
        "description": "Add a variable to an environment. The `value` can be either a string or a JSON object (default: string), as specified by the `is_json` boolean flag. Additionally, the inheritability of an environment variable can be determined through the `is_inheritable` flag (default: true). See the [Variables](https://docs.platform.sh/development/variables.html#platformsh-environment-variables) section in our documentation for more information. ",
        "text": "Add a variable to an environment. The `value` can be either a string or a JSON object (default: string), as specified by the `is_json` boolean flag. Additionally, the inheritability of an environment variable can be determined through the `is_inheritable` flag (default: true). See the [Variables](https://docs.platform.sh/development/variables.html#platformsh-environment-variables) section in our documentation for more information. ",
        "section": "/projects/{projectId}/environments/{environmentId}/variables",
        "subsections": "/projects/{projectId}/environments/{environmentId}/variables",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment-Variables/paths/~1projects~1{projectId}~1environments~1{environmentId}~1variables/post",
        "relurl": "/#tag/Environment-Variables/paths/~1projects~1{projectId}~1environments~1{environmentId}~1variables/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "b32338d2afe8fa342d4b77465ae32b4ea865b141",
        "title": "Get a commit object",
        "description": "Retrieve, by hash, an object representing a commit in the repository backing a project. This endpoint functions similarly to `git cat-file -p \u003ccommit-id\u003e`. The returned object contains the hash of the Git tree that it belongs to, as well as the ID of parent commits.  The commit represented by a parent ID can be retrieved using this endpoint, while the tree state represented by this commit can be retrieved using the [Get a tree object](#tag/Git-Repo%2Fpaths%2F~1projects~1%7BprojectId%7D~1git~1trees~1%7BrepositoryTreeId%7D%2Fget) endpoint. ",
        "text": "Retrieve, by hash, an object representing a commit in the repository backing a project. This endpoint functions similarly to `git cat-file -p \u003ccommit-id\u003e`. The returned object contains the hash of the Git tree that it belongs to, as well as the ID of parent commits.  The commit represented by a parent ID can be retrieved using this endpoint, while the tree state represented by this commit can be retrieved using the [Get a tree object](#tag/Git-Repo%2Fpaths%2F~1projects~1%7BprojectId%7D~1git~1trees~1%7BrepositoryTreeId%7D%2Fget) endpoint. ",
        "section": "/projects/{projectId}/git/commits/{repositoryCommitId}",
        "subsections": "/projects/{projectId}/git/commits/{repositoryCommitId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Repository/paths/~1projects~1{projectId}~1git~1commits~1{repositoryCommitId}/get",
        "relurl": "/#tag/Repository/paths/~1projects~1{projectId}~1git~1commits~1{repositoryCommitId}/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "c6338837292cd2f1dd5f24ca8b7326f59e6aba75",
        "title": "Retrieve project usage records",
        "description": "",
        "text": "",
        "section": "/records/usage",
        "subsections": "/records/usage",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulDataPlatformRecordUsage::index",
        "relurl": "/#operation/RestfulDataPlatformRecordUsage::index"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "9c3e6f9301f9d0e952fb84a0317dce293872b982",
        "title": "Get information about a single region",
        "description": "",
        "text": "",
        "section": "/regions/{regionId}",
        "subsections": "/regions/{regionId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulEntityBlimpPlatformRegion::view",
        "relurl": "/#operation/RestfulEntityBlimpPlatformRegion::view"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "3a782030dd41f7159270778a03ff7e4c70ad1a99",
        "title": "Clear project build cache",
        "description": "On rare occasions, a project's build cache can become corrupted. This endpoint will entirely flush the project's build cache. More information on [clearing the build cache can be found in our user documentation.](https://docs.platform.sh/development/troubleshoot.html#clear-the-build-cache) ",
        "text": "On rare occasions, a project's build cache can become corrupted. This endpoint will entirely flush the project's build cache. More information on [clearing the build cache can be found in our user documentation.](https://docs.platform.sh/development/troubleshoot.html#clear-the-build-cache) ",
        "section": "/projects/{projectId}/clear_build_cache",
        "subsections": "/projects/{projectId}/clear_build_cache",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project/paths/~1projects~1{projectId}~1clear_build_cache/post",
        "relurl": "/#tag/Project/paths/~1projects~1{projectId}~1clear_build_cache/post"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "b221a12d81e8a145260a5b938c1f0d7efa75a703",
        "title": "Get list of orders",
        "description": "",
        "text": "",
        "section": "/orders",
        "subsections": "/orders",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulDataPlatformOrders::index",
        "relurl": "/#operation/RestfulDataPlatformOrders::index"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "a3693548c58b7ddd71aaf4d14dd93bced544a868",
        "title": "Get a project domain",
        "description": "Retrieve information about a single user-specified domain associated with a project. ",
        "text": "Retrieve information about a single user-specified domain associated with a project. ",
        "section": "/projects/{projectId}/domains/{domainId}",
        "subsections": "/projects/{projectId}/domains/{domainId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Domain-Management/paths/~1projects~1{projectId}~1domains~1{domainId}/get",
        "relurl": "/#tag/Domain-Management/paths/~1projects~1{projectId}~1domains~1{domainId}/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "113e4936e5b7b6e1c799501edcfa7510da4bb41f",
        "title": "Delete a project domain",
        "description": "Delete a single user-specified domain associated with a project. ",
        "text": "Delete a single user-specified domain associated with a project. ",
        "section": "/projects/{projectId}/domains/{domainId}",
        "subsections": "/projects/{projectId}/domains/{domainId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Domain-Management/paths/~1projects~1{projectId}~1domains~1{domainId}/delete",
        "relurl": "/#tag/Domain-Management/paths/~1projects~1{projectId}~1domains~1{domainId}/delete"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "8696aeeeb2d32ee7a1d1b2c08a62655aa714f31a",
        "title": "Update a project domain",
        "description": "Update the information associated with a single user-specified domain associated with a project. ",
        "text": "Update the information associated with a single user-specified domain associated with a project. ",
        "section": "/projects/{projectId}/domains/{domainId}",
        "subsections": "/projects/{projectId}/domains/{domainId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Domain-Management/paths/~1projects~1{projectId}~1domains~1{domainId}/patch",
        "relurl": "/#tag/Domain-Management/paths/~1projects~1{projectId}~1domains~1{domainId}/patch"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "432a077292a247d898991146baade1c541555881",
        "title": "Get a route's info",
        "description": "Get details of a route from an environment using the `id` of the entry retrieved by the [Get environment routes list](#tag/Environment-Routes%2Fpaths%2F~1projects~1%7BprojectId%7D~1environments~1%7BenvironmentId%7D~1routes%2Fget) endpoint. ",
        "text": "Get details of a route from an environment using the `id` of the entry retrieved by the [Get environment routes list](#tag/Environment-Routes%2Fpaths%2F~1projects~1%7BprojectId%7D~1environments~1%7BenvironmentId%7D~1routes%2Fget) endpoint. ",
        "section": "/projects/{projectId}/environments/{environmentId}/routes/{routeId}",
        "subsections": "/projects/{projectId}/environments/{environmentId}/routes/{routeId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Routing/paths/~1projects~1{projectId}~1environments~1{environmentId}~1routes~1{routeId}/get",
        "relurl": "/#tag/Routing/paths/~1projects~1{projectId}~1environments~1{environmentId}~1routes~1{routeId}/get"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "3ccd37aa781a2608897e0eefedc30417bacfd17d",
        "title": "Delete a route",
        "description": "Remove a route from an environment using the `id` of the entry retrieved by the [Get environment routes list](#tag/Environment-Routes%2Fpaths%2F~1projects~1%7BprojectId%7D~1environments~1%7BenvironmentId%7D~1routes%2Fget) endpoint.  This endpoint modifies an environment's `.platform/routes.yaml` file. For routes to propagate to child environments, the child environments must be synchronized with their parent. ",
        "text": "Remove a route from an environment using the `id` of the entry retrieved by the [Get environment routes list](#tag/Environment-Routes%2Fpaths%2F~1projects~1%7BprojectId%7D~1environments~1%7BenvironmentId%7D~1routes%2Fget) endpoint.  This endpoint modifies an environment's `.platform/routes.yaml` file. For routes to propagate to child environments, the child environments must be synchronized with their parent. ",
        "section": "/projects/{projectId}/environments/{environmentId}/routes/{routeId}",
        "subsections": "/projects/{projectId}/environments/{environmentId}/routes/{routeId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Routing/paths/~1projects~1{projectId}~1environments~1{environmentId}~1routes~1{routeId}/delete",
        "relurl": "/#tag/Routing/paths/~1projects~1{projectId}~1environments~1{environmentId}~1routes~1{routeId}/delete"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "5f2197b1fb5e6a3ddf0ec4a276b24339970cbd8d",
        "title": "Update a route",
        "description": "Update a route in an environment using the `id` of the entry retrieved by the [Get environment routes list](#tag/Environment-Routes%2Fpaths%2F~1projects~1%7BprojectId%7D~1environments~1%7BenvironmentId%7D~1routes%2Fget) endpoint.  This endpoint modifies an environment's `.platform/routes.yaml` file. For routes to propagate to child environments, the child environments must be synchronized with their parent. ",
        "text": "Update a route in an environment using the `id` of the entry retrieved by the [Get environment routes list](#tag/Environment-Routes%2Fpaths%2F~1projects~1%7BprojectId%7D~1environments~1%7BenvironmentId%7D~1routes%2Fget) endpoint.  This endpoint modifies an environment's `.platform/routes.yaml` file. For routes to propagate to child environments, the child environments must be synchronized with their parent. ",
        "section": "/projects/{projectId}/environments/{environmentId}/routes/{routeId}",
        "subsections": "/projects/{projectId}/environments/{environmentId}/routes/{routeId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Routing/paths/~1projects~1{projectId}~1environments~1{environmentId}~1routes~1{routeId}/patch",
        "relurl": "/#tag/Routing/paths/~1projects~1{projectId}~1environments~1{environmentId}~1routes~1{routeId}/patch"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "c6acae5f5dfc6c37d3b1bc240e56556f8ed62938",
        "title": "Get information about a single project",
        "description": "This endpoint returns a project's endpoint and management console URLs, as well as information about the project owner. ",
        "text": "This endpoint returns a project's endpoint and management console URLs, as well as information about the project owner. ",
        "section": "/locate/{projectId}",
        "subsections": "/locate/{projectId}",
        "image": "",
        "url": "https://api.platform.sh/docs/#operation/RestfulEntityPlatformProject::view",
        "relurl": "/#operation/RestfulEntityPlatformProject::view"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "1a9ab927b3e0f7cadfbd7b4400ce925e65142355",
        "title": "Cert Management",
        "description": "User-supplied SSL/TLS certificates can be managed using these endpoints. For more information, see our [Third-party TLS certificate](https://docs.platform.sh/golive/steps/tls.html#optional-configure-a-third-party-tls-certificate) documentation. These endpoints are not for managing certificates that are automatically supplied by Platform.sh via Let's Encrypt. ",
        "text": "User-supplied SSL/TLS certificates can be managed using these endpoints. For more information, see our [Third-party TLS certificate](https://docs.platform.sh/golive/steps/tls.html#optional-configure-a-third-party-tls-certificate) documentation. These endpoints are not for managing certificates that are automatically supplied by Platform.sh via Let's Encrypt. ",
        "section": "",
        "subsections": "",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Cert-Management",
        "relurl": "/#tag/Cert-Management"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "76b68475404fa86d72a8d8e537414088cc4f637d",
        "title": "Environment",
        "description": "On Platform.sh, an environment encompasses a single instance of your entire application stack, the services used by the application, the application's data storage, and the environment's backups.  In general, an environment represents a single branch or merge request in the Git repository backing a project. It is a virtual cluster of read-only application and service containers with read-write mounts for application and service data.  On Platform.sh, the `master` branch is your production environment—thus, merging changes to master will put those changes to production. ",
        "text": "On Platform.sh, an environment encompasses a single instance of your entire application stack, the services used by the application, the application's data storage, and the environment's backups.  In general, an environment represents a single branch or merge request in the Git repository backing a project. It is a virtual cluster of read-only application and service containers with read-write mounts for application and service data.  On Platform.sh, the `master` branch is your production environment—thus, merging changes to master will put those changes to production. ",
        "section": "",
        "subsections": "",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment",
        "relurl": "/#tag/Environment"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "57de2285df1215de8d27ba788a90338273f2583e",
        "title": "Environment Backups",
        "description": "A snapshot is a complete backup of an environment, including all the persistent data from all services running in an environment and all files present in mounted volumes.  These endpoints can be used to trigger the creation of new snapshots, get information about existing snapshots, delete existing snapshots or restore a snapshot. More information about snapshots can be found in our [Snapshot and Restore](https://docs.platform.sh/administration/snapshot-and-restore.html) documentation. ",
        "text": "A snapshot is a complete backup of an environment, including all the persistent data from all services running in an environment and all files present in mounted volumes.  These endpoints can be used to trigger the creation of new snapshots, get information about existing snapshots, delete existing snapshots or restore a snapshot. More information about snapshots can be found in our [Snapshot and Restore](https://docs.platform.sh/administration/snapshot-and-restore.html) documentation. ",
        "section": "",
        "subsections": "",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment-Backups",
        "relurl": "/#tag/Environment-Backups"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "8ec5ad6d09283593249644e4c6d8fb17f1c62fdc",
        "title": "Environment Access",
        "description": "These endpoints can be used to manipulate the access control list of a environment at the environment level—that is, users and roles defined here will apply specifically to a single environment and its children within a project. For more project-level access control, see the section entitled [Project Access](#tag/Project-Access). ",
        "text": "These endpoints can be used to manipulate the access control list of a environment at the environment level—that is, users and roles defined here will apply specifically to a single environment and its children within a project. For more project-level access control, see the section entitled [Project Access](#tag/Project-Access). ",
        "section": "",
        "subsections": "",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment-Access",
        "relurl": "/#tag/Environment-Access"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "a9e0a9ed41c2f573c294e2f2012322177025715a",
        "title": "Environment Variables",
        "description": "These endpoints manipulate user-defined variables which are bound to a specific environment, as well as (optionally) the children of an environment. These variables can be made available at both build time and runtime. For more information on environment variables, see the [Variables](https://docs.platform.sh/development/variables.html#platformsh-environment-variables) section of the documentation. ",
        "text": "These endpoints manipulate user-defined variables which are bound to a specific environment, as well as (optionally) the children of an environment. These variables can be made available at both build time and runtime. For more information on environment variables, see the [Variables](https://docs.platform.sh/development/variables.html#platformsh-environment-variables) section of the documentation. ",
        "section": "",
        "subsections": "",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Environment-Variables",
        "relurl": "/#tag/Environment-Variables"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "8b954b2447a56c78c442097df48516e399489284",
        "title": "Project",
        "description": "## Project Overview  On Platform.sh, a Project is backed by a single Git repository and encompasses your entire application stack, the services used by your application, the application's data storage, the production and staging environments, and the backups of those environments.  When you create a new project, you start with a single [Environment](#tag/Environment) called *Master*, corresponding to the master branch in the Git repository of the project—this will be your production environment.  If you connect your project to an external Git repo using one of our [Third-Party Integrations](#tag/Third-Party-Integrations) a new development environment can be created for each branch or pull request created in the repository. When a new development environment is created, the production environment's data will be cloned on-the-fly, giving you an isolated, production-ready test environment.  This set of API endpoints can be used to retrieve a list of projects associated with an API key, as well as create and update the parameters of existing projects. ",
        "text": "## Project Overview  On Platform.sh, a Project is backed by a single Git repository and encompasses your entire application stack, the services used by your application, the application's data storage, the production and staging environments, and the backups of those environments.  When you create a new project, you start with a single [Environment](#tag/Environment) called *Master*, corresponding to the master branch in the Git repository of the project—this will be your production environment.  If you connect your project to an external Git repo using one of our [Third-Party Integrations](#tag/Third-Party-Integrations) a new development environment can be created for each branch or pull request created in the repository. When a new development environment is created, the production environment's data will be cloned on-the-fly, giving you an isolated, production-ready test environment.  This set of API endpoints can be used to retrieve a list of projects associated with an API key, as well as create and update the parameters of existing projects. ",
        "section": "",
        "subsections": "",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project",
        "relurl": "/#tag/Project"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "9b623a7b9b985afa32b9fef8a7a47ae377e3328b",
        "title": "Project Access",
        "description": "These endpoints can be used to manipulate the access control list at the project level. Users and roles defined here will apply globally to all environments within a project. For more fine-grained, environment-level access control, see the section entitled [Environment Access](#tag/Environment-Access). ",
        "text": "These endpoints can be used to manipulate the access control list at the project level. Users and roles defined here will apply globally to all environments within a project. For more fine-grained, environment-level access control, see the section entitled [Environment Access](#tag/Environment-Access). ",
        "section": "",
        "subsections": "",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project-Access",
        "relurl": "/#tag/Project-Access"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "51cd13d5c68b783eec5ccb0a4b6e04a7a8032aef",
        "title": "Project Variables",
        "description": "These endpoints manipulate user-defined variables which are bound to an entire project. These variables are accessible to all environments within a single project, and they can be made available at both build time and runtime. For more information on project variables, see the [Variables](https://docs.platform.sh/development/variables.html#project-variables) section of the documentation. ",
        "text": "These endpoints manipulate user-defined variables which are bound to an entire project. These variables are accessible to all environments within a single project, and they can be made available at both build time and runtime. For more information on project variables, see the [Variables](https://docs.platform.sh/development/variables.html#project-variables) section of the documentation. ",
        "section": "",
        "subsections": "",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project-Variables",
        "relurl": "/#tag/Project-Variables"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "06999b68862bba6e39daf2698ffd1f23aaca4e9e",
        "title": "Project Settings",
        "description": "These endpoints can be used to retrieve and manipulate project-level settings. Only the `initialize` property can be set by end users. It is used to initialize a project from an existing Git repository.  The other properties can only be set by a privileged user. ",
        "text": "These endpoints can be used to retrieve and manipulate project-level settings. Only the `initialize` property can be set by end users. It is used to initialize a project from an existing Git repository.  The other properties can only be set by a privileged user. ",
        "section": "",
        "subsections": "",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Project-Settings",
        "relurl": "/#tag/Project-Settings"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "2ff915fe8ff6c2e28c84a3c093fc60c395412eb4",
        "title": "Repository",
        "description": "The Git repository backing projects hosted on Platform.sh can be accessed in a **read-only** manner through the `/projects/{projectId}/git/*` family of endpoints. With these endpoints, you can retrieve objects from the Git repository in the same way that you would in a local environment. ",
        "text": "The Git repository backing projects hosted on Platform.sh can be accessed in a **read-only** manner through the `/projects/{projectId}/git/*` family of endpoints. With these endpoints, you can retrieve objects from the Git repository in the same way that you would in a local environment. ",
        "section": "",
        "subsections": "",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Repository",
        "relurl": "/#tag/Repository"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "6c3e5de52214e2e4cf535b574bbc95f56b1decdd",
        "title": "Domain Management",
        "description": "These endpoints can be used to add, modify, or remove domains from a project. For more information on how domains function on Platform.sh, see the [Domains](https://docs.platform.sh/administration/web/configure-project.html#domains) section of our documentation. ",
        "text": "These endpoints can be used to add, modify, or remove domains from a project. For more information on how domains function on Platform.sh, see the [Domains](https://docs.platform.sh/administration/web/configure-project.html#domains) section of our documentation. ",
        "section": "",
        "subsections": "",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Domain-Management",
        "relurl": "/#tag/Domain-Management"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "0cbd030ed48282c4a9358366580555b1e381759b",
        "title": "Routing",
        "description": "These endpoints modify an environment's `.platform/routes.yaml` file. For routes to propagate to child environments, the child environments must be synchronized with their parent.  \u003e **Warning**: These endpoints create a new commit in the project repository. \u003e This may lead to merge conflicts if you are using an external Git provider \u003e through our integrations. \u003e \u003e **We strongly recommend that you specify your routes in your `.platform/routes.yaml` \u003e file if you use an external Git integration such as GitHub or GitLab.**  More information about routing can be found in the [Routes](https://docs.platform.sh/configuration/routes.html) section of the documentation. ",
        "text": "These endpoints modify an environment's `.platform/routes.yaml` file. For routes to propagate to child environments, the child environments must be synchronized with their parent.  \u003e **Warning**: These endpoints create a new commit in the project repository. \u003e This may lead to merge conflicts if you are using an external Git provider \u003e through our integrations. \u003e \u003e **We strongly recommend that you specify your routes in your `.platform/routes.yaml` \u003e file if you use an external Git integration such as GitHub or GitLab.**  More information about routing can be found in the [Routes](https://docs.platform.sh/configuration/routes.html) section of the documentation. ",
        "section": "",
        "subsections": "",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Routing",
        "relurl": "/#tag/Routing"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "950d9560e933b3d2d4b3957cd2a77739adbe533b",
        "title": "Source Operations",
        "description": "\u003e *WARNING: These endpoints are currently in alpha release and may not yet \u003e be available on all projects or on all regions.*  These endpoints interact with source code operations as defined in the `source.operations` key in a project's `.platform.app.yaml` configuration. More information on source code operations is [available in our user documentation](https://docs.platform.sh/configuration/app/source-operations.html). ",
        "text": "\u003e *WARNING: These endpoints are currently in alpha release and may not yet \u003e be available on all projects or on all regions.*  These endpoints interact with source code operations as defined in the `source.operations` key in a project's `.platform.app.yaml` configuration. More information on source code operations is [available in our user documentation](https://docs.platform.sh/configuration/app/source-operations.html). ",
        "section": "",
        "subsections": "",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Source-Operations",
        "relurl": "/#tag/Source-Operations"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "978658e53d748758c8956329e59845740be1f769",
        "title": "Deployment Target",
        "description": "Platform.sh is capable of deploying the production environments of projects in multiple topologies: both in clusters of containers, and as dedicated virtual machines. This is an internal API that can only be used by privileged users. ",
        "text": "Platform.sh is capable of deploying the production environments of projects in multiple topologies: both in clusters of containers, and as dedicated virtual machines. This is an internal API that can only be used by privileged users. ",
        "section": "",
        "subsections": "",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Deployment-Target",
        "relurl": "/#tag/Deployment-Target"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "5d66bb8aa1a86f344adb28176c52c58f92aa3075",
        "title": "Deployments",
        "description": "The deployments endpoints gives detailed information about the actual deployment of an active environment. Currently, it returns the _current_ deployment with information about the different apps, services, and routes contained within. ",
        "text": "The deployments endpoints gives detailed information about the actual deployment of an active environment. Currently, it returns the _current_ deployment with information about the different apps, services, and routes contained within. ",
        "section": "",
        "subsections": "",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Deployments",
        "relurl": "/#tag/Deployments"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "5dd45707994d9587231f053f7fde11a30a970a67",
        "title": "Third-Party Integrations",
        "description": "Platform.sh can easily integrate with many third-party services, including Git hosting services (GitHub, GitLab, and Bitbucket), health notification services (email, Slack, PagerDuty), performance analytics platforms (New Relic, Blackfire, Tideways), and webhooks.  For clarification about what information each field requires, see the [External Integrations](https://docs.platform.sh/administration/integrations.html) documentation. NOTE: The names of the CLI arguments listed in the documentation are not always named exactly the same as the required body fields in the API request. ",
        "text": "Platform.sh can easily integrate with many third-party services, including Git hosting services (GitHub, GitLab, and Bitbucket), health notification services (email, Slack, PagerDuty), performance analytics platforms (New Relic, Blackfire, Tideways), and webhooks.  For clarification about what information each field requires, see the [External Integrations](https://docs.platform.sh/administration/integrations.html) documentation. NOTE: The names of the CLI arguments listed in the documentation are not always named exactly the same as the required body fields in the API request. ",
        "section": "",
        "subsections": "",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Third-Party-Integrations",
        "relurl": "/#tag/Third-Party-Integrations"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "6423ec52247e0d1c9ddfdbe3b175788074c8852d",
        "title": "Subscriptions",
        "description": "Each project is represented by a subscription that holds the plan information. These endpoints can be used to go to a larger plan, add more storage, or subscribe to optional features. ",
        "text": "Each project is represented by a subscription that holds the plan information. These endpoints can be used to go to a larger plan, add more storage, or subscribe to optional features. ",
        "section": "",
        "subsections": "",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Subscriptions",
        "relurl": "/#tag/Subscriptions"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "c9193fc136841718c066295fe1d907bc92aa1b39",
        "title": "Orders",
        "description": "These endpoints can be used to retrieve order information from our billing system. Here you can view information about your bill for our services, include the billed amount and a link to a PDF of the bill. ",
        "text": "These endpoints can be used to retrieve order information from our billing system. Here you can view information about your bill for our services, include the billed amount and a link to a PDF of the bill. ",
        "section": "",
        "subsections": "",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Orders",
        "relurl": "/#tag/Orders"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "37147770ead0c2e79b8dfb2207f94032a3091f7d",
        "title": "Vouchers",
        "description": "These endpoints can be used to retrieve vouchers associated with a particular user as well as apply a voucher to a particular user. ",
        "text": "These endpoints can be used to retrieve vouchers associated with a particular user as well as apply a voucher to a particular user. ",
        "section": "",
        "subsections": "",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Vouchers",
        "relurl": "/#tag/Vouchers"
    },
    {
        "site": "apidocs",
        "source": "secondary",
        "rank": 5,
        "documentId": "0cf7e59e605a33e1fe7e2e64b5cfa10a457689a0",
        "title": "Records",
        "description": "These endpoints retrieve information about which plans were assigned to a particular project at which time. ",
        "text": "These endpoints retrieve information about which plans were assigned to a particular project at which time. ",
        "section": "",
        "subsections": "",
        "image": "",
        "url": "https://api.platform.sh/docs/#tag/Records",
        "relurl": "/#tag/Records"
    }
]