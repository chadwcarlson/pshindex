[
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "42a9b6c9ab0332d8918387c519417562b185340a",
        "title": "How to deny abusive IP addresses on Platform.sh",
        "description": "Goal To block a certain set of abusive IP addresses before they hit the application layer. This can be IPs identified to cause high load or otherwise harm the system, e.g. crawlers / bots, malicious scripts, hacking attempts, or IPs that should be blocked for any other operational reason. Assumptions Access to a project hosted on https://platform.sh/ Administrator rights on the project Knowledge on using the and the web UI Problems Sometimes a website can be stressed by traffic originating from IP addresses that are causing high load or any other negative impact on the application environment. In some cases it makes sense for those IP addresses to be added to a deny liset in order to stabilize system resource usage. In other cases certain IP ranges might need to be blocked for a different operational reason. http://platform.sh/ allows for such a deny list to be configured either through the CLI, or through the project’s web UI. Depending on personal taste or technical familiarity level, there are 2 ways to go about configuring IP deny list on the project. Steps (Via the CLI) If the Platform CLI is preferred, these are the steps to take in order to block IP addresses. 1. Log in to Platform CLI $ platform login 2. Find the project ID Note the project ID of the project to enable the deny list for: $ platform project:list 3. Find the environment ID Note the environment ID for the environment on the project that needs securing: $ platform environment:list -p 4. Create the deny list Run the following command to create the deny list and include the IP that needs to be blocked: platform httpaccess --access deny: -p -e (optional: --no-wait) Note: If multiple IPs are meant to be denied, this entire block has to be repeated: --access deny: Example: platform httpaccess --access deny: --access deny: ",
        "text": "Goal To block a certain set of abusive IP addresses before they hit the application layer. This can be IPs identified to cause high load or otherwise harm the system, e.g. crawlers / bots, malicious scripts, hacking attempts, or IPs that should be blocked for any other operational reason. Assumptions Access to a project hosted on https://platform.sh/ Administrator rights on the project Knowledge on using the and the web UI Problems Sometimes a website can be stressed by traffic originating from IP addresses that are causing high load or any other negative impact on the application environment. In some cases it makes sense for those IP addresses to be added to a deny liset in order to stabilize system resource usage. In other cases certain IP ranges might need to be blocked for a different operational reason. http://platform.sh/ allows for such a deny list to be configured either through the CLI, or through the project’s web UI. Depending on personal taste or technical familiarity level, there are 2 ways to go about configuring IP deny list on the project. Steps (Via the CLI) If the Platform CLI is preferred, these are the steps to take in order to block IP addresses. 1. Log in to Platform CLI $ platform login 2. Find the project ID Note the project ID of the project to enable the deny list for: $ platform project:list 3. Find the environment ID Note the environment ID for the environment on the project that needs securing: $ platform environment:list -p 4. Create the deny list Run the following command to create the deny list and include the IP that needs to be blocked: platform httpaccess --access deny: -p -e (optional: --no-wait) Note: If multiple IPs are meant to be denied, this entire block has to be repeated: --access deny: Example: platform httpaccess --access deny: --access deny: ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-deny-abusive-ip-addresses-on-platform-sh/169",
        "relurl": "/t/how-to-deny-abusive-ip-addresses-on-platform-sh/169"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "fda5eba3f5ac3251c123c5a2e1c3dbb0af771537",
        "title": "How to overwrite Spring Data MongoDB variable to access Platform.sh services",
        "description": "Goal In this tutorial, we’ll cover how you can overwrite https://spring.io/ https://spring.io/projects/spring-data-mongodb configurations to access the services in http://Platform.sh. Preparation This tutorial assumes you have A working Spring Data MongoDB application with Java 8 or higher A text editor of your choice. Problems http://Platform.sh has a https://github.com/platformsh/config-reader-java that provides a streamlined and easy to use way to interact with a http://Platform.sh environment and its https://docs.platform.sh/configuration/services.html. However, you can also use the application regularly and overwrite those configurations when you deploy your application on http://Platform.sh. That is useful when you either already have one app and want to move to http://Platform.sh or keep the default configuration to run locally. Steps To keep the configurations Spring has, by default, the application.properties file where you can set the settings that you wish on your application. Furthermore, . Give a Spring Data MongoDB application that you’re running locally with either an empty or non applications.properties file: You can overwrite those configurations on the platform.app.yaml the application configuration to http://Platform.sh. As shown in the configuration below. name: app type: \"java:11\" disk: 1024 hooks: build: mvn clean install relationships: database: 'mongodb:mongodb' # The configuration of app when it is exposed to the web. web: commands: start: | export USER=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].username\"` export PASSWORD=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].password\"` export HOST=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].host\"` export DATABASE=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].path\"` java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError \\ -Dspring.data.mongodb.database=$DATABASE \\ -Dspring.data.mongodb.host=$HOST \\ -Dspring.data.mongodb.username=$USER \\ -Dspring.data.mongodb.password=$PASSWORD \\ target/spring-boot.jar --server.port=$PORT Therefore, you can have the configuration or just migrate the application that already exists to http://Platform.sh. References https://spring.io/ https://spring.io/projects/spring-data-mongodb https://docs.spring.io/spring-boot/docs/current/reference/html/appendix-application-properties.html https://docs.platform.sh/frameworks/spring.html https://github.com/platformsh-examples/java-overwrite-configuration/tree/master/spring-mongodb ",
        "text": "Goal In this tutorial, we’ll cover how you can overwrite https://spring.io/ https://spring.io/projects/spring-data-mongodb configurations to access the services in http://Platform.sh. Preparation This tutorial assumes you have A working Spring Data MongoDB application with Java 8 or higher A text editor of your choice. Problems http://Platform.sh has a https://github.com/platformsh/config-reader-java that provides a streamlined and easy to use way to interact with a http://Platform.sh environment and its https://docs.platform.sh/configuration/services.html. However, you can also use the application regularly and overwrite those configurations when you deploy your application on http://Platform.sh. That is useful when you either already have one app and want to move to http://Platform.sh or keep the default configuration to run locally. Steps To keep the configurations Spring has, by default, the application.properties file where you can set the settings that you wish on your application. Furthermore, . Give a Spring Data MongoDB application that you’re running locally with either an empty or non applications.properties file: You can overwrite those configurations on the platform.app.yaml the application configuration to http://Platform.sh. As shown in the configuration below. name: app type: \"java:11\" disk: 1024 hooks: build: mvn clean install relationships: database: 'mongodb:mongodb' # The configuration of app when it is exposed to the web. web: commands: start: | export USER=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].username\"` export PASSWORD=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].password\"` export HOST=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].host\"` export DATABASE=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].path\"` java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError \\ -Dspring.data.mongodb.database=$DATABASE \\ -Dspring.data.mongodb.host=$HOST \\ -Dspring.data.mongodb.username=$USER \\ -Dspring.data.mongodb.password=$PASSWORD \\ target/spring-boot.jar --server.port=$PORT Therefore, you can have the configuration or just migrate the application that already exists to http://Platform.sh. References https://spring.io/ https://spring.io/projects/spring-data-mongodb https://docs.spring.io/spring-boot/docs/current/reference/html/appendix-application-properties.html https://docs.platform.sh/frameworks/spring.html https://github.com/platformsh-examples/java-overwrite-configuration/tree/master/spring-mongodb ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-overwrite-spring-data-mongodb-variable-to-access-platform-sh-services/528",
        "relurl": "/t/how-to-overwrite-spring-data-mongodb-variable-to-access-platform-sh-services/528"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "ba3b8c6cf146e8c479811339d69b6cdf41e2c1a4",
        "title": "How to flush the Fastly cache",
        "description": "Goal To purge all cached assets from the CDN layer sitting in front of your site. Assumptions You will need: To have Fastly cache running in front of your site The ability to run curl from your local terminal, Working access to the platform command for your project OR Read access to the Variables on your project through the web console. Problems For most Enterprise or Premium plans, The http://Platform.sh setup team has configured Fastly CDN for you, and manages it on your behalf with a built-in account. This means you do not have direct administration access using the Fastly web UI, where a ‘Purge’ button can be found. However, you have been issued with an “API” key that allows you access to many actions on your account, and you can initiate a purge using this. To purge a single page Fastly lets you . curl -X PURGE $url That’s it! To Purge all You need use the API key. All projects with Fastly configured should have had the env:FASTLY_API_TOKEN set in their /master/settings/variables Some older dedicated plans do not have it here, and the API token is instead stored in a text file on the server. If you can’t find the value in the environment variables, check for a file called /mnt/shared/fastly_tokens.txt on your server. This key can be used to construct a purge_all API request. The to construct a request like this: POST /service/SU1Z0isxPaozGVKXdv0eY/purge_all HTTP/1.1 Fastly-Key: YOUR_FASTLY_TOKEN Accept: application/json So, from a local CLI, run this sequence to retrieve your keys: (or you can just set the variables to the values you can see in the web console yourself) FASTLY_SERVICE_ID=$(platform --project=$PROJECTID --environment=master variable:get env:FASTLY_SERVICE_ID --pipe) FASTLY_API_TOKEN=$(platform --project=$PROJECTID --environment=master variable:get env:FASTLY_API_TOKEN --pipe) # Legacy: If these environment variables do not exist on your installation, try looking for a file called `/mnt/shared/fastly_tokens.txt` instead. FASTLY_API_URL=\"https://api.fastly.com\" curl -H \"Fastly-Key: $FASTLY_API_TOKEN\" \\ -X POST \"${FASTLY_API_URL}/service/${FASTLY_SERVICE_ID}/purge_all\" eg: curl -H Fastly-Key: nfDDFUahPwV4wQiJOMbLwJsCBy3Wh4TQ -X POST https://api.fastly.com/service/nOBmPqIK5M71NgZMhLYTZk/purge_all {\"status\":\"ok\"} Verify it really worked To empirically test, you could retrieve a page from your site and see if it looks right , or immediately make a network request and check the cache headers and dates: curl -I -s $url HTTP/2 200 ... cache-control: max-age=3600, public last-modified: Fri, 06 Mar 2020 01:36:16 GMT date: Fri, 06 Mar 2020 02:26:54 GMT ... via: 1.1 varnish age: 0 x-served-by: cache-syd10142-SYD, cache-hkg17927-HKG x-cache: MISS, MISS The x-served-by and via confirm that a cache is working. Unless someone else has already requested this page just ahead of you, you should see a MISS in the x-cache. Either way, the age (in seconds) should be low, and the date should be fresh. Ignore the last-modified Verify via API (just FYI) To verify the command ran as expected, you can retrieve a list of the , with a command like: curl -H \"Fastly-Key: $FASTLY_API_TOKEN\" -X GET \"${FASTLY_API_URL}/events?page[number]=1\u0026page[size]=10\" …which should return a response including something like: { \"id\": \"miaXP6k1OLwUdOIXcKkiHG\", \"type\": \"event\", \"attributes\": { \"customer_id\": \"rSam6DCAgeyDU7gSoC5pJi\", \"description\": \"Purge all was performed\", \"event_type\": \"service.purge_all\", \"ip\": \"118.173.60.218, 118.173.60.218\", \"metadata\": {}, \"service_id\": \"ZknOBmPqIK5M71NgZMhLYT\", \"user_id\": \"TdqCvp120Tm185r3gn4DYR\", \"created_at\": \"2020-03-06T01:35:35Z\", \"admin\": false } }, Conclusion This is a quick method to flush your CDN cache as needed. There are many other actions possible through the https://docs.fastly.com/api/ though not all will be available to the account granted with that API key. If you are using a CMS like Drupal, then a https://www.drupal.org/project/fastly can be installed to integrate cache management into your CMS. This provides an easier interface to actions like this. This approach also lets the CMS invoke a CDN purge when things change, so is recommended if you need to tune things finer.",
        "text": "Goal To purge all cached assets from the CDN layer sitting in front of your site. Assumptions You will need: To have Fastly cache running in front of your site The ability to run curl from your local terminal, Working access to the platform command for your project OR Read access to the Variables on your project through the web console. Problems For most Enterprise or Premium plans, The http://Platform.sh setup team has configured Fastly CDN for you, and manages it on your behalf with a built-in account. This means you do not have direct administration access using the Fastly web UI, where a ‘Purge’ button can be found. However, you have been issued with an “API” key that allows you access to many actions on your account, and you can initiate a purge using this. To purge a single page Fastly lets you . curl -X PURGE $url That’s it! To Purge all You need use the API key. All projects with Fastly configured should have had the env:FASTLY_API_TOKEN set in their /master/settings/variables Some older dedicated plans do not have it here, and the API token is instead stored in a text file on the server. If you can’t find the value in the environment variables, check for a file called /mnt/shared/fastly_tokens.txt on your server. This key can be used to construct a purge_all API request. The to construct a request like this: POST /service/SU1Z0isxPaozGVKXdv0eY/purge_all HTTP/1.1 Fastly-Key: YOUR_FASTLY_TOKEN Accept: application/json So, from a local CLI, run this sequence to retrieve your keys: (or you can just set the variables to the values you can see in the web console yourself) FASTLY_SERVICE_ID=$(platform --project=$PROJECTID --environment=master variable:get env:FASTLY_SERVICE_ID --pipe) FASTLY_API_TOKEN=$(platform --project=$PROJECTID --environment=master variable:get env:FASTLY_API_TOKEN --pipe) # Legacy: If these environment variables do not exist on your installation, try looking for a file called `/mnt/shared/fastly_tokens.txt` instead. FASTLY_API_URL=\"https://api.fastly.com\" curl -H \"Fastly-Key: $FASTLY_API_TOKEN\" \\ -X POST \"${FASTLY_API_URL}/service/${FASTLY_SERVICE_ID}/purge_all\" eg: curl -H Fastly-Key: nfDDFUahPwV4wQiJOMbLwJsCBy3Wh4TQ -X POST https://api.fastly.com/service/nOBmPqIK5M71NgZMhLYTZk/purge_all {\"status\":\"ok\"} Verify it really worked To empirically test, you could retrieve a page from your site and see if it looks right , or immediately make a network request and check the cache headers and dates: curl -I -s $url HTTP/2 200 ... cache-control: max-age=3600, public last-modified: Fri, 06 Mar 2020 01:36:16 GMT date: Fri, 06 Mar 2020 02:26:54 GMT ... via: 1.1 varnish age: 0 x-served-by: cache-syd10142-SYD, cache-hkg17927-HKG x-cache: MISS, MISS The x-served-by and via confirm that a cache is working. Unless someone else has already requested this page just ahead of you, you should see a MISS in the x-cache. Either way, the age (in seconds) should be low, and the date should be fresh. Ignore the last-modified Verify via API (just FYI) To verify the command ran as expected, you can retrieve a list of the , with a command like: curl -H \"Fastly-Key: $FASTLY_API_TOKEN\" -X GET \"${FASTLY_API_URL}/events?page[number]=1\u0026page[size]=10\" …which should return a response including something like: { \"id\": \"miaXP6k1OLwUdOIXcKkiHG\", \"type\": \"event\", \"attributes\": { \"customer_id\": \"rSam6DCAgeyDU7gSoC5pJi\", \"description\": \"Purge all was performed\", \"event_type\": \"service.purge_all\", \"ip\": \"118.173.60.218, 118.173.60.218\", \"metadata\": {}, \"service_id\": \"ZknOBmPqIK5M71NgZMhLYT\", \"user_id\": \"TdqCvp120Tm185r3gn4DYR\", \"created_at\": \"2020-03-06T01:35:35Z\", \"admin\": false } }, Conclusion This is a quick method to flush your CDN cache as needed. There are many other actions possible through the https://docs.fastly.com/api/ though not all will be available to the account granted with that API key. If you are using a CMS like Drupal, then a https://www.drupal.org/project/fastly can be installed to integrate cache management into your CMS. This provides an easier interface to actions like this. This approach also lets the CMS invoke a CDN purge when things change, so is recommended if you need to tune things finer.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-flush-the-fastly-cache/484",
        "relurl": "/t/how-to-flush-the-fastly-cache/484"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "b36b50ada8f8f87e2781ab63f8b134cc299e82e0",
        "title": "How to Check my Fastly CDN peformance (Cache hit ratio)",
        "description": "Goal To review how effective the cache layer is, you may want to check the “Hit Ratio” of the content it is serving. A busy read-only site with its cache preferences properly configured can be seen to serve percentages in the high 90’s; while if you see a cache ratio down below 60%, the server is probably working pretty hard, and some things can be done - either at the app level, of just in the Response-Headers, to improve cache-ability and responsiveness. Assumptions You will need: A http://Platform.sh plan with managed Fastly cache enabled. This usually means an “Enterprise Dedicated” plan. Read access to your hosting server - the ability to ssh in and read a file there. This is best done by installing and using the platform CLI tool, though plain ssh work as well. Access to the curl tool. if you don’t have it installed locally, you can use it when ssh-ed in to your instance. Challenge For managed-CDN plans, http://Platform.sh looks after your Fastly subscriptions. Which is great, because cache management is hard. But this does mean that you don’t get direct access to some of the reporting tools that Fastly provides. While you may not get dashboard access, you do get to use the https://developer.fastly.com/reference/api/ with an access token that’s been attached to your account. Steps 1. SSH in to your production instance platform ssh --environment=production 2. Retrieve your API token This will have been added to your production instance, conventionally in the location /mnt/shared/fastly_tokens.txt $ cat /mnt/shared/fastly_tokens.txt This should return something like: Service: myprojectname Service ID: C8L8ykr4PCC65vQ8TK97td API Token: 74919c95e59ec799bc82641811e4e3b4 So, save that as variables: $ FASTLY_SERVICE_ID=\"C8L8ykr4PCC65vQ8TK97td\" $ FASTLY_API_TOKEN=\"74919c95e59ec799bc82641811e4e3b4\" 3. Use the Fastly API to ask for your statistics Historical stats can be retrieved from the https://developer.fastly.com/reference/api/metrics-stats/historical-stats/ curl \"https://api.fastly.com/stats/service/${FASTLY_SERVICE_ID}/field/hit_ratio?from=2+days+ago\" \\ -XGET \\ -H \"Fastly-Key: ${FASTLY_API_TOKEN}\" \\ -H \"Accept: Application/json\" \\ -k 4. Check the result It’s machine-readable, not so much human-readable, but: Hopefully you’ll see something like: { \"msg\" : null, \"status\" : \"success\", \"data\" : [ { \"service_id\" : \"C8L8ykr4PCC65vQ8TK97td\", \"hit_ratio\" : 0.818625522455654, \"start_time\" : 1589846900 } ], \"meta\" : { \"to\" : \"2020-05-22 12:21:00 UTC\", \"by\" : \"day\", \"from\" : \"2020-05-24 12:21:00 UTC\", \"region\" : \"all\" } } the hit_ratio there indicates that 81% of all requests were cacheable, and being returned by the CDN. Conclusion http://Platform.sh plans with managed Fastly CDN accounts do provide you access to an amount of management options, but you have to work through the API to get at them. This example gives you the recent “hit ratio”, but there are https://developer.fastly.com/reference/api/metrics-stats/historical-stats/ also, that can be retrieved in a similar way. See also https://community.platform.sh/t/how-to-flush-the-fastly-cache/484 ",
        "text": "Goal To review how effective the cache layer is, you may want to check the “Hit Ratio” of the content it is serving. A busy read-only site with its cache preferences properly configured can be seen to serve percentages in the high 90’s; while if you see a cache ratio down below 60%, the server is probably working pretty hard, and some things can be done - either at the app level, of just in the Response-Headers, to improve cache-ability and responsiveness. Assumptions You will need: A http://Platform.sh plan with managed Fastly cache enabled. This usually means an “Enterprise Dedicated” plan. Read access to your hosting server - the ability to ssh in and read a file there. This is best done by installing and using the platform CLI tool, though plain ssh work as well. Access to the curl tool. if you don’t have it installed locally, you can use it when ssh-ed in to your instance. Challenge For managed-CDN plans, http://Platform.sh looks after your Fastly subscriptions. Which is great, because cache management is hard. But this does mean that you don’t get direct access to some of the reporting tools that Fastly provides. While you may not get dashboard access, you do get to use the https://developer.fastly.com/reference/api/ with an access token that’s been attached to your account. Steps 1. SSH in to your production instance platform ssh --environment=production 2. Retrieve your API token This will have been added to your production instance, conventionally in the location /mnt/shared/fastly_tokens.txt $ cat /mnt/shared/fastly_tokens.txt This should return something like: Service: myprojectname Service ID: C8L8ykr4PCC65vQ8TK97td API Token: 74919c95e59ec799bc82641811e4e3b4 So, save that as variables: $ FASTLY_SERVICE_ID=\"C8L8ykr4PCC65vQ8TK97td\" $ FASTLY_API_TOKEN=\"74919c95e59ec799bc82641811e4e3b4\" 3. Use the Fastly API to ask for your statistics Historical stats can be retrieved from the https://developer.fastly.com/reference/api/metrics-stats/historical-stats/ curl \"https://api.fastly.com/stats/service/${FASTLY_SERVICE_ID}/field/hit_ratio?from=2+days+ago\" \\ -XGET \\ -H \"Fastly-Key: ${FASTLY_API_TOKEN}\" \\ -H \"Accept: Application/json\" \\ -k 4. Check the result It’s machine-readable, not so much human-readable, but: Hopefully you’ll see something like: { \"msg\" : null, \"status\" : \"success\", \"data\" : [ { \"service_id\" : \"C8L8ykr4PCC65vQ8TK97td\", \"hit_ratio\" : 0.818625522455654, \"start_time\" : 1589846900 } ], \"meta\" : { \"to\" : \"2020-05-22 12:21:00 UTC\", \"by\" : \"day\", \"from\" : \"2020-05-24 12:21:00 UTC\", \"region\" : \"all\" } } the hit_ratio there indicates that 81% of all requests were cacheable, and being returned by the CDN. Conclusion http://Platform.sh plans with managed Fastly CDN accounts do provide you access to an amount of management options, but you have to work through the API to get at them. This example gives you the recent “hit ratio”, but there are https://developer.fastly.com/reference/api/metrics-stats/historical-stats/ also, that can be retrieved in a similar way. See also https://community.platform.sh/t/how-to-flush-the-fastly-cache/484 ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-check-my-fastly-cdn-peformance-cache-hit-ratio/566",
        "relurl": "/t/how-to-check-my-fastly-cdn-peformance-cache-hit-ratio/566"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "8e59e86395060653ab9d1ba14659202021de2199",
        "title": "How to use the latest npm version",
        "description": "Goal NPM is a package manager for the JavaScript programming language. This guide will explain how to keep the latest version. Assumptions You will need: an active https://platform.sh/ project A text editor of your choice. Steps Explicitly, set the npm as the latest version as the code below shows. dependencies: nodejs: npm: latest References https://docs.platform.sh/languages/nodejs.html https://docs.platform.sh/languages/nodejs/nvm.html ",
        "text": "Goal NPM is a package manager for the JavaScript programming language. This guide will explain how to keep the latest version. Assumptions You will need: an active https://platform.sh/ project A text editor of your choice. Steps Explicitly, set the npm as the latest version as the code below shows. dependencies: nodejs: npm: latest References https://docs.platform.sh/languages/nodejs.html https://docs.platform.sh/languages/nodejs/nvm.html ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-use-the-latest-npm-version/562",
        "relurl": "/t/how-to-use-the-latest-npm-version/562"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "75c9cbef75a0b5e57b7cac8efc696438f8da11eb",
        "title": "How to access MySQL/MariaDB credentials on Platform.sh",
        "description": "Goal Access credentials for MySQL/MariaDB from within a http://Platform.sh application using Node.js. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/mysql.html .platform/services.yaml for the given service on account if developing locally MySQL installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html MySQL/MariaDB relationship like so, in .platform.app.yaml: relationships: database: \"db:mysql\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-nodejs Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-nodejs for minimum requirements. $ npm install platformsh-config --save 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. const config = require(\"platformsh-config\").config(); 3. Read the credentials // Get the credentials to connect to the MySQL/MariaDB service. const credentials = config.credentials('database'); Steps (Manual) 1. Load the environment variable relationships = JSON.parse(new Buffer(process.env['PLATFORM_RELATIONSHIPS'], 'base64').toString()); 2. Read the credentials const credentials = relationships['database']; Use path is the database name that will be needed to connect. Pass the needed properties to your MySQL library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is a an object matching the relationship JSON object, which includes the appropriate user, password, host, database name, and other pertinent information. { \"username\": \"user\", \"scheme\": \"mysql\", \"service\": \"db\", \"fragment\": null, \"ip\": \"169.254.147.122\", \"hostname\": \"czwb2d7zzunu67lh77infwkm6i.mysql.service._.eu-3.platformsh.site\", \"public\": false, \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"database.internal\", \"rel\": \"mysql\", \"query\": { \"is_master\": true }, \"path\": \"main\", \"password\": \"\", \"type\": \"mysql:10.2\", \"port\": 3306 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get MySQL/MariaDB credentials in Node.js. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-nodejs can be useful for inspecting the project environment: // Checks whether the code is running in a build environment config.inBuild() // Checks whether the code is running in a runtime environment config.inRuntime() and for reading environment variables as attributes of config: // Available in Build and at Runtime config.appDir; // Available at Runtime only config.branch; config.smtpHost; The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "text": "Goal Access credentials for MySQL/MariaDB from within a http://Platform.sh application using Node.js. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/mysql.html .platform/services.yaml for the given service on account if developing locally MySQL installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html MySQL/MariaDB relationship like so, in .platform.app.yaml: relationships: database: \"db:mysql\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-nodejs Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-nodejs for minimum requirements. $ npm install platformsh-config --save 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. const config = require(\"platformsh-config\").config(); 3. Read the credentials // Get the credentials to connect to the MySQL/MariaDB service. const credentials = config.credentials('database'); Steps (Manual) 1. Load the environment variable relationships = JSON.parse(new Buffer(process.env['PLATFORM_RELATIONSHIPS'], 'base64').toString()); 2. Read the credentials const credentials = relationships['database']; Use path is the database name that will be needed to connect. Pass the needed properties to your MySQL library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is a an object matching the relationship JSON object, which includes the appropriate user, password, host, database name, and other pertinent information. { \"username\": \"user\", \"scheme\": \"mysql\", \"service\": \"db\", \"fragment\": null, \"ip\": \"169.254.147.122\", \"hostname\": \"czwb2d7zzunu67lh77infwkm6i.mysql.service._.eu-3.platformsh.site\", \"public\": false, \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"database.internal\", \"rel\": \"mysql\", \"query\": { \"is_master\": true }, \"path\": \"main\", \"password\": \"\", \"type\": \"mysql:10.2\", \"port\": 3306 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get MySQL/MariaDB credentials in Node.js. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-nodejs can be useful for inspecting the project environment: // Checks whether the code is running in a build environment config.inBuild() // Checks whether the code is running in a runtime environment config.inRuntime() and for reading environment variables as attributes of config: // Available in Build and at Runtime config.appDir; // Available at Runtime only config.branch; config.smtpHost; The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-access-mysql-mariadb-credentials-on-platform-sh/149",
        "relurl": "/t/how-to-access-mysql-mariadb-credentials-on-platform-sh/149"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e4e26a648cd947662617792a323599f918c1f549",
        "title": "How to access Memcached credentials on Platform.sh",
        "description": "Goal Access credentials for Memcached from within a http://Platform.sh application using Node.js. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/memcached.html .platform/services.yaml for the given service on account if developing locally Memcached installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html Memcached relationship like so, in .platform.app.yaml: relationships: memcachedcache: \"cachemc:memcached\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-nodejs Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-nodejs for minimum requirements. $ npm install platformsh-config --save 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. const config = require(\"platformsh-config\").config(); 3. Read the credentials // Get the credentials to connect to the Memcached service. const credentials = config.credentials('memcachedcache'); Steps (Manual) 1. Load the environment variable relationships = JSON.parse(new Buffer(process.env['PLATFORM_RELATIONSHIPS'], 'base64').toString()); 2. Read the credentials credentials = relationships['memcachedcache']; Use In most cases, you will need only the host and port properties to connect to Memcached. Pass those to your Memecached library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is a an object matching the relationship JSON object, which includes the appropriate user, password, host, database name, and other pertinent information. { \"service\": \"cachemc\", \"ip\": \"169.254.192.212\", \"hostname\": \"tmimatsnz2dv26qlpuespenf3u.memcached.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"memcachedcache.internal\", \"rel\": \"memcached\", \"scheme\": \"memcached\", \"type\": \"memcached:1.4\", \"port\": 11211 } Conclusions: Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get Memcached credentials in Node.js. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-nodejs can be useful for inspecting the project environment: // Checks whether the code is running in a build environment config.inBuild() // Checks whether the code is running in a runtime environment config.inRuntime() and for reading environment variables as attributes of config: // Available in Build and at Runtime config.appDir; // Available at Runtime only config.branch; config.smtpHost; The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "text": "Goal Access credentials for Memcached from within a http://Platform.sh application using Node.js. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/memcached.html .platform/services.yaml for the given service on account if developing locally Memcached installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html Memcached relationship like so, in .platform.app.yaml: relationships: memcachedcache: \"cachemc:memcached\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-nodejs Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-nodejs for minimum requirements. $ npm install platformsh-config --save 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. const config = require(\"platformsh-config\").config(); 3. Read the credentials // Get the credentials to connect to the Memcached service. const credentials = config.credentials('memcachedcache'); Steps (Manual) 1. Load the environment variable relationships = JSON.parse(new Buffer(process.env['PLATFORM_RELATIONSHIPS'], 'base64').toString()); 2. Read the credentials credentials = relationships['memcachedcache']; Use In most cases, you will need only the host and port properties to connect to Memcached. Pass those to your Memecached library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is a an object matching the relationship JSON object, which includes the appropriate user, password, host, database name, and other pertinent information. { \"service\": \"cachemc\", \"ip\": \"169.254.192.212\", \"hostname\": \"tmimatsnz2dv26qlpuespenf3u.memcached.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"memcachedcache.internal\", \"rel\": \"memcached\", \"scheme\": \"memcached\", \"type\": \"memcached:1.4\", \"port\": 11211 } Conclusions: Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get Memcached credentials in Node.js. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-nodejs can be useful for inspecting the project environment: // Checks whether the code is running in a build environment config.inBuild() // Checks whether the code is running in a runtime environment config.inRuntime() and for reading environment variables as attributes of config: // Available in Build and at Runtime config.appDir; // Available at Runtime only config.branch; config.smtpHost; The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-access-memcached-credentials-on-platform-sh/147",
        "relurl": "/t/how-to-access-memcached-credentials-on-platform-sh/147"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "83a7ccb4fdbab0a788c48c32fb2f5b8861de7a60",
        "title": "How to access RabbitMQ credentials on Platform.sh",
        "description": "Goal: Access credentials for RabbitMQ from within a http://Platform.sh application using Python. Assumptions: an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/rabbitmq.html .platform/services.yaml for the given service on account if developing locally RabbitMQ installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html RabbitMQ relationship like so, in .platform.app.yaml: relationships: rabbitmqqueue: \"queuerabbit:rabbitmq\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-python Accessing environment variables manually Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-python for minimum requirements. $ pip install platformshconfig 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. from platformshconfig import Config config = Config() 3. Read the credentials # Get the credentials to connect to the RabbitMQ service. credentials = config.credentials('rabbitmqqueue') Steps (Manual) 1. Load the environment variable import os, json, base64 relationships = json.loads(base64.b64decode(os.environ[\"PLATFORM_RELATIONSHIPS\"])) 2. Read the credentials credentials = relationships['rabbitmqqueue'] Use In most cases, you will need only the host and port properties to connect to RabbitMQ. Pass those to your RabbitMQ library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is now a dictionary matching the relationship JSON object. { \"username\": \"guest\", \"scheme\": \"amqp\", \"service\": \"queuerabbit\", \"ip\": \"169.254.107.244\", \"hostname\": \"qmx5lbdjoolco7dqjr7cx26r7q.rabbitmq.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"rabbitmqqueue.internal\", \"rel\": \"rabbitmq\", \"password\": \"guest\", \"type\": \"rabbitmq:3.5\", \"port\": 5672 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get RabbitMQ credentials in Python. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-python can be useful for inspecting the project environment: # Checks whether the code is running in a build environment config.inBuild() # Checks whether the code is running in a runtime environment config.inRuntime() and for reading environment variables as attributes of config: # Available in Build and at Runtime config.appDir # Available at Runtime only config.branch config.smtpHost The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "text": "Goal: Access credentials for RabbitMQ from within a http://Platform.sh application using Python. Assumptions: an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/rabbitmq.html .platform/services.yaml for the given service on account if developing locally RabbitMQ installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html RabbitMQ relationship like so, in .platform.app.yaml: relationships: rabbitmqqueue: \"queuerabbit:rabbitmq\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-python Accessing environment variables manually Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-python for minimum requirements. $ pip install platformshconfig 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. from platformshconfig import Config config = Config() 3. Read the credentials # Get the credentials to connect to the RabbitMQ service. credentials = config.credentials('rabbitmqqueue') Steps (Manual) 1. Load the environment variable import os, json, base64 relationships = json.loads(base64.b64decode(os.environ[\"PLATFORM_RELATIONSHIPS\"])) 2. Read the credentials credentials = relationships['rabbitmqqueue'] Use In most cases, you will need only the host and port properties to connect to RabbitMQ. Pass those to your RabbitMQ library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is now a dictionary matching the relationship JSON object. { \"username\": \"guest\", \"scheme\": \"amqp\", \"service\": \"queuerabbit\", \"ip\": \"169.254.107.244\", \"hostname\": \"qmx5lbdjoolco7dqjr7cx26r7q.rabbitmq.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"rabbitmqqueue.internal\", \"rel\": \"rabbitmq\", \"password\": \"guest\", \"type\": \"rabbitmq:3.5\", \"port\": 5672 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get RabbitMQ credentials in Python. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-python can be useful for inspecting the project environment: # Checks whether the code is running in a build environment config.inBuild() # Checks whether the code is running in a runtime environment config.inRuntime() and for reading environment variables as attributes of config: # Available in Build and at Runtime config.appDir # Available at Runtime only config.branch config.smtpHost The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-access-rabbitmq-credentials-on-platform-sh/151",
        "relurl": "/t/how-to-access-rabbitmq-credentials-on-platform-sh/151"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "578d4e3ececc995c4be735729c380b7a4eb2461e",
        "title": "How to access Redis credentials on Platform.sh",
        "description": "Goal Access credentials for Redis from within a http://Platform.sh application using PHP. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/redis.html .platform/services.yaml for the given service on account if developing locally Redis installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html Redis relationship like so, in .platform.app.yaml: relationships: rediscache: \"cache:redis\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-php Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-php for minimum requirements. $ composer install platformsh/config-reader 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. use Platformsh\\ConfigReader\\Config; $config = new Config(); 3. Read the credentials // Get the credentials to connect to the Redis service. $credentials = $config-credentials('rediscache'); Steps (Manual) 1. Load the environment variable $relationships = json_decode(base64_decode(getenv('PLATFORM_RELATIONSHIPS')), TRUE); 2. Read the credentials $credentials = $relationships['rediscache']; Use In most cases, you will need only the host and port properties to connect to Redis. Pass those to your Redis library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is now an array matching the relationship JSON object. { \"username\": null, \"scheme\": \"redis\", \"service\": \"cache\", \"fragment\": null, \"ip\": \"169.254.22.205\", \"hostname\": \"2wye4dawv22vhvozyec3o5hxfm.redis.service._.eu-3.platformsh.site\", \"public\": false, \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"rediscache.internal\", \"rel\": \"redis\", \"query\": [ ], \"path\": null, \"password\": null, \"type\": \"redis:5.0\", \"port\": 6379 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get Redis credentials in PHP. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-php can be useful for inspecting the project environment: // Checks whether the code is running in a build environment $config-inBuild(); // Checks whether the code is running in a runtime environment $config-inRuntime(); and for reading environment variables as attributes of config: // Available in Build and at Runtime $config-appDir; // Available at Runtime only $config-branch; $config-smtpHost; The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "text": "Goal Access credentials for Redis from within a http://Platform.sh application using PHP. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/redis.html .platform/services.yaml for the given service on account if developing locally Redis installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html Redis relationship like so, in .platform.app.yaml: relationships: rediscache: \"cache:redis\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-php Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-php for minimum requirements. $ composer install platformsh/config-reader 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. use Platformsh\\ConfigReader\\Config; $config = new Config(); 3. Read the credentials // Get the credentials to connect to the Redis service. $credentials = $config-credentials('rediscache'); Steps (Manual) 1. Load the environment variable $relationships = json_decode(base64_decode(getenv('PLATFORM_RELATIONSHIPS')), TRUE); 2. Read the credentials $credentials = $relationships['rediscache']; Use In most cases, you will need only the host and port properties to connect to Redis. Pass those to your Redis library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is now an array matching the relationship JSON object. { \"username\": null, \"scheme\": \"redis\", \"service\": \"cache\", \"fragment\": null, \"ip\": \"169.254.22.205\", \"hostname\": \"2wye4dawv22vhvozyec3o5hxfm.redis.service._.eu-3.platformsh.site\", \"public\": false, \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"rediscache.internal\", \"rel\": \"redis\", \"query\": [ ], \"path\": null, \"password\": null, \"type\": \"redis:5.0\", \"port\": 6379 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get Redis credentials in PHP. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-php can be useful for inspecting the project environment: // Checks whether the code is running in a build environment $config-inBuild(); // Checks whether the code is running in a runtime environment $config-inRuntime(); and for reading environment variables as attributes of config: // Available in Build and at Runtime $config-appDir; // Available at Runtime only $config-branch; $config-smtpHost; The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-access-redis-credentials-on-platform-sh/152",
        "relurl": "/t/how-to-access-redis-credentials-on-platform-sh/152"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "4f91d2c8a0031e327247e117b949cf722dd93a57",
        "title": "How to access Elasticsearch credentials on Platform.sh",
        "description": "Goal Access credentials for Elasticsearch from within a http://Platform.sh application using Python. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/elasticsearch.html .platform/services.yaml for the given service on account if developing locally Elasticsearch installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html Elasticsearch relationship like so, in .platform.app.yaml: relationships: essearch: \"searchelastic:elasticsearch\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-python Accessing environment variables manually Steps (Config Reader) 1. Install the library Install the http://Platform.sh Configuration Reader library. See the https://github.com/platformsh/config-reader-python for minimum requirements. $ pip install platformshconfig 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. from platformshconfig import Config config = Config() 3. Read the credentials # Get the credentials to connect to the Elasticsearch service. credentials = config.credentials('essearch') If the relationship is named something else, modify the credentials() call accordingly. Steps (Manual) 1. Load and decode the environment variable import os, json, base64 relationships = json.loads(base64.b64decode(os.environ[\"PLATFORM_RELATIONSHIPS\"])) 2. Get the credentials credentials = relationships['essearch'] If the relationship is named something else, modify that line accordingly. Use In most cases, you will need only the scheme, host, and port properties to connect to Elasticsearch. Pass those to your Elasticsearch library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is now a dictionary matching the relationship JSON object. { \"username\": null, \"scheme\": \"http\", \"service\": \"searchelastic\", \"fragment\": null, \"ip\": \"169.254.101.149\", \"hostname\": \"j2dkzht3gs2yr66fb2brhoj4zu.elasticsearch.service._.eu-3.platformsh.site\", \"public\": false, \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"essearch.internal\", \"rel\": \"elasticsearch\", \"query\": [ ], \"path\": null, \"password\": null, \"type\": \"elasticsearch:5.4\", \"port\": 9200 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get Elasticsearch credentials in Python. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-python can be useful for inspecting the project environment: # Checks whether the code is running in a build environment config.inBuild() # Checks whether the code is running in a runtime environment config.inRuntime() and for reading environment variables as attributes of config: # Available in Build and at Runtime config.appDir # Available at Runtime only config.branch config.smtpHost The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "text": "Goal Access credentials for Elasticsearch from within a http://Platform.sh application using Python. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/elasticsearch.html .platform/services.yaml for the given service on account if developing locally Elasticsearch installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html Elasticsearch relationship like so, in .platform.app.yaml: relationships: essearch: \"searchelastic:elasticsearch\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-python Accessing environment variables manually Steps (Config Reader) 1. Install the library Install the http://Platform.sh Configuration Reader library. See the https://github.com/platformsh/config-reader-python for minimum requirements. $ pip install platformshconfig 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. from platformshconfig import Config config = Config() 3. Read the credentials # Get the credentials to connect to the Elasticsearch service. credentials = config.credentials('essearch') If the relationship is named something else, modify the credentials() call accordingly. Steps (Manual) 1. Load and decode the environment variable import os, json, base64 relationships = json.loads(base64.b64decode(os.environ[\"PLATFORM_RELATIONSHIPS\"])) 2. Get the credentials credentials = relationships['essearch'] If the relationship is named something else, modify that line accordingly. Use In most cases, you will need only the scheme, host, and port properties to connect to Elasticsearch. Pass those to your Elasticsearch library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is now a dictionary matching the relationship JSON object. { \"username\": null, \"scheme\": \"http\", \"service\": \"searchelastic\", \"fragment\": null, \"ip\": \"169.254.101.149\", \"hostname\": \"j2dkzht3gs2yr66fb2brhoj4zu.elasticsearch.service._.eu-3.platformsh.site\", \"public\": false, \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"essearch.internal\", \"rel\": \"elasticsearch\", \"query\": [ ], \"path\": null, \"password\": null, \"type\": \"elasticsearch:5.4\", \"port\": 9200 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get Elasticsearch credentials in Python. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-python can be useful for inspecting the project environment: # Checks whether the code is running in a build environment config.inBuild() # Checks whether the code is running in a runtime environment config.inRuntime() and for reading environment variables as attributes of config: # Available in Build and at Runtime config.appDir # Available at Runtime only config.branch config.smtpHost The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-access-elasticsearch-credentials-on-platform-sh/146",
        "relurl": "/t/how-to-access-elasticsearch-credentials-on-platform-sh/146"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "884bc41074079bab17248fe65e762901d3e6179d",
        "title": "How to access MongoDB credentials on Platform.sh",
        "description": "Goal Access credentials for MongoDB from within a http://Platform.sh application using Node.js. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/mongodb.html .platform/services.yaml for the given service on account if developing locally MongoDB installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html MongoDB relationship like so, in .platform.app.yaml: relationships: database: \"dbmongo:mongodb\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-nodejs Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-nodejs for minimum requirements. $ npm install platformsh-config --save 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. const config = require(\"platformsh-config\").config(); 3. Read the credentials // Get the credentials to connect to the MongoDB service. const credentials = config.credentials('database'); Steps (Manual) 1. Load and decode the environment variable relationships = JSON.parse(new Buffer(process.env['PLATFORM_RELATIONSHIPS'], 'base64').toString()); 2. Read the credentials credentials = relationships['database']; Use In most cases, you will need only the host and port properties to connect to MongoDB. Pass those to your MongoDB library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is a an object matching the relationship JSON object. { \"username\": \"main\", \"scheme\": \"mongodb\", \"service\": \"dbmongo\", \"ip\": \"169.254.78.213\", \"hostname\": \"okqb7mbggt4wbyiszla2zy4boq.mongodb.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"database.internal\", \"rel\": \"mongodb\", \"path\": \"main\", \"query\": { \"is_master\": true }, \"password\": \"main\", \"type\": \"mongodb:3.6\", \"port\": 27017 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get MongoDB credentials in Node.js. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-nodejs can be useful for connecting to MongoDB without creating the full DSN string using formattedCredentials: // Define formatted credentials var client = await MongoClient.connect(config.formattedCredentials('database', 'mongodb')); The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "text": "Goal Access credentials for MongoDB from within a http://Platform.sh application using Node.js. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/mongodb.html .platform/services.yaml for the given service on account if developing locally MongoDB installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html MongoDB relationship like so, in .platform.app.yaml: relationships: database: \"dbmongo:mongodb\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-nodejs Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-nodejs for minimum requirements. $ npm install platformsh-config --save 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. const config = require(\"platformsh-config\").config(); 3. Read the credentials // Get the credentials to connect to the MongoDB service. const credentials = config.credentials('database'); Steps (Manual) 1. Load and decode the environment variable relationships = JSON.parse(new Buffer(process.env['PLATFORM_RELATIONSHIPS'], 'base64').toString()); 2. Read the credentials credentials = relationships['database']; Use In most cases, you will need only the host and port properties to connect to MongoDB. Pass those to your MongoDB library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is a an object matching the relationship JSON object. { \"username\": \"main\", \"scheme\": \"mongodb\", \"service\": \"dbmongo\", \"ip\": \"169.254.78.213\", \"hostname\": \"okqb7mbggt4wbyiszla2zy4boq.mongodb.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"database.internal\", \"rel\": \"mongodb\", \"path\": \"main\", \"query\": { \"is_master\": true }, \"password\": \"main\", \"type\": \"mongodb:3.6\", \"port\": 27017 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get MongoDB credentials in Node.js. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-nodejs can be useful for connecting to MongoDB without creating the full DSN string using formattedCredentials: // Define formatted credentials var client = await MongoClient.connect(config.formattedCredentials('database', 'mongodb')); The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-access-mongodb-credentials-on-platform-sh/145",
        "relurl": "/t/how-to-access-mongodb-credentials-on-platform-sh/145"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "ddc9f9bafab94adf778bb07f7759c42990245dce",
        "title": "How to access PostgreSQL credentials on Platform.sh",
        "description": "Goal: Access credentials for PostgreSQL from within a http://Platform.sh application using PHP. Assumptions: an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/postgresql.html .platform/services.yaml for the given service on account if developing locally PostgreSQL installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html PostgreSQL relationship like so, in .platform.app.yaml: relationships: database: \"dbpostgres:postgresql\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-php Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-php for minimum requirements. $ composer install platformsh/config-reader 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. use Platformsh\\ConfigReader\\Config; $config = new Config(); 3. Read the credentials // Get the credentials to connect to the PostgreSQL service. $credentials = $config-credentials('database'); Steps (Manual) 1. Load and decode the environment variable $relationships = json_decode(base64_decode(getenv('PLATFORM_RELATIONSHIPS')), TRUE); 2. Read the credentials $credentials = $relationships['database']; Use: path is the database name that will be needed to connect. Pass the needed properties to your PostgreSQL library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is now an object matching the relationship JSON object. { \"username\": \"main\", \"scheme\": \"pgsql\", \"service\": \"dbpostgres\", \"ip\": \"169.254.231.161\", \"hostname\": \"tkydopuhmksuhglz7ox4x4de6m.postgresql.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"database.internal\", \"rel\": \"postgresql\", \"path\": \"main\", \"query\": { \"is_master\": true }, \"password\": \"main\", \"type\": \"postgresql:11\", \"port\": 5432 } Conclusions: Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get PostgreSQL credentials in PHP. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-php can be useful for connecting to PostgreSQL without creating the full DSN string using FormattedCredentials: // Define formatted credentials $formatted = $config-FormattedCredentials(\"database\", \"pdo_pgsql\") The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "text": "Goal: Access credentials for PostgreSQL from within a http://Platform.sh application using PHP. Assumptions: an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/postgresql.html .platform/services.yaml for the given service on account if developing locally PostgreSQL installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html PostgreSQL relationship like so, in .platform.app.yaml: relationships: database: \"dbpostgres:postgresql\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-php Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-php for minimum requirements. $ composer install platformsh/config-reader 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. use Platformsh\\ConfigReader\\Config; $config = new Config(); 3. Read the credentials // Get the credentials to connect to the PostgreSQL service. $credentials = $config-credentials('database'); Steps (Manual) 1. Load and decode the environment variable $relationships = json_decode(base64_decode(getenv('PLATFORM_RELATIONSHIPS')), TRUE); 2. Read the credentials $credentials = $relationships['database']; Use: path is the database name that will be needed to connect. Pass the needed properties to your PostgreSQL library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is now an object matching the relationship JSON object. { \"username\": \"main\", \"scheme\": \"pgsql\", \"service\": \"dbpostgres\", \"ip\": \"169.254.231.161\", \"hostname\": \"tkydopuhmksuhglz7ox4x4de6m.postgresql.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"database.internal\", \"rel\": \"postgresql\", \"path\": \"main\", \"query\": { \"is_master\": true }, \"password\": \"main\", \"type\": \"postgresql:11\", \"port\": 5432 } Conclusions: Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get PostgreSQL credentials in PHP. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-php can be useful for connecting to PostgreSQL without creating the full DSN string using FormattedCredentials: // Define formatted credentials $formatted = $config-FormattedCredentials(\"database\", \"pdo_pgsql\") The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-access-postgresql-credentials-on-platform-sh/150",
        "relurl": "/t/how-to-access-postgresql-credentials-on-platform-sh/150"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "a45103b61ac17e5cfda8e6f66e015164e27b2d03",
        "title": "How to access Solr credentials on Platform.sh",
        "description": "Goal Access credentials for Solr from within a http://Platform.sh application using Python. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/solr.html .platform/services.yaml for the given service on account if developing locally Solr installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html Solr relationship like so, in .platform.app.yaml: relationships: solrsearch: \"searchsolr:solr\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-python Accessing environment variables manually Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-python for minimum requirements. $ pip install platformshconfig 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. from platformshconfig import Config config = Config() 3. Read the credentials # Get the credentials to connect to the Solr service. credentials = config.credentials('solrsearch') Steps (Manual) 1. Load the environment variable import os, json, base64 relationships = json.loads(base64.b64decode(os.environ[\"PLATFORM_RELATIONSHIPS\"])) 2. Read the credentials credentials = relationships['solrsearch'] Use In most cases, you will need only the host and port properties to connect to Solr. Pass those to your Solr library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is a dictionary matching the relationship JSON object. { \"service\": \"searchsolr\", \"ip\": \"169.254.103.29\", \"hostname\": \"swhnppdg3ugcd5sktkga73wjoi.solr.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"solrsearh.internal\", \"rel\": \"solr\", \"path\": \"solr\\/collection1\", \"scheme\": \"solr\", \"type\": \"solr:7.6\", \"port\": 8080 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get Solr credentials in Python. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-python can be useful for connecting to Solr without creating the full DSN string using FormattedCredentials: # Define formatted credentials formatted_url = config.formatted_credentials('solrsearch', 'pysolr') The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "text": "Goal Access credentials for Solr from within a http://Platform.sh application using Python. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/solr.html .platform/services.yaml for the given service on account if developing locally Solr installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html Solr relationship like so, in .platform.app.yaml: relationships: solrsearch: \"searchsolr:solr\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-python Accessing environment variables manually Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-python for minimum requirements. $ pip install platformshconfig 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. from platformshconfig import Config config = Config() 3. Read the credentials # Get the credentials to connect to the Solr service. credentials = config.credentials('solrsearch') Steps (Manual) 1. Load the environment variable import os, json, base64 relationships = json.loads(base64.b64decode(os.environ[\"PLATFORM_RELATIONSHIPS\"])) 2. Read the credentials credentials = relationships['solrsearch'] Use In most cases, you will need only the host and port properties to connect to Solr. Pass those to your Solr library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is a dictionary matching the relationship JSON object. { \"service\": \"searchsolr\", \"ip\": \"169.254.103.29\", \"hostname\": \"swhnppdg3ugcd5sktkga73wjoi.solr.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"solrsearh.internal\", \"rel\": \"solr\", \"path\": \"solr\\/collection1\", \"scheme\": \"solr\", \"type\": \"solr:7.6\", \"port\": 8080 } Conclusions Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get Solr credentials in Python. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-python can be useful for connecting to Solr without creating the full DSN string using FormattedCredentials: # Define formatted credentials formatted_url = config.formatted_credentials('solrsearch', 'pysolr') The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-access-solr-credentials-on-platform-sh/153",
        "relurl": "/t/how-to-access-solr-credentials-on-platform-sh/153"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "eaf0b7ad65f79cd63b7b9491f8b0f0ad53ff964a",
        "title": "How to Configure Load Balancer in a multiple applications",
        "description": "Goal This guide will explain how to configure a simple load balancing setup in a Single Application at http://Platform.sh. The critical point is that this Load balancer configuration happens transparently, that is, the application does not know about the LB. For that to work, it is very important that your application follows good practices. For example, ensure that your application is stateless. https://thenewstack.io/best-practices-for-developing-cloud-native-applications-and-microservice-architectures/ is a good practice for having your application in the cloud. One of the advantages of stateless services is that you can bring up multiple instances of your application. Since there’s no per-instance state, any instance can handle any request. This is a natural fit for load balancers, which can be leveraged to help scale the service. They’re also readily available on cloud platforms. Assumptions You either have java Applications, and you want to run at http://platform.sh or you already have a Java application running at http://platform.sh A text editor of your choice. Steps https://community.platform.sh/t/how-to-configure-multi-applications-with-applications-yaml/552 Given that you already have a multi-application running in with .platform/applications.yaml The next step is “copy/paste” the information that you want the replicated machine. In this sample, we’ll only overwrite the name. - \u0026appdef name: app type: 'java:8' disk: 1024 source: root: app hooks: build: mvn clean install mounts: 'server/': source: local source_path: server_source web: commands: start: | cp target/dependency/webapp-runner.jar server/webapp-runner.jar cp target/tomcat.war server/tomcat.war cd server \u0026\u0026 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError webapp-runner.jar --port $PORT tomcat.war - ",
        "text": "Goal This guide will explain how to configure a simple load balancing setup in a Single Application at http://Platform.sh. The critical point is that this Load balancer configuration happens transparently, that is, the application does not know about the LB. For that to work, it is very important that your application follows good practices. For example, ensure that your application is stateless. https://thenewstack.io/best-practices-for-developing-cloud-native-applications-and-microservice-architectures/ is a good practice for having your application in the cloud. One of the advantages of stateless services is that you can bring up multiple instances of your application. Since there’s no per-instance state, any instance can handle any request. This is a natural fit for load balancers, which can be leveraged to help scale the service. They’re also readily available on cloud platforms. Assumptions You either have java Applications, and you want to run at http://platform.sh or you already have a Java application running at http://platform.sh A text editor of your choice. Steps https://community.platform.sh/t/how-to-configure-multi-applications-with-applications-yaml/552 Given that you already have a multi-application running in with .platform/applications.yaml The next step is “copy/paste” the information that you want the replicated machine. In this sample, we’ll only overwrite the name. - \u0026appdef name: app type: 'java:8' disk: 1024 source: root: app hooks: build: mvn clean install mounts: 'server/': source: local source_path: server_source web: commands: start: | cp target/dependency/webapp-runner.jar server/webapp-runner.jar cp target/tomcat.war server/tomcat.war cd server \u0026\u0026 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError webapp-runner.jar --port $PORT tomcat.war - ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-configure-load-balancer-in-a-multiple-applications/554",
        "relurl": "/t/how-to-configure-load-balancer-in-a-multiple-applications/554"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "ef183595513b2be194048ce8bf3e715d36d83ad3",
        "title": "How to Configure Load Balancer in a Single Application",
        "description": "Goal This guide will explain how to configure a simple load balancing setup in a Single Application at http://Platform.sh. The critical point is that this Load balancer configuration happens transparently, that is, the application does not know about the LB. For that to work, it is very important that your application follows good practices. For example, ensure that your application is stateless. https://thenewstack.io/best-practices-for-developing-cloud-native-applications-and-microservice-architectures/ is a good practice for having your application in the cloud. One of the advantages of stateless services is that you can bring up multiple instances of your application. Since there’s no per-instance state, any instance can handle any request. This is a natural fit for load balancers, which can be leveraged to help scale the service. They’re also readily available on cloud platforms. What is load balancing? At a basic level, load balancing works to distribute web traffic requests among different servers to ensure high availability and optimal traffic management while avoiding overload of any one server and defending against denial of service attacks. Load balancers increase capacity and reliability. How does the Load Balancer work at http://Platform.sh? To make a Load balancer possible, http://Platform.sh provides https://docs.platform.sh/configuration/services/varnish.html, an HTTP accelerator designed for content-heavy dynamic web sites as well as APIs. In Varnish you will find three different methods (“directors”) for load balancing: round robin fallback random Assumptions You either have an application, and you want to run at http://platform.sh or you already have an application running at http://platform.sh A text editor of your choice. The example below uses a Java application, but any stateless application in any language should work the same. Steps 1. Use network storage Because this configuration involves multiple application instances, they will each have their own local storage. To share files between applications you must use a https://docs.platform.sh/configuration/services/network-storage.html service. See the documentation for specific instructions. 2. Define the application Define your application in the https://docs.platform.sh/configuration/app/multi-app.html file, rather than in .platform.app.yaml as usual. The syntax is the same, but represented as a YAML array. The application should also be defined as a . - \u0026appdef name: app type: 'java:8' disk: 1024 source: root: / hooks: build: mvn clean install mounts: 'server/': source: local source_path: server_source web: commands: start: | cp target/dependency/webapp-runner.jar server/webapp-runner.jar cp target/tomcat.war server/tomcat.war cd server \u0026\u0026 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError webapp-runner.jar --port $PORT tomcat.war Next, add a second definition in the same file by aliasing the first definition. Add the following lines to applications.yaml: - ",
        "text": "Goal This guide will explain how to configure a simple load balancing setup in a Single Application at http://Platform.sh. The critical point is that this Load balancer configuration happens transparently, that is, the application does not know about the LB. For that to work, it is very important that your application follows good practices. For example, ensure that your application is stateless. https://thenewstack.io/best-practices-for-developing-cloud-native-applications-and-microservice-architectures/ is a good practice for having your application in the cloud. One of the advantages of stateless services is that you can bring up multiple instances of your application. Since there’s no per-instance state, any instance can handle any request. This is a natural fit for load balancers, which can be leveraged to help scale the service. They’re also readily available on cloud platforms. What is load balancing? At a basic level, load balancing works to distribute web traffic requests among different servers to ensure high availability and optimal traffic management while avoiding overload of any one server and defending against denial of service attacks. Load balancers increase capacity and reliability. How does the Load Balancer work at http://Platform.sh? To make a Load balancer possible, http://Platform.sh provides https://docs.platform.sh/configuration/services/varnish.html, an HTTP accelerator designed for content-heavy dynamic web sites as well as APIs. In Varnish you will find three different methods (“directors”) for load balancing: round robin fallback random Assumptions You either have an application, and you want to run at http://platform.sh or you already have an application running at http://platform.sh A text editor of your choice. The example below uses a Java application, but any stateless application in any language should work the same. Steps 1. Use network storage Because this configuration involves multiple application instances, they will each have their own local storage. To share files between applications you must use a https://docs.platform.sh/configuration/services/network-storage.html service. See the documentation for specific instructions. 2. Define the application Define your application in the https://docs.platform.sh/configuration/app/multi-app.html file, rather than in .platform.app.yaml as usual. The syntax is the same, but represented as a YAML array. The application should also be defined as a . - \u0026appdef name: app type: 'java:8' disk: 1024 source: root: / hooks: build: mvn clean install mounts: 'server/': source: local source_path: server_source web: commands: start: | cp target/dependency/webapp-runner.jar server/webapp-runner.jar cp target/tomcat.war server/tomcat.war cd server \u0026\u0026 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError webapp-runner.jar --port $PORT tomcat.war Next, add a second definition in the same file by aliasing the first definition. Add the following lines to applications.yaml: - ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-configure-load-balancer-in-a-single-application/553",
        "relurl": "/t/how-to-configure-load-balancer-in-a-single-application/553"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "2eb87e9ab2fc0e657b26e8ed97c05458e7004548",
        "title": "How to configure multi-applications with applications.yaml",
        "description": "Goal This “how to” guide will explain about create multiple applications with applications.yaml file with Java. Assumptions You either have a Java application, and you want to run at http://platform.sh or you already have a Java application running at http://platform.sh A text editor of your choice. Steps Create a .platform/applications.yml file at the root directory of your project, this file will have the each application’s configuration. In this file, we’re going to use YAML’s built-in “anchors” to share configuration typically found in a .platform.app.yaml file between multiple applications. When we talk about YAML achor there are two important points: The anchor ‘\u0026’ which defines a chunk of configuration The alias ‘*’ used to refer to that chunk elsewhere In the code below, we define the anchorappdef that contains the settings of the first application, app, and we use the alias for the second application, app2. That becomes the basis of the first application, which we can then overwrite with information, such as the app’s unique name. - \u0026appdef name: app type: 'java:8' disk: 1024 source: root: app hooks: build: mvn clean install mounts: 'server/': source: local source_path: server_source web: commands: start: | cp target/dependency/webapp-runner.jar server/webapp-runner.jar cp target/tomcat.war server/tomcat.war cd server \u0026\u0026 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError webapp-runner.jar --port $PORT tomcat.war - ",
        "text": "Goal This “how to” guide will explain about create multiple applications with applications.yaml file with Java. Assumptions You either have a Java application, and you want to run at http://platform.sh or you already have a Java application running at http://platform.sh A text editor of your choice. Steps Create a .platform/applications.yml file at the root directory of your project, this file will have the each application’s configuration. In this file, we’re going to use YAML’s built-in “anchors” to share configuration typically found in a .platform.app.yaml file between multiple applications. When we talk about YAML achor there are two important points: The anchor ‘\u0026’ which defines a chunk of configuration The alias ‘*’ used to refer to that chunk elsewhere In the code below, we define the anchorappdef that contains the settings of the first application, app, and we use the alias for the second application, app2. That becomes the basis of the first application, which we can then overwrite with information, such as the app’s unique name. - \u0026appdef name: app type: 'java:8' disk: 1024 source: root: app hooks: build: mvn clean install mounts: 'server/': source: local source_path: server_source web: commands: start: | cp target/dependency/webapp-runner.jar server/webapp-runner.jar cp target/tomcat.war server/tomcat.war cd server \u0026\u0026 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError webapp-runner.jar --port $PORT tomcat.war - ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-configure-multi-applications-with-applications-yaml/552",
        "relurl": "/t/how-to-configure-multi-applications-with-applications-yaml/552"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "565c67842415ac7af245608af259d2a0df191268",
        "title": "How to Migrate my Java Application to Platform.sh",
        "description": "Goal This tutorial will cover how to migrate your Java application to http://Platform.sh. Preparation This tutorial assumes you have A working Java 8 or higher application A text editor of your choice. Problems The Java Application is ready, and now the team wants to migrate to http://Platform.sh and enjoy the benefits of cloud computing naturally. Steps https://docs.platform.sh/ https://docs.platform.sh/gettingstarted/own-code/create-project.html Create https://docs.platform.sh/configuration/routes.html , https://docs.platform.sh/configuration/services.html and https://docs.platform.sh/configuration/app-containers.html . You can integrate to existing , , or just use the remote repository that http://Platform.sh will provide to you. Furthermore, we also have those guides to help you to migrate with your Spring or Jakarta EE application: https://community.platform.sh/t/how-to-overwrite-spring-data-variable-to-access-platform-sh-services/518 https://community.platform.sh/t/how-to-overwrite-spring-data-mongodb-variable-to-access-platform-sh-services/528 https://community.platform.sh/t/how-to-overwrite-variables-to-payara-jpa-access-platform-sh-sql-services/519 https://community.platform.sh/t/how-to-overwrite-variables-to-payara-jpa-access-platform-sh-sql-services/519 https://community.platform.sh/t/how-to-migrate-your-java-application-from-heroku-to-platform-sh/441 References https://community.platform.sh/t/tips-about-java-commands-to-run-your-application-at-platform-sh/531 https://platform.sh/blog/2019/java-hello-world-at-platform.sh/ https://github.com/platformsh/config-reader-java https://docs.platform.sh/languages/java.html https://docs.platform.sh/languages/java/frameworks.html https://github.com/platformsh/java-quick-start ",
        "text": "Goal This tutorial will cover how to migrate your Java application to http://Platform.sh. Preparation This tutorial assumes you have A working Java 8 or higher application A text editor of your choice. Problems The Java Application is ready, and now the team wants to migrate to http://Platform.sh and enjoy the benefits of cloud computing naturally. Steps https://docs.platform.sh/ https://docs.platform.sh/gettingstarted/own-code/create-project.html Create https://docs.platform.sh/configuration/routes.html , https://docs.platform.sh/configuration/services.html and https://docs.platform.sh/configuration/app-containers.html . You can integrate to existing , , or just use the remote repository that http://Platform.sh will provide to you. Furthermore, we also have those guides to help you to migrate with your Spring or Jakarta EE application: https://community.platform.sh/t/how-to-overwrite-spring-data-variable-to-access-platform-sh-services/518 https://community.platform.sh/t/how-to-overwrite-spring-data-mongodb-variable-to-access-platform-sh-services/528 https://community.platform.sh/t/how-to-overwrite-variables-to-payara-jpa-access-platform-sh-sql-services/519 https://community.platform.sh/t/how-to-overwrite-variables-to-payara-jpa-access-platform-sh-sql-services/519 https://community.platform.sh/t/how-to-migrate-your-java-application-from-heroku-to-platform-sh/441 References https://community.platform.sh/t/tips-about-java-commands-to-run-your-application-at-platform-sh/531 https://platform.sh/blog/2019/java-hello-world-at-platform.sh/ https://github.com/platformsh/config-reader-java https://docs.platform.sh/languages/java.html https://docs.platform.sh/languages/java/frameworks.html https://github.com/platformsh/java-quick-start ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-migrate-my-java-application-to-platform-sh/529",
        "relurl": "/t/how-to-migrate-my-java-application-to-platform-sh/529"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "b10571b969602f927f2b07117ccd443adaf6d72c",
        "title": "How to use the Platform.sh CLI from the web container",
        "description": "Goal Have the http://Platform.SH CLI installed and authenticated in the web container. Assumptions Access to a project hosted on http://Platform.sh Your project account has administrator rights Knowledge on using the project web interface or Problems Using the http://Platform.sh CLI directly from the web (application) container can be required to automate certain tasks, like renewing the SSL certificate or triggering a snapshot based on a cron schedule. Due to the non-interactive deployment flow, using a regular username and password authentication is not possible. We will use token-based authentication to have the CLI installed and configured on each deployment. Steps 0. Create a dedicated account to use for automated tasks This step is optional, but _ strongly recommended_: for automated tasks, a dedicated user should be created and added to the project. Adding an API token to a project is a security risk, as it means the token will be visible to the other project members who have SSH access, so they will then also have access to the token’s account. 1. Create an API token Log in and navigate to Account settings API tokens, available https://accounts.platform.sh/user/api-tokens. Click on Create API token. https://community.platform.sh/uploads/default/718bd2e55a0aa3257ee8850e654a66b6af264b75 You will be asked for the token name - enter a name to easily identify your token in the future, in case of multiple tokens (CLI automated is one example). Click on Create API token to save the token. https://community.platform.sh/uploads/default/8da6eff40eec38ba02da0bc02f7cd149e49bfc03 Once done, the newly created token will be displayed at the top of the page, and can be copied to the clipboard using the Copy button. After this, you will not be able to view the API token again. https://community.platform.sh/uploads/default/9c567702a32fc496e8c7e4d3b8ea0cc5e7546b09 2. Add the token as an environment variable in the project Once the API token is added as an environment variable in the project, it will be automatically detected and used by the CLI tool. After the installation is done, you should see the PLATFORMSH_CLI_TOKEN variable when running env. Option 1: Using the web interface Open the project web interface and navigate to the environment in which you want to use the CLI, then click on Configure environment and go to the Variables tab. Add a variable named env:PLATFORMSH_CLI_TOKEN and set its value to the previously created token. https://community.platform.sh/uploads/default/49b4472ba7c3fd557210a40fe39e4ca89a75e902 Option 2: Using the CLI If you have the CLI tool installed and configured, you can add the variable from the command line: platform variable:create -e --level environment --name env:PLATFORMSH_CLI_TOKEN --sensitive true --value Replace the and with the correct values for your use case. 3. Install the CLI tool in the application container Once the variable has been added to the environment, it is required to download and install the CLI tool in a build hook. Modify .platform.app.yaml in the project to include: hooks: build: | curl -sS https://platform.sh/cli/installer | php This will download the CLI to a known directory, .platformsh/bin , which will be added to the PATH at runtime (via the .environment file). Redeploy your environment and log into the application container with SSH, then run platform. You should see the welcome prompt and a list of your current projects. Conclusion Having the CLI tool set up in an environment opens up further automation possibilities - renewing the SSL certificate, triggering snapshots, etc.",
        "text": "Goal Have the http://Platform.SH CLI installed and authenticated in the web container. Assumptions Access to a project hosted on http://Platform.sh Your project account has administrator rights Knowledge on using the project web interface or Problems Using the http://Platform.sh CLI directly from the web (application) container can be required to automate certain tasks, like renewing the SSL certificate or triggering a snapshot based on a cron schedule. Due to the non-interactive deployment flow, using a regular username and password authentication is not possible. We will use token-based authentication to have the CLI installed and configured on each deployment. Steps 0. Create a dedicated account to use for automated tasks This step is optional, but _ strongly recommended_: for automated tasks, a dedicated user should be created and added to the project. Adding an API token to a project is a security risk, as it means the token will be visible to the other project members who have SSH access, so they will then also have access to the token’s account. 1. Create an API token Log in and navigate to Account settings API tokens, available https://accounts.platform.sh/user/api-tokens. Click on Create API token. https://community.platform.sh/uploads/default/718bd2e55a0aa3257ee8850e654a66b6af264b75 You will be asked for the token name - enter a name to easily identify your token in the future, in case of multiple tokens (CLI automated is one example). Click on Create API token to save the token. https://community.platform.sh/uploads/default/8da6eff40eec38ba02da0bc02f7cd149e49bfc03 Once done, the newly created token will be displayed at the top of the page, and can be copied to the clipboard using the Copy button. After this, you will not be able to view the API token again. https://community.platform.sh/uploads/default/9c567702a32fc496e8c7e4d3b8ea0cc5e7546b09 2. Add the token as an environment variable in the project Once the API token is added as an environment variable in the project, it will be automatically detected and used by the CLI tool. After the installation is done, you should see the PLATFORMSH_CLI_TOKEN variable when running env. Option 1: Using the web interface Open the project web interface and navigate to the environment in which you want to use the CLI, then click on Configure environment and go to the Variables tab. Add a variable named env:PLATFORMSH_CLI_TOKEN and set its value to the previously created token. https://community.platform.sh/uploads/default/49b4472ba7c3fd557210a40fe39e4ca89a75e902 Option 2: Using the CLI If you have the CLI tool installed and configured, you can add the variable from the command line: platform variable:create -e --level environment --name env:PLATFORMSH_CLI_TOKEN --sensitive true --value Replace the and with the correct values for your use case. 3. Install the CLI tool in the application container Once the variable has been added to the environment, it is required to download and install the CLI tool in a build hook. Modify .platform.app.yaml in the project to include: hooks: build: | curl -sS https://platform.sh/cli/installer | php This will download the CLI to a known directory, .platformsh/bin , which will be added to the PATH at runtime (via the .environment file). Redeploy your environment and log into the application container with SSH, then run platform. You should see the welcome prompt and a list of your current projects. Conclusion Having the CLI tool set up in an environment opens up further automation possibilities - renewing the SSL certificate, triggering snapshots, etc.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-use-the-platform-sh-cli-from-the-web-container/126",
        "relurl": "/t/how-to-use-the-platform-sh-cli-from-the-web-container/126"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e31b14ceb71374402b78c16df608ead85d567d75",
        "title": "How to make a Vue.js single page application (SPA) on Platform.sh",
        "description": "Goal This guide shows how to deploy a https://vuejs.org/ application on http://Platform.sh, using https://cli.vuejs.org/ . Assumptions To go through this guide, you will need: An empty http://Platform.sh project (you can https://accounts.platform.sh/platform/trial/general/setup to start your free trial) Your SSH key loaded in your https://linux.die.net/man/1/ssh-agent and configured in the https://accounts.platform.sh/user The https://github.com/platformsh/platformsh-cli installed. The https://cli.vuejs.org/guide/installation.html installed. Steps 1. Create a Vue.js application Create a new Vue.js project with Vue CLI (ignore this step if you want to deploy an existing Vue.js application on http://Platform.sh): vue create vuejs-platformsh cd vuejs-platformsh Set the platform Git remote: platform project:set-remote Add the https://npmjs.com/package/vue-cli-plugin-platformsh : vue add platformsh This plugin will add the http://Platform.sh configuration files to your project and extract the http://Platform.sh environment variables. 2. Deploy your application to http://Platform.sh Commit and push your code to deploy your application: git add --all git commit -m \"Vue.js on Platform.sh.\" git push platform master 3. Test your application on http://Platform.sh platform url This opens a browser tab with your Vue.js application running. 4. Fetch http://Platform.sh environment variables The plugin will automatically extract the http://Platform.sh environment variables. To fetch those, you simply need to import the following package: import platformshVar from 'platformsh_variables' Conclusion Using the Vue CLI, it’s very easy to setup and deploy a new or an existing Vue.js application on http://Platform.sh.",
        "text": "Goal This guide shows how to deploy a https://vuejs.org/ application on http://Platform.sh, using https://cli.vuejs.org/ . Assumptions To go through this guide, you will need: An empty http://Platform.sh project (you can https://accounts.platform.sh/platform/trial/general/setup to start your free trial) Your SSH key loaded in your https://linux.die.net/man/1/ssh-agent and configured in the https://accounts.platform.sh/user The https://github.com/platformsh/platformsh-cli installed. The https://cli.vuejs.org/guide/installation.html installed. Steps 1. Create a Vue.js application Create a new Vue.js project with Vue CLI (ignore this step if you want to deploy an existing Vue.js application on http://Platform.sh): vue create vuejs-platformsh cd vuejs-platformsh Set the platform Git remote: platform project:set-remote Add the https://npmjs.com/package/vue-cli-plugin-platformsh : vue add platformsh This plugin will add the http://Platform.sh configuration files to your project and extract the http://Platform.sh environment variables. 2. Deploy your application to http://Platform.sh Commit and push your code to deploy your application: git add --all git commit -m \"Vue.js on Platform.sh.\" git push platform master 3. Test your application on http://Platform.sh platform url This opens a browser tab with your Vue.js application running. 4. Fetch http://Platform.sh environment variables The plugin will automatically extract the http://Platform.sh environment variables. To fetch those, you simply need to import the following package: import platformshVar from 'platformsh_variables' Conclusion Using the Vue CLI, it’s very easy to setup and deploy a new or an existing Vue.js application on http://Platform.sh.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-make-a-vue-js-single-page-application-spa-on-platform-sh/125",
        "relurl": "/t/how-to-make-a-vue-js-single-page-application-spa-on-platform-sh/125"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "5e5b7c74d6538b26edc10e307d3c05ed914960a7",
        "title": "How to monitor long running scripts with pv",
        "description": "Goal Sometimes you want to run a long running process without feedback. https://linux.die.net/man/1/pv is a tool that allows you to do just that. Example Running a mysqldump mysqldump | pv --progress --size 100m /tmp/dump.sql Importing a mysql dump pv dump.sql | mysql Copying a large file pv /originallocation/largefile.bin /otherlocation/largefile.bin Create a zip file pv largefile.txt | zip largefile.zip Create a tar archive tar -czf - ./foldertotar | (pv -p --timer --rate --bytes backup.tgz) Problem Sounds cool, but pv isn’t available by default. Solution Luckily, there is an easy solution. Simply add the pv package in your .platform.app.yaml file. This will automatically install pv. So you can use it. dependencies: nodejs: # Specify one NPM package per line. pv: '~1.0.1' Note: you can do this in any container type. PHP, nodejs, golang, … This works because there is a https://github.com/roccomuso/pv that does the heavy lifting for us. For more information on dependencies, check ",
        "text": "Goal Sometimes you want to run a long running process without feedback. https://linux.die.net/man/1/pv is a tool that allows you to do just that. Example Running a mysqldump mysqldump | pv --progress --size 100m /tmp/dump.sql Importing a mysql dump pv dump.sql | mysql Copying a large file pv /originallocation/largefile.bin /otherlocation/largefile.bin Create a zip file pv largefile.txt | zip largefile.zip Create a tar archive tar -czf - ./foldertotar | (pv -p --timer --rate --bytes backup.tgz) Problem Sounds cool, but pv isn’t available by default. Solution Luckily, there is an easy solution. Simply add the pv package in your .platform.app.yaml file. This will automatically install pv. So you can use it. dependencies: nodejs: # Specify one NPM package per line. pv: '~1.0.1' Note: you can do this in any container type. PHP, nodejs, golang, … This works because there is a https://github.com/roccomuso/pv that does the heavy lifting for us. For more information on dependencies, check ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-monitor-long-running-scripts-with-pv/537",
        "relurl": "/t/how-to-monitor-long-running-scripts-with-pv/537"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "ed74c8a025b1e01a544851e27045c6002576f27b",
        "title": "How to configure Java with New Relic At Platform.sh",
        "description": "Goal This tutorial will explain how to configure New Relic into a Java application at http://Platform.sh. Assumptions You either have a Java application, and you want to run at http://Platform.sh or you already have a Java application running at http://Platform.sh A text editor of your choice. Problems New Relic is a technology company that develops cloud-based software to help website and application owners track the performance of their services. That might be useful because you can track everything from performance issues to tiny errors within your code. Every minute the agent posts metric time slice and event data to the New Relic user interface, where the owner of that data can sign in and use the data to see how their website is performing. Steps To set up new-relic in the Java project, we have two ways: Using the maven project Download the code through application.app.yaml. Maven This section explains how to configure Maven to download and unzip the newrelic-java.zip file, which contains all New Relic Java agent components. To set up the application with New Relic, you have two options: To set up the application with New Relic, you have two options: Configuring and download from Maven Downloading on your own Configure your pom.xml to download newrelic-java.zip. For example: com.newrelic.agent.java JAVA_AGENT_VERSION provided zip Replace JAVA_AGENT_VERSION with the https://docs.newrelic.com/docs/agents/java-agent/getting-started/java-release-notes . Unzip newrelic-java.zip by configuring maven-dependency-plugin in your pom.xml. For example: org.apache.maven.plugins 3.1.1 unpack-newrelic package unpack-dependencies com.newrelic.agent.java newrelic-java **/newrelic.yml false false true ${project.build.directory} The next step is to configure the https://docs.platform.sh/configuration/app-containers.html file to: Set the agent in the JVM parameters Overwrite the application file with the proper license key and application name. You can also do it using the https://docs.platform.sh/development/variables.html or . Therefore this configuration will work outside the code very useful when the application is on a public repository. name: app type: 'java:8' disk: 1024 hooks: build: | mvn clean package rm -rf newrelic/ mv target/newrelic/ newrelic/ mounts: 'server/': source: local source_path: server_source variables: env: NEW_RELIC_LICENSE_KEY: NEW_RELIC_APP_NAME: web: commands: start: | cp target/dependency/webapp-runner.jar server/webapp-runner.jar cp target/tomcat.war server/tomcat.war java -jar \\ -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError \\ -javaagent:/app/newrelic/newrelic.jar //the other parameters here Manual installation To use this installation it is only required that you modify .platform.app.yaml , which will download and set the New Relic Java agent for you. name: app type: 'java:8' disk: 1024 variables: env: NEW_RELIC_URL: https://download.newrelic.com/newrelic/java-agent/newrelic-agent/current/newrelic-java.zip NEW_RELIC_LICENSE_KEY: NEW_RELIC_APP_NAME: hooks: build: | mvn clean package rm -rf newrelic curl -O $NEW_RELIC_URL unzip newrelic-java.zip web: commands: start: | java -jar \\ -Xmx$(jq .info.limits.memory /run/config.json)m \\ -XX:+ExitOnOutOfMemoryError \\ -javaagent:/app/newrelic/newrelic.jar //the left of the commands here References https://newrelic.com/ https://docs.newrelic.com/docs/agents/java-agent/configuration/java-agent-configuration-config-file https://docs.newrelic.com/docs/agents/java-agent/additional-installation/install-java-agent-using-maven https://docs.newrelic.com/docs/agents/java-agent/installation/install-java-agent https://github.com/platformsh-examples/new-relic-java ",
        "text": "Goal This tutorial will explain how to configure New Relic into a Java application at http://Platform.sh. Assumptions You either have a Java application, and you want to run at http://Platform.sh or you already have a Java application running at http://Platform.sh A text editor of your choice. Problems New Relic is a technology company that develops cloud-based software to help website and application owners track the performance of their services. That might be useful because you can track everything from performance issues to tiny errors within your code. Every minute the agent posts metric time slice and event data to the New Relic user interface, where the owner of that data can sign in and use the data to see how their website is performing. Steps To set up new-relic in the Java project, we have two ways: Using the maven project Download the code through application.app.yaml. Maven This section explains how to configure Maven to download and unzip the newrelic-java.zip file, which contains all New Relic Java agent components. To set up the application with New Relic, you have two options: To set up the application with New Relic, you have two options: Configuring and download from Maven Downloading on your own Configure your pom.xml to download newrelic-java.zip. For example: com.newrelic.agent.java JAVA_AGENT_VERSION provided zip Replace JAVA_AGENT_VERSION with the https://docs.newrelic.com/docs/agents/java-agent/getting-started/java-release-notes . Unzip newrelic-java.zip by configuring maven-dependency-plugin in your pom.xml. For example: org.apache.maven.plugins 3.1.1 unpack-newrelic package unpack-dependencies com.newrelic.agent.java newrelic-java **/newrelic.yml false false true ${project.build.directory} The next step is to configure the https://docs.platform.sh/configuration/app-containers.html file to: Set the agent in the JVM parameters Overwrite the application file with the proper license key and application name. You can also do it using the https://docs.platform.sh/development/variables.html or . Therefore this configuration will work outside the code very useful when the application is on a public repository. name: app type: 'java:8' disk: 1024 hooks: build: | mvn clean package rm -rf newrelic/ mv target/newrelic/ newrelic/ mounts: 'server/': source: local source_path: server_source variables: env: NEW_RELIC_LICENSE_KEY: NEW_RELIC_APP_NAME: web: commands: start: | cp target/dependency/webapp-runner.jar server/webapp-runner.jar cp target/tomcat.war server/tomcat.war java -jar \\ -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError \\ -javaagent:/app/newrelic/newrelic.jar //the other parameters here Manual installation To use this installation it is only required that you modify .platform.app.yaml , which will download and set the New Relic Java agent for you. name: app type: 'java:8' disk: 1024 variables: env: NEW_RELIC_URL: https://download.newrelic.com/newrelic/java-agent/newrelic-agent/current/newrelic-java.zip NEW_RELIC_LICENSE_KEY: NEW_RELIC_APP_NAME: hooks: build: | mvn clean package rm -rf newrelic curl -O $NEW_RELIC_URL unzip newrelic-java.zip web: commands: start: | java -jar \\ -Xmx$(jq .info.limits.memory /run/config.json)m \\ -XX:+ExitOnOutOfMemoryError \\ -javaagent:/app/newrelic/newrelic.jar //the left of the commands here References https://newrelic.com/ https://docs.newrelic.com/docs/agents/java-agent/configuration/java-agent-configuration-config-file https://docs.newrelic.com/docs/agents/java-agent/additional-installation/install-java-agent-using-maven https://docs.newrelic.com/docs/agents/java-agent/installation/install-java-agent https://github.com/platformsh-examples/new-relic-java ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-configure-java-with-new-relic-at-platform-sh/533",
        "relurl": "/t/how-to-configure-java-with-new-relic-at-platform-sh/533"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "76a4d76b36e0fd06b9aed49d73de38907faba264",
        "title": "Improving GraphQL performance by caching POST requests",
        "description": "At http://Platform.sh we recently came across a project that utilized GraphQL to pull in data for its front-end, but was experiencing performance issues with slow GraphQL queries. We found a way to conveniently cache those query results and greatly speed up the project as a result. Queries to GraphQL are normally done as POST requests, and caching the results of POST requests is typically bad practice. In our use case, some queries are taking upwards of 5s to execute, which results in a rather poor user experience. Combining caching with a short TTL, (say, 60 seconds) the slightly-outdated content is preferable to an incredibly slow user experience. We start at https://fastly.com, and create a custom CDN endpoint to the GraphQL URI. This passes all requests (to that new endpoint) through the Fastly service, which uses Varnish to cache requests. Fastly allows the user to upload a custom VCL, using the as a starting point. The only change we need to make is to ensure that POST requests are treated in the same manner as GET requests. This results in two small changes: The block if (req.method != \"HEAD\" \u0026\u0026 req.method != \"GET\" \u0026\u0026 req.method != \"FASTLYPURGE\") { return(pass); } … should be changed to: if (req.method != \"HEAD\" \u0026\u0026 req.method != \"GET\" \u0026\u0026 req.method != \"POST\" \u0026\u0026 req.method != \"FASTLYPURGE\") { return(pass); } And similarly, the block if ((beresp.status == 500 || beresp.status == 503) \u0026\u0026 req.restarts ",
        "text": "At http://Platform.sh we recently came across a project that utilized GraphQL to pull in data for its front-end, but was experiencing performance issues with slow GraphQL queries. We found a way to conveniently cache those query results and greatly speed up the project as a result. Queries to GraphQL are normally done as POST requests, and caching the results of POST requests is typically bad practice. In our use case, some queries are taking upwards of 5s to execute, which results in a rather poor user experience. Combining caching with a short TTL, (say, 60 seconds) the slightly-outdated content is preferable to an incredibly slow user experience. We start at https://fastly.com, and create a custom CDN endpoint to the GraphQL URI. This passes all requests (to that new endpoint) through the Fastly service, which uses Varnish to cache requests. Fastly allows the user to upload a custom VCL, using the as a starting point. The only change we need to make is to ensure that POST requests are treated in the same manner as GET requests. This results in two small changes: The block if (req.method != \"HEAD\" \u0026\u0026 req.method != \"GET\" \u0026\u0026 req.method != \"FASTLYPURGE\") { return(pass); } … should be changed to: if (req.method != \"HEAD\" \u0026\u0026 req.method != \"GET\" \u0026\u0026 req.method != \"POST\" \u0026\u0026 req.method != \"FASTLYPURGE\") { return(pass); } And similarly, the block if ((beresp.status == 500 || beresp.status == 503) \u0026\u0026 req.restarts ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/improving-graphql-performance-by-caching-post-requests/530",
        "relurl": "/t/improving-graphql-performance-by-caching-post-requests/530"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "b7e514098b82f3c01c55e3005005cf8dcab4b94e",
        "title": "How to Overwrite Spring Data variable to access Platform.sh services",
        "description": "Goal In this tutorial, we’ll cover how you can overwrite https://spring.io/ configurations to access the services in http://Platform.sh. Preparation This tutorial assumes you have A working Spring application with Java 8 or higher A text editor of your choice. Problems http://Platform.sh has a https://github.com/platformsh/config-reader-java library that provides a streamlined and easy to interact with a http://Platform.sh environment and its https://docs.platform.sh/configuration/services.html. However, you can also use the application regularly and overwrite those configurations when you deploy your application on http://Platform.sh. That is useful when you either already have one app and want to move to http://Platform.sh or keep the default configuration to run locally. Steps To keep the configurations Spring has by default, the application.properties file is where you can set the settings that you wish on your application. Furthermore, . E.g.: Give a Spring Data JPA application that you’re running locally PostgreSQL with those properties on the applications.properties: ## default connection pool spring.datasource.hikari.connectionTimeout=20000 spring.datasource.hikari.maximumPoolSize=5 ## PostgreSQL spring.datasource.url=jdbc:postgresql://localhost:5432/people spring.datasource.username=postgres spring.datasource.password=password spring.jpa.hibernate.ddl-auto=update You can overwrite those configurations on the platform.app.yaml, the application configuration file for http://Platform.sh: name: app type: \"java:11\" disk: 1024 hooks: build: mvn clean install relationships: database: \"dbpostgres:postgresql\" web: commands: start: | export DB_PORT=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].port\"` export USER=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].username\"` export PASSWORD=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].password\"` export HOST=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].host\"` export DATABASE=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].path\"` export URL=\"jdbc:postgresql://${HOST}:${DB_PORT}/${DATABASE}\" java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError \\ -Dspring.datasource.url=$URL \\ -Dspring.datasource.username=$USER \\ -Dspring.datasource.password=$PASSWORD \\ target/spring-boot.jar --server.port=$PORT Therefore, you can have the configuration or just migrate the application that already exists to http://Platform.sh. References https://spring.io/ https://docs.spring.io/spring-boot/docs/current/reference/html/appendix-application-properties.html https://docs.platform.sh/frameworks/spring.html https://github.com/platformsh-examples/java-overwrite-configuration/tree/master/spring-jpa ",
        "text": "Goal In this tutorial, we’ll cover how you can overwrite https://spring.io/ configurations to access the services in http://Platform.sh. Preparation This tutorial assumes you have A working Spring application with Java 8 or higher A text editor of your choice. Problems http://Platform.sh has a https://github.com/platformsh/config-reader-java library that provides a streamlined and easy to interact with a http://Platform.sh environment and its https://docs.platform.sh/configuration/services.html. However, you can also use the application regularly and overwrite those configurations when you deploy your application on http://Platform.sh. That is useful when you either already have one app and want to move to http://Platform.sh or keep the default configuration to run locally. Steps To keep the configurations Spring has by default, the application.properties file is where you can set the settings that you wish on your application. Furthermore, . E.g.: Give a Spring Data JPA application that you’re running locally PostgreSQL with those properties on the applications.properties: ## default connection pool spring.datasource.hikari.connectionTimeout=20000 spring.datasource.hikari.maximumPoolSize=5 ## PostgreSQL spring.datasource.url=jdbc:postgresql://localhost:5432/people spring.datasource.username=postgres spring.datasource.password=password spring.jpa.hibernate.ddl-auto=update You can overwrite those configurations on the platform.app.yaml, the application configuration file for http://Platform.sh: name: app type: \"java:11\" disk: 1024 hooks: build: mvn clean install relationships: database: \"dbpostgres:postgresql\" web: commands: start: | export DB_PORT=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].port\"` export USER=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].username\"` export PASSWORD=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].password\"` export HOST=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].host\"` export DATABASE=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].path\"` export URL=\"jdbc:postgresql://${HOST}:${DB_PORT}/${DATABASE}\" java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError \\ -Dspring.datasource.url=$URL \\ -Dspring.datasource.username=$USER \\ -Dspring.datasource.password=$PASSWORD \\ target/spring-boot.jar --server.port=$PORT Therefore, you can have the configuration or just migrate the application that already exists to http://Platform.sh. References https://spring.io/ https://docs.spring.io/spring-boot/docs/current/reference/html/appendix-application-properties.html https://docs.platform.sh/frameworks/spring.html https://github.com/platformsh-examples/java-overwrite-configuration/tree/master/spring-jpa ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-overwrite-spring-data-variable-to-access-platform-sh-services/518",
        "relurl": "/t/how-to-overwrite-spring-data-variable-to-access-platform-sh-services/518"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "b1004e7b9065e8b447509de310e6ddf03e199021",
        "title": "How to Migrate your Java 8 application to Java 11 on Platform.sh",
        "description": "Goal In this tutorial, we’ll cover the how and why of upgrading a Java 8 project to Java 11 on http://Platform.sh. Preparation This tutorial assumes you have A working Java 8 application already deployed on http://Platform.sh. A text editor of your choice. Problems Java 8, the old Java LTS release, is no longer maintained. As an unmaintained version it will no longer receive security fixes, and so over time there will be more and more known-but-unfixed security issues found with it. Java 11 is the new LTS release, and is both more secure and more performant. https://twitter.com/leonardopanga ’s https://medium.com/criciumadev/its-time-migrating-to-java-11-5eb3868354f9 covers the major new features, including: Next LTS version Full support for containers Support parallel full garbage collection on G1. Free Application Class-Data Sharing feature. Heap allocation on alternative memory devices. New default set of root authority certificates. New https://wiki.openjdk.java.net/display/zgc/Main and http://openjdk.java.net/jeps/318 garbage collectors. Ahead-of-time compilation and GraalVM. Transport Layer Security (TLS) 1.3. http://openjdk.java.net/jeps/254 http://openjdk.java.net/jeps/248 Several benchmarks on the JVM improvements https://www.optaplanner.org/blog/2019/01/17/HowMuchFasterIsJava11.html . Furthermore, code written for Java 8 doesn’t need to be updated to run on the Java 11 JVM, https://caff.de/posts/java-11-vs-8-performance/ . To keep your code working you need to be aware of https://openjdk.java.net/jeps/320 . If you are using those modules you’ll need to re-add them in your dependency management tool. See . Steps To update the application, you need to update a single file: the .platform.app.yaml file. On a development branch (not production), find the type property: name: app type: \"java:8\" Update this line to Java 11: name: app type: \"java:11\" Commit the changes and push. http://Platform.sh will then build a new container using Java 11 instead of Java 8. The new version will be used for both the build process (compilation) and the running environment. Once you’re satisfied that the update is safe to complete, merge this new branch to master and push. References https://docs.platform.sh/languages/java.html ",
        "text": "Goal In this tutorial, we’ll cover the how and why of upgrading a Java 8 project to Java 11 on http://Platform.sh. Preparation This tutorial assumes you have A working Java 8 application already deployed on http://Platform.sh. A text editor of your choice. Problems Java 8, the old Java LTS release, is no longer maintained. As an unmaintained version it will no longer receive security fixes, and so over time there will be more and more known-but-unfixed security issues found with it. Java 11 is the new LTS release, and is both more secure and more performant. https://twitter.com/leonardopanga ’s https://medium.com/criciumadev/its-time-migrating-to-java-11-5eb3868354f9 covers the major new features, including: Next LTS version Full support for containers Support parallel full garbage collection on G1. Free Application Class-Data Sharing feature. Heap allocation on alternative memory devices. New default set of root authority certificates. New https://wiki.openjdk.java.net/display/zgc/Main and http://openjdk.java.net/jeps/318 garbage collectors. Ahead-of-time compilation and GraalVM. Transport Layer Security (TLS) 1.3. http://openjdk.java.net/jeps/254 http://openjdk.java.net/jeps/248 Several benchmarks on the JVM improvements https://www.optaplanner.org/blog/2019/01/17/HowMuchFasterIsJava11.html . Furthermore, code written for Java 8 doesn’t need to be updated to run on the Java 11 JVM, https://caff.de/posts/java-11-vs-8-performance/ . To keep your code working you need to be aware of https://openjdk.java.net/jeps/320 . If you are using those modules you’ll need to re-add them in your dependency management tool. See . Steps To update the application, you need to update a single file: the .platform.app.yaml file. On a development branch (not production), find the type property: name: app type: \"java:8\" Update this line to Java 11: name: app type: \"java:11\" Commit the changes and push. http://Platform.sh will then build a new container using Java 11 instead of Java 8. The new version will be used for both the build process (compilation) and the running environment. Once you’re satisfied that the update is safe to complete, merge this new branch to master and push. References https://docs.platform.sh/languages/java.html ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-migrate-your-java-8-application-to-java-11-on-platform-sh/499",
        "relurl": "/t/how-to-migrate-your-java-8-application-to-java-11-on-platform-sh/499"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "4f41fc199792a82171e62fa0f8a345bca5b641e4",
        "title": "How to Overwrite variables to Payara JPA access Platform.sh SQL services",
        "description": "Goal In this tutorial, we’ll cover how you can overwrite https://www.payara.fish/ configurations to access the services in http://Platform.sh. Preparation This tutorial assumes you have A working Payara application with Java 8 or higher A text editor of your choice. Problems http://Platform.sh has a https://github.com/platformsh/config-reader-java library that provides a streamlined and easy to use way to interact with a http://Platform.sh environment and https://docs.platform.sh/configuration/services.html . However, you can also use the application regularly and overwrite those configurations when you deploy your application on http://Platform.sh. That is useful when you either already have one app and want to move to http://Platform.sh or keep the default configuration to run locally. Steps First, to use a database as a data source in Payara Micro, you’ll need to have the database already running. Once you have that in place, download the JDBC driver for the database and put it into your WEB-INF/lib directory. In your web.xml add the following: java:global/JPAExampleDataSource org.postgresql.ds.PGSimpleDataSource ${server.host} 5432 ${server.database} ${server.user} ${server.password} You can overwrite those configurations on the platform.app.yaml the application configuration to http://Platform.sh. As shown in the configuration below. name: app type: \"java:8\" disk: 1024 hooks: build: mvn clean package payara-micro:bundle relationships: database: \"db:postgresql\" web: commands: start: | export HOST=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].host\"` export PASSWORD=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].password\"` export USER=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].username\"` export DATABASE=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].path\"` java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError \\ -Dserver.host=$HOST \\ -Dserver.database=$DATABASE \\ -Dserver.user=$USER \\ -Dserver.password=$PASSWORD \\ target/microprofile-microbundle.jar --port $PORT Therefore, you can have the configuration or just migrate the application that already exists to http://Platform.sh. References https://www.payara.fish/ https://github.com/payara/Payara-Examples/tree/master/payara-micro/jpa-datasource-example https://blog.payara.fish/setting-up-a-data-source-in-payara-micro https://docs.platform.sh/frameworks/jakarta.html https://github.com/platformsh-examples/java-overwrite-configuration/blob/master/payara/README.md ",
        "text": "Goal In this tutorial, we’ll cover how you can overwrite https://www.payara.fish/ configurations to access the services in http://Platform.sh. Preparation This tutorial assumes you have A working Payara application with Java 8 or higher A text editor of your choice. Problems http://Platform.sh has a https://github.com/platformsh/config-reader-java library that provides a streamlined and easy to use way to interact with a http://Platform.sh environment and https://docs.platform.sh/configuration/services.html . However, you can also use the application regularly and overwrite those configurations when you deploy your application on http://Platform.sh. That is useful when you either already have one app and want to move to http://Platform.sh or keep the default configuration to run locally. Steps First, to use a database as a data source in Payara Micro, you’ll need to have the database already running. Once you have that in place, download the JDBC driver for the database and put it into your WEB-INF/lib directory. In your web.xml add the following: java:global/JPAExampleDataSource org.postgresql.ds.PGSimpleDataSource ${server.host} 5432 ${server.database} ${server.user} ${server.password} You can overwrite those configurations on the platform.app.yaml the application configuration to http://Platform.sh. As shown in the configuration below. name: app type: \"java:8\" disk: 1024 hooks: build: mvn clean package payara-micro:bundle relationships: database: \"db:postgresql\" web: commands: start: | export HOST=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].host\"` export PASSWORD=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].password\"` export USER=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].username\"` export DATABASE=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".database[0].path\"` java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError \\ -Dserver.host=$HOST \\ -Dserver.database=$DATABASE \\ -Dserver.user=$USER \\ -Dserver.password=$PASSWORD \\ target/microprofile-microbundle.jar --port $PORT Therefore, you can have the configuration or just migrate the application that already exists to http://Platform.sh. References https://www.payara.fish/ https://github.com/payara/Payara-Examples/tree/master/payara-micro/jpa-datasource-example https://blog.payara.fish/setting-up-a-data-source-in-payara-micro https://docs.platform.sh/frameworks/jakarta.html https://github.com/platformsh-examples/java-overwrite-configuration/blob/master/payara/README.md ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-overwrite-variables-to-payara-jpa-access-platform-sh-sql-services/519",
        "relurl": "/t/how-to-overwrite-variables-to-payara-jpa-access-platform-sh-sql-services/519"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "6e5e4cf509e1096d0f6721cbc9fd394f3cca2465",
        "title": "How to overwrite configuration to Jakarta/MicroProfile to access Platform.sh services",
        "description": "Goal In this tutorial, we’ll cover how you can overwrite Jakarta EE/MicroProfile configurations to access the services in http://Platform.sh. Preparation This tutorial assumes you have: A working Jakarta EE/MicroProfile application with Java 8 or higher A text editor of your choice. Problems http://Platform.sh has a https://github.com/platformsh/config-reader-java library that provides a streamlined and easy to interact with a http://Platform.sh environment and its https://docs.platform.sh/configuration/services.html. However, you can also use the application regularly and overwrite those configurations when you deploy your application on http://Platform.sh. That is useful when you either already have one app and want to move to http://Platform.sh or keep the default configuration to run locally. Steps For https://microprofile.io// https://jakarta.ee/ the configurations and settings are set by https://github.com/eclipse/microprofile-config . It is as a convention and not as standard, which by default it uses the microprofile-config.properties file where you can set the settings and overwrite it. To have a https://projects.eclipse.org/projects/ee4j.nosql application that connects a https://www.mongodb.com/ database locally, the configuration will be like this: document=document document.database=conferences document.settings.jakarta.nosql.host=localhost:27017 document.provider=org.eclipse.jnosql.diana.mongodb.document.MongoDBDocumentConfiguration You can overwrite those configurations in platform.app.yaml, the application configuration file for http://Platform.sh: name: app type: \"java:8\" disk: 800 hooks: build: mvn -U -DskipTests clean package payara-micro:bundle relationships: mongodb: 'mongodb:mongodb' web: commands: start: | export MONGO_PORT=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".mongodb[0].port\"` export MONGO_HOST=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".mongodb[0].host\"` export MONGO_ADDRESS=\"${MONGO_HOST}:${MONGO_PORT}\" export MONGO_PASSWORD=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".mongodb[0].password\"` export MONGO_USER=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".mongodb[0].username\"` export MONGO_DATABASE=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".mongodb[0].path\"` java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -Ddocument.settings.jakarta.nosql.host=$MONGO_ADDRESS \\ -Ddocument.database=$MONGO_DATABASE -Ddocument.settings.jakarta.nosql.user=$MONGO_USER \\ -Ddocument.settings.jakarta.nosql.password=$MONGO_PASSWORD \\ -Ddocument.settings.mongodb.authentication.source=$MONGO_DATABASE \\ target/heroes-microbundle.jar --port $PORT Therefore, you can have the configuration or just migrate the application that already exists to http://Platform.sh. References https://jakarta.ee/ https://microprofile.io/ https://github.com/eclipse/microprofile-config https://www.jnosql.org/ https://docs.platform.sh/frameworks/jakarta.html https://github.com/platformsh-examples/java-overwrite-configuration/tree/master/jakarta-nosql ",
        "text": "Goal In this tutorial, we’ll cover how you can overwrite Jakarta EE/MicroProfile configurations to access the services in http://Platform.sh. Preparation This tutorial assumes you have: A working Jakarta EE/MicroProfile application with Java 8 or higher A text editor of your choice. Problems http://Platform.sh has a https://github.com/platformsh/config-reader-java library that provides a streamlined and easy to interact with a http://Platform.sh environment and its https://docs.platform.sh/configuration/services.html. However, you can also use the application regularly and overwrite those configurations when you deploy your application on http://Platform.sh. That is useful when you either already have one app and want to move to http://Platform.sh or keep the default configuration to run locally. Steps For https://microprofile.io// https://jakarta.ee/ the configurations and settings are set by https://github.com/eclipse/microprofile-config . It is as a convention and not as standard, which by default it uses the microprofile-config.properties file where you can set the settings and overwrite it. To have a https://projects.eclipse.org/projects/ee4j.nosql application that connects a https://www.mongodb.com/ database locally, the configuration will be like this: document=document document.database=conferences document.settings.jakarta.nosql.host=localhost:27017 document.provider=org.eclipse.jnosql.diana.mongodb.document.MongoDBDocumentConfiguration You can overwrite those configurations in platform.app.yaml, the application configuration file for http://Platform.sh: name: app type: \"java:8\" disk: 800 hooks: build: mvn -U -DskipTests clean package payara-micro:bundle relationships: mongodb: 'mongodb:mongodb' web: commands: start: | export MONGO_PORT=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".mongodb[0].port\"` export MONGO_HOST=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".mongodb[0].host\"` export MONGO_ADDRESS=\"${MONGO_HOST}:${MONGO_PORT}\" export MONGO_PASSWORD=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".mongodb[0].password\"` export MONGO_USER=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".mongodb[0].username\"` export MONGO_DATABASE=`echo $PLATFORM_RELATIONSHIPS|base64 -d|jq -r \".mongodb[0].path\"` java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -Ddocument.settings.jakarta.nosql.host=$MONGO_ADDRESS \\ -Ddocument.database=$MONGO_DATABASE -Ddocument.settings.jakarta.nosql.user=$MONGO_USER \\ -Ddocument.settings.jakarta.nosql.password=$MONGO_PASSWORD \\ -Ddocument.settings.mongodb.authentication.source=$MONGO_DATABASE \\ target/heroes-microbundle.jar --port $PORT Therefore, you can have the configuration or just migrate the application that already exists to http://Platform.sh. References https://jakarta.ee/ https://microprofile.io/ https://github.com/eclipse/microprofile-config https://www.jnosql.org/ https://docs.platform.sh/frameworks/jakarta.html https://github.com/platformsh-examples/java-overwrite-configuration/tree/master/jakarta-nosql ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-overwrite-configuration-to-jakarta-microprofile-to-access-platform-sh-services/520",
        "relurl": "/t/how-to-overwrite-configuration-to-jakarta-microprofile-to-access-platform-sh-services/520"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "01dec78a8c614f5c91236814c7bd2b7a58d94e48",
        "title": "How to Configure your Java Application with Log4J at Platform.sh",
        "description": "Goal In this tutorial, we’ll cover the how and why to use log in your application using https://logging.apache.org/log4j/2.x/ http://platform.sh. Preparation This tutorial assumes you have A working Java 8 application already deployed on http://platform.sh. A text editor of your choice. Problems Logging is the process of writing log messages during the execution of a program to a central place. This logging allows you to report and persist error and warning messages as well as info messages (e.g., runtime statistics) so that the messages can later be retrieved and analyzed. Steps 1. http://Platform.sh log file In http://Platform.sh we have two options to log your Java application. The first one is to use the stdout where http://Platform.sh will handle the folder it includes to avoid the oversized in the disk issue. This log will be at the /var/log/app.log: https://docs.platform.sh/development/logs.html In your log4j.properties you can set it, e.g.: log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.Target=System.out log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n All log files are trimmed to 100 MB automatically. But if you need to have complete logs, you can set up cron which will upload them to third-party storage. https://www.contextualcode.com/ made a https://gitlab.com/contextualcode/platformsh-store-logs-at-s3 how to achieve it. 2. The custom log folder The second option is to create a new mount directory and then use it to handle your logs. The advantage is that we can create multiple records as you wish; however, you will then be responsible for purging old log files to avoid filling up the disk. In your log4j.properties you can set, e.g.: log4j.rootLogger=DEBUG, R log4j.appender.R=org.apache.log4j.RollingFileAppender log4j.appender.R.MaxFileSize=1MB log4j.appender.R.MaxBackupIndex=20 log4j.appender.R.File=/app/log/app.log log4j.appender.R.Append=true log4j.appender.R.DatePattern='.'yyyy-MM-dd'.log' log4j.appender.R.layout=org.apache.log4j.PatternLayout log4j.appender.R.layout.ConversionPattern=%d{MM/dd/yyyy HH:mm:ss,SSS} %-5p %c - %m%n This time we need to create a to create your log, so we need to go in the https://docs.platform.sh/configuration/app-containers.html to define a writable folder to the log. mounts: 'log/': source: local source_path: log_source Commit the changes and push to redeploy. References https://dzone.com/articles/logging-with-log4j-in-java https://www.vogella.com/tutorials/Logging/article.html https://docs.platform.sh/languages/java.html https://docs.platform.sh/development/logs.html ",
        "text": "Goal In this tutorial, we’ll cover the how and why to use log in your application using https://logging.apache.org/log4j/2.x/ http://platform.sh. Preparation This tutorial assumes you have A working Java 8 application already deployed on http://platform.sh. A text editor of your choice. Problems Logging is the process of writing log messages during the execution of a program to a central place. This logging allows you to report and persist error and warning messages as well as info messages (e.g., runtime statistics) so that the messages can later be retrieved and analyzed. Steps 1. http://Platform.sh log file In http://Platform.sh we have two options to log your Java application. The first one is to use the stdout where http://Platform.sh will handle the folder it includes to avoid the oversized in the disk issue. This log will be at the /var/log/app.log: https://docs.platform.sh/development/logs.html In your log4j.properties you can set it, e.g.: log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.Target=System.out log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n All log files are trimmed to 100 MB automatically. But if you need to have complete logs, you can set up cron which will upload them to third-party storage. https://www.contextualcode.com/ made a https://gitlab.com/contextualcode/platformsh-store-logs-at-s3 how to achieve it. 2. The custom log folder The second option is to create a new mount directory and then use it to handle your logs. The advantage is that we can create multiple records as you wish; however, you will then be responsible for purging old log files to avoid filling up the disk. In your log4j.properties you can set, e.g.: log4j.rootLogger=DEBUG, R log4j.appender.R=org.apache.log4j.RollingFileAppender log4j.appender.R.MaxFileSize=1MB log4j.appender.R.MaxBackupIndex=20 log4j.appender.R.File=/app/log/app.log log4j.appender.R.Append=true log4j.appender.R.DatePattern='.'yyyy-MM-dd'.log' log4j.appender.R.layout=org.apache.log4j.PatternLayout log4j.appender.R.layout.ConversionPattern=%d{MM/dd/yyyy HH:mm:ss,SSS} %-5p %c - %m%n This time we need to create a to create your log, so we need to go in the https://docs.platform.sh/configuration/app-containers.html to define a writable folder to the log. mounts: 'log/': source: local source_path: log_source Commit the changes and push to redeploy. References https://dzone.com/articles/logging-with-log4j-in-java https://www.vogella.com/tutorials/Logging/article.html https://docs.platform.sh/languages/java.html https://docs.platform.sh/development/logs.html ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-configure-your-java-application-with-log4j-at-platform-sh/514",
        "relurl": "/t/how-to-configure-your-java-application-with-log4j-at-platform-sh/514"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "c7a1b88d335d55ff3c8d8b614183f4a86a8f3e34",
        "title": "How to migrate your Java Application from Heroku to Platform.sh",
        "description": "Goal Migrate a Heroku Java Application to a http://Platform.sh service. Assumptions You will need: an active application on http://Platform.sh configured to an empty database an active application on Heroku Steps 1. Git clone the Heroku Repository git clone heroku_repository.git 2. Create the three basic http://Platform.sh file .platform/routes.yaml : http://Platform.sh allows you to define the https://docs.platform.sh/configuration/routes.html. .platform/services.yaml: http://Platform.sh allows you to completely define and configure the topology and https://docs.platform.sh/configuration/services.html . .platform.app.yaml: You control your application and the way it will be built and deployed on http://Platform.sh https://docs.platform.sh/configuration/app-containers.html . 3. Set the Application File At the .platform.app.yaml we’ll set the minimum configuration to run a plain configuration be aware that it does not include database services such as MySQL, MariaDB and so on. # This file describes an application. You can have multiple applications # in the same project. # # See https://docs.platform.sh/user_guide/reference/platform-app-yaml.html # The name of this app. Must be unique within a project. name: app # The runtime the application uses. type: 'java:8' disk: 1024 # The hooks executed at various points in the lifecycle of the application. hooks: build: mvn clean install mounts: 'server/': source: local source_path: server_source # The relationships of the application with services or other applications. # # The left-hand side is the name of the relationship as it will be exposed # to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand # side is in the form ` : `. # The configuration of app when it is exposed to the web. web: commands: start: | cp target/dependency/webapp-runner.jar server/webapp-runner.jar cp target/tomcat.war server/tomcat.war cd server \u0026\u0026 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError webapp-runner.jar --port $PORT tomcat.war 4. Add http://Platform.sh Remote Repository git remote add platform platform_repository.git 5. Push the changes to the Repository git push platform master Conclusion By adding a .platform.app.yaml file to a project, a Java application’s build process can be migrated from Heroku. In order to fully migrate, the next step is to use the Heroku CLI to dump your database and import it to a service defined on your http://Platform.sh project. Additional resources: https://community.platform.sh/t/how-to-migrate-a-postgresql-database-from-heroku-to-platform-sh/301 https://github.com/platformsh-examples/tomcat-webapp-runner ",
        "text": "Goal Migrate a Heroku Java Application to a http://Platform.sh service. Assumptions You will need: an active application on http://Platform.sh configured to an empty database an active application on Heroku Steps 1. Git clone the Heroku Repository git clone heroku_repository.git 2. Create the three basic http://Platform.sh file .platform/routes.yaml : http://Platform.sh allows you to define the https://docs.platform.sh/configuration/routes.html. .platform/services.yaml: http://Platform.sh allows you to completely define and configure the topology and https://docs.platform.sh/configuration/services.html . .platform.app.yaml: You control your application and the way it will be built and deployed on http://Platform.sh https://docs.platform.sh/configuration/app-containers.html . 3. Set the Application File At the .platform.app.yaml we’ll set the minimum configuration to run a plain configuration be aware that it does not include database services such as MySQL, MariaDB and so on. # This file describes an application. You can have multiple applications # in the same project. # # See https://docs.platform.sh/user_guide/reference/platform-app-yaml.html # The name of this app. Must be unique within a project. name: app # The runtime the application uses. type: 'java:8' disk: 1024 # The hooks executed at various points in the lifecycle of the application. hooks: build: mvn clean install mounts: 'server/': source: local source_path: server_source # The relationships of the application with services or other applications. # # The left-hand side is the name of the relationship as it will be exposed # to the application in the PLATFORM_RELATIONSHIPS variable. The right-hand # side is in the form ` : `. # The configuration of app when it is exposed to the web. web: commands: start: | cp target/dependency/webapp-runner.jar server/webapp-runner.jar cp target/tomcat.war server/tomcat.war cd server \u0026\u0026 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError webapp-runner.jar --port $PORT tomcat.war 4. Add http://Platform.sh Remote Repository git remote add platform platform_repository.git 5. Push the changes to the Repository git push platform master Conclusion By adding a .platform.app.yaml file to a project, a Java application’s build process can be migrated from Heroku. In order to fully migrate, the next step is to use the Heroku CLI to dump your database and import it to a service defined on your http://Platform.sh project. Additional resources: https://community.platform.sh/t/how-to-migrate-a-postgresql-database-from-heroku-to-platform-sh/301 https://github.com/platformsh-examples/tomcat-webapp-runner ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-migrate-your-java-application-from-heroku-to-platform-sh/441",
        "relurl": "/t/how-to-migrate-your-java-application-from-heroku-to-platform-sh/441"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "7ac02fd7f0e887eaff88fe68dc56af15532a2c6e",
        "title": "How to set up Wordpress to use the HTTP cache of Platform.sh",
        "description": "Goal Configure Wordpress to send proper cache-control headers, so the can be used. Assumptions A WordPress installation on http://Platform.sh that uses Composer, for example one that uses the https://github.com/platformsh/template-wordpress and is configured for a https://community.platform.sh/t/how-to-install-wordpress-plugins-and-themes-with-composer An https://docs.platform.sh/development/ssh.html configured on the project account https://getcomposer.org/ installed locally Problems By default, Wordpress doesn’t set proper headers to allow the build-in to work efficiently. You can test if your site is cached based on the x-platform-cache response header which will probably be a MISS with the default Wordpress installation. Steps 1. Install and enable the cache-control plugin The main configuration that the HTTP cache needs to work properly, is Cache-Control header. Since Wordpress does not include an option to set those headers in their core, you need to use a 3rd party plugin. The example used here is https://wordpress.org/plugins/cache-control/. If you have set up your project to use a composer-based installation of plugins ( https://community.platform.sh/t/how-to-install-wordpress-plugins-and-themes-with-composer ), you can use composer require wpackagist-plugin/cache-control locally to update your composer files. After that commit and push them so the plugin gets deployed to your project. In the Admin-Dashboard go to Plugins, find Cache-Control and click Activate. 2. Verify and adjust the header values In the plugin settings, you can adjust the time different sites is being cached (e.g home page, post sites, etc). You can adjust that as necessary, following good practises. https://community.platform.sh/uploads/default/83103faec078789e4fa3c155e364e73aa38d72aa 3. Settings for static assets For static assets, the Wordpress template includes a default Cache-Control of 600 in your .platform.app.yaml file. To adjust that (if needed), you find the expires setting in the . 4. Verify your site is being cached To check if the response is cached, you can use curl -I to retrieve the headers. While the first response might be a MISS (because the page has not been cached yet), as soon as you send the same request again, it should return HIT. curl -I https://master-7rqtwti-af6kbo7ndasc2.eu-3.platformsh.site/ HTTP/2 200 cache-control: max-age=300 … x-platform-cache: HIT … Conclusion With this setup, your Wordpress site can leverage the http://Platform.sh HTTP Cache and you have control over the caching behaviour for different parts of your Wordpress site.",
        "text": "Goal Configure Wordpress to send proper cache-control headers, so the can be used. Assumptions A WordPress installation on http://Platform.sh that uses Composer, for example one that uses the https://github.com/platformsh/template-wordpress and is configured for a https://community.platform.sh/t/how-to-install-wordpress-plugins-and-themes-with-composer An https://docs.platform.sh/development/ssh.html configured on the project account https://getcomposer.org/ installed locally Problems By default, Wordpress doesn’t set proper headers to allow the build-in to work efficiently. You can test if your site is cached based on the x-platform-cache response header which will probably be a MISS with the default Wordpress installation. Steps 1. Install and enable the cache-control plugin The main configuration that the HTTP cache needs to work properly, is Cache-Control header. Since Wordpress does not include an option to set those headers in their core, you need to use a 3rd party plugin. The example used here is https://wordpress.org/plugins/cache-control/. If you have set up your project to use a composer-based installation of plugins ( https://community.platform.sh/t/how-to-install-wordpress-plugins-and-themes-with-composer ), you can use composer require wpackagist-plugin/cache-control locally to update your composer files. After that commit and push them so the plugin gets deployed to your project. In the Admin-Dashboard go to Plugins, find Cache-Control and click Activate. 2. Verify and adjust the header values In the plugin settings, you can adjust the time different sites is being cached (e.g home page, post sites, etc). You can adjust that as necessary, following good practises. https://community.platform.sh/uploads/default/83103faec078789e4fa3c155e364e73aa38d72aa 3. Settings for static assets For static assets, the Wordpress template includes a default Cache-Control of 600 in your .platform.app.yaml file. To adjust that (if needed), you find the expires setting in the . 4. Verify your site is being cached To check if the response is cached, you can use curl -I to retrieve the headers. While the first response might be a MISS (because the page has not been cached yet), as soon as you send the same request again, it should return HIT. curl -I https://master-7rqtwti-af6kbo7ndasc2.eu-3.platformsh.site/ HTTP/2 200 cache-control: max-age=300 … x-platform-cache: HIT … Conclusion With this setup, your Wordpress site can leverage the http://Platform.sh HTTP Cache and you have control over the caching behaviour for different parts of your Wordpress site.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-set-up-wordpress-to-use-the-http-cache-of-platform-sh/508",
        "relurl": "/t/how-to-set-up-wordpress-to-use-the-http-cache-of-platform-sh/508"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "5bcf53ac327f598f8d0723b98a3bee07469bd3ed",
        "title": "How to install Wordpress plugins and themes with Composer",
        "description": "Goal Add external plugins or themes from the official directory to your Wordpress installation using composer. Assumptions A WordPress installation on http://Platform.sh that uses Composer, for example one that uses the https://github.com/platformsh/template-wordpress . An https://docs.platform.sh/development/ssh.html configured on the project account https://getcomposer.org/ installed locally Problems Since http://Platform.sh deploys the code read-only (and thus makes many Wordpress sites more secure by default), the integrated mechanism to add and update both plugins and themes doesn’t work. Luckily, https://wpackagist.org/ mirrors the WordPress plugin and theme directories as a Composer repository. This means you can add all plugins and themes covered by the directory using composer. Steps 1. Add the external repository Edit your composer.json file and add wpackagist as a repository. \"repositories\":[ { \"type\":\"composer\", \"url\":\"https://wpackagist.org\" } ], 2. Add a plugin or theme via composer You can use composer require to add a plugin or a theme as a dependency. For plugins, use wpackagist-plugin as the vendor name, for themes use wpackagist-theme. Examples: Plugin: composer require wpackagist-plugin/cache-control Theme: composer require wpackagist-theme/neve Composer will update your composer.json and composer.lock files accordingly. 3. Push to your repository By pushing those changes to http://Platform.sh, the build process will automatically install the themes and plugins in the right folder. git add composer.json composer.lock git commit -m 'adding themes/plugins' git push 4. Enable plugins/themes in the WP-Admin Dashboard The admin interface will show the plugins/themes and will allow you to enable them directly via the interface. https://community.platform.sh/uploads/default/190a0168bbfa12f70e4697173dde9e705688e27c https://community.platform.sh/uploads/default/d5cb437818dd8dfec2045255c7074593fea81340 Conclusion By using composer to install 3rd party plugins and themes, you get a reliable build process and make sure the code is deployed safely. This avoids committing plugins and themes into your git repository. It also makes it easy to keep plugins and themes up to date. You can follow https://community.platform.sh/t/how-to-upgrade-wordpress-core-and-dependencies-with-composer guide on how to keep your Wordpress site updated.",
        "text": "Goal Add external plugins or themes from the official directory to your Wordpress installation using composer. Assumptions A WordPress installation on http://Platform.sh that uses Composer, for example one that uses the https://github.com/platformsh/template-wordpress . An https://docs.platform.sh/development/ssh.html configured on the project account https://getcomposer.org/ installed locally Problems Since http://Platform.sh deploys the code read-only (and thus makes many Wordpress sites more secure by default), the integrated mechanism to add and update both plugins and themes doesn’t work. Luckily, https://wpackagist.org/ mirrors the WordPress plugin and theme directories as a Composer repository. This means you can add all plugins and themes covered by the directory using composer. Steps 1. Add the external repository Edit your composer.json file and add wpackagist as a repository. \"repositories\":[ { \"type\":\"composer\", \"url\":\"https://wpackagist.org\" } ], 2. Add a plugin or theme via composer You can use composer require to add a plugin or a theme as a dependency. For plugins, use wpackagist-plugin as the vendor name, for themes use wpackagist-theme. Examples: Plugin: composer require wpackagist-plugin/cache-control Theme: composer require wpackagist-theme/neve Composer will update your composer.json and composer.lock files accordingly. 3. Push to your repository By pushing those changes to http://Platform.sh, the build process will automatically install the themes and plugins in the right folder. git add composer.json composer.lock git commit -m 'adding themes/plugins' git push 4. Enable plugins/themes in the WP-Admin Dashboard The admin interface will show the plugins/themes and will allow you to enable them directly via the interface. https://community.platform.sh/uploads/default/190a0168bbfa12f70e4697173dde9e705688e27c https://community.platform.sh/uploads/default/d5cb437818dd8dfec2045255c7074593fea81340 Conclusion By using composer to install 3rd party plugins and themes, you get a reliable build process and make sure the code is deployed safely. This avoids committing plugins and themes into your git repository. It also makes it easy to keep plugins and themes up to date. You can follow https://community.platform.sh/t/how-to-upgrade-wordpress-core-and-dependencies-with-composer guide on how to keep your Wordpress site updated.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-install-wordpress-plugins-and-themes-with-composer/507",
        "relurl": "/t/how-to-install-wordpress-plugins-and-themes-with-composer/507"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "4b3f5c7dd6da631abb6463f7ac91bb628c70d289",
        "title": "How to use environment variables with nuxtjs on Platform.sh",
        "description": "Goal To use environment variables in a https://nuxtjs.org app on http://Platform.sh Problem Nuxt currently provides a very https://nuxtjs.org/api/configuration-env which uses webpack substitution to inject your env vars at build time. This works most of the time, but on http://Platform.Sh the build process is environment-agnostic. A build can, and will be reused on different environments. This means that environment variables are not available in the build hook. So nuxt is not able to compile them. Building nuxt in the deploy hook, is also not possible, since the file system is read only. Work around: nuxt-env Luckily, there is a work around. There’s a nuxtjs plugin called https://github.com/samtgarson/nuxt-env. This allows you to define the environment variables that you want to access at runtime. First, make sure you have nuxt-env installed by running yarn add nuxt-env on your local pc, and commit that yarn.lock file to your repository Then, you can add the env vars you want to injects to your nuxt.config.js file // nuxt.config.js // Tell nuxt-env which env vars you want to inject modules: [ 'other-nuxt-module', ['nuxt-env', { keys: [ 'TEST_ENV_VAR', // Basic usage—equivalent of { key: 'TEST_ENV_VAR' } { key: 'OTHER_ENV_VAR', default: 'defaultValue' } // Specify a default value { key: 'THIRD_ENV_VAR', secret: true } // Only inject the var server side { key: 'ANOTHER_ENV_VAR', name: 'MY_ENV_VAR' } // Rename the variable ] }] ] ",
        "text": "Goal To use environment variables in a https://nuxtjs.org app on http://Platform.sh Problem Nuxt currently provides a very https://nuxtjs.org/api/configuration-env which uses webpack substitution to inject your env vars at build time. This works most of the time, but on http://Platform.Sh the build process is environment-agnostic. A build can, and will be reused on different environments. This means that environment variables are not available in the build hook. So nuxt is not able to compile them. Building nuxt in the deploy hook, is also not possible, since the file system is read only. Work around: nuxt-env Luckily, there is a work around. There’s a nuxtjs plugin called https://github.com/samtgarson/nuxt-env. This allows you to define the environment variables that you want to access at runtime. First, make sure you have nuxt-env installed by running yarn add nuxt-env on your local pc, and commit that yarn.lock file to your repository Then, you can add the env vars you want to injects to your nuxt.config.js file // nuxt.config.js // Tell nuxt-env which env vars you want to inject modules: [ 'other-nuxt-module', ['nuxt-env', { keys: [ 'TEST_ENV_VAR', // Basic usage—equivalent of { key: 'TEST_ENV_VAR' } { key: 'OTHER_ENV_VAR', default: 'defaultValue' } // Specify a default value { key: 'THIRD_ENV_VAR', secret: true } // Only inject the var server side { key: 'ANOTHER_ENV_VAR', name: 'MY_ENV_VAR' } // Rename the variable ] }] ] ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-use-environment-variables-with-nuxtjs-on-platform-sh/505",
        "relurl": "/t/how-to-use-environment-variables-with-nuxtjs-on-platform-sh/505"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "215a7641ac7bf5885b62eb096983e40b8f1d0035",
        "title": "How to deploy Nextcloud to Platform.sh",
        "description": "Goal https://nextcloud.com/ is a full, self-hosted alternative to tools such as G Suite or Microsoft 360. It includes a wide range of features, such as file management and sharing, calendars, document editing, voice and video conferencing, chat, and more. This how-to guide will show a basic Nextcloud installation on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account The http://Platform.sh https://github.com/platformsh/platformsh-cli installed. Steps Create a new http://Platform.sh project Install Nextcloud from the CLI Log into Nextcloud Set up integrations 1. Create a new project You can create a new project using the http://Platform.sh CLI tool: platform create After following the instructions, you will get a confirmation that a new project has been created. Copy the new project ID. 2. Initialize your project with your new project ID Replace with the ID from the previous step and execute this command: platform environment:init -p -e master https://github.com/platformsh-templates/nextcloud 3. Log into Nextcloud The username and password for the administration account were created for you during deployment, and can be viewed in the deploy log: platform log deploy -p Using the URL from Step 1, and the credentials listed in the logs, visit your new Nextcloud installation to complete the setup. 4. Set up integrations Depending on how you intend to use Nextcloud, you will want to enhance the basic installation with some integrations. Common integrations that will give Nextcloud full functionality include: IMAP server for https://docs.nextcloud.com/server/18/admin_manual/configuration_server/email_configuration.html?highlight=imap https://docs.nextcloud.com/server/18/admin_manual/configuration_files/external_storage_configuration_gui.html?highlight=dropbox (eg. Dropbox, S3, SFTP) for enabling file exchange https://nextcloud.com/collaboraonline/ or https://nextcloud.com/onlyoffice/ integration for collaborative document editing Conclusion The basic installation of Nextcloud on http://Platform.sh is straightforward using the CLI and the provided template. Nextcloud users will still need to look after some integrations before the full potential of Nextcloud is available.",
        "text": "Goal https://nextcloud.com/ is a full, self-hosted alternative to tools such as G Suite or Microsoft 360. It includes a wide range of features, such as file management and sharing, calendars, document editing, voice and video conferencing, chat, and more. This how-to guide will show a basic Nextcloud installation on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account The http://Platform.sh https://github.com/platformsh/platformsh-cli installed. Steps Create a new http://Platform.sh project Install Nextcloud from the CLI Log into Nextcloud Set up integrations 1. Create a new project You can create a new project using the http://Platform.sh CLI tool: platform create After following the instructions, you will get a confirmation that a new project has been created. Copy the new project ID. 2. Initialize your project with your new project ID Replace with the ID from the previous step and execute this command: platform environment:init -p -e master https://github.com/platformsh-templates/nextcloud 3. Log into Nextcloud The username and password for the administration account were created for you during deployment, and can be viewed in the deploy log: platform log deploy -p Using the URL from Step 1, and the credentials listed in the logs, visit your new Nextcloud installation to complete the setup. 4. Set up integrations Depending on how you intend to use Nextcloud, you will want to enhance the basic installation with some integrations. Common integrations that will give Nextcloud full functionality include: IMAP server for https://docs.nextcloud.com/server/18/admin_manual/configuration_server/email_configuration.html?highlight=imap https://docs.nextcloud.com/server/18/admin_manual/configuration_files/external_storage_configuration_gui.html?highlight=dropbox (eg. Dropbox, S3, SFTP) for enabling file exchange https://nextcloud.com/collaboraonline/ or https://nextcloud.com/onlyoffice/ integration for collaborative document editing Conclusion The basic installation of Nextcloud on http://Platform.sh is straightforward using the CLI and the provided template. Nextcloud users will still need to look after some integrations before the full potential of Nextcloud is available.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-deploy-nextcloud-to-platform-sh/504",
        "relurl": "/t/how-to-deploy-nextcloud-to-platform-sh/504"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "76c4705b4f68c18b7f6c4d4b2aaf4e735e4e65a7",
        "title": "How to deploy Mattermost (Slack alternative) on Platform.sh",
        "description": "Goal This how-to guide will guide you through deploying https://mattermost.com on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account The http://Platform.sh https://github.com/platformsh/platformsh-cli installed. Steps Create a new http://Platform.sh project Install Mattermost from the CLI Create a first account and team Connect it to your Desktop and Mobile Mattermost clients 1. Create a new project You can create a new project using the http://Platform.sh CLI tool: platform create After following the instructions, you will get a confirmation that a new project has been created. Copy the new project ID. 2. Initialize your project with your new project ID Replace with the ID from the previous step and execute this command: platform environment:init -p -e master https://github.com/platformsh-templates/mattermost 3. Create a first account and team Use the URL from the first step to visit your new Mattermost website, and complete the steps in the setup wizard. https://community.platform.sh/uploads/default/a1e2299cda49fda087cb063991ba338e0a87c55a https://community.platform.sh/uploads/default/c74db48c0d2b5d42f2244554d37782db21822e51 4. Connect it to your Desktop and Mobile Mattermost clients Once you have created an account and a team on your Mattermost server, you can log into the server from your desktop client or mobile app. See https://mattermost.com/download/ on how to get those. The server URL is the same URL that was displayed in Step 1 of this how-to guide, the URL to your running Mattermost server. The authentication that follows is the account you created in Step 3. https://community.platform.sh/uploads/default/3f93f8e58b15fafa55425e916a45e7f25b6e86e8 Conclusion Using the http://Platform.sh CLI and the Mattermost template provided by http://Platform.sh, it is a straightforward process to set up a Mattermost server.",
        "text": "Goal This how-to guide will guide you through deploying https://mattermost.com on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account The http://Platform.sh https://github.com/platformsh/platformsh-cli installed. Steps Create a new http://Platform.sh project Install Mattermost from the CLI Create a first account and team Connect it to your Desktop and Mobile Mattermost clients 1. Create a new project You can create a new project using the http://Platform.sh CLI tool: platform create After following the instructions, you will get a confirmation that a new project has been created. Copy the new project ID. 2. Initialize your project with your new project ID Replace with the ID from the previous step and execute this command: platform environment:init -p -e master https://github.com/platformsh-templates/mattermost 3. Create a first account and team Use the URL from the first step to visit your new Mattermost website, and complete the steps in the setup wizard. https://community.platform.sh/uploads/default/a1e2299cda49fda087cb063991ba338e0a87c55a https://community.platform.sh/uploads/default/c74db48c0d2b5d42f2244554d37782db21822e51 4. Connect it to your Desktop and Mobile Mattermost clients Once you have created an account and a team on your Mattermost server, you can log into the server from your desktop client or mobile app. See https://mattermost.com/download/ on how to get those. The server URL is the same URL that was displayed in Step 1 of this how-to guide, the URL to your running Mattermost server. The authentication that follows is the account you created in Step 3. https://community.platform.sh/uploads/default/3f93f8e58b15fafa55425e916a45e7f25b6e86e8 Conclusion Using the http://Platform.sh CLI and the Mattermost template provided by http://Platform.sh, it is a straightforward process to set up a Mattermost server.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-deploy-mattermost-slack-alternative-on-platform-sh/503",
        "relurl": "/t/how-to-deploy-mattermost-slack-alternative-on-platform-sh/503"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "b46491195d8dc786efc05650514b0d95782c7abc",
        "title": "How to create, initialize, and delete a Platform.sh project from the command line",
        "description": "Goal To fully manage the life cycle of sites on http://Platform.sh, including the creation of new projects, initializing from a hosted GitHub repository, and deleting projects. Assumptions on account The https://github.com/platformsh/platformsh-cli installed Problems You will need a public GitHub project that already has the required https://docs.platform.sh/configuration/yaml.html to work with http://Platform.sh. Steps 1. Creating projects To create a new project, use the platform create command. https://community.platform.sh/uploads/default/eebff1e7f8af01f1ef3a797f23cabb821edfd799 2. Initializing projects from a GitHub repository The platform environment:init command requires both the Project ID of the newly created project and the URL of the GitHub project. Note that as of version 3.40.8 of the http://Platform.sh CLI, the environment:init command is considered beta and not listed via platform list. platform environment:init -p -e master Note that the GitHub URL is the https: URL from the browser, not the git: URL. https://community.platform.sh/uploads/default/bc695ccb5ad79b9a1f838d93a8189883d057f4ce Open the new site in your browser with the following command: platform url -p platform url -p ov4iu6fcb35jg Environment ID [master]: Enter a number to open a URL [0] https://master-7rqtwti-ov4iu6fcb35jg.de-2.platformsh.site/ [1] https://www.master-7rqtwti-ov4iu6fcb35jg.de-2.platformsh.site/ [2] http://master-7rqtwti-ov4iu6fcb35jg.de-2.platformsh.site/ [3] http://www.master-7rqtwti-ov4iu6fcb35jg.de-2.platformsh.site/ 0 https://master-7rqtwti-ov4iu6fcb35jg.de-2.platformsh.site/ https://community.platform.sh/uploads/default/7c010a9f1063622054a23af85d7ac144ce8dedfb 3. Deleting projects Delete the project with the following command: platform project:delete -p You will be prompted to type the name of the project as a security confirmation, since deleting a project is irreversible. platform project:delete -p ov4iu6fcb35jg You are about to delete the project: New Drupal Site (ov4iu6fcb35jg) * This action is irreversible. * Your site will no longer be accessible. * All data associated with this project will be deleted, including backups. * You will be charged at the end of the month for any remaining project costs. Are you sure you want to delete this project? [y/N] y Type the project title to confirm: New Drupal Site Conclusion The http://Platform.sh CLI enables full life cycle management of your sites. It is not even necessary to use Git directly when creating, initializing, and deleting sites.",
        "text": "Goal To fully manage the life cycle of sites on http://Platform.sh, including the creation of new projects, initializing from a hosted GitHub repository, and deleting projects. Assumptions on account The https://github.com/platformsh/platformsh-cli installed Problems You will need a public GitHub project that already has the required https://docs.platform.sh/configuration/yaml.html to work with http://Platform.sh. Steps 1. Creating projects To create a new project, use the platform create command. https://community.platform.sh/uploads/default/eebff1e7f8af01f1ef3a797f23cabb821edfd799 2. Initializing projects from a GitHub repository The platform environment:init command requires both the Project ID of the newly created project and the URL of the GitHub project. Note that as of version 3.40.8 of the http://Platform.sh CLI, the environment:init command is considered beta and not listed via platform list. platform environment:init -p -e master Note that the GitHub URL is the https: URL from the browser, not the git: URL. https://community.platform.sh/uploads/default/bc695ccb5ad79b9a1f838d93a8189883d057f4ce Open the new site in your browser with the following command: platform url -p platform url -p ov4iu6fcb35jg Environment ID [master]: Enter a number to open a URL [0] https://master-7rqtwti-ov4iu6fcb35jg.de-2.platformsh.site/ [1] https://www.master-7rqtwti-ov4iu6fcb35jg.de-2.platformsh.site/ [2] http://master-7rqtwti-ov4iu6fcb35jg.de-2.platformsh.site/ [3] http://www.master-7rqtwti-ov4iu6fcb35jg.de-2.platformsh.site/ 0 https://master-7rqtwti-ov4iu6fcb35jg.de-2.platformsh.site/ https://community.platform.sh/uploads/default/7c010a9f1063622054a23af85d7ac144ce8dedfb 3. Deleting projects Delete the project with the following command: platform project:delete -p You will be prompted to type the name of the project as a security confirmation, since deleting a project is irreversible. platform project:delete -p ov4iu6fcb35jg You are about to delete the project: New Drupal Site (ov4iu6fcb35jg) * This action is irreversible. * Your site will no longer be accessible. * All data associated with this project will be deleted, including backups. * You will be charged at the end of the month for any remaining project costs. Are you sure you want to delete this project? [y/N] y Type the project title to confirm: New Drupal Site Conclusion The http://Platform.sh CLI enables full life cycle management of your sites. It is not even necessary to use Git directly when creating, initializing, and deleting sites.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-create-initialize-and-delete-a-platform-sh-project-from-the-command-line/188",
        "relurl": "/t/how-to-create-initialize-and-delete-a-platform-sh-project-from-the-command-line/188"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "30fd4bbad0a3a7e72af39de3a66f009724485caf",
        "title": "How to access InfluxDB credentials on Platform.sh",
        "description": "Goal Access credentials for InfluxDB from within a http://Platform.sh application using PHP. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/influxdb.html .platform/services.yaml for the given service on account if developing locally InfluxDB installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html InfluxDB relationship like so, in .platform.app.yaml: relationships: database: \"timedb:influxdb\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-php Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-php for minimum requirements. $ composer install platformsh/config-reader 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. use Platformsh\\ConfigReader\\Config; $config = new Config(); 3. Read the credentials // Get the credentials to connect to the InfluxDB service. $credentials = $config-credentials('database'); Steps (Manual) 1. Load and decode the environment variable $relationships = json_decode(base64_decode(getenv('PLATFORM_RELATIONSHIPS')), TRUE); 2. Read the credentials $credentials = $relationships['database']; Use: In most cases, you will need only the host and port properties to connect to InfluxDB. Pass those to your InfluxDB library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is now an array matching the relationship JSON object. { \"service\": \"timedb\", \"ip\": \"169.254.113.144\", \"hostname\": \"haz5rys6n2dsnjusqa54os3ii4.influxdb.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"database.internal\", \"rel\": \"influxdb\", \"scheme\": \"http\", \"type\": \"influxdb:1.3\", \"port\": 8086 } Conclusions: Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get InfluxDB credentials in PHP. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-php can be useful for inspecting the project environment: // Checks whether the code is running in a build environment $config-inBuild(); // Checks whether the code is running in a runtime environment $config-inRuntime(); and for reading environment variables as attributes of config: // Available in Build and at Runtime $config-appDir; // Available at Runtime only $config-branch; $config-smtpHost; The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "text": "Goal Access credentials for InfluxDB from within a http://Platform.sh application using PHP. Assumptions an active https://platform.sh/ project properly https://docs.platform.sh/configuration/services/influxdb.html .platform/services.yaml for the given service on account if developing locally InfluxDB installed if developing locally properly https://docs.platform.sh/configuration/app/relationships.html InfluxDB relationship like so, in .platform.app.yaml: relationships: database: \"timedb:influxdb\" If developing locally, remember to first open a tunnel to the project environment using the . $ platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" This will open an SSH tunnel to your current http://Platform.sh environment and expose a local environment variable that mimics the relationships array on http://Platform.sh. Check the https://docs.platform.sh/gettingstarted/local/tethered.html and https://community.platform.sh/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69 for more information. Problems http://Platform.sh service credentials are made available to applications as the PLATFORM_RELATIONSHIPS environment variable, which is a base64-encoded JSON string that has to be decoded before it can be used. There are two primary options for accessing service credentials on http://Platform.sh that can be used within an application: Using the https://github.com/platformsh/config-reader-php Accessing environment variables directly Steps (Config Reader) 1. Install the library Install the configuration library. See the https://github.com/platformsh/config-reader-php for minimum requirements. $ composer install platformsh/config-reader 2. Create a Config object Creating a Config object provides access to the http://Platform.sh environment. use Platformsh\\ConfigReader\\Config; $config = new Config(); 3. Read the credentials // Get the credentials to connect to the InfluxDB service. $credentials = $config-credentials('database'); Steps (Manual) 1. Load and decode the environment variable $relationships = json_decode(base64_decode(getenv('PLATFORM_RELATIONSHIPS')), TRUE); 2. Read the credentials $credentials = $relationships['database']; Use: In most cases, you will need only the host and port properties to connect to InfluxDB. Pass those to your InfluxDB library’s setup routine in your application. Most of the time the other values may be ignored. In either case, credentials is now an array matching the relationship JSON object. { \"service\": \"timedb\", \"ip\": \"169.254.113.144\", \"hostname\": \"haz5rys6n2dsnjusqa54os3ii4.influxdb.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"database.internal\", \"rel\": \"influxdb\", \"scheme\": \"http\", \"type\": \"influxdb:1.3\", \"port\": 8086 } Conclusions: Using either the language-specific http://Platform.sh Configuration library or direct access methods for environment variables, an application can get InfluxDB credentials in PHP. http://Platform.sh supports configuration libraries for multiple languages. The https://github.com/platformsh/config-reader-php can be useful for inspecting the project environment: // Checks whether the code is running in a build environment $config-inBuild(); // Checks whether the code is running in a runtime environment $config-inRuntime(); and for reading environment variables as attributes of config: // Available in Build and at Runtime $config-appDir; // Available at Runtime only $config-branch; $config-smtpHost; The APIs for each language are written to be as consistent as possible, but seek out each library’s documentation for specific differences.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-access-influxdb-credentials-on-platform-sh/148",
        "relurl": "/t/how-to-access-influxdb-credentials-on-platform-sh/148"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "35d22561178c1d9924ed7eeb6e4aedc851234607",
        "title": "How to run Grocy on Platform.sh",
        "description": "Goal https://grocy.info/ is an awesome app one can use to manage groceries, food stock, recipes, chores and more. It’s like a personal ERP, I am using it to hoard responsibly without wasting food and other household items. Here’s how it looks like: https://community.platform.sh/uploads/default/d8d9662e07dddf277803bfac89bca2b18fb5ffdd It is very easy to run Grocy on http://Platform.sh, and I will show you here what it takes. You can always https://github.com/kotnik/grocy-platformsh , and have it running in seconds. Assumptions You will need: http://Platform.sh project, it can be development sized one too. Steps Clone your http://Platform.sh project locally first: platform get It should be fresh and empty project. Now, let’s add Grocy as submodule so we can upgrade easily when new version is released: cd git submodule add https://github.com/grocy/grocy.git git submodule init git submodule update cd grocy git checkout v2.6.1 cd .. git add . git commit -am 'Added Grocy 2.6.1' OK, latest Grocy is ready. Now, let’s add some info so http://Platform.sh knows how to run it. Grocy itself is not using any additional services, all the data is kept in a file, so let’s create empty .platform/services.yaml file: # No need for services Routes can be default ones, have this in .platform/routes.yaml: \"https://{default}/\": type: upstream upstream: \"app:http\" \"http://{default}/\": type: redirect to: \"https://{default}/\" Good. Now, let’s create .platform.app.yaml. You can take https://github.com/kotnik/grocy-platformsh/blob/master/.platform.app.yaml , but here we’ll cover it section by section. Grocy, at the time this is written, needs PHP 7.2 so let’s use that: name: 'app' type: 'php:7.2' It keeps all the data in single file with db extension it searches for in data/ directory. So, let’s give it writable permanent mount there: disk: 512 mounts: '/data': source: local source_path: 'data' For build it only requires yarn around, so let’s get it: build: flavor: none dependencies: nodejs: yarn: \"*\" Now, this is the build hook: hooks: build: | set -e mkdir -p www cd www ln -s ../data/data data cd .. mv grocy/public www mv grocy/controllers www mv grocy/helpers www mv grocy/localization www mv grocy/middleware www mv grocy/migrations www mv grocy/publication_assets www mv grocy/services www mv grocy/views www mv grocy/composer.* www mv grocy/.yarnrc www mv grocy/yarn.* www mv grocy/*.php www mv grocy/*.json www cd www composer install yarn install This results with built app in www sub-directory. Notice the trick we did with the data directory. By Grocy’s own installation documentation, /data directory keeps cache and plugins as well (structure is provided in the repo of the app itself), not just the data created by user. So we link it to the permanent mount and then copy the provided plugins in the deploy hook: hooks: deploy: | set -e mkdir -p data/data/viewcache cp -rp grocy/data/* data/data cp config.php data/data Deploy hook also provides configuration file config.php. You can look at the defaults, but at least you should disable PHP’s error reporting since it breaks session management: ",
        "text": "Goal https://grocy.info/ is an awesome app one can use to manage groceries, food stock, recipes, chores and more. It’s like a personal ERP, I am using it to hoard responsibly without wasting food and other household items. Here’s how it looks like: https://community.platform.sh/uploads/default/d8d9662e07dddf277803bfac89bca2b18fb5ffdd It is very easy to run Grocy on http://Platform.sh, and I will show you here what it takes. You can always https://github.com/kotnik/grocy-platformsh , and have it running in seconds. Assumptions You will need: http://Platform.sh project, it can be development sized one too. Steps Clone your http://Platform.sh project locally first: platform get It should be fresh and empty project. Now, let’s add Grocy as submodule so we can upgrade easily when new version is released: cd git submodule add https://github.com/grocy/grocy.git git submodule init git submodule update cd grocy git checkout v2.6.1 cd .. git add . git commit -am 'Added Grocy 2.6.1' OK, latest Grocy is ready. Now, let’s add some info so http://Platform.sh knows how to run it. Grocy itself is not using any additional services, all the data is kept in a file, so let’s create empty .platform/services.yaml file: # No need for services Routes can be default ones, have this in .platform/routes.yaml: \"https://{default}/\": type: upstream upstream: \"app:http\" \"http://{default}/\": type: redirect to: \"https://{default}/\" Good. Now, let’s create .platform.app.yaml. You can take https://github.com/kotnik/grocy-platformsh/blob/master/.platform.app.yaml , but here we’ll cover it section by section. Grocy, at the time this is written, needs PHP 7.2 so let’s use that: name: 'app' type: 'php:7.2' It keeps all the data in single file with db extension it searches for in data/ directory. So, let’s give it writable permanent mount there: disk: 512 mounts: '/data': source: local source_path: 'data' For build it only requires yarn around, so let’s get it: build: flavor: none dependencies: nodejs: yarn: \"*\" Now, this is the build hook: hooks: build: | set -e mkdir -p www cd www ln -s ../data/data data cd .. mv grocy/public www mv grocy/controllers www mv grocy/helpers www mv grocy/localization www mv grocy/middleware www mv grocy/migrations www mv grocy/publication_assets www mv grocy/services www mv grocy/views www mv grocy/composer.* www mv grocy/.yarnrc www mv grocy/yarn.* www mv grocy/*.php www mv grocy/*.json www cd www composer install yarn install This results with built app in www sub-directory. Notice the trick we did with the data directory. By Grocy’s own installation documentation, /data directory keeps cache and plugins as well (structure is provided in the repo of the app itself), not just the data created by user. So we link it to the permanent mount and then copy the provided plugins in the deploy hook: hooks: deploy: | set -e mkdir -p data/data/viewcache cp -rp grocy/data/* data/data cp config.php data/data Deploy hook also provides configuration file config.php. You can look at the defaults, but at least you should disable PHP’s error reporting since it breaks session management: ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-run-grocy-on-platform-sh/498",
        "relurl": "/t/how-to-run-grocy-on-platform-sh/498"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "4c63027166447ff170ee0cd627011ecf64160bc0",
        "title": "How to run CryptPad on Platform.sh",
        "description": "Goal Here you can find what is needed to run https://cryptpad.fr/what-is-cryp.htpadtml on http://Platform.sh. You will see soon that it doesn’t take much at all, and very quickly you will be able to collaborate on slides, sheets, and other types of documents in secure and private manner. TL;DR Clone CryptPad, https://github.com/kotnik/cryptpad/commit/72a8fd5f7c5b8781c95d6fa31691d3b439f80dfe , push to fresh http://Platform.sh project, profffit. Here’s what you can collaborate on with CryptPad: https://community.platform.sh/uploads/default/4e3dcd0eb8384ed8d1d13689faf1f9a597808d5d It’s like Google Docs or Microsoft Office365, but on your own: you own the data, and it is protected at rest. Interested? Let’s go. Assumptions You will need: Basic knowledge of how http://Platform.sh works. You have run at least one project on http://Platform.sh, so we don’t have to go over git and other primitives here. Steps Before we go to steps you can pick up this complete https://github.com/kotnik/cryptpad/tree/platform , push it to your http://Platform.sh project, and you will have fully working CryptPad copy. You can see this repo https://master-7rqtwti-6sbs6vfvslbwc.eu-3.platformsh.site/ . And now, here are all the steps I took to make it happen. 1. Fork it - Clone it - Branch it So I forked https://github.com/xwiki-labs/cryptpad to my own Github account and cloned it. I will paste my own command, but please adjust it for your own username and URL: git clone git@github.com:kotnik/cryptpad.git cd cryptpad Ok, origin remote is Github one. Sweet. Next, I wanted to use the latest CryptPad release, which at the time of writing this is 3.12.0, and name that branch platform, you can guess why. Let’s go: git checkout -b platform tags/3.12.0 Easier to do than to describe. 2. Create http://Platform.sh project So go and create new project. It can be trial too. It’s fast, I’ll wait. 3. Hook it up - Basics As you already probably know, there are only a few things http://Platform.sh needs to know before it is able to run your application: Your app description: what it runs on, how to build it, how to serve it, etc. It’s that .platform.app.yaml file. Services your app is using. It’s easy here because CryptPad uses no external services and keeps everything on the disk. Your routes, how to access your application. And only here we have one small gotcha that is crucial, we’ll cover it below. But first, let’s add http://Platform.sh project remote to our repo: platform project:set-remote 4. Hook it up - Describing the app Here it is, full and fully commented .platform.app.yaml file: name: cryptpad # CryptPad needs to run on Node 12. type: nodejs:12 variables: env: # This tells the app how to create encryption key. Refer its documentation for more info. FRESH: 1 # No, don't sweat it, you're behind proxy and we got TLS all figured out :slight_smile:. USE_SSL: \"false\" web: commands: # I love Express framework apps, they're so easy to run! start: \"node ./server.js\" build: # Let me handle it, don't be smart. flavor: none hooks: build: | npm install # Make our life easier with official Platform.sh helper. npm install platformsh-config@2.3.1 bower install # This is the configuration file, more about this one later. cp config.platformsh.js config/config.js dependencies: nodejs: bower: \"^1.8.8\" mounts: datastore: source: local source_path: datastore data: source: local source_path: data block: source: local source_path: block blob: source: local source_path: blob # Starting with half of a gigabyte. It's easy to bump it up, so you choose. disk: 512 5. Hook it up - Services and routes Now we need to tell http://Platform.sh what we need for the application to run, and where one can find it. As for services, there’s nothing we need, so use this for .platform/services.yaml file: # No needs You do need it, even an empty one. Next, here are the routes you need to put in your .platform/routes.yaml file: \"https://{default}/\": type: upstream upstream: \"cryptpad:http\" \"https://{default}/cryptpad_websocket\": type: upstream upstream: \"cryptpad:http\" cache: enabled: false And this is the only tricky part of this how-to, the one I figured out the hard way: CryptPad uses WebSockets to talk with browsers and you must have that second entry in routes.yaml to disable caching it, or your app will simply not work. We but CryptPad people should have this endpoint mentioned on their installation page. 6. Hook it up - Configure app itself CryptPad looks at config/config.js file by default for configuration instructions. This file is rather long so I will not include it here, you can https://github.com/kotnik/cryptpad/blob/72a8fd5f7c5b8781c95d6fa31691d3b439f80dfe/config.platformsh.js and save it to config.platform.js. It’s well documented so you should have no problems figuring out what to do. 7. Run it This is it, after defining your application (with .platform.app.yaml, .platform/services.yaml and .platform/routes.yaml in place) and instructing how your CryptPad should behave (file config.platform.js is ready), you can simply commit all this and push it to your http://Platform.sh project: git add .platform.app.yaml .platform/services.yaml .platform/routes.yaml config.platform.js git commit -m 'Platformize' git push platform -u platform:master After building and deploying your application, your project will inform you where your CryptPad is and you can start using it right away! Happy hacking on http://Platform.sh!",
        "text": "Goal Here you can find what is needed to run https://cryptpad.fr/what-is-cryp.htpadtml on http://Platform.sh. You will see soon that it doesn’t take much at all, and very quickly you will be able to collaborate on slides, sheets, and other types of documents in secure and private manner. TL;DR Clone CryptPad, https://github.com/kotnik/cryptpad/commit/72a8fd5f7c5b8781c95d6fa31691d3b439f80dfe , push to fresh http://Platform.sh project, profffit. Here’s what you can collaborate on with CryptPad: https://community.platform.sh/uploads/default/4e3dcd0eb8384ed8d1d13689faf1f9a597808d5d It’s like Google Docs or Microsoft Office365, but on your own: you own the data, and it is protected at rest. Interested? Let’s go. Assumptions You will need: Basic knowledge of how http://Platform.sh works. You have run at least one project on http://Platform.sh, so we don’t have to go over git and other primitives here. Steps Before we go to steps you can pick up this complete https://github.com/kotnik/cryptpad/tree/platform , push it to your http://Platform.sh project, and you will have fully working CryptPad copy. You can see this repo https://master-7rqtwti-6sbs6vfvslbwc.eu-3.platformsh.site/ . And now, here are all the steps I took to make it happen. 1. Fork it - Clone it - Branch it So I forked https://github.com/xwiki-labs/cryptpad to my own Github account and cloned it. I will paste my own command, but please adjust it for your own username and URL: git clone git@github.com:kotnik/cryptpad.git cd cryptpad Ok, origin remote is Github one. Sweet. Next, I wanted to use the latest CryptPad release, which at the time of writing this is 3.12.0, and name that branch platform, you can guess why. Let’s go: git checkout -b platform tags/3.12.0 Easier to do than to describe. 2. Create http://Platform.sh project So go and create new project. It can be trial too. It’s fast, I’ll wait. 3. Hook it up - Basics As you already probably know, there are only a few things http://Platform.sh needs to know before it is able to run your application: Your app description: what it runs on, how to build it, how to serve it, etc. It’s that .platform.app.yaml file. Services your app is using. It’s easy here because CryptPad uses no external services and keeps everything on the disk. Your routes, how to access your application. And only here we have one small gotcha that is crucial, we’ll cover it below. But first, let’s add http://Platform.sh project remote to our repo: platform project:set-remote 4. Hook it up - Describing the app Here it is, full and fully commented .platform.app.yaml file: name: cryptpad # CryptPad needs to run on Node 12. type: nodejs:12 variables: env: # This tells the app how to create encryption key. Refer its documentation for more info. FRESH: 1 # No, don't sweat it, you're behind proxy and we got TLS all figured out :slight_smile:. USE_SSL: \"false\" web: commands: # I love Express framework apps, they're so easy to run! start: \"node ./server.js\" build: # Let me handle it, don't be smart. flavor: none hooks: build: | npm install # Make our life easier with official Platform.sh helper. npm install platformsh-config@2.3.1 bower install # This is the configuration file, more about this one later. cp config.platformsh.js config/config.js dependencies: nodejs: bower: \"^1.8.8\" mounts: datastore: source: local source_path: datastore data: source: local source_path: data block: source: local source_path: block blob: source: local source_path: blob # Starting with half of a gigabyte. It's easy to bump it up, so you choose. disk: 512 5. Hook it up - Services and routes Now we need to tell http://Platform.sh what we need for the application to run, and where one can find it. As for services, there’s nothing we need, so use this for .platform/services.yaml file: # No needs You do need it, even an empty one. Next, here are the routes you need to put in your .platform/routes.yaml file: \"https://{default}/\": type: upstream upstream: \"cryptpad:http\" \"https://{default}/cryptpad_websocket\": type: upstream upstream: \"cryptpad:http\" cache: enabled: false And this is the only tricky part of this how-to, the one I figured out the hard way: CryptPad uses WebSockets to talk with browsers and you must have that second entry in routes.yaml to disable caching it, or your app will simply not work. We but CryptPad people should have this endpoint mentioned on their installation page. 6. Hook it up - Configure app itself CryptPad looks at config/config.js file by default for configuration instructions. This file is rather long so I will not include it here, you can https://github.com/kotnik/cryptpad/blob/72a8fd5f7c5b8781c95d6fa31691d3b439f80dfe/config.platformsh.js and save it to config.platform.js. It’s well documented so you should have no problems figuring out what to do. 7. Run it This is it, after defining your application (with .platform.app.yaml, .platform/services.yaml and .platform/routes.yaml in place) and instructing how your CryptPad should behave (file config.platform.js is ready), you can simply commit all this and push it to your http://Platform.sh project: git add .platform.app.yaml .platform/services.yaml .platform/routes.yaml config.platform.js git commit -m 'Platformize' git push platform -u platform:master After building and deploying your application, your project will inform you where your CryptPad is and you can start using it right away! Happy hacking on http://Platform.sh!",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-run-cryptpad-on-platform-sh/486",
        "relurl": "/t/how-to-run-cryptpad-on-platform-sh/486"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "2569574f10d19de4ba0d9dca8f591363d57bdb39",
        "title": "How to fail a build on failing unit tests",
        "description": "Goal Every time a push is made, run all unit tests. If tests fail, the build should fail and not be deployed. Assumptions During the build phase no services are available. That means only tests that can run with only your code (no database, no web requests, etc.) can be run. Most testing frameworks have a way to define test suites and run only a selected set of them. This guide assumes you have defined a test suite that is safe to run in isolation. Although this example uses PHP the process is essentially identical for any language, provide its test runner follows standard Unix error conventions. Steps 1. Ensure that your test framework is included in your list of project dependencies For a PHP application that would mean composer.json. For a Node.js application it would mean package.json. For PHP: { \"requires-dev\": { \"phpunit/phpunit\": \"^7.5\" } } (By default http://Platform.sh installs dev dependencies as well, so it will be downloaded by default.) 2. Add set -e as the first line of your build hook In .platform.app.yaml, locate your build hook and add set -e as the first line. If one does not exist go ahead and create it. hooks: build: set -e That tells the system to fail the build if any command in the build hook returns an error code. 3. Add your test command as the last step of your build hook After all other tasks in your build hook have run, add a line to run your tests. For PHP your build hook will look like this: hooks: build: set -e # Possibly other stuff here vendor/bin/phpunit For Node.js it will look like this: hooks: build: set -e # Possibly other stuff here npm test 4. Commit the result and push git add composer.json .platform.app.yaml git commit -m \"Enable build tests\" Conclusion If the test command fails a test it will return a non-0 error code to the shell. The set -e flag means that error will cause the whole build process to fail and the container will not be deployed. If no tests fail then the test command will return 0, allowing the build to proceed. The output of the test runner will be available in the build log.",
        "text": "Goal Every time a push is made, run all unit tests. If tests fail, the build should fail and not be deployed. Assumptions During the build phase no services are available. That means only tests that can run with only your code (no database, no web requests, etc.) can be run. Most testing frameworks have a way to define test suites and run only a selected set of them. This guide assumes you have defined a test suite that is safe to run in isolation. Although this example uses PHP the process is essentially identical for any language, provide its test runner follows standard Unix error conventions. Steps 1. Ensure that your test framework is included in your list of project dependencies For a PHP application that would mean composer.json. For a Node.js application it would mean package.json. For PHP: { \"requires-dev\": { \"phpunit/phpunit\": \"^7.5\" } } (By default http://Platform.sh installs dev dependencies as well, so it will be downloaded by default.) 2. Add set -e as the first line of your build hook In .platform.app.yaml, locate your build hook and add set -e as the first line. If one does not exist go ahead and create it. hooks: build: set -e That tells the system to fail the build if any command in the build hook returns an error code. 3. Add your test command as the last step of your build hook After all other tasks in your build hook have run, add a line to run your tests. For PHP your build hook will look like this: hooks: build: set -e # Possibly other stuff here vendor/bin/phpunit For Node.js it will look like this: hooks: build: set -e # Possibly other stuff here npm test 4. Commit the result and push git add composer.json .platform.app.yaml git commit -m \"Enable build tests\" Conclusion If the test command fails a test it will return a non-0 error code to the shell. The set -e flag means that error will cause the whole build process to fail and the container will not be deployed. If no tests fail then the test command will return 0, allowing the build to proceed. The output of the test runner will be available in the build log.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-fail-a-build-on-failing-unit-tests/57",
        "relurl": "/t/how-to-fail-a-build-on-failing-unit-tests/57"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "7c9e418c901c5b3455a910819f44d89b7335231a",
        "title": "How to determine database usage of an environment",
        "description": "Goal Determine approximate database usage of a http://platform.sh/ project using the https://github.com/platformsh/platformsh-cli tool. Assumptions One or more http://platform.sh/ projects https://github.com/platformsh/platformsh-cli tool installed locally https://docs.platform.sh/development/ssh.html configured with the project account(s) Problems As a general rule, the database service should have at least 15% of disk space available on a http://Platform.sh project environment, so it is useful to be able to approximate the current disk usage of a project’s database(s). Steps 1. Log in to Platform CLI $ platform login 2. Find the project ID Note the project ID of the project you’re checking: $ platform project:list 3. Find the environment ID Note the environment ID for the environment you’re checking. $ platform environment:list -p 4. Run the database size command $ platform db:size -p -e The command will return something like: +----------------+-----------------+--------+ | Allocated disk | Estimated usage | % used | +----------------+-----------------+--------+ | 14.6 GiB | 9.4 GiB | ~ 64% | +----------------+-----------------+--------+ Warning This is an estimate of the databases disk usage. It does not represent its real size on disk. Conclusion Obtaining an approximate measure of your database usage can be done from the command line using the https://github.com/platformsh/platformsh-cli tool.",
        "text": "Goal Determine approximate database usage of a http://platform.sh/ project using the https://github.com/platformsh/platformsh-cli tool. Assumptions One or more http://platform.sh/ projects https://github.com/platformsh/platformsh-cli tool installed locally https://docs.platform.sh/development/ssh.html configured with the project account(s) Problems As a general rule, the database service should have at least 15% of disk space available on a http://Platform.sh project environment, so it is useful to be able to approximate the current disk usage of a project’s database(s). Steps 1. Log in to Platform CLI $ platform login 2. Find the project ID Note the project ID of the project you’re checking: $ platform project:list 3. Find the environment ID Note the environment ID for the environment you’re checking. $ platform environment:list -p 4. Run the database size command $ platform db:size -p -e The command will return something like: +----------------+-----------------+--------+ | Allocated disk | Estimated usage | % used | +----------------+-----------------+--------+ | 14.6 GiB | 9.4 GiB | ~ 64% | +----------------+-----------------+--------+ Warning This is an estimate of the databases disk usage. It does not represent its real size on disk. Conclusion Obtaining an approximate measure of your database usage can be done from the command line using the https://github.com/platformsh/platformsh-cli tool.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-determine-database-usage-of-an-environment/180",
        "relurl": "/t/how-to-determine-database-usage-of-an-environment/180"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "73211ec386367945926c44387c419d97efb10ac8",
        "title": "How to compile CSS using SASS on Platform.sh",
        "description": "Goal Have your SCSS Code compiled to CSS during deployment on http://Platform.sh Assumptions An empty http://Platform.sh project A local git repository, with Platform.as as a remote Knowledge of https://community.platform.sh/t/how-to-serve-a-static-html-page-on-platform-sh/51 Steps 1. Create a HTML file and link style.css. ./web/index.html \r\n\r\nFoobar\r\n\r\n lorem ipsum\r\n\r\n 2. Create a SCSS file . ./scss/style.scss $colors: ( background: rgb(26, 25, 43), text: rgb(255, 255, 255), ); body { background-color: map-get($colors, 'background'); color: map-get($colors, 'text'); } 3. Define default routes. ./.platform/routes.yaml https://{default}/: type: upstream upstream: sasshowto:http 4. Add empty services (we don’t need any in this example). ./.platform/services.yaml # empty 5. Add .platform.app.yaml configuration. ./.platform.app.yaml name: sasshowto # Any type will work as we just serve static HTML type: \"php:7.3\" # Install sass from npm dependencies: nodejs: sass: '~1.17.2' # Compile sass to css during the build hook and save output to web/style.css with compressed css for production hooks: build: | sass --style compressed scss/style.scss web/style.css disk: 256 web: locations: \"/\": # This tells Nginx to serve from the base directory root: \"web\" index: - \"index.html\" 6. Add, commit, and push these files to your empty http://Platform.sh project. git add . git commit -m \"Compile SASS to CSS\" git push -u platform master 7. Test by visiting the URL of your environment. Check that white text on a dark blue background is visible on the site. platform url Conclusion Every time the project is pushed to http://Platform.sh, compressed CSS will be generated from your SCSS-File and put into web/style.css.",
        "text": "Goal Have your SCSS Code compiled to CSS during deployment on http://Platform.sh Assumptions An empty http://Platform.sh project A local git repository, with Platform.as as a remote Knowledge of https://community.platform.sh/t/how-to-serve-a-static-html-page-on-platform-sh/51 Steps 1. Create a HTML file and link style.css. ./web/index.html \r\n\r\nFoobar\r\n\r\n lorem ipsum\r\n\r\n 2. Create a SCSS file . ./scss/style.scss $colors: ( background: rgb(26, 25, 43), text: rgb(255, 255, 255), ); body { background-color: map-get($colors, 'background'); color: map-get($colors, 'text'); } 3. Define default routes. ./.platform/routes.yaml https://{default}/: type: upstream upstream: sasshowto:http 4. Add empty services (we don’t need any in this example). ./.platform/services.yaml # empty 5. Add .platform.app.yaml configuration. ./.platform.app.yaml name: sasshowto # Any type will work as we just serve static HTML type: \"php:7.3\" # Install sass from npm dependencies: nodejs: sass: '~1.17.2' # Compile sass to css during the build hook and save output to web/style.css with compressed css for production hooks: build: | sass --style compressed scss/style.scss web/style.css disk: 256 web: locations: \"/\": # This tells Nginx to serve from the base directory root: \"web\" index: - \"index.html\" 6. Add, commit, and push these files to your empty http://Platform.sh project. git add . git commit -m \"Compile SASS to CSS\" git push -u platform master 7. Test by visiting the URL of your environment. Check that white text on a dark blue background is visible on the site. platform url Conclusion Every time the project is pushed to http://Platform.sh, compressed CSS will be generated from your SCSS-File and put into web/style.css.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-compile-css-using-sass-on-platform-sh/129",
        "relurl": "/t/how-to-compile-css-using-sass-on-platform-sh/129"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "563413d4acae78b93350a4c6adebacf58672992c",
        "title": "How to set up automated environment tasks",
        "description": "Goal To have one more more environments automatically backed up through snapshots triggered without manual intervention. Assumptions Access to a project hosted on http://platform.sh/ Your project account has administrator rights Knowledge on using the project web interface or the CLI tool set up in the environment, as described https://community.platform.sh/t/how-to-use-the-platform-sh-cli-from-the-web-container/ Problems The snapshot process is not automated and requires a manual trigger using the CLI or web interface. Steps 1. Check the CLI tool is correctly installed in the environment After logging in with SSH in the environment, you should be able to run platform and see the welcome prompt, together with a list of your projects. 2. Add a cron for automated environment snapshots Edit your .platform.app.yaml file and add the snapshot command in a cron. Example: crons: auto_snapshot: # The cron task will run everyday at 4 am (UTC) spec: '0 4 * * *' cmd: | if [ \"$PLATFORM_BRANCH\" = master ]; then platform backup:create --yes --no-wait fi The PLATFORM_BRANCH variable check ensures automatic snapshots are done only on the master environment. The --yes flag skips user interaction for the snapshot command. Ensure the --no-wait parameter is added to the command, in order to make the operation non-blocking (otherwise, your site will be down until the snapshot operation is completed). 3. Add a cron for automated SSL certificate renewal The provided Let’s Encrypt certificates have to be renewed every 3 months. The renewal is done automatically on every deployment, but if you do not deploy that often it is possible to have the SSL certificate expire. In order to ensure this does not happen, you can configure a cron job to automatically redeploy your environment. If there are no code changes, this will happen very fast and ensure your SSL certificate is refreshed. To do this, you need to edit your .platform.app.yaml just like before and add a cron job like in this example: crons: auto_renewcert: # Force a redeploy at 10 am (UTC) on the 1st and 15th of every month. spec: '0 10 1,15 * *' cmd: | if [ \"$PLATFORM_BRANCH\" = master ]; then platform redeploy --yes --no-wait fi Like above, PLATFORM_BRANCH variable check ensures the redeployment is done only on the master environment. Conclusion After setting up the CLI tool in the environment, we learned how to use it in order to trigger various maintenance tasks for the project.",
        "text": "Goal To have one more more environments automatically backed up through snapshots triggered without manual intervention. Assumptions Access to a project hosted on http://platform.sh/ Your project account has administrator rights Knowledge on using the project web interface or the CLI tool set up in the environment, as described https://community.platform.sh/t/how-to-use-the-platform-sh-cli-from-the-web-container/ Problems The snapshot process is not automated and requires a manual trigger using the CLI or web interface. Steps 1. Check the CLI tool is correctly installed in the environment After logging in with SSH in the environment, you should be able to run platform and see the welcome prompt, together with a list of your projects. 2. Add a cron for automated environment snapshots Edit your .platform.app.yaml file and add the snapshot command in a cron. Example: crons: auto_snapshot: # The cron task will run everyday at 4 am (UTC) spec: '0 4 * * *' cmd: | if [ \"$PLATFORM_BRANCH\" = master ]; then platform backup:create --yes --no-wait fi The PLATFORM_BRANCH variable check ensures automatic snapshots are done only on the master environment. The --yes flag skips user interaction for the snapshot command. Ensure the --no-wait parameter is added to the command, in order to make the operation non-blocking (otherwise, your site will be down until the snapshot operation is completed). 3. Add a cron for automated SSL certificate renewal The provided Let’s Encrypt certificates have to be renewed every 3 months. The renewal is done automatically on every deployment, but if you do not deploy that often it is possible to have the SSL certificate expire. In order to ensure this does not happen, you can configure a cron job to automatically redeploy your environment. If there are no code changes, this will happen very fast and ensure your SSL certificate is refreshed. To do this, you need to edit your .platform.app.yaml just like before and add a cron job like in this example: crons: auto_renewcert: # Force a redeploy at 10 am (UTC) on the 1st and 15th of every month. spec: '0 10 1,15 * *' cmd: | if [ \"$PLATFORM_BRANCH\" = master ]; then platform redeploy --yes --no-wait fi Like above, PLATFORM_BRANCH variable check ensures the redeployment is done only on the master environment. Conclusion After setting up the CLI tool in the environment, we learned how to use it in order to trigger various maintenance tasks for the project.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-set-up-automated-environment-tasks/127",
        "relurl": "/t/how-to-set-up-automated-environment-tasks/127"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "857da84ef00e2fabd0700a1594a1b917ce872c4a",
        "title": "How to use wkhtmltopdf on platform.sh",
        "description": "Goal https://wkhtmltopdf.org/ is a tool to generate PDF files from HTML. It can also be used from within https://www.drupal.org/project/wkhtmltopdf. This article will explain how to install it on http://platform.sh Installing You can install the wkhtmltopdf-binary using the https://rubygems.org/gems/wkhtmltopdf-binary/ . You do not need to run ruby to be able to use this. Add this to your .platform.app.yaml dependencies to install: dependencies: ruby: \"wkhtmltopdf-binary\": \" 0.12.5.1\" Note that you can specify the binary version. Check the https://rubygems.org/gems/wkhtmltopdf-binary/ for a full list of all versions. Important: Make sure you also add wkhtmltopdf -V to your build hook. This is needed because of the way the ruby gem works. The first time it runs, it unpacks the correct binary into your /app folder. And since we have a read-only files system by design, this can only be done in the build hook. hooks: build: | wkhtmltopdf -V Usage When you’re using the https://www.drupal.org/project/wkhtmltopdf it should already be working. You can also use a wrapper e.g. https://github.com/mikehaertl/phpwkhtmltopdf https://github.com/mikehaertl/phpwkhtmltopdf The default examples should work use mikehaertl\\wkhtmlto\\Pdf; // You can pass a filename, a HTML string, an URL or an options array to the constructor $pdf = new Pdf('/path/to/page.html'); if (!$pdf-saveAs('/path/to/page.pdf')) { $error = $pdf-getError(); // ... handle error here } If you have any other way of using it, make sure you are calling the alias wkhtmltopdf. Don’t use /app/.global/gems/wkhtmltopdf-binary-0.12.5/bin/wkhtmltopdf because that will cause problems upon upgrading the version. e.g. 0.0\" to this: dependencies: ruby: \"wkhtmltopdf-binary\": \" 0.12.5\" or any other version you specifically require.",
        "text": "Goal https://wkhtmltopdf.org/ is a tool to generate PDF files from HTML. It can also be used from within https://www.drupal.org/project/wkhtmltopdf. This article will explain how to install it on http://platform.sh Installing You can install the wkhtmltopdf-binary using the https://rubygems.org/gems/wkhtmltopdf-binary/ . You do not need to run ruby to be able to use this. Add this to your .platform.app.yaml dependencies to install: dependencies: ruby: \"wkhtmltopdf-binary\": \" 0.12.5.1\" Note that you can specify the binary version. Check the https://rubygems.org/gems/wkhtmltopdf-binary/ for a full list of all versions. Important: Make sure you also add wkhtmltopdf -V to your build hook. This is needed because of the way the ruby gem works. The first time it runs, it unpacks the correct binary into your /app folder. And since we have a read-only files system by design, this can only be done in the build hook. hooks: build: | wkhtmltopdf -V Usage When you’re using the https://www.drupal.org/project/wkhtmltopdf it should already be working. You can also use a wrapper e.g. https://github.com/mikehaertl/phpwkhtmltopdf https://github.com/mikehaertl/phpwkhtmltopdf The default examples should work use mikehaertl\\wkhtmlto\\Pdf; // You can pass a filename, a HTML string, an URL or an options array to the constructor $pdf = new Pdf('/path/to/page.html'); if (!$pdf-saveAs('/path/to/page.pdf')) { $error = $pdf-getError(); // ... handle error here } If you have any other way of using it, make sure you are calling the alias wkhtmltopdf. Don’t use /app/.global/gems/wkhtmltopdf-binary-0.12.5/bin/wkhtmltopdf because that will cause problems upon upgrading the version. e.g. 0.0\" to this: dependencies: ruby: \"wkhtmltopdf-binary\": \" 0.12.5\" or any other version you specifically require.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-use-wkhtmltopdf-on-platform-sh/445",
        "relurl": "/t/how-to-use-wkhtmltopdf-on-platform-sh/445"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "25d431e2584a4013b37088f9dcc7d770845986e2",
        "title": "Set up XDebug on Dedicated (Pro) server clusters",
        "description": "Use your local development environment to do breakpoint debugging on your remote server(s) This document is mostly about the nuts-and-bolts of establishing communications between your local development environment and the remote servers. It’s not a HOWTO use your IDE, it’s about how to diagnose network or service issues that are specific to the Platform hosting environment. Assumptions You will need: A local copy of your project site, with all code files available An IDE such as PHPStorm with integrated XDebug support. XDebug extensions installed and activated on your environments Conceptual overview Before getting started, it’s helpful to understand what happens at every step in an xdebugging process. You normally don’t have to worry about some of these layers, but if any one of them goes wrong, nothing will work, so this may help to narrow in on what does and doesn’t work, when you are setting up for the first time. Once all the server configurations are in place, and your IDE and the tunnels are set up to listen, what happens is this: You initiate an xdebug session from your browser, by requesting http://your.site?XDEBUG_SESSION_START=yourxdebugkey This request is routed to the outer (cache) layer of the hosting platform, where the presence of the XDEBUG_SESSION_START key should tell the system not to cache it. The request next passes to the load-balancing router, which forwards it to one of three web heads. It reaches the (nginx) webserver, which also recognises the XDEBUG_SESSION_START key, and so passes it to a dedicated xdebug-enabled php service for processing. This is to avoid the performance cost of running xdebug resident or on-demand inside a production PHP service. PHP script execution begins, and this starts sending stack trace messages to a socket on that server ( /run/platform/${PROJECTID}${SUFFIX}/xdebug.sock ). Your development environment has a tunnel open to each of the three web heads, which listens to that remote socket(s), and relays these messages to a local socket (port 9000) on your development machine. Your IDE is configured to listen to that port, so the incoming message triggers IDE debugging. Your IDE is configured to map the paths of php code files on the server to your local project folders, so it’s able to show you where in the local code, the remote process is currently stepping through. You can use your IDE to step through the execution stack, evaluate state, or run to breakpoints. This sends messages back over the tunnel to the running process, which then executes on the server. Eventually, execution and page build completes, and the response is sent back from the server, and your browser displays the page. For subsequent browser requests, an XDEBUG_SESSION cookie should have been set, and should provide the same effect as the XDEBUG_SESSION_START parameter for subsequent requests. That is what is supposed to happen when all is well. The routing, tunneling and the multi-head delegation of requests are the quirks specific to this hosting environment that you may need to know. Other tutorials on https://devdocs.magento.com/guides/v2.3/cloud/howtos/debug.html or https://crosp.net/blog/software-development/web/php/understanding-and-using-xdebug-with-phpstorm-and-magento-remotely/ should be referred to for deeper HOWTOs. For reference, the config settings on the server that make this happen can be inspected on the server at /etc/platform/$USER/php-fpm.xdebug.conf /etc/platform/$USER/php.xdebug.ini Activity logs are kept separate from the usual access logs, and are seen at /var/log/platform/$USER/xdebug.access.log /var/log/platform/$USER/php5-fpm-xdebug.log Remember, you are sometimes talking to three different servers at once. A https://docs.platform.sh/dedicated/overview.html (Previously known as “Pro”, “Platform Enterprise” or “PE”) site with integrated deployment management has several web heads in the . This makes connecting to “the server” indeterminate, as any one of three may be the server for a request, so keep this in mind as we go forwards. Getting started Getting the server configured For a “Dedicated” cluster, you need to have https://accounts.platform.sh/support . This may already be done for you, so please check before raising another ticket. They will have set xdebug_enable: true on your project, and provided you with a unique xdebug_key to use to initiate the session. Your xdebug_key is usually different between your production and your staging environments. Take note of which you are using. If xdebug has already been enabled, a record of your key may have been helpfully left in a text file /mnt/shared/ for your reference. If it’s not there, you can usually retrieve the xdebug key with a command like: platform ssh --environment=staging 'grep -A 3 XDebug /etc/platform/$USER/nginx.conf' # XDebug Configuration ## map \"$cookie_xdebug_session$arg_xdebug_session_start$arg_xdebug_session_stop\" $php_backend { \"Gd6QdPZaqnnSet32\" \"unix:///run/platform/myproject_stg/php5-xdebug.sock\"; = Your XDEBUGKEY=Gd6QdPZaqnnSet32 If you can’t find that key in your nginx.conf file either, then xdebug is probably not yet enabled for you, and you should raise the request. Local environment Assume any debugging should be happening on staging in the first case. PROJECTID=[projectID from ticket] BRANCH='staging' Or if you already have the project cloned locally: PROJECTID=$(platform project:info id) HOSTNAME=$(platform environment:info edge_hostname) XDebug talking to multiple webheads To listen to multiple possible sources of incoming xdebug connections, XDebug communication from all three heads need to be tunneled back to a port (9000) on our local environment. Most public XDebug tutorials won’t allow for this multiple-head issue. Open tunnels to the servers Here’s a small script that sets up 3 simultaneous tunnels: # Set these: PROJECTID=\"xxxxxxxxxxxxx\" XDEBUGKEY=\"yyyyyyyyyyyyy\" # Optionally change these: BRANCH=\"staging\"; # or 'master' PORT=9000 # These are the per-instance configurations you may need to change. # Review these URL=$( platform --project=${PROJECTID} --environment=${BRANCH} route:get 'https://{default}/' --property=url ) [ $BRANCH = 'staging' ] \u0026\u0026 SUFFIX='_stg' || SUFFIX='' SOCKETPATH=\"/run/platform/${PROJECTID}${SUFFIX}/xdebug.sock\" WEBHEADS=( $(platform --project=${PROJECTID} --environment=${BRANCH} ssh --pipe --all) ) # These are settings to be used for setting up the tunnels # Now open the tunnels for WEBHEAD in $WEBHEADS ; do echo \"Will listen to xdebug on webhead ${WEBHEAD}\" echo \"Clearing old xdebug socket on instance.\" ssh ${WEBHEAD} \"rm ${SOCKETPATH}\" echo \"Opening port forwarding to xdebug socket. Listening on port ${PORT} in the background.\" ssh -R ${SOCKETPATH}:localhost:${PORT} -N ${WEBHEAD} \u0026 done At this point, the remote server(s) should be sending you messages back down that tunnel. You next need to connect a listener on your end to do something with that info. To close the tunnels kill $(jobs -p) If using zsh, then “jobs -p” doesn’t work as expected. Instead, “kill %1 %2 %3 %4” may work. In my experience, the ssh tunnels time out on their own in about 10 minutes if idle. Timeout, server ssh.platform.cloud not responding. [1] Exit 255 ssh -R ${SOCKETPATH}:localhost:${PORT} -N ${WEBHEAD} To start debugging. Launch your browser with the key in the URL open \"${URL}?XDEBUG_SESSION_START=${XDEBUGKEY}\" Diagnostic: Ensure the request is hitting the server(s) correctly When that key is used (and is correct) during a browser session, transactions will be getting logged in xdebug.access.log on the server(s). You can check activity in these files to ensure that the request is even getting through - that it’s not being cached, and that the parameter is being passed through the router without being stripped. The following command will summarize the most recent xdebug.access.log from all webheads at once. WEBHEADS=$(platform ssh --project=$PROJECTID --environment=staging --pipe --all) echo $WEBHEADS | xargs -I% ssh % 'tail /var/log/platform/$USER/xdebug.access.log' Reasons the XDebug session may not be getting logged in xdebug.access.log You have three web heads, the load balancer may be sending your request to any one of them. You need to check all logs on all instances at once, not just one. Your $XDEBUGKEY is wrong. Double-check against the value in /etc/platform/$USER/nginx.conf. The URL you used was for a different branch than the one you are looking at The outside cache layer is intercepting the request. You can check if it’s cached using wget -I The inside router is not recognising or honoring the XDEBUGKEY Nginx is performing a redirect or an access denied before the request can be routed to php. You’ll need your IP to be whitelisted if using HTTP access controls. The xdebug php service (site-$USER-xdebug-php) is not running or responding. You should see it in the server process list (ps -axf). Investigating Things don’t always go smoothly, so here is a process of elimination to ensure that all things are set up as expected. To verify that xdebug configs have been deployed on the host(s) You can see the settings on the servers in /etc/platform/${PROJECTID}${SUFFIX}/php.xdebug.ini, looking like xdebug.remote_host = unix:///run/platform/xxxxxxxxxxxxx_stg/xdebug.sock To verify that xdebug is being loaded by PHP You may be able to check out a phpinfo diagnostic from within your web application and confirm xdebug is running. In Drupal this can be found underneath reports. Don’t verify using php -m command Note: Running basic diagnostics like commandline php -i on the server may not show that xdebug is enabled as php can be configured to use different settings for commandline than it does for web requests. The files php-fpm.conf, php-cli.conf, and php-fpm.xdebug.conf (deployed into in the apps etc folder) are different in that way, and are each used depending upon context of the request. To verify that the xdebug process is active xdebug.access.log The xdebug.conf tells us that the logs are at /var/log/platform/$USER/xdebug.access.log Tailing that log should show some current activity when a browser session activates xdebug with the xdebug key. When connected to an instance, tail -f /var/log/platform/${USER}/xdebug.access.log Or called directly from your environment: echo tail /var/log/platform/${PROJECTID}${SUFFIX}/xdebug.access.log | platform --project=${PROJECTID} --environment=${BRANCH} ssh echo tail /var/log/platform/${PROJECTID}${SUFFIX}/error.log | platform --project=${PROJECTID} --environment=${BRANCH} ssh Beware, the requests will actually happen on more than one instance, so you will only see some of the requests. Use tmux or similar tools to watch them all simultaneously. Gotcha if using non-standard project names Most dedicated hosting plans name your docroot after your project ID. Such as qazqaz234qaz or qazqaz234qaz_stg. Thus is the assumption used in the tunnel script that is configured to listen to the xdebug socket /run/platform/qazqaz234qaz/php5-xdebug.sock . However, if you are using a non-standard or legacy docroot name, some of these paths need to be updated. The socket may instead be something like /run/platform/shoppingsite/php5-xdebug.sock. Diagnosing if the cache layer is interfering If your outer cache layer (eg Fastly) is returning a previously cached version of the page, you will seem to be getting debuggable transactions at first, but later requests will fail to debug, sometimes unpredictably. Use curl -I \"${URL}?XDEBUG_SESSION_START=${XDEBUGKEY}\" a couple of times in a row, and look for x-cache: HIT, HIT in the response. If this is happening, you need to bypass the cache. This may be possible by disabling your cache, or configuring it to use the presence of the XDEBUG_SESSION_START argument to prevent caching - using the cache management configurations if you have access to them. To verify that the socket is open for communication with the debugger client After the client sets up a tunnel from their development side, the socket file mentioned in the configs should be seen on each of the servers. ls -la /run/platform/${USER}/xdebug.sock You should check to see that the last-modified date on it is recent - that reflects the last time the socket was set up. Beware timezones on the server are likely to be quite different to your own! Compare the date against the server time! echo $(( $(date +%s) - $(stat -c%Y \"/run/platform/${USER}/xdebug.sock\") )) seconds old.` Note: Don’t get distracted by php5-xdebug.sock seen in the same directory, that’s an internal socket used for communication between php-fpm and nginx To verify that messages are being sent down the pipe to the debugger client Listen for a bit When the tunnels are active, port 9000 on the developers machine is a window into the xdebug process. If it seems that xdebug is not firing at all, on the server you may sniff what’s happening on port 9000 with something like As a very basic test, running netcat --listen --local-port 9000 on the developer machine, and then visiting the website with the XDEBUG_SESSION_START key in the URL (or the XDEBUG_CONFIG set in a CLI environment) should result in the first raw xdebug message being shown on your console. https://xdebug.org/docs/remote . Doing this will stall the server, as you will not be able to respond with the expected sort of acknowledgements (just exit out) but if you get any sort of initial packet sent to that port, it shows that something xdebug is happening, and you need to work on the tunnels. It seems that unless you acknowledge that first message appropriately, no subsequent ones will be sent, and the server will hang there until you kill one end of the conversation, so there is a limit to what can be done without a real debugging tool, but this may at least prove that messages are getting through to the developers desktop. https://hackernoon.com/how-debug-php-applications-with-dephpugger-98cc234d917c is a quick CLI tool for this, but you probably want to just go straight to using a real IDE. Using an IDE to listen to xdebug messages With something like PHPStorm, you can just ‘start listening’ to port 9000 and when the first message arrives from the server, the wizard will ask you to match the incoming request (eg /app/${PROJECTID}_stg/index.php ) to a local file to begin breakpoint debugging. If you don’t have a local checkout of the project, well, you need to go get one to proceed now. XDebug on cli Note that xdebugging on the CLI does NOT log into the access log (not even the xdebug.access.log which is for web requests) so looking for clues there will not help. You can trigger xdebug behaviour on the CLI using a custom php.ini, by setting an environment variable export XDEBUG_CONFIG=\"remote_enable=1\" or by specifying everything up front in the commandline arguments …though all methods ALSO need XDEBUG_CONFIG to at least be set to SOMETHING. To test XDebug is working in a snippet As a single command is the most straightforward for testing, XDebug can be triggered minimally with: # On the host: SOCKETPATH=\"unix:///run/platform/${USER}/xdebug.sock\" export XDEBUG_CONFIG=\"remote_enable=1 remote_host=$SOCKETPATH\" php \\ -dzend_extension=xdebug.so \\ index.php If you have a listener open on port 9000 on your local dev, it’ll start getting messages. I haven’t been able to find a way to get logs of these transactions, so it’s up to you to be listening correctly. To use the php.xdebug.ini To work as designed however, a php.xdebug.ini has been provided. To use that, you should invoke php, source the special ini, and also must set XDEBUG_CONFIG to non-null in your session. PHPXDEBUG=/etc/platform/${USER}/php.xdebug.ini export XDEBUG_CONFIG=true php -c $PHPXDEBUG index.php … and stuff should be coming down the socket. Interesting snippets: PHPXDEBUG=/etc/platform/$USER/php.xdebug.ini php -c $PHPXDEBUG -r ‘echo(ini_get(“xdebug.idekey”));’ Go and trace your project The real fun begins after the tunnels are set up and the XDebug communications are happening. Other tutorials from your IDE will probably be more helpful than can be covered here.",
        "text": "Use your local development environment to do breakpoint debugging on your remote server(s) This document is mostly about the nuts-and-bolts of establishing communications between your local development environment and the remote servers. It’s not a HOWTO use your IDE, it’s about how to diagnose network or service issues that are specific to the Platform hosting environment. Assumptions You will need: A local copy of your project site, with all code files available An IDE such as PHPStorm with integrated XDebug support. XDebug extensions installed and activated on your environments Conceptual overview Before getting started, it’s helpful to understand what happens at every step in an xdebugging process. You normally don’t have to worry about some of these layers, but if any one of them goes wrong, nothing will work, so this may help to narrow in on what does and doesn’t work, when you are setting up for the first time. Once all the server configurations are in place, and your IDE and the tunnels are set up to listen, what happens is this: You initiate an xdebug session from your browser, by requesting http://your.site?XDEBUG_SESSION_START=yourxdebugkey This request is routed to the outer (cache) layer of the hosting platform, where the presence of the XDEBUG_SESSION_START key should tell the system not to cache it. The request next passes to the load-balancing router, which forwards it to one of three web heads. It reaches the (nginx) webserver, which also recognises the XDEBUG_SESSION_START key, and so passes it to a dedicated xdebug-enabled php service for processing. This is to avoid the performance cost of running xdebug resident or on-demand inside a production PHP service. PHP script execution begins, and this starts sending stack trace messages to a socket on that server ( /run/platform/${PROJECTID}${SUFFIX}/xdebug.sock ). Your development environment has a tunnel open to each of the three web heads, which listens to that remote socket(s), and relays these messages to a local socket (port 9000) on your development machine. Your IDE is configured to listen to that port, so the incoming message triggers IDE debugging. Your IDE is configured to map the paths of php code files on the server to your local project folders, so it’s able to show you where in the local code, the remote process is currently stepping through. You can use your IDE to step through the execution stack, evaluate state, or run to breakpoints. This sends messages back over the tunnel to the running process, which then executes on the server. Eventually, execution and page build completes, and the response is sent back from the server, and your browser displays the page. For subsequent browser requests, an XDEBUG_SESSION cookie should have been set, and should provide the same effect as the XDEBUG_SESSION_START parameter for subsequent requests. That is what is supposed to happen when all is well. The routing, tunneling and the multi-head delegation of requests are the quirks specific to this hosting environment that you may need to know. Other tutorials on https://devdocs.magento.com/guides/v2.3/cloud/howtos/debug.html or https://crosp.net/blog/software-development/web/php/understanding-and-using-xdebug-with-phpstorm-and-magento-remotely/ should be referred to for deeper HOWTOs. For reference, the config settings on the server that make this happen can be inspected on the server at /etc/platform/$USER/php-fpm.xdebug.conf /etc/platform/$USER/php.xdebug.ini Activity logs are kept separate from the usual access logs, and are seen at /var/log/platform/$USER/xdebug.access.log /var/log/platform/$USER/php5-fpm-xdebug.log Remember, you are sometimes talking to three different servers at once. A https://docs.platform.sh/dedicated/overview.html (Previously known as “Pro”, “Platform Enterprise” or “PE”) site with integrated deployment management has several web heads in the . This makes connecting to “the server” indeterminate, as any one of three may be the server for a request, so keep this in mind as we go forwards. Getting started Getting the server configured For a “Dedicated” cluster, you need to have https://accounts.platform.sh/support . This may already be done for you, so please check before raising another ticket. They will have set xdebug_enable: true on your project, and provided you with a unique xdebug_key to use to initiate the session. Your xdebug_key is usually different between your production and your staging environments. Take note of which you are using. If xdebug has already been enabled, a record of your key may have been helpfully left in a text file /mnt/shared/ for your reference. If it’s not there, you can usually retrieve the xdebug key with a command like: platform ssh --environment=staging 'grep -A 3 XDebug /etc/platform/$USER/nginx.conf' # XDebug Configuration ## map \"$cookie_xdebug_session$arg_xdebug_session_start$arg_xdebug_session_stop\" $php_backend { \"Gd6QdPZaqnnSet32\" \"unix:///run/platform/myproject_stg/php5-xdebug.sock\"; = Your XDEBUGKEY=Gd6QdPZaqnnSet32 If you can’t find that key in your nginx.conf file either, then xdebug is probably not yet enabled for you, and you should raise the request. Local environment Assume any debugging should be happening on staging in the first case. PROJECTID=[projectID from ticket] BRANCH='staging' Or if you already have the project cloned locally: PROJECTID=$(platform project:info id) HOSTNAME=$(platform environment:info edge_hostname) XDebug talking to multiple webheads To listen to multiple possible sources of incoming xdebug connections, XDebug communication from all three heads need to be tunneled back to a port (9000) on our local environment. Most public XDebug tutorials won’t allow for this multiple-head issue. Open tunnels to the servers Here’s a small script that sets up 3 simultaneous tunnels: # Set these: PROJECTID=\"xxxxxxxxxxxxx\" XDEBUGKEY=\"yyyyyyyyyyyyy\" # Optionally change these: BRANCH=\"staging\"; # or 'master' PORT=9000 # These are the per-instance configurations you may need to change. # Review these URL=$( platform --project=${PROJECTID} --environment=${BRANCH} route:get 'https://{default}/' --property=url ) [ $BRANCH = 'staging' ] \u0026\u0026 SUFFIX='_stg' || SUFFIX='' SOCKETPATH=\"/run/platform/${PROJECTID}${SUFFIX}/xdebug.sock\" WEBHEADS=( $(platform --project=${PROJECTID} --environment=${BRANCH} ssh --pipe --all) ) # These are settings to be used for setting up the tunnels # Now open the tunnels for WEBHEAD in $WEBHEADS ; do echo \"Will listen to xdebug on webhead ${WEBHEAD}\" echo \"Clearing old xdebug socket on instance.\" ssh ${WEBHEAD} \"rm ${SOCKETPATH}\" echo \"Opening port forwarding to xdebug socket. Listening on port ${PORT} in the background.\" ssh -R ${SOCKETPATH}:localhost:${PORT} -N ${WEBHEAD} \u0026 done At this point, the remote server(s) should be sending you messages back down that tunnel. You next need to connect a listener on your end to do something with that info. To close the tunnels kill $(jobs -p) If using zsh, then “jobs -p” doesn’t work as expected. Instead, “kill %1 %2 %3 %4” may work. In my experience, the ssh tunnels time out on their own in about 10 minutes if idle. Timeout, server ssh.platform.cloud not responding. [1] Exit 255 ssh -R ${SOCKETPATH}:localhost:${PORT} -N ${WEBHEAD} To start debugging. Launch your browser with the key in the URL open \"${URL}?XDEBUG_SESSION_START=${XDEBUGKEY}\" Diagnostic: Ensure the request is hitting the server(s) correctly When that key is used (and is correct) during a browser session, transactions will be getting logged in xdebug.access.log on the server(s). You can check activity in these files to ensure that the request is even getting through - that it’s not being cached, and that the parameter is being passed through the router without being stripped. The following command will summarize the most recent xdebug.access.log from all webheads at once. WEBHEADS=$(platform ssh --project=$PROJECTID --environment=staging --pipe --all) echo $WEBHEADS | xargs -I% ssh % 'tail /var/log/platform/$USER/xdebug.access.log' Reasons the XDebug session may not be getting logged in xdebug.access.log You have three web heads, the load balancer may be sending your request to any one of them. You need to check all logs on all instances at once, not just one. Your $XDEBUGKEY is wrong. Double-check against the value in /etc/platform/$USER/nginx.conf. The URL you used was for a different branch than the one you are looking at The outside cache layer is intercepting the request. You can check if it’s cached using wget -I The inside router is not recognising or honoring the XDEBUGKEY Nginx is performing a redirect or an access denied before the request can be routed to php. You’ll need your IP to be whitelisted if using HTTP access controls. The xdebug php service (site-$USER-xdebug-php) is not running or responding. You should see it in the server process list (ps -axf). Investigating Things don’t always go smoothly, so here is a process of elimination to ensure that all things are set up as expected. To verify that xdebug configs have been deployed on the host(s) You can see the settings on the servers in /etc/platform/${PROJECTID}${SUFFIX}/php.xdebug.ini, looking like xdebug.remote_host = unix:///run/platform/xxxxxxxxxxxxx_stg/xdebug.sock To verify that xdebug is being loaded by PHP You may be able to check out a phpinfo diagnostic from within your web application and confirm xdebug is running. In Drupal this can be found underneath reports. Don’t verify using php -m command Note: Running basic diagnostics like commandline php -i on the server may not show that xdebug is enabled as php can be configured to use different settings for commandline than it does for web requests. The files php-fpm.conf, php-cli.conf, and php-fpm.xdebug.conf (deployed into in the apps etc folder) are different in that way, and are each used depending upon context of the request. To verify that the xdebug process is active xdebug.access.log The xdebug.conf tells us that the logs are at /var/log/platform/$USER/xdebug.access.log Tailing that log should show some current activity when a browser session activates xdebug with the xdebug key. When connected to an instance, tail -f /var/log/platform/${USER}/xdebug.access.log Or called directly from your environment: echo tail /var/log/platform/${PROJECTID}${SUFFIX}/xdebug.access.log | platform --project=${PROJECTID} --environment=${BRANCH} ssh echo tail /var/log/platform/${PROJECTID}${SUFFIX}/error.log | platform --project=${PROJECTID} --environment=${BRANCH} ssh Beware, the requests will actually happen on more than one instance, so you will only see some of the requests. Use tmux or similar tools to watch them all simultaneously. Gotcha if using non-standard project names Most dedicated hosting plans name your docroot after your project ID. Such as qazqaz234qaz or qazqaz234qaz_stg. Thus is the assumption used in the tunnel script that is configured to listen to the xdebug socket /run/platform/qazqaz234qaz/php5-xdebug.sock . However, if you are using a non-standard or legacy docroot name, some of these paths need to be updated. The socket may instead be something like /run/platform/shoppingsite/php5-xdebug.sock. Diagnosing if the cache layer is interfering If your outer cache layer (eg Fastly) is returning a previously cached version of the page, you will seem to be getting debuggable transactions at first, but later requests will fail to debug, sometimes unpredictably. Use curl -I \"${URL}?XDEBUG_SESSION_START=${XDEBUGKEY}\" a couple of times in a row, and look for x-cache: HIT, HIT in the response. If this is happening, you need to bypass the cache. This may be possible by disabling your cache, or configuring it to use the presence of the XDEBUG_SESSION_START argument to prevent caching - using the cache management configurations if you have access to them. To verify that the socket is open for communication with the debugger client After the client sets up a tunnel from their development side, the socket file mentioned in the configs should be seen on each of the servers. ls -la /run/platform/${USER}/xdebug.sock You should check to see that the last-modified date on it is recent - that reflects the last time the socket was set up. Beware timezones on the server are likely to be quite different to your own! Compare the date against the server time! echo $(( $(date +%s) - $(stat -c%Y \"/run/platform/${USER}/xdebug.sock\") )) seconds old.` Note: Don’t get distracted by php5-xdebug.sock seen in the same directory, that’s an internal socket used for communication between php-fpm and nginx To verify that messages are being sent down the pipe to the debugger client Listen for a bit When the tunnels are active, port 9000 on the developers machine is a window into the xdebug process. If it seems that xdebug is not firing at all, on the server you may sniff what’s happening on port 9000 with something like As a very basic test, running netcat --listen --local-port 9000 on the developer machine, and then visiting the website with the XDEBUG_SESSION_START key in the URL (or the XDEBUG_CONFIG set in a CLI environment) should result in the first raw xdebug message being shown on your console. https://xdebug.org/docs/remote . Doing this will stall the server, as you will not be able to respond with the expected sort of acknowledgements (just exit out) but if you get any sort of initial packet sent to that port, it shows that something xdebug is happening, and you need to work on the tunnels. It seems that unless you acknowledge that first message appropriately, no subsequent ones will be sent, and the server will hang there until you kill one end of the conversation, so there is a limit to what can be done without a real debugging tool, but this may at least prove that messages are getting through to the developers desktop. https://hackernoon.com/how-debug-php-applications-with-dephpugger-98cc234d917c is a quick CLI tool for this, but you probably want to just go straight to using a real IDE. Using an IDE to listen to xdebug messages With something like PHPStorm, you can just ‘start listening’ to port 9000 and when the first message arrives from the server, the wizard will ask you to match the incoming request (eg /app/${PROJECTID}_stg/index.php ) to a local file to begin breakpoint debugging. If you don’t have a local checkout of the project, well, you need to go get one to proceed now. XDebug on cli Note that xdebugging on the CLI does NOT log into the access log (not even the xdebug.access.log which is for web requests) so looking for clues there will not help. You can trigger xdebug behaviour on the CLI using a custom php.ini, by setting an environment variable export XDEBUG_CONFIG=\"remote_enable=1\" or by specifying everything up front in the commandline arguments …though all methods ALSO need XDEBUG_CONFIG to at least be set to SOMETHING. To test XDebug is working in a snippet As a single command is the most straightforward for testing, XDebug can be triggered minimally with: # On the host: SOCKETPATH=\"unix:///run/platform/${USER}/xdebug.sock\" export XDEBUG_CONFIG=\"remote_enable=1 remote_host=$SOCKETPATH\" php \\ -dzend_extension=xdebug.so \\ index.php If you have a listener open on port 9000 on your local dev, it’ll start getting messages. I haven’t been able to find a way to get logs of these transactions, so it’s up to you to be listening correctly. To use the php.xdebug.ini To work as designed however, a php.xdebug.ini has been provided. To use that, you should invoke php, source the special ini, and also must set XDEBUG_CONFIG to non-null in your session. PHPXDEBUG=/etc/platform/${USER}/php.xdebug.ini export XDEBUG_CONFIG=true php -c $PHPXDEBUG index.php … and stuff should be coming down the socket. Interesting snippets: PHPXDEBUG=/etc/platform/$USER/php.xdebug.ini php -c $PHPXDEBUG -r ‘echo(ini_get(“xdebug.idekey”));’ Go and trace your project The real fun begins after the tunnels are set up and the XDebug communications are happening. Other tutorials from your IDE will probably be more helpful than can be covered here.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/set-up-xdebug-on-dedicated-pro-server-clusters/403",
        "relurl": "/t/set-up-xdebug-on-dedicated-pro-server-clusters/403"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "464345847de3070ff40f5ba7bcf9209ca1ae6a07",
        "title": "How to configure a php mail library (like Drupal 7 phpmailer) to use the correct server",
        "description": "Goal To send mail, using a third party PHP library, In a Drupal environment. Assumptions This is for Drupal 7, and its https://www.drupal.org/project/phpmailer. However, the concepts may apply to any PHP app that needs SMTP mail server details provided to it during configuration. You have admin access to your Drupal site. You have write access to your sites settings.php (by way of git deployments). You have read the initial , and taken notes that you need to explicitly enable emails on dev environments if needed. Problems Mail doesn’t send, because I’m using an advanced library. Test emails are not delivered. Watchdog reports, or drush wd-show shows messages like Error sending e-mail SMTP Error: SMTP connect() failed. By default, using mail() on a http://Platform.sh environment is already configured to just work, as The PHP runtime is configured to send email automatically via the assigned SendGrid sub-account. However, if using additional mail libraries like SwiftMailer, Drupals https://www.drupal.org/project/smtp or https://www.drupal.org/project/phpmailer, the basic PHP mail() is deliberately bypassed, and additional SMTP settings are exposed, and must now be configured by you. Steps If using Drupal 7 phpmailer: 1. Check the settings At the admin page found at /admin/config/system/phpmailer check the Primary SMTP server. It should not be localhost. It may need to be set to whatever the value of the Platform.sh-provided environment variable PLATFORM_SMTP_HOST is for your hosting environment. This may be different depending on region or plan. . Hint: you can see the environment variables available to your runtime by visiting your phpinfo page at /admin/reports/status/php - though it’s also available by ssh-ing in also. platform ssh ‘echo PLATFORM_SMTP_HOST is $PLATFORM_SMTP_HOST’ Copy that host value (an IP) into your Primary SMTP server setting at /admin/config/system/phpmailer. The SMTP port should remain 25. Test that mail works by entering your email address in the Test Configuration section when you save. 2. Configure these settings in your settings.php file, not just the UI For protection against database overwrites, and to ensure the settings remain correct if you have to move servers around, it can be better to copy this environment variable out of the live environment, and into Drupals configuration settings directly. Inspecting the settings ( drush vget smtp ) tells us that the configuration key we need to update is smtp_host. Edit your sites settings.php or equivalent, and add a line like: $conf['smtp_host'] = getenv('PLATFORM_SMTP_HOST'); Test email sending works. Conclusion The mail delivery subsystem is now configured to send mail using the named server explicitly. If things change, the correct SMTP server should be updated also. As noted earlier, the default behaviour with no special mail handling subsystem, is expected to work automatically. It’s only the more configurable ones that may have incorrect placeholders, or may require you to provide a value explicitly.",
        "text": "Goal To send mail, using a third party PHP library, In a Drupal environment. Assumptions This is for Drupal 7, and its https://www.drupal.org/project/phpmailer. However, the concepts may apply to any PHP app that needs SMTP mail server details provided to it during configuration. You have admin access to your Drupal site. You have write access to your sites settings.php (by way of git deployments). You have read the initial , and taken notes that you need to explicitly enable emails on dev environments if needed. Problems Mail doesn’t send, because I’m using an advanced library. Test emails are not delivered. Watchdog reports, or drush wd-show shows messages like Error sending e-mail SMTP Error: SMTP connect() failed. By default, using mail() on a http://Platform.sh environment is already configured to just work, as The PHP runtime is configured to send email automatically via the assigned SendGrid sub-account. However, if using additional mail libraries like SwiftMailer, Drupals https://www.drupal.org/project/smtp or https://www.drupal.org/project/phpmailer, the basic PHP mail() is deliberately bypassed, and additional SMTP settings are exposed, and must now be configured by you. Steps If using Drupal 7 phpmailer: 1. Check the settings At the admin page found at /admin/config/system/phpmailer check the Primary SMTP server. It should not be localhost. It may need to be set to whatever the value of the Platform.sh-provided environment variable PLATFORM_SMTP_HOST is for your hosting environment. This may be different depending on region or plan. . Hint: you can see the environment variables available to your runtime by visiting your phpinfo page at /admin/reports/status/php - though it’s also available by ssh-ing in also. platform ssh ‘echo PLATFORM_SMTP_HOST is $PLATFORM_SMTP_HOST’ Copy that host value (an IP) into your Primary SMTP server setting at /admin/config/system/phpmailer. The SMTP port should remain 25. Test that mail works by entering your email address in the Test Configuration section when you save. 2. Configure these settings in your settings.php file, not just the UI For protection against database overwrites, and to ensure the settings remain correct if you have to move servers around, it can be better to copy this environment variable out of the live environment, and into Drupals configuration settings directly. Inspecting the settings ( drush vget smtp ) tells us that the configuration key we need to update is smtp_host. Edit your sites settings.php or equivalent, and add a line like: $conf['smtp_host'] = getenv('PLATFORM_SMTP_HOST'); Test email sending works. Conclusion The mail delivery subsystem is now configured to send mail using the named server explicitly. If things change, the correct SMTP server should be updated also. As noted earlier, the default behaviour with no special mail handling subsystem, is expected to work automatically. It’s only the more configurable ones that may have incorrect placeholders, or may require you to provide a value explicitly.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-configure-a-php-mail-library-like-drupal-7-phpmailer-to-use-the-correct-server/437",
        "relurl": "/t/how-to-configure-a-php-mail-library-like-drupal-7-phpmailer-to-use-the-correct-server/437"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "712240881c66b83d9fdcaf337719d6ead3239b78",
        "title": "How do I get a postgres connection string for prisma2 framework",
        "description": "Goal To create a postgresql connection string from the PLATFORM_RELATIONSHIPS environment variable. Assumptions You have nodejs installed in your container. You have postgresql You know how to work with prisma2 Problem You can get database credentials using the $PLATFORM_RELATIONSHIPS variable echo $PLATFORM_RELATIONSHIPS | base64 --decode | json_pp But… the prisma2 framework assumes you have a postgresql connection string https://github.com/prisma/prisma2/blob/master/docs/core/connectors/postgresql.md . Proposed solution We can make a simple nodejs script that takes the base64 decoded string from PLATFORM_RELATIONSHIPS, and converts it into a postgresql connection string. In a bash script, that would look like this: $ DECODED_RELATIONSHIPS=$(echo $PLATFORM_RELATIONSHIPS | base64 --decode) $ nodejs get_postgres_querystring.js $DECODED_RELATIONSHIPS postgresdatabase postgres://main:main@postgresdatabase.internal:5432/main Where the last variable, postgresdatabase, is the name of the relationship. And get_postgres_querystring.js looks like this: var myArgs = process.argv.slice(2); var relationships = myArgs[0]; var db = myArgs[1]; var json_rel = JSON.parse(relationships); var json_db = json_rel[db][0]; var url = \"postgres://\"+json_db[\"username\"]+\":\"+json_db[\"password\"]+\"@\"+json_db[\"host\"]+\":\"+json_db[\"port\"]+\"/\"+json_db[\"path\"]; console.log(url); Integrate into your environment Create a .environment file According to the documentation we can create an that allows us to set environment variables. Create a .environment file and put what we have learned above in it, but add the output to an environment variable using export $ DECODED_RELATIONSHIPS=$(echo $PLATFORM_RELATIONSHIPS | base64 --decode) $ connect_string=$(nodejs qs.js $DECODED_RELATIONSHIPS postgresdatabase) $ export PS_CON_STRING=\"$connect_string\" Using the environment variable You can now use env(\"PS_CON_STRING\") in your https://github.com/prisma/prisma2/blob/master/docs/core/connectors/postgresql.md .",
        "text": "Goal To create a postgresql connection string from the PLATFORM_RELATIONSHIPS environment variable. Assumptions You have nodejs installed in your container. You have postgresql You know how to work with prisma2 Problem You can get database credentials using the $PLATFORM_RELATIONSHIPS variable echo $PLATFORM_RELATIONSHIPS | base64 --decode | json_pp But… the prisma2 framework assumes you have a postgresql connection string https://github.com/prisma/prisma2/blob/master/docs/core/connectors/postgresql.md . Proposed solution We can make a simple nodejs script that takes the base64 decoded string from PLATFORM_RELATIONSHIPS, and converts it into a postgresql connection string. In a bash script, that would look like this: $ DECODED_RELATIONSHIPS=$(echo $PLATFORM_RELATIONSHIPS | base64 --decode) $ nodejs get_postgres_querystring.js $DECODED_RELATIONSHIPS postgresdatabase postgres://main:main@postgresdatabase.internal:5432/main Where the last variable, postgresdatabase, is the name of the relationship. And get_postgres_querystring.js looks like this: var myArgs = process.argv.slice(2); var relationships = myArgs[0]; var db = myArgs[1]; var json_rel = JSON.parse(relationships); var json_db = json_rel[db][0]; var url = \"postgres://\"+json_db[\"username\"]+\":\"+json_db[\"password\"]+\"@\"+json_db[\"host\"]+\":\"+json_db[\"port\"]+\"/\"+json_db[\"path\"]; console.log(url); Integrate into your environment Create a .environment file According to the documentation we can create an that allows us to set environment variables. Create a .environment file and put what we have learned above in it, but add the output to an environment variable using export $ DECODED_RELATIONSHIPS=$(echo $PLATFORM_RELATIONSHIPS | base64 --decode) $ connect_string=$(nodejs qs.js $DECODED_RELATIONSHIPS postgresdatabase) $ export PS_CON_STRING=\"$connect_string\" Using the environment variable You can now use env(\"PS_CON_STRING\") in your https://github.com/prisma/prisma2/blob/master/docs/core/connectors/postgresql.md .",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-get-a-postgres-connection-string-for-prisma2-framework/427",
        "relurl": "/t/how-do-i-get-a-postgres-connection-string-for-prisma2-framework/427"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "5888f35719f97cac1eb7fa44e5a5736c5ced5b2e",
        "title": "How to back up environment variables",
        "description": "Introduction You always need to store some sensitive data for your application to work. But you should never store them in your git repository. For this, you can use environment variables to define per-environment configuration settings and other sensitive information. Each project depends on those settings and will not work without the correct value in each variable. The default ‘restore from backup’ procedure on http://platform.sh will restore your code and data but it will not store environment variables because of safety concerns. This article describes how you can backup your variables in a secure and easy way. Note: This article is based on a https://www.contextualcode.com/Blog/backup-environment-variables-on-platform.sh written by Ivan Ternovtsiy from Contextual Code. Contextual Code relies heavily on http://Platform.Sh to host their customer’s websites. Feel free to check out their original blog post and website. Prerequisites You will need: https://www.php.net/manual/en/openssl.installation.php PHP extension. Installed on http://platform.sh by default. https://aws.amazon.com/cli/ installed and configured with access to the backup bucket dependencies: python2: awscli: '*' installed and configured with access to project variables Set your token in the PLATFORMSH_CLI_TOKEN environment variable. update .platform.app.yaml hooks: build: | if [ ! -z \"$PLATFORMSH_CLI_TOKEN\" ]; then curl -sS https://platform.sh/cli/installer | php fi Backup To configure a daily backup, follow these steps. It will add a daily backup with 7 days of retention. Storage data files are named PROJECT_BRANCH_EnvVars_DAY.json. The command MUST be executed on the http://platform.sh instance, otherwise, it will not be able to access sensitive variables value. Set http://Platform.sh variables: platform variable:create --level=project --name=BACKUP_ENVVAR_S3_DIRECTORY --value=\"s3://your-bucket/platformsh-env-variables\" --json=false --sensitive=false --prefix=env --visible-build=false --visible-runtime=true Copy backup utility tool from https://gitlab.com/contextualcode/platformsh-migration/raw/master/utils/backup_environment_variables.php cd /bin/php wget https://gitlab.com/contextualcode/platformsh-migration/raw/master/utils/backup_environment_variables.php Add backup_env_vars_to_s3 cron ( .platform.app.yaml): backup_env_vars_to_s3: spec: '45 23 * * *' cmd: php bin/php/backup_environment_variables.php Commit the changes: git add .platform.app.yaml bin/php/backup_environment_variables.php git commit -m \"Add backup environment variables to AWS S3\" git push Under the hood Below is a summary of what the code is actually doing, but feel free to check the source code. It’s always a good idea to inspect third party code before running it, especially when sensitive information is at risk. read list of environment variables using “platform variables” command. read actual value of sensitive variable using php “getenv()” function. read variable options (level, is sensitive, is inheritable, is json) build json with ‘method’, ‘iv’, ‘data’, ‘tag’. ‘method’ - encryption method. AES-256-GCM for PHP7.1+ and AES-256-CBC otherwise. ‘iv’ - random initialization vector, used during encryption. ‘tag’ - encryption tag. Used for AES-256-GCM, empty for AES-256-CBC. ‘data’ - encrypted json with variables list. store json to file and push it to S3 using aws s3 cp command. Restore The ‘restore’ command can be triggered from a http://platform.sh instance or outside. Keep in mind that execution outside of http://platform.sh will not be able to perform “same exists” checks for sensitive variables. As a result, it will trigger the update of all variable disregard existing variable values. To restore values from http://Platform.sh. On http://platform.sh, copy file from https://gitlab.com/contextualcode/platformsh-migration/raw/master/utils/restore_environment_variables.php cd /tmp wget https://gitlab.com/contextualcode/platformsh-migration/raw/master/utils/restore_environment_variables.php Check the latest backup filename and restore variables from it. Use --dry-run to see what will be created/updated. aws s3 ls s3://your-bucket/platformsh-env-variables/ php /tmp/restore_environment_variables.php --dry-run --s3dir=s3://your-bucket/platformsh-env-variables --s3filename=projectname_stageEnvVars_FRI.json --override-existing --secret=ChangeTheSecret After changing per environment variable, http://platform.sh will trigger a redeploy, so you’ll need to re-execute step 2 a few times to get all the variables restored. Under the hood Below is a summary of what the code is actually doing, but feel free to check the source code. It’s always a good idea to inspect third party code before running it, especially when sensitive information is at risk. download file from S3 using aws s3 cp. decrypt it with PHP “openssl_decrypt” function compare variables in file with existing variables create variables using platform variable:create command. ignore variables with the same value as already defined in the environment skip existing variables unless --override-existing option specified. update variables using platform variable:update command. Conclusion That’s it! You should now have a backup and restore process for your environment variables!",
        "text": "Introduction You always need to store some sensitive data for your application to work. But you should never store them in your git repository. For this, you can use environment variables to define per-environment configuration settings and other sensitive information. Each project depends on those settings and will not work without the correct value in each variable. The default ‘restore from backup’ procedure on http://platform.sh will restore your code and data but it will not store environment variables because of safety concerns. This article describes how you can backup your variables in a secure and easy way. Note: This article is based on a https://www.contextualcode.com/Blog/backup-environment-variables-on-platform.sh written by Ivan Ternovtsiy from Contextual Code. Contextual Code relies heavily on http://Platform.Sh to host their customer’s websites. Feel free to check out their original blog post and website. Prerequisites You will need: https://www.php.net/manual/en/openssl.installation.php PHP extension. Installed on http://platform.sh by default. https://aws.amazon.com/cli/ installed and configured with access to the backup bucket dependencies: python2: awscli: '*' installed and configured with access to project variables Set your token in the PLATFORMSH_CLI_TOKEN environment variable. update .platform.app.yaml hooks: build: | if [ ! -z \"$PLATFORMSH_CLI_TOKEN\" ]; then curl -sS https://platform.sh/cli/installer | php fi Backup To configure a daily backup, follow these steps. It will add a daily backup with 7 days of retention. Storage data files are named PROJECT_BRANCH_EnvVars_DAY.json. The command MUST be executed on the http://platform.sh instance, otherwise, it will not be able to access sensitive variables value. Set http://Platform.sh variables: platform variable:create --level=project --name=BACKUP_ENVVAR_S3_DIRECTORY --value=\"s3://your-bucket/platformsh-env-variables\" --json=false --sensitive=false --prefix=env --visible-build=false --visible-runtime=true Copy backup utility tool from https://gitlab.com/contextualcode/platformsh-migration/raw/master/utils/backup_environment_variables.php cd /bin/php wget https://gitlab.com/contextualcode/platformsh-migration/raw/master/utils/backup_environment_variables.php Add backup_env_vars_to_s3 cron ( .platform.app.yaml): backup_env_vars_to_s3: spec: '45 23 * * *' cmd: php bin/php/backup_environment_variables.php Commit the changes: git add .platform.app.yaml bin/php/backup_environment_variables.php git commit -m \"Add backup environment variables to AWS S3\" git push Under the hood Below is a summary of what the code is actually doing, but feel free to check the source code. It’s always a good idea to inspect third party code before running it, especially when sensitive information is at risk. read list of environment variables using “platform variables” command. read actual value of sensitive variable using php “getenv()” function. read variable options (level, is sensitive, is inheritable, is json) build json with ‘method’, ‘iv’, ‘data’, ‘tag’. ‘method’ - encryption method. AES-256-GCM for PHP7.1+ and AES-256-CBC otherwise. ‘iv’ - random initialization vector, used during encryption. ‘tag’ - encryption tag. Used for AES-256-GCM, empty for AES-256-CBC. ‘data’ - encrypted json with variables list. store json to file and push it to S3 using aws s3 cp command. Restore The ‘restore’ command can be triggered from a http://platform.sh instance or outside. Keep in mind that execution outside of http://platform.sh will not be able to perform “same exists” checks for sensitive variables. As a result, it will trigger the update of all variable disregard existing variable values. To restore values from http://Platform.sh. On http://platform.sh, copy file from https://gitlab.com/contextualcode/platformsh-migration/raw/master/utils/restore_environment_variables.php cd /tmp wget https://gitlab.com/contextualcode/platformsh-migration/raw/master/utils/restore_environment_variables.php Check the latest backup filename and restore variables from it. Use --dry-run to see what will be created/updated. aws s3 ls s3://your-bucket/platformsh-env-variables/ php /tmp/restore_environment_variables.php --dry-run --s3dir=s3://your-bucket/platformsh-env-variables --s3filename=projectname_stageEnvVars_FRI.json --override-existing --secret=ChangeTheSecret After changing per environment variable, http://platform.sh will trigger a redeploy, so you’ll need to re-execute step 2 a few times to get all the variables restored. Under the hood Below is a summary of what the code is actually doing, but feel free to check the source code. It’s always a good idea to inspect third party code before running it, especially when sensitive information is at risk. download file from S3 using aws s3 cp. decrypt it with PHP “openssl_decrypt” function compare variables in file with existing variables create variables using platform variable:create command. ignore variables with the same value as already defined in the environment skip existing variables unless --override-existing option specified. update variables using platform variable:update command. Conclusion That’s it! You should now have a backup and restore process for your environment variables!",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-back-up-environment-variables/430",
        "relurl": "/t/how-to-back-up-environment-variables/430"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e0d8fb131896d9cbf64c9994144674381a27772f",
        "title": "How to use cmake in a Platform.sh build hook",
        "description": "Goal Sometimes a library may require cmake in order to compile. By default you only have make available. An example would be libgit2 how would you get this to work on http://platform.sh ? Assumptions You will need: An SSH key configured on your http://Platform.sh account A http://Platform.sh project and a .platform.app.yaml to edit Steps 1. Add at the beginning of your build hook in .platform.app.yaml Setting VERSION to the version of cmake you want. export VERSION=\"3.16.0-rc4\" wget https://github.com/Kitware/CMake/releases/download/v$VERSION/cmake-$VERSION-Linux-x86_64.tar.gz tar xzf cmake-$VERSION-Linux-x86_64.tar.gz export PATH=$PATH:/$PWD/cmake-$VERSION-Linux-x86_64/bin 2. Potentially cleanup at the end of the build step if you don’t want cmake in production rm -rf /$PWD/cmake-$VERSION-Linux-x86_64 Conclusion Any dependencies that require cmake in their build should very probably work now.",
        "text": "Goal Sometimes a library may require cmake in order to compile. By default you only have make available. An example would be libgit2 how would you get this to work on http://platform.sh ? Assumptions You will need: An SSH key configured on your http://Platform.sh account A http://Platform.sh project and a .platform.app.yaml to edit Steps 1. Add at the beginning of your build hook in .platform.app.yaml Setting VERSION to the version of cmake you want. export VERSION=\"3.16.0-rc4\" wget https://github.com/Kitware/CMake/releases/download/v$VERSION/cmake-$VERSION-Linux-x86_64.tar.gz tar xzf cmake-$VERSION-Linux-x86_64.tar.gz export PATH=$PATH:/$PWD/cmake-$VERSION-Linux-x86_64/bin 2. Potentially cleanup at the end of the build step if you don’t want cmake in production rm -rf /$PWD/cmake-$VERSION-Linux-x86_64 Conclusion Any dependencies that require cmake in their build should very probably work now.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-use-cmake-in-a-platform-sh-build-hook/405",
        "relurl": "/t/how-to-use-cmake-in-a-platform-sh-build-hook/405"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "51e57f5aa87d82f80ed2381075eb46173d0f463c",
        "title": "How do I check the disk usage for elastic search?",
        "description": "Goal Get the available disk size of elasticsearch Assumptions You already have elasticsearch set-up and running. Steps 1. Log in to the app container using ssh 2. Query the elastic search api Run this curl snippet assuming your elasticsearch relationship is called elasticsearch. If the name is different, change the snippet below to reflect that. curl -XGET 'elasticsearch.internal:9200/_cat/allocation?v\u0026pretty' shards disk.indices disk.used disk.avail disk.total disk.percent host ip node 4 124.7kb 22.6mb 1.8gb 1.9gb 1 127.0.0.0 127.0.0.0 2kv5ycbzprs4bomyaeu6zkvoh4 ",
        "text": "Goal Get the available disk size of elasticsearch Assumptions You already have elasticsearch set-up and running. Steps 1. Log in to the app container using ssh 2. Query the elastic search api Run this curl snippet assuming your elasticsearch relationship is called elasticsearch. If the name is different, change the snippet below to reflect that. curl -XGET 'elasticsearch.internal:9200/_cat/allocation?v\u0026pretty' shards disk.indices disk.used disk.avail disk.total disk.percent host ip node 4 124.7kb 22.6mb 1.8gb 1.9gb 1 127.0.0.0 127.0.0.0 2kv5ycbzprs4bomyaeu6zkvoh4 ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-check-the-disk-usage-for-elastic-search/395",
        "relurl": "/t/how-do-i-check-the-disk-usage-for-elastic-search/395"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "37dfd98155449c271f605e29309ca66291acff16",
        "title": "How to reset a project's code and data",
        "description": "Goal To reset a project entirely to a new code base. Assumptions You will need: A working http://Platform.sh project Administrator access to it Git and SSH configured and accessible Problems Normally the easiest way to “start fresh” with a project on http://Platform.sh is to create a new project and optionally delete the old one. However, there are some cases where that may not be viable: When using a trial project, deleting the project created with the trial will also delete the trial, even if the trial period has not expired. You may want to retain non-code configuration on the project (users, access control, etc.) but still reset all code and data on the project. These steps will “reset” a project to a fresh state. Warning: By design, these instructions will result in code and data loss. Do not proceed unless code and data loss is really what you want to do. Steps 1. Remove all services and application data Edit the services.yaml file and remove all lines. Just leave it as an empty file. Then update your .platform.app.yaml file. Remove the relationships block, all mount declarations and change the name key to any new value. Optionally you may remove the hooks section to make the deploy a bit faster but that is not necessary. If you have multiple .platform.app.yaml files, do the same for all of them. Commit these changes and git push to the master branch. The environment will rebuild with no services (and thus deleting all of the previously-specified services) and with a blank application container. Additionally, delete any non-master branches in Git as they will not work correctly after this process is complete. 2. Select a new code base to deploy If you have an existing Git repository you wish to deploy, ensure it has the https://docs.platform.sh/gettingstarted/own-code/project-configuration.html . If you would like to use a one of http://Platform.sh’s pre-made templates, git clone the appropriate repository from GitHub to your local computer. Now reset the Git history in the repository. cd into the directory you just cloned, then run: $ rm -rf .git git init git add . git commit -m \"Add Platform.sh template.\" 3. Force push to the project Add a Git remote for the project to the local Git repository you just created. You can find the Git URL to use in the Web Console by going to the master environment and selecting the “Git” dropdown. Just copy the remote URL itself, not the full command. Add that remote to the project (using the Git URL you just copied): git remote add platform abc123@git.eu-3.platform.sh:abc123git Then “Force push” to the master branch of the project: git push --force -u platform master That will completely overwrite the master branch on http://Platform.sh with the code in your new repository, and set your local branch to track the project’s master branch so you don’t need to specify it in the future. The new code will build and deploy a new master environment with the configuration in Git. Conclusion The project will now have a fresh Git history with new code, and completely empty services based on what was defined in the new code base. Any users or environment variables that had been defined previously, however, will remain intact.",
        "text": "Goal To reset a project entirely to a new code base. Assumptions You will need: A working http://Platform.sh project Administrator access to it Git and SSH configured and accessible Problems Normally the easiest way to “start fresh” with a project on http://Platform.sh is to create a new project and optionally delete the old one. However, there are some cases where that may not be viable: When using a trial project, deleting the project created with the trial will also delete the trial, even if the trial period has not expired. You may want to retain non-code configuration on the project (users, access control, etc.) but still reset all code and data on the project. These steps will “reset” a project to a fresh state. Warning: By design, these instructions will result in code and data loss. Do not proceed unless code and data loss is really what you want to do. Steps 1. Remove all services and application data Edit the services.yaml file and remove all lines. Just leave it as an empty file. Then update your .platform.app.yaml file. Remove the relationships block, all mount declarations and change the name key to any new value. Optionally you may remove the hooks section to make the deploy a bit faster but that is not necessary. If you have multiple .platform.app.yaml files, do the same for all of them. Commit these changes and git push to the master branch. The environment will rebuild with no services (and thus deleting all of the previously-specified services) and with a blank application container. Additionally, delete any non-master branches in Git as they will not work correctly after this process is complete. 2. Select a new code base to deploy If you have an existing Git repository you wish to deploy, ensure it has the https://docs.platform.sh/gettingstarted/own-code/project-configuration.html . If you would like to use a one of http://Platform.sh’s pre-made templates, git clone the appropriate repository from GitHub to your local computer. Now reset the Git history in the repository. cd into the directory you just cloned, then run: $ rm -rf .git git init git add . git commit -m \"Add Platform.sh template.\" 3. Force push to the project Add a Git remote for the project to the local Git repository you just created. You can find the Git URL to use in the Web Console by going to the master environment and selecting the “Git” dropdown. Just copy the remote URL itself, not the full command. Add that remote to the project (using the Git URL you just copied): git remote add platform abc123@git.eu-3.platform.sh:abc123git Then “Force push” to the master branch of the project: git push --force -u platform master That will completely overwrite the master branch on http://Platform.sh with the code in your new repository, and set your local branch to track the project’s master branch so you don’t need to specify it in the future. The new code will build and deploy a new master environment with the configuration in Git. Conclusion The project will now have a fresh Git history with new code, and completely empty services based on what was defined in the new code base. Any users or environment variables that had been defined previously, however, will remain intact.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-reset-a-projects-code-and-data/382",
        "relurl": "/t/how-to-reset-a-projects-code-and-data/382"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "0e5d13e6ac4b9bd49722778f95fe3682c71e5c1e",
        "title": "How to deploy a Vue.js Single Page Application (SPA) with a Golang API on Platform.sh",
        "description": "Goal Deploy a Vue.js Single Page Application (SPA) with a Golang API backend on http://Platform.sh Assumptions an empty http://Platform.sh project with Medium plan Node.js installed locally Vue.js CLI installed locally (npm install -g @vue/cli): https://cli.vuejs.org/ Problems Two strategies are possible when building an SPA with http://Platform.sh: creating two separate projects for Vue.js frontend and Golang API backend hosting both apps within a single multi-app Platform project (requires at least a Medium plan) This How-to shows the second option. Steps (Multi-app SPA) 1. Project structure Ultimately, the project structure will look like the following (one common .platform directory, and one .platform.app.yaml file per app): vuespa/ .git/ .platform/ routes.yaml services.yaml hello_world_backend/ .platform.app.yaml hello_world.go hello_world_frontend/ .platform.app.yaml node_modules/ public/ src/ App.vue main.js babel.config.js package-lock.json package.json 2. Set Up Golang App 1. Create the Go Project Create and enter a project directory vuespa. Create a hello_world_backend directory, and then create the following hello_world.go file within that directory: // vuespa/hello_world_backend/hello_world.go package main import ( \"encoding/json\" \"fmt\" \"net/http\" psh \"github.com/platformsh/gohelper\" \"github.com/rs/cors\" ) // Greetings is a basic Greetings to user type Greetings struct { Message string `json:\"message\"` } // sayHello returns greetings to user in JSON format func sayHello(w http.ResponseWriter, r *http.Request) { greetings := Greetings{Message: \"Hello World!\"} returnedJSON, err := json.Marshal(greetings) if err != nil { http.Error(w, \"Internal server error\", http.StatusInternalServerError) } w.Header().Set(\"Content-Type\", \"application/json\") fmt.Fprintf(w, \"%s\", returnedJSON) } func main() { // Load Platform.sh environment variables in order to retrieve correct port p, err := psh.NewPlatformInfo() if err != nil { panic(\"Not in a Platform.sh Environment.\") } // Initialize HTTP server mux := http.NewServeMux() // Set up the /say-hello API endpoint mux.HandleFunc(\"/say-hello\", sayHello) // Enable CORS and Whitelist the frontend domain in order to comply with // CORS Allowed Origins policy handler := cors.New(cors.Options{ AllowedOrigins: []string{\"https://your-platformsh-frontend-url\"}, }).Handler(mux) // Launch HTTP server with custom port retrieved in Platform.sh env var http.ListenAndServe(\":\"+p.Port, handler) } The above exposes a say-hello REST endpoint returning a greetings message to user. CORS need to be properly handled as both frontend and backend apps are not hosted at the same URLs. Ideally the frontend URL whitelisted in CORS (https://your-platformsh-frontend-url) should be retrieved through an environment variable. For more information about CORS please visit: https://wikipedia.org/wiki/Cross-origin_resource_sharing The frontend url will not be made available until the project is pushed to http://Platform.sh for the first time, so it may be necessary to push the code once to establish those routes, and then commit the url once they are defined. In general, they will take the forms https://https://master-7rqtwti- . .platformsh.site for the frontend url and https://https://backend.master-7rqtwti- . .platformsh.site for the backend url. 2. Set up the http://Platform.sh configuration Create a .platform.app.yaml within the directory: # vuespa/hello_world_backend/.platform.app.yaml name: go-backend # Use the Golang 1.12 image type: golang:1.12 # A Medium plan is necessary for multi-app size: M hooks: # Get dependencies and build Go app build: | go get ./... go build -o bin/app web: upstream: socket_family: tcp protocol: http # Launch the Go server commands: start: ./bin/app locations: /: allow: false passthru: true disk: 1024 3. Set Up Vue.js App 1. Initialize the Vue.js project Create the Vue.js base project with the Vue CLI, selecting the default install option when prompted: $ vue create hello_world_frontend cd into hello_world_frontend and install the axios dependency: $ npm install axios 2. Update The Vue.js Project Remove everything inside the src directory except the main.js file and add the following App.vue file: Retrieved the following greetings message from API Go backend: {{ msg }} \r\n\r\n Error while getting message from Go API backend: {{ msgError }} \r\n\r\n ",
        "text": "Goal Deploy a Vue.js Single Page Application (SPA) with a Golang API backend on http://Platform.sh Assumptions an empty http://Platform.sh project with Medium plan Node.js installed locally Vue.js CLI installed locally (npm install -g @vue/cli): https://cli.vuejs.org/ Problems Two strategies are possible when building an SPA with http://Platform.sh: creating two separate projects for Vue.js frontend and Golang API backend hosting both apps within a single multi-app Platform project (requires at least a Medium plan) This How-to shows the second option. Steps (Multi-app SPA) 1. Project structure Ultimately, the project structure will look like the following (one common .platform directory, and one .platform.app.yaml file per app): vuespa/ .git/ .platform/ routes.yaml services.yaml hello_world_backend/ .platform.app.yaml hello_world.go hello_world_frontend/ .platform.app.yaml node_modules/ public/ src/ App.vue main.js babel.config.js package-lock.json package.json 2. Set Up Golang App 1. Create the Go Project Create and enter a project directory vuespa. Create a hello_world_backend directory, and then create the following hello_world.go file within that directory: // vuespa/hello_world_backend/hello_world.go package main import ( \"encoding/json\" \"fmt\" \"net/http\" psh \"github.com/platformsh/gohelper\" \"github.com/rs/cors\" ) // Greetings is a basic Greetings to user type Greetings struct { Message string `json:\"message\"` } // sayHello returns greetings to user in JSON format func sayHello(w http.ResponseWriter, r *http.Request) { greetings := Greetings{Message: \"Hello World!\"} returnedJSON, err := json.Marshal(greetings) if err != nil { http.Error(w, \"Internal server error\", http.StatusInternalServerError) } w.Header().Set(\"Content-Type\", \"application/json\") fmt.Fprintf(w, \"%s\", returnedJSON) } func main() { // Load Platform.sh environment variables in order to retrieve correct port p, err := psh.NewPlatformInfo() if err != nil { panic(\"Not in a Platform.sh Environment.\") } // Initialize HTTP server mux := http.NewServeMux() // Set up the /say-hello API endpoint mux.HandleFunc(\"/say-hello\", sayHello) // Enable CORS and Whitelist the frontend domain in order to comply with // CORS Allowed Origins policy handler := cors.New(cors.Options{ AllowedOrigins: []string{\"https://your-platformsh-frontend-url\"}, }).Handler(mux) // Launch HTTP server with custom port retrieved in Platform.sh env var http.ListenAndServe(\":\"+p.Port, handler) } The above exposes a say-hello REST endpoint returning a greetings message to user. CORS need to be properly handled as both frontend and backend apps are not hosted at the same URLs. Ideally the frontend URL whitelisted in CORS (https://your-platformsh-frontend-url) should be retrieved through an environment variable. For more information about CORS please visit: https://wikipedia.org/wiki/Cross-origin_resource_sharing The frontend url will not be made available until the project is pushed to http://Platform.sh for the first time, so it may be necessary to push the code once to establish those routes, and then commit the url once they are defined. In general, they will take the forms https://https://master-7rqtwti- . .platformsh.site for the frontend url and https://https://backend.master-7rqtwti- . .platformsh.site for the backend url. 2. Set up the http://Platform.sh configuration Create a .platform.app.yaml within the directory: # vuespa/hello_world_backend/.platform.app.yaml name: go-backend # Use the Golang 1.12 image type: golang:1.12 # A Medium plan is necessary for multi-app size: M hooks: # Get dependencies and build Go app build: | go get ./... go build -o bin/app web: upstream: socket_family: tcp protocol: http # Launch the Go server commands: start: ./bin/app locations: /: allow: false passthru: true disk: 1024 3. Set Up Vue.js App 1. Initialize the Vue.js project Create the Vue.js base project with the Vue CLI, selecting the default install option when prompted: $ vue create hello_world_frontend cd into hello_world_frontend and install the axios dependency: $ npm install axios 2. Update The Vue.js Project Remove everything inside the src directory except the main.js file and add the following App.vue file: Retrieved the following greetings message from API Go backend: {{ msg }} \r\n\r\n Error while getting message from Go API backend: {{ msgError }} \r\n\r\n ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-deploy-a-vue-js-single-page-application-spa-with-a-golang-api-on-platform-sh/182",
        "relurl": "/t/how-to-deploy-a-vue-js-single-page-application-spa-with-a-golang-api-on-platform-sh/182"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "cae4fea0e0859ea12fcfccb106b8ad7d89d8bdc9",
        "title": "How to transfer files between two different projects",
        "description": "Transfer files between two different projects with SSH or SCP Goal The goal is to transfer files between two separate projects. Assumptions You will need two projects set up on http://Platform.sh: One project (undntpvafhdn4 in this guide) that acts as the data source. One project (xksjd6v6od7iq) that is the data sink (where we want to copy data to). Problems Communications between projects relies on SSH, so managing the keys needs to be automated. Steps 1. Preparing the data sink On the data sink, add .ssh as a writeable mount (this is to allow SSH to write into known_hosts). Your .platform.app.yaml has to contain this block: mounts: \"/.ssh\": source: local source_path: .ssh In the build hook, https://community.platform.sh/t/how-to-use-the-platform-sh-cli-from-the-web-container/126 . This is used to dynamically retrieve the data source’s SSH connection string. If the remote will not change, you can also hardcode the path and avoid this step. hooks: build: | echo \"### INSTALLING Platform.sh CLI ... ###\" curl -sS https://platform.sh/cli/installer | php 2. Transferring files Connect to the data sink project using the -A option for SSH (this allows SSH key forwarding and connections to the data source project from the data sink project): ssh -A \"$(platform ssh -p xksjd6v6od7iq --pipe)\" ___ _ _ __ _ | _ \\ |__ _| |_ / _|___ _ _ _ __ __| |_ | _/ / _` | _| _/ _ \\ '_| ' \\ _(_- ssh-keygen -f id_rsa_transfer Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in id_rsa_transfer. Your public key has been saved in id_rsa_transfer.pub. The key fingerprint is: SHA256: The key's randomart image is: Add the generated public key into .ssh/authorized_keys in the data source project and commit the change: cat id_rsa_transfer.pub /.ssh/authorized_keys cd git commit -m \"Add SSH key\" git push Add the key in the data sink project: cat id_rsa_transfer /.ssh/id_rsa cat id_rsa_transfer.pub /.ssh/id_rsa.pub cd Edit the build hook to set the correct permissions on the keys: hooks: build: | chmod 0600 .ssh/id_rsa chmod 0600 .ssh/id_rsa.pub Commit the changes: git commit -m \"Add SSH key\" git push On the data source project, add a separate http://Platform.sh user in the Console and add the newly generated public key in the new account. Provide the least privileges possible (Viewer), for security reasons. Now, connect in the data sink project without the -A switch: ssh xksjd6v6od7iq-master-7rqtwti--app@ssh.eu-3.platform.sh ___ _ _ __ _ | _ \\ |__ _| |_ / _|___ _ _ _ __ __| |_ | _/ / _` | _| _/ _ \\ '_| ' \\ _(_- ",
        "text": "Transfer files between two different projects with SSH or SCP Goal The goal is to transfer files between two separate projects. Assumptions You will need two projects set up on http://Platform.sh: One project (undntpvafhdn4 in this guide) that acts as the data source. One project (xksjd6v6od7iq) that is the data sink (where we want to copy data to). Problems Communications between projects relies on SSH, so managing the keys needs to be automated. Steps 1. Preparing the data sink On the data sink, add .ssh as a writeable mount (this is to allow SSH to write into known_hosts). Your .platform.app.yaml has to contain this block: mounts: \"/.ssh\": source: local source_path: .ssh In the build hook, https://community.platform.sh/t/how-to-use-the-platform-sh-cli-from-the-web-container/126 . This is used to dynamically retrieve the data source’s SSH connection string. If the remote will not change, you can also hardcode the path and avoid this step. hooks: build: | echo \"### INSTALLING Platform.sh CLI ... ###\" curl -sS https://platform.sh/cli/installer | php 2. Transferring files Connect to the data sink project using the -A option for SSH (this allows SSH key forwarding and connections to the data source project from the data sink project): ssh -A \"$(platform ssh -p xksjd6v6od7iq --pipe)\" ___ _ _ __ _ | _ \\ |__ _| |_ / _|___ _ _ _ __ __| |_ | _/ / _` | _| _/ _ \\ '_| ' \\ _(_- ssh-keygen -f id_rsa_transfer Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in id_rsa_transfer. Your public key has been saved in id_rsa_transfer.pub. The key fingerprint is: SHA256: The key's randomart image is: Add the generated public key into .ssh/authorized_keys in the data source project and commit the change: cat id_rsa_transfer.pub /.ssh/authorized_keys cd git commit -m \"Add SSH key\" git push Add the key in the data sink project: cat id_rsa_transfer /.ssh/id_rsa cat id_rsa_transfer.pub /.ssh/id_rsa.pub cd Edit the build hook to set the correct permissions on the keys: hooks: build: | chmod 0600 .ssh/id_rsa chmod 0600 .ssh/id_rsa.pub Commit the changes: git commit -m \"Add SSH key\" git push On the data source project, add a separate http://Platform.sh user in the Console and add the newly generated public key in the new account. Provide the least privileges possible (Viewer), for security reasons. Now, connect in the data sink project without the -A switch: ssh xksjd6v6od7iq-master-7rqtwti--app@ssh.eu-3.platform.sh ___ _ _ __ _ | _ \\ |__ _| |_ / _|___ _ _ _ __ __| |_ | _/ / _` | _| _/ _ \\ '_| ' \\ _(_- ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-transfer-files-between-two-different-projects/373",
        "relurl": "/t/how-to-transfer-files-between-two-different-projects/373"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "c1cf7d083669e62203e7081e3e140e5cab538941",
        "title": "How to export database table data to a CSV file with platform-cli",
        "description": "Goal You want to export a database table (or a part of it) to a CSV file. Assumptions You will need: Platform CLI Platform SQL The platform CLI tool allows you to connect to your database using the command platform sql You can also specify which query you want to run. This will show you all the tables in your database. platform sql \"SHOW TABLES\" Exporting data With this, piping it to a file then becomes trivial. Just add --raw to your command to make the output machine readable. If there is more than one column, it will be separated by a TAB character. platform sql \"SHOW TABLES\" --raw all_tables.csv To export data from the table items, in the database main simply do: platform sql \"SELECT id, title FROM items LIMIT 10\" --raw --schema=main tab_separated_output.csv The resulting file can be opened with any spreadsheet software (libreoffice, openoffice, MS Office). Simply select TAB as a column separator. Warning Note that these queries will run on your database server. If the tables are large, exporting them can take a while. More information For more information on what is possible with platform sql try platform sql --help or checkout the platform cli git repository https://github.com/platformsh/platformsh-cli",
        "text": "Goal You want to export a database table (or a part of it) to a CSV file. Assumptions You will need: Platform CLI Platform SQL The platform CLI tool allows you to connect to your database using the command platform sql You can also specify which query you want to run. This will show you all the tables in your database. platform sql \"SHOW TABLES\" Exporting data With this, piping it to a file then becomes trivial. Just add --raw to your command to make the output machine readable. If there is more than one column, it will be separated by a TAB character. platform sql \"SHOW TABLES\" --raw all_tables.csv To export data from the table items, in the database main simply do: platform sql \"SELECT id, title FROM items LIMIT 10\" --raw --schema=main tab_separated_output.csv The resulting file can be opened with any spreadsheet software (libreoffice, openoffice, MS Office). Simply select TAB as a column separator. Warning Note that these queries will run on your database server. If the tables are large, exporting them can take a while. More information For more information on what is possible with platform sql try platform sql --help or checkout the platform cli git repository https://github.com/platformsh/platformsh-cli",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-export-database-table-data-to-a-csv-file-with-platform-cli/370",
        "relurl": "/t/how-to-export-database-table-data-to-a-csv-file-with-platform-cli/370"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "bf74fa534d67c25445dd96e9866e498e6529abd0",
        "title": "How to serve a static HTML page on Platform.sh",
        "description": "Goal To serve a static HTML page on http://Platform.sh. Assumptions You will need: An empty http://Platform.sh project SSH key configured to the http://Platform.sh account Problems In addition to your HTML file, http://Platform.sh needs three YAML files to configure an application in a project. These files define the routing, configure the web server, and set minimal defaults for everything else. Steps 1. Set git remote to http://Platform.sh project $ git init $ platform project:set-remote 2. Create your HTML file: ./web/index.html \r\n\r\nHello World\r\n\r\n 3. Define routes ./.platform/routes.yaml https://{default}/: type: upstream upstream: htmlhowto:http 4. Add empty services ./.platform/services.yaml # empty 5. Add .platform.app.yaml ./.platform.app.yaml # The name of this app. Must be unique within a project. name: htmlhowto # Any type will work. There is no \"plain HTML\" type. type: \"python:3.7\" # There is no need for a writable file mount, so set it to the smallest possible size. disk: 256 # Configure the web server to serve our static site. web: commands: # Run a no-op process that uses no CPU resources, since this is a static site. start: sleep infinity locations: # This tells Nginx to serve from the base directory \"/\": root: \"web\" index: - \"index.html\" This is a stripped down application configuration for serving a static file on http://Platform.sh. You can find more information about additional commands that can be included in .platform.app.yaml in the . 6. Add, commit, and push these files to your empty http://Platform.sh project git add . git commit -m \"Adding configuration to serve a static html file\" git push platform master 7. Test by visiting the URL of your environment. Conclusion By adding a route to routes.yaml, and by adding the proper web server configuration to .platform.app.yaml, a project is able to serve static HTML files.",
        "text": "Goal To serve a static HTML page on http://Platform.sh. Assumptions You will need: An empty http://Platform.sh project SSH key configured to the http://Platform.sh account Problems In addition to your HTML file, http://Platform.sh needs three YAML files to configure an application in a project. These files define the routing, configure the web server, and set minimal defaults for everything else. Steps 1. Set git remote to http://Platform.sh project $ git init $ platform project:set-remote 2. Create your HTML file: ./web/index.html \r\n\r\nHello World\r\n\r\n 3. Define routes ./.platform/routes.yaml https://{default}/: type: upstream upstream: htmlhowto:http 4. Add empty services ./.platform/services.yaml # empty 5. Add .platform.app.yaml ./.platform.app.yaml # The name of this app. Must be unique within a project. name: htmlhowto # Any type will work. There is no \"plain HTML\" type. type: \"python:3.7\" # There is no need for a writable file mount, so set it to the smallest possible size. disk: 256 # Configure the web server to serve our static site. web: commands: # Run a no-op process that uses no CPU resources, since this is a static site. start: sleep infinity locations: # This tells Nginx to serve from the base directory \"/\": root: \"web\" index: - \"index.html\" This is a stripped down application configuration for serving a static file on http://Platform.sh. You can find more information about additional commands that can be included in .platform.app.yaml in the . 6. Add, commit, and push these files to your empty http://Platform.sh project git add . git commit -m \"Adding configuration to serve a static html file\" git push platform master 7. Test by visiting the URL of your environment. Conclusion By adding a route to routes.yaml, and by adding the proper web server configuration to .platform.app.yaml, a project is able to serve static HTML files.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-serve-a-static-html-page-on-platform-sh/51",
        "relurl": "/t/how-to-serve-a-static-html-page-on-platform-sh/51"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "db73646ee7ba37254005122c1b7feaf44bc42317",
        "title": "How to automatically update Composer dependencies with Source Operations",
        "description": "Goal This guide details how to automatically update Composer dependencies on a specific environment, so that you can test the changes before deploying to production. Assumptions You will need: A Composer based http://Platform.sh application (you can start with one of our many templates for https://github.com/platformsh/template-drupal8 , https://github.com/platformsh/template-wordpress, https://github.com/platformsh/template-magento2ce, https://github.com/platformsh/template-symfony4 , https://github.com/platformsh/template-laravel) An SSH key loaded in an SSH agent and configured in the https://accounts.platform.sh/user The http://Platform.sh CLI https://docs.platform.sh/gettingstarted/cli.html Problems Keeping your code base and its dependencies up to date is critical for so many reasons, and it is always possible to forget and miss a security upgrade. Even though http://Platform.sh makes it easy to update dependencies, thanks to its support for all the famous package managers (Composer, npm, gem…), it is better to automate that process, so that this issue never happens. That is the goal of this how-to guide. Steps 1. Install the CLI on the application container Create a machine user that you invite to your project. Get an API token from this machine user account (read the https://docs.platform.sh/gettingstarted/cli/api-tokens.html ) and run the following command: cd my-platformsh-project/ platform variable:create -e master --level environment --name env:PLATFORMSH_CLI_TOKEN --sensitive true --value 'your API token' Your local CLI will automatically detect the current project and add the env:PLATFORMSH_CLI_TOKEN environment variable to your project. Then install the CLI on your application container via a new build hook defined in .platform.app.yaml: hooks: build: | curl -sS https://platform.sh/cli/installer | php 2. Enable Source Operations Create a dedicated update-dependencies branch where we will automatically run and test Composer updates. platform branch update-dependencies -e master On that newly created branch, add the following lines in the .platform.app.yaml: source: operations: update: command: | composer update git add composer.lock git commit -m \"Update Composer dependencies.\" This configuration defines an arbitrary update source operation which will run the composer update command and commit the changes to the composer.lock file, before redeploying the environment on which it has been triggered. 3. Automatically trigger the update source operation Define a new cron entry to automatically trigger the update source operation in .platform.app.yaml: crons: update: # Trigger the update source operation every day at 00:00. spec: '0 0 * * *' cmd: | if [ \"$PLATFORM_BRANCH\" = update-dependencies ]; then platform environment:sync code data --no-wait --yes platform source-operation:run update --no-wait --yes fi Every day, this cron will synchronize the update-dependencies environment with its parent master, and trigger the update source operation on it. 4. Deploy the changes Use Git to deploy the changes: git add .platform.app.yaml git commit -m \"Enable automated Composer updates on the update-dependencies branch via cron.\" git push platform update-dependencies Conclusion This is how easy it is to automate the update of Composer dependencies (or any other package manager dependencies if you are not using PHP) on http://Platform.sh. The next step should be to enable a http://Platform.sh notification alert (Email, Slack…) so that you know when the environment has been updated and you can test the changes before deploying those to production.",
        "text": "Goal This guide details how to automatically update Composer dependencies on a specific environment, so that you can test the changes before deploying to production. Assumptions You will need: A Composer based http://Platform.sh application (you can start with one of our many templates for https://github.com/platformsh/template-drupal8 , https://github.com/platformsh/template-wordpress, https://github.com/platformsh/template-magento2ce, https://github.com/platformsh/template-symfony4 , https://github.com/platformsh/template-laravel) An SSH key loaded in an SSH agent and configured in the https://accounts.platform.sh/user The http://Platform.sh CLI https://docs.platform.sh/gettingstarted/cli.html Problems Keeping your code base and its dependencies up to date is critical for so many reasons, and it is always possible to forget and miss a security upgrade. Even though http://Platform.sh makes it easy to update dependencies, thanks to its support for all the famous package managers (Composer, npm, gem…), it is better to automate that process, so that this issue never happens. That is the goal of this how-to guide. Steps 1. Install the CLI on the application container Create a machine user that you invite to your project. Get an API token from this machine user account (read the https://docs.platform.sh/gettingstarted/cli/api-tokens.html ) and run the following command: cd my-platformsh-project/ platform variable:create -e master --level environment --name env:PLATFORMSH_CLI_TOKEN --sensitive true --value 'your API token' Your local CLI will automatically detect the current project and add the env:PLATFORMSH_CLI_TOKEN environment variable to your project. Then install the CLI on your application container via a new build hook defined in .platform.app.yaml: hooks: build: | curl -sS https://platform.sh/cli/installer | php 2. Enable Source Operations Create a dedicated update-dependencies branch where we will automatically run and test Composer updates. platform branch update-dependencies -e master On that newly created branch, add the following lines in the .platform.app.yaml: source: operations: update: command: | composer update git add composer.lock git commit -m \"Update Composer dependencies.\" This configuration defines an arbitrary update source operation which will run the composer update command and commit the changes to the composer.lock file, before redeploying the environment on which it has been triggered. 3. Automatically trigger the update source operation Define a new cron entry to automatically trigger the update source operation in .platform.app.yaml: crons: update: # Trigger the update source operation every day at 00:00. spec: '0 0 * * *' cmd: | if [ \"$PLATFORM_BRANCH\" = update-dependencies ]; then platform environment:sync code data --no-wait --yes platform source-operation:run update --no-wait --yes fi Every day, this cron will synchronize the update-dependencies environment with its parent master, and trigger the update source operation on it. 4. Deploy the changes Use Git to deploy the changes: git add .platform.app.yaml git commit -m \"Enable automated Composer updates on the update-dependencies branch via cron.\" git push platform update-dependencies Conclusion This is how easy it is to automate the update of Composer dependencies (or any other package manager dependencies if you are not using PHP) on http://Platform.sh. The next step should be to enable a http://Platform.sh notification alert (Email, Slack…) so that you know when the environment has been updated and you can test the changes before deploying those to production.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-automatically-update-composer-dependencies-with-source-operations/337",
        "relurl": "/t/how-to-automatically-update-composer-dependencies-with-source-operations/337"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e4a83949ff3e1b3bad0fd690e845962bc7e16d5e",
        "title": "How to generate PDFs using Puppeteer and Headless Chrome",
        "description": "Goal To use https://github.com/GoogleChrome/puppeteer and https://developers.google.com/web/updates/2017/04/headless-chrome to create an https://expressjs.com/ application that generates PDFs of web sites on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account The https://docs.platform.sh/gettingstarted/cli.html installed locally Node.js and npm installed locally Problems Using headless Chrome to generate PDFs of a website requires properly connecting the chrome-headless service container to the Node library https://github.com/GoogleChrome/puppeteer by passing its credentials using the Node.js libary. Steps The project will ultimately have the following structure: . ├── .platform │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml ├── index.js ├── package.json ├── package-lock.json └── pdfs.js 1. Initialize the project Create an empty project on http://Platform.sh using the CLI. $ platform create Create a new project directory for the application on your local machine called pdfs and cd into it. Initialize the directory as a Git repository and set its remote to the newly created http://Platform.sh project using the outputted project ID. $ git init $ platform project:set-remote 2. Create the http://Platform.sh configuration files .platform/services.yaml Define the chrome-headless container using the supported version outlined in the https://docs.platform.sh/configuration/services/headless-chrome.html . headless: type: chrome-headless:73 .platform.app.yaml Configure the application nodejs: name: nodejs type: nodejs:10 relationships: headless: \"headless:http\" crons: cleanup: spec: '*/30 * * * *' cmd: rm pdfs/* web: commands: start: \"nodejs index.js\" mounts: \"/pdfs\": \"shared:files/pdfs\" disk: 512 The configuration uses nodejs 10, since it is required to use the Config Reader library with Puppeteer. It defines the mount pdfs that will act as a writable directory to save the PDFs the application generates. In order to prevent pdfs/ from filling up as people use it, a cron job is also defined that removes its contents every 30 minutes. .platform/routes.yaml Lastly, set up a basic routes configuration file, using the name of the application nodejs \"https://{default}/\": id: main type: upstream upstream: \"nodejs:http\" \"https://www.{default}/\": type: redirect to: \"https://{default}/\" 3. Write the pdfs.js file Create a file in the project directory called pdfs.js with the following contents: const puppeteer = require('puppeteer'); const platformsh = require('platformsh-config'); var exports = module.exports = {}; // Create an async function exports.makePDF = async function (url, pdfID) { try { // Connect to chrome-headless using pre-formatted puppeteer credentials let config = platformsh.config(); const formattedURL = config.formattedCredentials(\"headless\", \"puppeteer\"); const browser = await puppeteer.connect({browserURL: formattedURL}); // Open a new page to the given url and create the PDF const page = await browser.newPage(); await page.goto(url, {waitUntil: 'networkidle2'}); await page.pdf({ path: `pdfs/${pdfID}.pdf`, printBackground: true }); await browser.close(); return browser } catch (e) { return Promise.reject(e); } }; It defines an async function called makePDF as a module export. The Node.js Config Reader retrieves the library’s formatted credentials for Puppeteer to create the formattedURL string. path defines the saved location of the PDF, while printBackground allows background images on the page to be included in the generated PDF. Additional parameters for page.pdf() can be found in the . 4. Define index.js Create the file index.js that defines the ExpressJS application app: const fs = require('fs'); const uuidv4 = require('uuid/v4') const platformsh = require('platformsh-config'); const express = require('express'); // Require pdf file and its function var pdfs = require(\"./pdfs.js\"); // Build the application var app = express(); // Define the index route app.get('/', (req, res) = { res.writeHead(200, {\"Content-Type\": \"text/html\"}); res.write(` \r\n\r\nHeadless Chrome on Platform.sh\r\n\r\n Generate a PDF of a page\r\n\r\n Click submit to generate a PDF of the https://platform.sh/ , or paste in another URL. `); res.end(``); }) // Define PDF result route app.get('/result', async function(req, res){ // Create a randomly generated ID number for the current PDF var pdfID = uuidv4(); // Generate the PDF await pdfs.makePDF(req.query['pdfURL'], pdfID) // Define and download the file const file = `pdfs/${pdfID}.pdf`; res.download(file); }); // Create config object to get Platform.sh PORT credentials let config = platformsh.config(); // Start the server. app.listen(config.port, function() { console.log(`Listening on port ${config.port}`) }); In addition to the home route, index.js defines a /results path that calls makePDF() and passes a randomly generated ID that will become part of the name for the generated PDF file. 5. Define the application’s dependencies Include the application’s dependencies in package.json: { \"name\": \"chrome_headless\", \"version\": \"1.0.0\", \"description\": \"A simple example for taking screenshots with Puppeteer and headless Chrome on Platform.sh\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" \u0026\u0026 exit 1\" }, \"author\": \"Chad Carlson\", \"license\": \"MIT\", \"dependencies\": { \"platformsh-config\": \"^2.0.0\", \"puppeteer\": \"^1.14.0\", \"express\": \"^4.16.4\", \"uuid\": \"^3.3.2\" } } Then create the package-lock.json file by running $ npm install 6. Push to http://Platform.sh Commit the changes and push master to http://Platform.sh $ git add . $ git commit -m \"Create PDF generator application.\" $ git push platform master 7. Verify Use the command platform url when the build process has completed to visit the site. Click submit to generate a PDF of the http://Platform.sh website, or copy in another url to test the application. Conclusion Using ExpressJS, Puppeteer, and http://Platform.sh headless Chrome as a service, a simple application can be made that generates PDFs of an inputted url.",
        "text": "Goal To use https://github.com/GoogleChrome/puppeteer and https://developers.google.com/web/updates/2017/04/headless-chrome to create an https://expressjs.com/ application that generates PDFs of web sites on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account The https://docs.platform.sh/gettingstarted/cli.html installed locally Node.js and npm installed locally Problems Using headless Chrome to generate PDFs of a website requires properly connecting the chrome-headless service container to the Node library https://github.com/GoogleChrome/puppeteer by passing its credentials using the Node.js libary. Steps The project will ultimately have the following structure: . ├── .platform │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml ├── index.js ├── package.json ├── package-lock.json └── pdfs.js 1. Initialize the project Create an empty project on http://Platform.sh using the CLI. $ platform create Create a new project directory for the application on your local machine called pdfs and cd into it. Initialize the directory as a Git repository and set its remote to the newly created http://Platform.sh project using the outputted project ID. $ git init $ platform project:set-remote 2. Create the http://Platform.sh configuration files .platform/services.yaml Define the chrome-headless container using the supported version outlined in the https://docs.platform.sh/configuration/services/headless-chrome.html . headless: type: chrome-headless:73 .platform.app.yaml Configure the application nodejs: name: nodejs type: nodejs:10 relationships: headless: \"headless:http\" crons: cleanup: spec: '*/30 * * * *' cmd: rm pdfs/* web: commands: start: \"nodejs index.js\" mounts: \"/pdfs\": \"shared:files/pdfs\" disk: 512 The configuration uses nodejs 10, since it is required to use the Config Reader library with Puppeteer. It defines the mount pdfs that will act as a writable directory to save the PDFs the application generates. In order to prevent pdfs/ from filling up as people use it, a cron job is also defined that removes its contents every 30 minutes. .platform/routes.yaml Lastly, set up a basic routes configuration file, using the name of the application nodejs \"https://{default}/\": id: main type: upstream upstream: \"nodejs:http\" \"https://www.{default}/\": type: redirect to: \"https://{default}/\" 3. Write the pdfs.js file Create a file in the project directory called pdfs.js with the following contents: const puppeteer = require('puppeteer'); const platformsh = require('platformsh-config'); var exports = module.exports = {}; // Create an async function exports.makePDF = async function (url, pdfID) { try { // Connect to chrome-headless using pre-formatted puppeteer credentials let config = platformsh.config(); const formattedURL = config.formattedCredentials(\"headless\", \"puppeteer\"); const browser = await puppeteer.connect({browserURL: formattedURL}); // Open a new page to the given url and create the PDF const page = await browser.newPage(); await page.goto(url, {waitUntil: 'networkidle2'}); await page.pdf({ path: `pdfs/${pdfID}.pdf`, printBackground: true }); await browser.close(); return browser } catch (e) { return Promise.reject(e); } }; It defines an async function called makePDF as a module export. The Node.js Config Reader retrieves the library’s formatted credentials for Puppeteer to create the formattedURL string. path defines the saved location of the PDF, while printBackground allows background images on the page to be included in the generated PDF. Additional parameters for page.pdf() can be found in the . 4. Define index.js Create the file index.js that defines the ExpressJS application app: const fs = require('fs'); const uuidv4 = require('uuid/v4') const platformsh = require('platformsh-config'); const express = require('express'); // Require pdf file and its function var pdfs = require(\"./pdfs.js\"); // Build the application var app = express(); // Define the index route app.get('/', (req, res) = { res.writeHead(200, {\"Content-Type\": \"text/html\"}); res.write(` \r\n\r\nHeadless Chrome on Platform.sh\r\n\r\n Generate a PDF of a page\r\n\r\n Click submit to generate a PDF of the https://platform.sh/ , or paste in another URL. `); res.end(``); }) // Define PDF result route app.get('/result', async function(req, res){ // Create a randomly generated ID number for the current PDF var pdfID = uuidv4(); // Generate the PDF await pdfs.makePDF(req.query['pdfURL'], pdfID) // Define and download the file const file = `pdfs/${pdfID}.pdf`; res.download(file); }); // Create config object to get Platform.sh PORT credentials let config = platformsh.config(); // Start the server. app.listen(config.port, function() { console.log(`Listening on port ${config.port}`) }); In addition to the home route, index.js defines a /results path that calls makePDF() and passes a randomly generated ID that will become part of the name for the generated PDF file. 5. Define the application’s dependencies Include the application’s dependencies in package.json: { \"name\": \"chrome_headless\", \"version\": \"1.0.0\", \"description\": \"A simple example for taking screenshots with Puppeteer and headless Chrome on Platform.sh\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" \u0026\u0026 exit 1\" }, \"author\": \"Chad Carlson\", \"license\": \"MIT\", \"dependencies\": { \"platformsh-config\": \"^2.0.0\", \"puppeteer\": \"^1.14.0\", \"express\": \"^4.16.4\", \"uuid\": \"^3.3.2\" } } Then create the package-lock.json file by running $ npm install 6. Push to http://Platform.sh Commit the changes and push master to http://Platform.sh $ git add . $ git commit -m \"Create PDF generator application.\" $ git push platform master 7. Verify Use the command platform url when the build process has completed to visit the site. Click submit to generate a PDF of the http://Platform.sh website, or copy in another url to test the application. Conclusion Using ExpressJS, Puppeteer, and http://Platform.sh headless Chrome as a service, a simple application can be made that generates PDFs of an inputted url.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-generate-pdfs-using-puppeteer-and-headless-chrome/306",
        "relurl": "/t/how-to-generate-pdfs-using-puppeteer-and-headless-chrome/306"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "d49f923782a6cf2c04a948924ef990e0ecc6a1ba",
        "title": "How to set up and deploy a Ruby on Rails application on Platform.sh",
        "description": "Goal This guide shows how to deploy a toy https://rubyonrails.org/ 5 application on https://platform.sh running on Sqlite (please do not use Sqlite in production, ever). In a later post I’ll show how to deploy a robust, production-ready Rails with Postgres and Redis. Assumptions An empty https://platform.sh project - https://docs.platform.sh/gettingstarted/first-project.html An SSH key loaded in an SSH agent and configured in the https://accounts.platform.sh/user The http://Platform.sh CLI https://docs.platform.sh/gettingstarted/cli.html Some supported version of Ruby and the Rails gem installed. You can follow http://installrails.com/ to get both installed. Your best experience would be to use the same version of Ruby you will be running on the server (for example 2.6). Steps 1. Create a new Rails application using the CLI $ rails new rails-platformsh $ cd rails-platformsh 2. Configure the application for https://platform.sh Set the platform Git remote. $ platform project:set-remote Update the Ruby version directive in the Gemfile not to care about patch level, because locking patch-level is evil. # rails-platformsh/Gemfile ruby '~ 2.6.0' Then update the Gemfile.lock $ bundle update 3. Add the https://platform.sh configuration files Create the .platform.app.yaml https://docs.platform.sh/configuration/app-containers.html . $ touch .platform.app.yaml Add a basic https://docs.platform.sh/languages/ruby.html to the file. # rails-platformsh/.platform.app.yaml name: \"ruby_example\" type: \"ruby:2.6\" disk: 1024 hooks: build: | bundle install rake assets:precompile deploy: | rails db:migrate mounts: \"/log\": \"shared:files/log\" \"/tmp\": \"shared:files/tmp\" \"/db\": \"shared:files/db\" web: commands: start: 'rails server -p $PORT' Create the .platform/services.yaml https://docs.platform.sh/configuration/services.html and .platform/routes.yaml https://docs.platform.sh/configuration/routes.html files. $ touch .platform/routes.yaml $ touch .platform/services.yaml Add basic routes configuration: # rails-platformsh/.platform/routes.yaml \"https://{default}\": type: upstream upstream: \"ruby_example:http\" you can leave .platform/services.yaml empty for the moment. Later if you would like a Postgres or a MySQL… this is where you will put it. 4. Deploy the application to https://platform.sh Commit everything to the repository: git add --all git commit -m \"Initial commit\" Push the working branch to the platform remote: git push platform master Conclusion The Rails gem can be used in conjunction with the Platform CLI to easily create and configure a Ruby on Rails application for deployment on https://platform.sh.",
        "text": "Goal This guide shows how to deploy a toy https://rubyonrails.org/ 5 application on https://platform.sh running on Sqlite (please do not use Sqlite in production, ever). In a later post I’ll show how to deploy a robust, production-ready Rails with Postgres and Redis. Assumptions An empty https://platform.sh project - https://docs.platform.sh/gettingstarted/first-project.html An SSH key loaded in an SSH agent and configured in the https://accounts.platform.sh/user The http://Platform.sh CLI https://docs.platform.sh/gettingstarted/cli.html Some supported version of Ruby and the Rails gem installed. You can follow http://installrails.com/ to get both installed. Your best experience would be to use the same version of Ruby you will be running on the server (for example 2.6). Steps 1. Create a new Rails application using the CLI $ rails new rails-platformsh $ cd rails-platformsh 2. Configure the application for https://platform.sh Set the platform Git remote. $ platform project:set-remote Update the Ruby version directive in the Gemfile not to care about patch level, because locking patch-level is evil. # rails-platformsh/Gemfile ruby '~ 2.6.0' Then update the Gemfile.lock $ bundle update 3. Add the https://platform.sh configuration files Create the .platform.app.yaml https://docs.platform.sh/configuration/app-containers.html . $ touch .platform.app.yaml Add a basic https://docs.platform.sh/languages/ruby.html to the file. # rails-platformsh/.platform.app.yaml name: \"ruby_example\" type: \"ruby:2.6\" disk: 1024 hooks: build: | bundle install rake assets:precompile deploy: | rails db:migrate mounts: \"/log\": \"shared:files/log\" \"/tmp\": \"shared:files/tmp\" \"/db\": \"shared:files/db\" web: commands: start: 'rails server -p $PORT' Create the .platform/services.yaml https://docs.platform.sh/configuration/services.html and .platform/routes.yaml https://docs.platform.sh/configuration/routes.html files. $ touch .platform/routes.yaml $ touch .platform/services.yaml Add basic routes configuration: # rails-platformsh/.platform/routes.yaml \"https://{default}\": type: upstream upstream: \"ruby_example:http\" you can leave .platform/services.yaml empty for the moment. Later if you would like a Postgres or a MySQL… this is where you will put it. 4. Deploy the application to https://platform.sh Commit everything to the repository: git add --all git commit -m \"Initial commit\" Push the working branch to the platform remote: git push platform master Conclusion The Rails gem can be used in conjunction with the Platform CLI to easily create and configure a Ruby on Rails application for deployment on https://platform.sh.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-set-up-and-deploy-a-ruby-on-rails-application-on-platform-sh/176",
        "relurl": "/t/how-to-set-up-and-deploy-a-ruby-on-rails-application-on-platform-sh/176"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "f9e82243619cf2ce9f474c18711722530deea06b",
        "title": "How to enable commandline autocomplete support for the `platform` CLI tool",
        "description": "The https://docs.platform.sh/gettingstarted/cli.html contains a large library of commands, and trying to list them each time you can’t quite remember the names or options can fill up your screen really fast. Tab-completion will be able to help us out here. Goal We want autocomplete support (press tab to get command and option suggestions) for the platform CLI tool. Assumptions As it’s a symfony component, these commands can be programmatically extracted in a standard way. The https://github.com/bamarni/symfony-console-autocomplete tool does this for us. Instructions here for OSX, the https://github.com/bamarni/symfony-console-autocomplete Prerequisites If you don’t have bash-completion installed by default, get that first. Using brew on OSX. *nix sessions probably already have it. brew install bash-completion Add the startup line to your ~/.bash_profile like it advises. Scripts found in /usr/local/etc/bash_completion.d/ can support individual commands now. Steps Install symfony-console-autocomplete composer global require bamarni/symfony-console-autocomplete Now we can generate a dump of all symfony-console-based CLI commands, and use that dump to generate the autocomplete prompts. Auto-generate autocomplete hints for platform symfony-autocomplete platform --script-options=list /usr/local/etc/bash_completion.d/platform Restart your shell session (or at least source ~/.bash_profile) and autocomplete should be available for platform cli now. The location /usr/local/etc/bash_completion.d/ will differ depending on your OS, version, and preferred shell. . The only difference from the examples there is that we have to add --script-options=list to extract the commands for platform. Test Pressing [TAB] part-way through entering platform commands will now provide either autocompletion or a list of suggestions, for both commands and options to those commands. Examples: $ platform pr[TAB] platform project: $ platform project:[TAB] clear-build-cache curl get list variable:delete variable:set create delete info set-remote variable:get $ platform project:l[TAB] platform project:list $ platform project:list --[TAB] --columns --help --my --no-header --quiet --reverse --title --version --format --host --no --pipe --refresh --sort --verbose --yes ",
        "text": "The https://docs.platform.sh/gettingstarted/cli.html contains a large library of commands, and trying to list them each time you can’t quite remember the names or options can fill up your screen really fast. Tab-completion will be able to help us out here. Goal We want autocomplete support (press tab to get command and option suggestions) for the platform CLI tool. Assumptions As it’s a symfony component, these commands can be programmatically extracted in a standard way. The https://github.com/bamarni/symfony-console-autocomplete tool does this for us. Instructions here for OSX, the https://github.com/bamarni/symfony-console-autocomplete Prerequisites If you don’t have bash-completion installed by default, get that first. Using brew on OSX. *nix sessions probably already have it. brew install bash-completion Add the startup line to your ~/.bash_profile like it advises. Scripts found in /usr/local/etc/bash_completion.d/ can support individual commands now. Steps Install symfony-console-autocomplete composer global require bamarni/symfony-console-autocomplete Now we can generate a dump of all symfony-console-based CLI commands, and use that dump to generate the autocomplete prompts. Auto-generate autocomplete hints for platform symfony-autocomplete platform --script-options=list /usr/local/etc/bash_completion.d/platform Restart your shell session (or at least source ~/.bash_profile) and autocomplete should be available for platform cli now. The location /usr/local/etc/bash_completion.d/ will differ depending on your OS, version, and preferred shell. . The only difference from the examples there is that we have to add --script-options=list to extract the commands for platform. Test Pressing [TAB] part-way through entering platform commands will now provide either autocompletion or a list of suggestions, for both commands and options to those commands. Examples: $ platform pr[TAB] platform project: $ platform project:[TAB] clear-build-cache curl get list variable:delete variable:set create delete info set-remote variable:get $ platform project:l[TAB] platform project:list $ platform project:list --[TAB] --columns --help --my --no-header --quiet --reverse --title --version --format --host --no --pipe --refresh --sort --verbose --yes ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-enable-commandline-autocomplete-support-for-the-platform-cli-tool/312",
        "relurl": "/t/how-to-enable-commandline-autocomplete-support-for-the-platform-cli-tool/312"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "5b4e445b48f71f8bf68746295ff24a9da5a0e694",
        "title": "How to take screenshots using Puppeteer and Headless Chrome",
        "description": "Goal To use https://github.com/GoogleChrome/puppeteer and https://developers.google.com/web/updates/2017/04/headless-chrome to create an https://expressjs.com/ application that takes website screenshots on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account The https://docs.platform.sh/gettingstarted/cli.html installed locally Node.js and npm installed locally Problems Using headless Chrome to take screenshots of a website requires properly connecting the chrome-headless service container to the Node library https://github.com/GoogleChrome/puppeteer by passing its credentials using the Node.js libary. Steps The project will ultimately have the following structure: . ├── .platform │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml ├── index.js ├── package.json ├── package-lock.json └── screenshots.js 1. Initialize the project Create an empty project on http://Platform.sh using the CLI. $ platform create Create a new project directory for the application on your local machine called screenshots and cd into it. Initialize the directory as a Git repository and set its remote to the newly created http://Platform.sh project using the outputted project ID. $ git init $ platform project:set-remote 2. Create the http://Platform.sh configuration files .platform/services.yaml Define the chrome-headless container using the supported version outlined in the https://docs.platform.sh/configuration/services/headless-chrome.html . headless: type: chrome-headless:73 .platform.app.yaml Configure the application nodejs: name: nodejs type: nodejs:10 relationships: headless: \"headless:http\" crons: cleanup: spec: '*/30 * * * *' cmd: rm screenshots/* web: commands: start: \"nodejs index.js\" mounts: \"/screenshots\": \"shared:files/screenshots\" disk: 512 The configuration uses nodejs 10, since it is required to use the Config Reader library with Puppeteer. It defines the mount screenshots which will be the writable directory to save the screenshots the application takes. In order to prevent screenshots/ from filling up as people use it, a cron job is also defined that removes its contents every 30 minutes. .platform/routes.yaml Lastly, set up a basic routes configuration file, using the name of the application nodejs \"https://{default}/\": id: main type: upstream upstream: \"nodejs:http\" \"https://www.{default}/\": type: redirect to: \"https://{default}/\" 3. Write the screenshots.js file Create a file in the project directory called screenshots.js with the following contents: const puppeteer = require('puppeteer'); const platformsh = require('platformsh-config'); var exports = module.exports = {}; // Create an async function exports.takeScreenshot = async function (url, screenshotID) { try { // Connect to chrome-headless using pre-formatted puppeteer credentials let config = platformsh.config(); const formattedURL = config.formattedCredentials(\"headless\", \"puppeteer\"); const browser = await puppeteer.connect({browserURL: formattedURL}); // Open a new page to the given url and take the screenshot const page = await browser.newPage(); await page.goto(url); await page.screenshot({ fullPage: true, path: `screenshots/${screenshotID}.png` }); await browser.close(); return browser } catch (e) { return Promise.reject(e); } }; It defines an async function called takeScreenshot as a module export. The Node.js Config Reader retrieves the library’s formatted credentials for Puppeteer to create the formattedURL string. path defines the saved location of the screenshot, while fullPage configures Puppeteer to take a screenshot of the entire page, not just the view seen within a window. Additional parameters for page.screenshot() can be found in the . 4. Define index.js Create the file index.js that defines the ExpressJS application app: const fs = require('fs'); const uuidv4 = require('uuid/v4') const platformsh = require('platformsh-config'); const express = require('express'); // Require screenshot file and its function var screenshot = require(\"./screenshots.js\"); // Build the application var app = express() // Define the index route app.get('/', (req, res) = { res.writeHead(200, {\"Content-Type\": \"text/html\"}); res.write(` \r\n\r\nHeadless Chrome on Platform.sh\r\n\r\n Take a screenshot of a page\r\n\r\n Click submit to generate a png of the https://platform.sh/ , or paste in another URL. `); res.end(``); }) // Define screenshot result route app.get('/result', async function(req, res){ // Create a randomly generated ID number for the current screenshot var screenshotID = uuidv4(); // Take the screenshot await screenshot.takeScreenshot(req.query['screenshotURL'], screenshotID) // Define and download the file const file = `screenshots/${screenshotID}.png`; res.download(file); }); // Create config object to get Platform.sh PORT credentials let config = platformsh.config(); // Start the server. app.listen(config.port, function() { console.log(`Listening on port ${config.port}`) }); In addition to the home route, index.js defines a /results path that calls takeScreenshot() and passes a randomly generated ID that will become part of the name for the generated png file. 5. Define the application’s dependencies Include the application’s dependencies in package.json: { \"name\": \"chrome_headless\", \"version\": \"1.0.0\", \"description\": \"A simple example for taking screenshots with Puppeteer and headless Chrome on Platform.sh\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" \u0026\u0026 exit 1\" }, \"author\": \"Chad Carlson\", \"license\": \"MIT\", \"dependencies\": { \"platformsh-config\": \"^2.0.0\", \"puppeteer\": \"^1.14.0\", \"express\": \"^4.16.4\", \"uuid\": \"^3.3.2\" } } Then create the package-lock.json file by running $ npm install 6. Push to http://Platform.sh Commit the changes and push master to http://Platform.sh $ git add . $ git commit -m \"Create screenshot application.\" $ git push platform master 7. Verify Use the command platform url when the build process has completed to visit the site. Click submit to take a screenshot of the http://Platform.sh website, or copy in another url to test the application. Conclusion Using ExpressJS, Puppeteer, and http://Platform.sh headless Chrome as a service, a simple application can be made that takes screenshots of an inputted url.",
        "text": "Goal To use https://github.com/GoogleChrome/puppeteer and https://developers.google.com/web/updates/2017/04/headless-chrome to create an https://expressjs.com/ application that takes website screenshots on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account The https://docs.platform.sh/gettingstarted/cli.html installed locally Node.js and npm installed locally Problems Using headless Chrome to take screenshots of a website requires properly connecting the chrome-headless service container to the Node library https://github.com/GoogleChrome/puppeteer by passing its credentials using the Node.js libary. Steps The project will ultimately have the following structure: . ├── .platform │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml ├── index.js ├── package.json ├── package-lock.json └── screenshots.js 1. Initialize the project Create an empty project on http://Platform.sh using the CLI. $ platform create Create a new project directory for the application on your local machine called screenshots and cd into it. Initialize the directory as a Git repository and set its remote to the newly created http://Platform.sh project using the outputted project ID. $ git init $ platform project:set-remote 2. Create the http://Platform.sh configuration files .platform/services.yaml Define the chrome-headless container using the supported version outlined in the https://docs.platform.sh/configuration/services/headless-chrome.html . headless: type: chrome-headless:73 .platform.app.yaml Configure the application nodejs: name: nodejs type: nodejs:10 relationships: headless: \"headless:http\" crons: cleanup: spec: '*/30 * * * *' cmd: rm screenshots/* web: commands: start: \"nodejs index.js\" mounts: \"/screenshots\": \"shared:files/screenshots\" disk: 512 The configuration uses nodejs 10, since it is required to use the Config Reader library with Puppeteer. It defines the mount screenshots which will be the writable directory to save the screenshots the application takes. In order to prevent screenshots/ from filling up as people use it, a cron job is also defined that removes its contents every 30 minutes. .platform/routes.yaml Lastly, set up a basic routes configuration file, using the name of the application nodejs \"https://{default}/\": id: main type: upstream upstream: \"nodejs:http\" \"https://www.{default}/\": type: redirect to: \"https://{default}/\" 3. Write the screenshots.js file Create a file in the project directory called screenshots.js with the following contents: const puppeteer = require('puppeteer'); const platformsh = require('platformsh-config'); var exports = module.exports = {}; // Create an async function exports.takeScreenshot = async function (url, screenshotID) { try { // Connect to chrome-headless using pre-formatted puppeteer credentials let config = platformsh.config(); const formattedURL = config.formattedCredentials(\"headless\", \"puppeteer\"); const browser = await puppeteer.connect({browserURL: formattedURL}); // Open a new page to the given url and take the screenshot const page = await browser.newPage(); await page.goto(url); await page.screenshot({ fullPage: true, path: `screenshots/${screenshotID}.png` }); await browser.close(); return browser } catch (e) { return Promise.reject(e); } }; It defines an async function called takeScreenshot as a module export. The Node.js Config Reader retrieves the library’s formatted credentials for Puppeteer to create the formattedURL string. path defines the saved location of the screenshot, while fullPage configures Puppeteer to take a screenshot of the entire page, not just the view seen within a window. Additional parameters for page.screenshot() can be found in the . 4. Define index.js Create the file index.js that defines the ExpressJS application app: const fs = require('fs'); const uuidv4 = require('uuid/v4') const platformsh = require('platformsh-config'); const express = require('express'); // Require screenshot file and its function var screenshot = require(\"./screenshots.js\"); // Build the application var app = express() // Define the index route app.get('/', (req, res) = { res.writeHead(200, {\"Content-Type\": \"text/html\"}); res.write(` \r\n\r\nHeadless Chrome on Platform.sh\r\n\r\n Take a screenshot of a page\r\n\r\n Click submit to generate a png of the https://platform.sh/ , or paste in another URL. `); res.end(``); }) // Define screenshot result route app.get('/result', async function(req, res){ // Create a randomly generated ID number for the current screenshot var screenshotID = uuidv4(); // Take the screenshot await screenshot.takeScreenshot(req.query['screenshotURL'], screenshotID) // Define and download the file const file = `screenshots/${screenshotID}.png`; res.download(file); }); // Create config object to get Platform.sh PORT credentials let config = platformsh.config(); // Start the server. app.listen(config.port, function() { console.log(`Listening on port ${config.port}`) }); In addition to the home route, index.js defines a /results path that calls takeScreenshot() and passes a randomly generated ID that will become part of the name for the generated png file. 5. Define the application’s dependencies Include the application’s dependencies in package.json: { \"name\": \"chrome_headless\", \"version\": \"1.0.0\", \"description\": \"A simple example for taking screenshots with Puppeteer and headless Chrome on Platform.sh\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" \u0026\u0026 exit 1\" }, \"author\": \"Chad Carlson\", \"license\": \"MIT\", \"dependencies\": { \"platformsh-config\": \"^2.0.0\", \"puppeteer\": \"^1.14.0\", \"express\": \"^4.16.4\", \"uuid\": \"^3.3.2\" } } Then create the package-lock.json file by running $ npm install 6. Push to http://Platform.sh Commit the changes and push master to http://Platform.sh $ git add . $ git commit -m \"Create screenshot application.\" $ git push platform master 7. Verify Use the command platform url when the build process has completed to visit the site. Click submit to take a screenshot of the http://Platform.sh website, or copy in another url to test the application. Conclusion Using ExpressJS, Puppeteer, and http://Platform.sh headless Chrome as a service, a simple application can be made that takes screenshots of an inputted url.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-take-screenshots-using-puppeteer-and-headless-chrome/305",
        "relurl": "/t/how-to-take-screenshots-using-puppeteer-and-headless-chrome/305"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "00a1490102fc2574211cb363db019dfcef1494d9",
        "title": "How to modify my Platform.sh project plan and its resources",
        "description": "Goal Change a http://Platform.sh plan/and or modify project settings such as environments, storage, and users. Assumptions Owner status on the project to be changed Problems Requirements change and modifications to your current plan sometimes need to be implemented. Steps 1. Edit a Project Login to https://accounts.platform.sh/user to view the accessible projects. For the projects that you are the Account Owner for, you will see three vertical dots. Click those dots to pull up a drop-down menu and select edit to modify that project’s configuration. 2. Upgrade the plan size From the drop down menu, modify the Project’s current plan size. Changing the plan here will make visible changes to: what is included within the plan the cost of the plan. When finished, click the Update Plan button on the bottom of the page. Note: Production-sized http://Platform.sh subscriptions cannot be downgraded to a Development plan due to potential feature conflicts. If looking to downgrade a plan, https://accounts.platform.sh/platform/support 3. Modify number of environments To modify the number of environments, you can click on the drop down menu and select the number of environments you wish to add. Note: You can increase and decrease the number of environments yourself. 4. Modify storage sizing To modify the storage sizing, you can click on the drop down menu and select additional storage you would like to add (in GB). Note: You can only increase the storage sizing here. If you wish to decrease the amount of storage used, https://accounts.platform.sh/platform/support 5. Number of Users This page also shows the Account Owner the number of Users for the specific project in the same way. The number of users can also be modified https://community.platform.sh/t/how-to-add-or-remove-users-from-projects-using-the-command-line/91 . Conclusion Account owner has the power to modify their plan as well as view the costs associated with the change. For further information, please check our https://platform.sh/pricing .",
        "text": "Goal Change a http://Platform.sh plan/and or modify project settings such as environments, storage, and users. Assumptions Owner status on the project to be changed Problems Requirements change and modifications to your current plan sometimes need to be implemented. Steps 1. Edit a Project Login to https://accounts.platform.sh/user to view the accessible projects. For the projects that you are the Account Owner for, you will see three vertical dots. Click those dots to pull up a drop-down menu and select edit to modify that project’s configuration. 2. Upgrade the plan size From the drop down menu, modify the Project’s current plan size. Changing the plan here will make visible changes to: what is included within the plan the cost of the plan. When finished, click the Update Plan button on the bottom of the page. Note: Production-sized http://Platform.sh subscriptions cannot be downgraded to a Development plan due to potential feature conflicts. If looking to downgrade a plan, https://accounts.platform.sh/platform/support 3. Modify number of environments To modify the number of environments, you can click on the drop down menu and select the number of environments you wish to add. Note: You can increase and decrease the number of environments yourself. 4. Modify storage sizing To modify the storage sizing, you can click on the drop down menu and select additional storage you would like to add (in GB). Note: You can only increase the storage sizing here. If you wish to decrease the amount of storage used, https://accounts.platform.sh/platform/support 5. Number of Users This page also shows the Account Owner the number of Users for the specific project in the same way. The number of users can also be modified https://community.platform.sh/t/how-to-add-or-remove-users-from-projects-using-the-command-line/91 . Conclusion Account owner has the power to modify their plan as well as view the costs associated with the change. For further information, please check our https://platform.sh/pricing .",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-modify-my-platform-sh-project-plan-and-its-resources/130",
        "relurl": "/t/how-to-modify-my-platform-sh-project-plan-and-its-resources/130"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "287b8d60ed96f3001ebc1909cf19145ebfd57536",
        "title": "How to sync database backups to Amazon S3",
        "description": "Goal To transfer regular database backups to Amazon S3. Assumptions You will need: An SSH key configured on your http://Platform.sh account A project with a configured database ( https://docs.platform.sh/configuration/services/postgresql.html in this example) Problems While http://Platform.sh provides managed services that make configuring databases to application very easy, certain projects may require syncing regular backups to a specified location like Amazon S3. Steps The following steps refer to PostgreSQL specifically, though they could be generalized to other databases. Consult each service’s documentation for differences, and modify the credentials accordingly. 1. Create sensitive environment variables for AWS credentials AWS access keys are located in “My Security Credentials” under “Access Keys”. New keys can be created there as well. platform variable:create --level=project --name=AWS_ACCESS_KEY_ID --value= --json=false --sensitive=true --prefix=env --visible-build=true --visible-runtime=true platform variable:create --level=project --name=AWS_SECRET_ACCESS_KEY --value= ",
        "text": "Goal To transfer regular database backups to Amazon S3. Assumptions You will need: An SSH key configured on your http://Platform.sh account A project with a configured database ( https://docs.platform.sh/configuration/services/postgresql.html in this example) Problems While http://Platform.sh provides managed services that make configuring databases to application very easy, certain projects may require syncing regular backups to a specified location like Amazon S3. Steps The following steps refer to PostgreSQL specifically, though they could be generalized to other databases. Consult each service’s documentation for differences, and modify the credentials accordingly. 1. Create sensitive environment variables for AWS credentials AWS access keys are located in “My Security Credentials” under “Access Keys”. New keys can be created there as well. platform variable:create --level=project --name=AWS_ACCESS_KEY_ID --value= --json=false --sensitive=true --prefix=env --visible-build=true --visible-runtime=true platform variable:create --level=project --name=AWS_SECRET_ACCESS_KEY --value= ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-sync-database-backups-to-amazon-s3/293",
        "relurl": "/t/how-to-sync-database-backups-to-amazon-s3/293"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "1f55fddf1826d550f5cf3650fcc55c0806d3a069",
        "title": "How to migrate a PostgreSQL database from Heroku to Platform.sh",
        "description": "How to migrate a PostgreSQL database from Heroku to http://Platform.sh Goal Migrate data stored in a Heroku PostgreSQL resource to a http://Platform.sh service. Assumptions You will need: an active application on http://Platform.sh configured to an empty database an active application on Heroku configured to the target database http://Platform.sh Postgres https://community.platform.sh/t/how-to-access-postgresql-credentials-on-platform-sh/150 A local repository with the http://platform.sh/ project as git remote The https://docs.platform.sh/gettingstarted/cli.html installed locally Problems When migrating an existing PostgreSQL project from Heroku to http://Platform.sh, the corresponding resources must be migrated as well. Heroku and http://Platform.sh use different db_dump formats, as well as different styles of configuration settings. Steps 1. Download database dump from Heroku. Find the Resources tab of the Heroku App Dashboard and click on the name of the desired database add-on, for example Heroku Postgres. From the datastore dashboard, visit the Durability tab, create a manual backup of the data, and download the backup file. For the purposes of this guide, the location of the dump will be assumed to be ~/Downloads/db.dump. 2. Add extensions to http://Platform.sh configuration. Next, issue the following command to filter the list of SQL commands in the dump by the word “extension”, giving all Postgres extensions loaded into the database: $ pg_restore -l ~/Downloads/db.dump | grep -o -P '(? db.list Finally, use this list to generate a new, filtered SQL file from the original dump file: pg_restore --no-owner -L db.list ~/Downloads/db.dump db.sql This creates an intermediary file db.list, which may be removed. 4. Import database to http://Platform.sh note Importing a database snapshot is a destructive operation, which will overwrite data already in your database. Backing up data before completing this step is strongly recommended. The final step is importing the SQL file to the Platform database instance. The simplest way to do so is via the following command in the project directory: platform sql ",
        "text": "How to migrate a PostgreSQL database from Heroku to http://Platform.sh Goal Migrate data stored in a Heroku PostgreSQL resource to a http://Platform.sh service. Assumptions You will need: an active application on http://Platform.sh configured to an empty database an active application on Heroku configured to the target database http://Platform.sh Postgres https://community.platform.sh/t/how-to-access-postgresql-credentials-on-platform-sh/150 A local repository with the http://platform.sh/ project as git remote The https://docs.platform.sh/gettingstarted/cli.html installed locally Problems When migrating an existing PostgreSQL project from Heroku to http://Platform.sh, the corresponding resources must be migrated as well. Heroku and http://Platform.sh use different db_dump formats, as well as different styles of configuration settings. Steps 1. Download database dump from Heroku. Find the Resources tab of the Heroku App Dashboard and click on the name of the desired database add-on, for example Heroku Postgres. From the datastore dashboard, visit the Durability tab, create a manual backup of the data, and download the backup file. For the purposes of this guide, the location of the dump will be assumed to be ~/Downloads/db.dump. 2. Add extensions to http://Platform.sh configuration. Next, issue the following command to filter the list of SQL commands in the dump by the word “extension”, giving all Postgres extensions loaded into the database: $ pg_restore -l ~/Downloads/db.dump | grep -o -P '(? db.list Finally, use this list to generate a new, filtered SQL file from the original dump file: pg_restore --no-owner -L db.list ~/Downloads/db.dump db.sql This creates an intermediary file db.list, which may be removed. 4. Import database to http://Platform.sh note Importing a database snapshot is a destructive operation, which will overwrite data already in your database. Backing up data before completing this step is strongly recommended. The final step is importing the SQL file to the Platform database instance. The simplest way to do so is via the following command in the project directory: platform sql ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-migrate-a-postgresql-database-from-heroku-to-platform-sh/301",
        "relurl": "/t/how-to-migrate-a-postgresql-database-from-heroku-to-platform-sh/301"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "23cab84b964712ec40faf61364793a09aabdad48",
        "title": "How to configure caching for static assets",
        "description": "How to configure caching for static assets Goal Caching is a common and effective way to improve the performance of a website. Both http://Platform.sh and your browser can cache static files with the proper cache header configuration. This guide will explain how to configure those headers. Assumptions This guide applies only to static assets. For responses generated dynamically by the application, the application itself will need to include the appropriate cache headers in the response. Some static assets may be inappropriate to cache. This guide assumes that most or all files are safe to cache. Depending on the use case, it may make sense to cache files for a relatively short period (minutes) or a very long period (weeks). This guide applies equally to static files included with your application as well as those produced by your application or uploaded by a user. Problems If a file is cached and then updated, anywhere the old file is already cached will still keep the old copy until its cache expires. For that reason it is usually better to err on the side of shorter cache lifetimes if unsure. Steps 1. Determine which files to cache Different cache settings may be set for different files, based on either their directory or precise filename. For this example assume the following directory structure: / .platform/ .platform.app.yaml web/ index.php robots.txt uploads/ assets/ generated/ nocache/ Where the document root is web. uploads contains user-uploaded files that should be cached for 10 minutes. assets contains developer-provided files that may be cached for longer, such a 1 day. robots.txt is a miscellaneous file that is safe to cache. generated contains application-generated files, such as CSS or JS files, that may be cached for a long period but need to change quickly when code changes. nocache contains files that change so rapidly that caching them is not useful. 2. Set a default cache lifetime for all static files Locate the web.locations block in .platform.app.yaml for the document root. In this example it would look something like: web: locations: '/': root: 'web' As a sibling of root, add an expires key with a time period. To set an expiration time of 5 minutes, use: web: locations: '/': root: 'web' expires: 5m The time period can be in “s” (seconds), “m” (minutes), “h” (hours), “d” (days), “w” (weeks), “M” (30 day months), or “y” (365 day years). This configuration will automatically set a cache-control header for all static files to cache the file for 5 minutes. If you have multiple paths mapped under web.locations, each one may have its own expires declaration. See the for more details. 3. Configure directory-specific alternate cache lifetimes To change the cache information (or any other configuration) for a specific sub-path within the docroot, use the rules block: web: locations: '/': root: 'web' expires: 5m rules: '^/uploads': expires: 10m '^/assets': expires: 1d '^/nocache': expires: -1 Each rules block is a regular expression that may match the path. In this example: any URL that begins with /uploads will have a cache lifetime of 10 minutes any URL that begins with /assets will have a cache lifetime of 1 day any URL that begins with /nocache will explicitly have caching disabled and any other static files will have a cache lifetime of 5 minutes The rules block also makes it possible to target specific files by name or pattern. 4. Configure cache-busting URLs A common pattern is for certain files to have very long cache lifetimes but need to be cleared on-demand after a code deploy. That’s especially true for aggregated CSS or JS files. The standard technique for that is to include a “cache busting” query on the URL as produced by the application, which since it’s a different URL will not use the previously cached value. There are two steps to configure that pattern. First, set an especially long cache lifetime for the files in question. web: locations: '/': root: 'web' expires: 5m rules: '^/generated/*.(css|js)': expires: 2w That configuration will set a 2 week cache lifetime for any files under the generated directory that end in .css or .js. Second, when your application generates URLs to those files it needs to include a ?cache=somestring query parameter. When the application code changes then somestring will need to change, too. An easy way to do that is to base the string on the PLATFORM_TREE_ID environment variable, which will change if and only if the code in the application changed. For example, in PHP one could do: $somestring = substr(getenv('PLATFORM_TREE_ID'), 0, 6); That will produce a junk string that is the first 6 characters of the tree ID. When new code is deployed, PLATFORM_TREE_ID will change and thus $somestring will change as well. The application will then need to generate URLs that include that value, such as: https://www.example.com/generated/forum.css?cache=$somestring 5. Enable router caching In .platform/routes.yaml, ensure that caching is enabled for the appropriate route: \"https://{default}/\": type: upstream upstream: \"app:http\" cache: enabled: true cookies: ['/^SS?ESS/'] default_ttl: 5m The enabled statement turns on the cache, causing the router to cache any responses that have a proper cache header (both static and application-generated responses). The cookies statement is an array of cookie name regular expressions that should be excluded when calculating the cache. Generally it should be set to whatever your application’s cookie name is. In this example, cookies named SESS or SSESS will be excluded from the cache. If this setting is incorrect then any request with an active session will not be cached, even if it’s safe to do so. The default_ttl statement will set a cache header of 5 minutes on all static responses that do not already have one. In the example above all static files will have a cache header already so it has no effect. See the https://docs.platform.sh/configuration/routes/cache.html for more details. Conclusion All static files will now be cached for the specified period of time in the router and in each visitor’s browser cache. Dynamic application responses that have a cache header set by the application will be cached as well. Dynamic application responses that have no cache-header will not be cached at any level.",
        "text": "How to configure caching for static assets Goal Caching is a common and effective way to improve the performance of a website. Both http://Platform.sh and your browser can cache static files with the proper cache header configuration. This guide will explain how to configure those headers. Assumptions This guide applies only to static assets. For responses generated dynamically by the application, the application itself will need to include the appropriate cache headers in the response. Some static assets may be inappropriate to cache. This guide assumes that most or all files are safe to cache. Depending on the use case, it may make sense to cache files for a relatively short period (minutes) or a very long period (weeks). This guide applies equally to static files included with your application as well as those produced by your application or uploaded by a user. Problems If a file is cached and then updated, anywhere the old file is already cached will still keep the old copy until its cache expires. For that reason it is usually better to err on the side of shorter cache lifetimes if unsure. Steps 1. Determine which files to cache Different cache settings may be set for different files, based on either their directory or precise filename. For this example assume the following directory structure: / .platform/ .platform.app.yaml web/ index.php robots.txt uploads/ assets/ generated/ nocache/ Where the document root is web. uploads contains user-uploaded files that should be cached for 10 minutes. assets contains developer-provided files that may be cached for longer, such a 1 day. robots.txt is a miscellaneous file that is safe to cache. generated contains application-generated files, such as CSS or JS files, that may be cached for a long period but need to change quickly when code changes. nocache contains files that change so rapidly that caching them is not useful. 2. Set a default cache lifetime for all static files Locate the web.locations block in .platform.app.yaml for the document root. In this example it would look something like: web: locations: '/': root: 'web' As a sibling of root, add an expires key with a time period. To set an expiration time of 5 minutes, use: web: locations: '/': root: 'web' expires: 5m The time period can be in “s” (seconds), “m” (minutes), “h” (hours), “d” (days), “w” (weeks), “M” (30 day months), or “y” (365 day years). This configuration will automatically set a cache-control header for all static files to cache the file for 5 minutes. If you have multiple paths mapped under web.locations, each one may have its own expires declaration. See the for more details. 3. Configure directory-specific alternate cache lifetimes To change the cache information (or any other configuration) for a specific sub-path within the docroot, use the rules block: web: locations: '/': root: 'web' expires: 5m rules: '^/uploads': expires: 10m '^/assets': expires: 1d '^/nocache': expires: -1 Each rules block is a regular expression that may match the path. In this example: any URL that begins with /uploads will have a cache lifetime of 10 minutes any URL that begins with /assets will have a cache lifetime of 1 day any URL that begins with /nocache will explicitly have caching disabled and any other static files will have a cache lifetime of 5 minutes The rules block also makes it possible to target specific files by name or pattern. 4. Configure cache-busting URLs A common pattern is for certain files to have very long cache lifetimes but need to be cleared on-demand after a code deploy. That’s especially true for aggregated CSS or JS files. The standard technique for that is to include a “cache busting” query on the URL as produced by the application, which since it’s a different URL will not use the previously cached value. There are two steps to configure that pattern. First, set an especially long cache lifetime for the files in question. web: locations: '/': root: 'web' expires: 5m rules: '^/generated/*.(css|js)': expires: 2w That configuration will set a 2 week cache lifetime for any files under the generated directory that end in .css or .js. Second, when your application generates URLs to those files it needs to include a ?cache=somestring query parameter. When the application code changes then somestring will need to change, too. An easy way to do that is to base the string on the PLATFORM_TREE_ID environment variable, which will change if and only if the code in the application changed. For example, in PHP one could do: $somestring = substr(getenv('PLATFORM_TREE_ID'), 0, 6); That will produce a junk string that is the first 6 characters of the tree ID. When new code is deployed, PLATFORM_TREE_ID will change and thus $somestring will change as well. The application will then need to generate URLs that include that value, such as: https://www.example.com/generated/forum.css?cache=$somestring 5. Enable router caching In .platform/routes.yaml, ensure that caching is enabled for the appropriate route: \"https://{default}/\": type: upstream upstream: \"app:http\" cache: enabled: true cookies: ['/^SS?ESS/'] default_ttl: 5m The enabled statement turns on the cache, causing the router to cache any responses that have a proper cache header (both static and application-generated responses). The cookies statement is an array of cookie name regular expressions that should be excluded when calculating the cache. Generally it should be set to whatever your application’s cookie name is. In this example, cookies named SESS or SSESS will be excluded from the cache. If this setting is incorrect then any request with an active session will not be cached, even if it’s safe to do so. The default_ttl statement will set a cache header of 5 minutes on all static responses that do not already have one. In the example above all static files will have a cache header already so it has no effect. See the https://docs.platform.sh/configuration/routes/cache.html for more details. Conclusion All static files will now be cached for the specified period of time in the router and in each visitor’s browser cache. Dynamic application responses that have a cache header set by the application will be cached as well. Dynamic application responses that have no cache-header will not be cached at any level.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-configure-caching-for-static-assets/187",
        "relurl": "/t/how-to-configure-caching-for-static-assets/187"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "1ef7562eeee0768bb4ec1c923e12237acc8dd180",
        "title": "How do I install Apache Tika with Solr",
        "description": "Goal Install Apache Tika with Solr on http://Platform.sh Assumptions A https://github.com/platformsh/template-drupal8 project on http://Platform.sh. https://docs.platform.sh/configuration/services/solr.html on that project. Problems Apache Tika allows you to extract information from binary files (e.g. PDF files) and make them searchable in Solr. Steps 1. Install search modules using composer Install and configure search_api and search_api_solr: composer require drupal/search_api composer require drupal/search_api_solr More information is available for setting up Solr with Drupal 8 in the https://docs.platform.sh/frameworks/drupal8/solr.html . 2. Install search attachments module Install search_api_attachments using composer composer require drupal/search_api_attachments Search API Attachments enable pointing at the tika jar file to index PDF documents. 3. Install the Tika jar Modify or include a build hook to download the Tika jar file to the project by editing .platform.app.yaml. # .platform.app.yaml hooks: build: | mkdir -p /app/srv/bin cd /app/srv/bin \u0026\u0026 curl -OL http://download.nextag.com/apache/tika/tika-app-1.16.jar The build hook creates the directory /srv/bin and downloads the Tika jar executable into it. An https://github.com/thinktandem/platform-tika/blob/master/.platform.app.yaml is available where the full .platform.app.yaml can be found. Consult the documentation for more information about on http://Platform.sh. 4. Configure the search API attachments Visit /admin/config/search/search_api_attachments in a browser and add the method, java executable, and Tika paths configuration. These paths correspond to the paths entered in the .platform.app.yaml file for the build step. Conclusion Apache Tika is now setup with Solr on http://Platform.sh.",
        "text": "Goal Install Apache Tika with Solr on http://Platform.sh Assumptions A https://github.com/platformsh/template-drupal8 project on http://Platform.sh. https://docs.platform.sh/configuration/services/solr.html on that project. Problems Apache Tika allows you to extract information from binary files (e.g. PDF files) and make them searchable in Solr. Steps 1. Install search modules using composer Install and configure search_api and search_api_solr: composer require drupal/search_api composer require drupal/search_api_solr More information is available for setting up Solr with Drupal 8 in the https://docs.platform.sh/frameworks/drupal8/solr.html . 2. Install search attachments module Install search_api_attachments using composer composer require drupal/search_api_attachments Search API Attachments enable pointing at the tika jar file to index PDF documents. 3. Install the Tika jar Modify or include a build hook to download the Tika jar file to the project by editing .platform.app.yaml. # .platform.app.yaml hooks: build: | mkdir -p /app/srv/bin cd /app/srv/bin \u0026\u0026 curl -OL http://download.nextag.com/apache/tika/tika-app-1.16.jar The build hook creates the directory /srv/bin and downloads the Tika jar executable into it. An https://github.com/thinktandem/platform-tika/blob/master/.platform.app.yaml is available where the full .platform.app.yaml can be found. Consult the documentation for more information about on http://Platform.sh. 4. Configure the search API attachments Visit /admin/config/search/search_api_attachments in a browser and add the method, java executable, and Tika paths configuration. These paths correspond to the paths entered in the .platform.app.yaml file for the build step. Conclusion Apache Tika is now setup with Solr on http://Platform.sh.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-install-apache-tika-with-solr/290",
        "relurl": "/t/how-do-i-install-apache-tika-with-solr/290"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "2f4e8d8496af1c4d12403bc0a97cb1d791d9a966",
        "title": "How to develop locally on Platform.sh with a tethered connection",
        "description": "Goal Run a project on a local web server that connects to an application’s services with an SSH tunnel. Assumptions This guide uses the https://docs.platform.sh/gettingstarted/cli.html to connect to a project’s services, and assumes that it and Git are already installed. The guide also assumes that an SSH key is configured on the project account. Problems The local machine must be configured with the appropriate PHP extensions. From a local project, an SSH tunnel is required to connect to http://Platform.sh services and expose those relationships in a local environment variable. Steps 1. Check pcntl and posix Tunneling to the project’s services using the CLI requires the PHP extensions pcntl and posix. Check to make sure they are present, and install if the following output is not received: $ php -m | grep -E 'posix|pcntl' pcntl posix 2. Download local project copy List the projects on the account to find the project-ID: $ platform projects Your projects are: +--------------+------------------------------------------+-------------------------------------------------+ | ID | Title | URL | +--------------+------------------------------------------+-------------------------------------------------+ | | Platform Project | https://eu.platform.sh//#/projects/ | +--------------+------------------------------------------+-------------------------------------------------+ Download the project. $ platform get Directory [repo]: repo Environment [master]: dev Downloading project Platform Project () Cloning into 'my-project/repo'... remote: counting objects: 461, done. Receiving objects: 100% (461/461), 1.41 MiB | 249.00 KiB/s, done. Resolving deltas: 100% (225/225), done. The project Platform Project () was successfully downloaded to: repo 3. Open an SSH tunnel to the project. $ platform tunnel:open SSH tunnel opened on port 30000 to relationship: elasticsearch SSH tunnel opened on port 30001 to relationship: rabbitmq SSH tunnel opened on port 30002 to relationship: postgresql Check to make sure that the tunnels are visible. $ platform tunnel:list +-------+--------------+-------------+-----+---------------+ | Port | Project | Environment | App | Relationship | +-------+--------------+-------------+-----+---------------+ | 30000 | | dev | app | elasticsearch | | 30001 | | dev | app | rabbitmq | | 30002 | | dev | app | postgresql | +-------+--------------+-------------+-----+---------------+ If the tunnels are not visible here, check /Users//.platformsh/tunnels.log to see if the https://docs.platform.sh/development/ssh.html was configured incorrectly. 4. Export PLATFORM_RELATIONSHIPS Create a JSON encoded PLATFORM_RELATIONSHIPS environment variable to mimic the relationships array that will be visible to the application. export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" 5. Build Build the application. If a service is required locally but missing, the CLI will prompt for its installation. $ cd repo $ platform build 6. Run the application Now the project has been built and can be run locally. For a Django application: $ python manage.py runserver 7. Close the tunnel. $ platform tunnel:close Close tunnel to relationship elasticsearch on -dev--app? [Y/n] y Closed tunnel to elasticsearch on -dev--app Close tunnel to relationship rabbitmq on -dev--app? [Y/n] y Closed tunnel to rabbitmq on -dev--app Close tunnel to relationship postgresql on -dev--app? [Y/n] y Closed tunnel to postgresql on -dev--app Conclusion Once PHP extensions are properly configured and SSH tunnels are opened to a http://Platform.sh project’s services, an application can be built for local development. The CLI is used here to connect to PostgreSQL, ElasticSearch and RabbitMQ, but the commands are not specific to those https://docs.platform.sh/configuration/services.html or to the language used in the application. Additional information regarding local development best practices can be found in the https://docs.platform.sh/gettingstarted/local.html .",
        "text": "Goal Run a project on a local web server that connects to an application’s services with an SSH tunnel. Assumptions This guide uses the https://docs.platform.sh/gettingstarted/cli.html to connect to a project’s services, and assumes that it and Git are already installed. The guide also assumes that an SSH key is configured on the project account. Problems The local machine must be configured with the appropriate PHP extensions. From a local project, an SSH tunnel is required to connect to http://Platform.sh services and expose those relationships in a local environment variable. Steps 1. Check pcntl and posix Tunneling to the project’s services using the CLI requires the PHP extensions pcntl and posix. Check to make sure they are present, and install if the following output is not received: $ php -m | grep -E 'posix|pcntl' pcntl posix 2. Download local project copy List the projects on the account to find the project-ID: $ platform projects Your projects are: +--------------+------------------------------------------+-------------------------------------------------+ | ID | Title | URL | +--------------+------------------------------------------+-------------------------------------------------+ | | Platform Project | https://eu.platform.sh//#/projects/ | +--------------+------------------------------------------+-------------------------------------------------+ Download the project. $ platform get Directory [repo]: repo Environment [master]: dev Downloading project Platform Project () Cloning into 'my-project/repo'... remote: counting objects: 461, done. Receiving objects: 100% (461/461), 1.41 MiB | 249.00 KiB/s, done. Resolving deltas: 100% (225/225), done. The project Platform Project () was successfully downloaded to: repo 3. Open an SSH tunnel to the project. $ platform tunnel:open SSH tunnel opened on port 30000 to relationship: elasticsearch SSH tunnel opened on port 30001 to relationship: rabbitmq SSH tunnel opened on port 30002 to relationship: postgresql Check to make sure that the tunnels are visible. $ platform tunnel:list +-------+--------------+-------------+-----+---------------+ | Port | Project | Environment | App | Relationship | +-------+--------------+-------------+-----+---------------+ | 30000 | | dev | app | elasticsearch | | 30001 | | dev | app | rabbitmq | | 30002 | | dev | app | postgresql | +-------+--------------+-------------+-----+---------------+ If the tunnels are not visible here, check /Users//.platformsh/tunnels.log to see if the https://docs.platform.sh/development/ssh.html was configured incorrectly. 4. Export PLATFORM_RELATIONSHIPS Create a JSON encoded PLATFORM_RELATIONSHIPS environment variable to mimic the relationships array that will be visible to the application. export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" 5. Build Build the application. If a service is required locally but missing, the CLI will prompt for its installation. $ cd repo $ platform build 6. Run the application Now the project has been built and can be run locally. For a Django application: $ python manage.py runserver 7. Close the tunnel. $ platform tunnel:close Close tunnel to relationship elasticsearch on -dev--app? [Y/n] y Closed tunnel to elasticsearch on -dev--app Close tunnel to relationship rabbitmq on -dev--app? [Y/n] y Closed tunnel to rabbitmq on -dev--app Close tunnel to relationship postgresql on -dev--app? [Y/n] y Closed tunnel to postgresql on -dev--app Conclusion Once PHP extensions are properly configured and SSH tunnels are opened to a http://Platform.sh project’s services, an application can be built for local development. The CLI is used here to connect to PostgreSQL, ElasticSearch and RabbitMQ, but the commands are not specific to those https://docs.platform.sh/configuration/services.html or to the language used in the application. Additional information regarding local development best practices can be found in the https://docs.platform.sh/gettingstarted/local.html .",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69",
        "relurl": "/t/how-to-develop-locally-on-platform-sh-with-a-tethered-connection/69"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "755ed483076fb2073f5c0dfea720e7d5c55743ca",
        "title": "How to create and restore snapshots using the CLI",
        "description": "Goal Restore a https://platform.sh live environment from a created snapshot. Assumptions This guide requires: An application running on http://Platform.sh A local repository with the http://Platform.sh project as git remote The https://docs.platform.sh/gettingstarted/cli.html installed locally An https://docs.platform.sh/development/ssh.html configured on the project account admin role granted for the project This guide uses a https://github.com/platformsh/template-python3 for a http://Platform.sh application using Flask as well as MySQL and Redis for its services. The template runs a test on both MySQL and Redis and returns a status dictionary to the live page. Problems http://Platform.sh recommends that snapshots are created of the live environment before merging, or when the storage space of services is increased. For one reason or another, it may become necessary to restore an active environment to a previous working state using snapshots. Creation and restoration of snapshots can be executed from the UI and using the CLI. Additional information can be found in the https://pr-1044-6dxt2aq-t2llqeifuhpzg.eu.platform.sh/administration/snapshot-and-restore.html Steps 1. Create a snapshot The master environment is functioning as desired: $ curl https://master-7rqtwti- ..platformsh.site/ {\"mysql\":{\"return\":null,\"status\":\"OK\"},\"redis\":{\"return\":null,\"status\":\"OK\"}} From master create a snapshot using the http://Platform.sh CLI if one has not already been created. $ platform snapshot:create Creating a snapshot of master Waiting for the snapshot to complete... Waiting for the activity khlktlgy4r5uc (User created a backup of Master): Backing up master Backup name is [============================] 12 secs (complete) A snapshot of environment master has been created Snapshot name: 2. Changes were made and merged to Master In another branch dev, some changes were made. For the Python 3 template example, the Redis test was changed from 70 r.set(key_name, \"bar\") to 70 r.set(key_name, \"BEAR\") which will result in an Exception for the failed test. It was not caught in time, and dev was merged into master. $ platform environment:merge Now the live site is failing: $ curl https://master-7rqtwti- ..platformsh.site/ {\"mysql\":{\"return\":null,\"status\":\"OK\"},\"redis\":{\"error\":[\"Traceback (most recent call last):\\n\",\" File \\\"server.py\\\", line 30, in wrap_test\\n result = callback(*args, **kwargs)\\n\",\" File \\\"server.py\\\", line 71, in test_redis\\n assert value == r.get(key_name)\\n\",\"AssertionError\\n\"], \"status\":\"ERROR\"}} 3. Retrieve Snapshot Name Actions must take place from the branch the snapshot was taken from, and will not be available from dev. $ git checkout dev Switched to branch 'dev' $ platform snapshots No snapshots found List the saved snapshots to retrieve the snapshot name. $ git checkout master Switched to branch 'master' Your branch is up to date with 'platform/master'. $ platform snapshots Snapshots on the project ( ), environment Master (master): +---------------------------+----------------------------+----------+----------+---------+ | Created | Snapshot name | Progress | State | Result | +---------------------------+----------------------------+----------+----------+---------+ | 2019-03-11T15:16:16-04:00 | | 100% | complete | success | +---------------------------+----------------------------+----------+----------+---------+ 4. Restore the snapshot The snapshot can be restored to the original environment: $ platform snapshot:restore Are you sure you want to restore the snapshot from 2019-03-11T15:16:16-04:00 to environment Master (master)? [Y/n] Y Restoring snapshot to Master (master) Waiting for the restore to complete... Waiting for the activity tb7sf5gd3ivfw (User restored environment master from backup ): Provisioning certificates Environment certificates - certificate 7455422: expiring on 2019-06-09 17:53:35+00:00, covering master-7rqtwti- ..platformsh.site [============================] 35 secs (complete) The snapshot was successfully restored It can also be restored to another active branch. In this case, the target is the branch feature-x: platform snapshot:restore --target=feature-x 5. Verify the restoration was successful Check that the snapshot has been restored to the environment. $ curl https://master-7rqtwti- ..platformsh.site/ {\"mysql\":{\"return\":null,\"status\":\"OK\"},\"redis\":{\"return\":null,\"status\":\"OK\"}} $ $ curl https://feature-x-c2qo5ma- ..platformsh.site/ {\"mysql\":{\"return\":null,\"status\":\"OK\"},\"redis\":{\"return\":null,\"status\":\"OK\"}} Conclusion Once a snapshot is created, undesired changes can be reverted on an active branch using the http://Platform.sh CLI.",
        "text": "Goal Restore a https://platform.sh live environment from a created snapshot. Assumptions This guide requires: An application running on http://Platform.sh A local repository with the http://Platform.sh project as git remote The https://docs.platform.sh/gettingstarted/cli.html installed locally An https://docs.platform.sh/development/ssh.html configured on the project account admin role granted for the project This guide uses a https://github.com/platformsh/template-python3 for a http://Platform.sh application using Flask as well as MySQL and Redis for its services. The template runs a test on both MySQL and Redis and returns a status dictionary to the live page. Problems http://Platform.sh recommends that snapshots are created of the live environment before merging, or when the storage space of services is increased. For one reason or another, it may become necessary to restore an active environment to a previous working state using snapshots. Creation and restoration of snapshots can be executed from the UI and using the CLI. Additional information can be found in the https://pr-1044-6dxt2aq-t2llqeifuhpzg.eu.platform.sh/administration/snapshot-and-restore.html Steps 1. Create a snapshot The master environment is functioning as desired: $ curl https://master-7rqtwti- ..platformsh.site/ {\"mysql\":{\"return\":null,\"status\":\"OK\"},\"redis\":{\"return\":null,\"status\":\"OK\"}} From master create a snapshot using the http://Platform.sh CLI if one has not already been created. $ platform snapshot:create Creating a snapshot of master Waiting for the snapshot to complete... Waiting for the activity khlktlgy4r5uc (User created a backup of Master): Backing up master Backup name is [============================] 12 secs (complete) A snapshot of environment master has been created Snapshot name: 2. Changes were made and merged to Master In another branch dev, some changes were made. For the Python 3 template example, the Redis test was changed from 70 r.set(key_name, \"bar\") to 70 r.set(key_name, \"BEAR\") which will result in an Exception for the failed test. It was not caught in time, and dev was merged into master. $ platform environment:merge Now the live site is failing: $ curl https://master-7rqtwti- ..platformsh.site/ {\"mysql\":{\"return\":null,\"status\":\"OK\"},\"redis\":{\"error\":[\"Traceback (most recent call last):\\n\",\" File \\\"server.py\\\", line 30, in wrap_test\\n result = callback(*args, **kwargs)\\n\",\" File \\\"server.py\\\", line 71, in test_redis\\n assert value == r.get(key_name)\\n\",\"AssertionError\\n\"], \"status\":\"ERROR\"}} 3. Retrieve Snapshot Name Actions must take place from the branch the snapshot was taken from, and will not be available from dev. $ git checkout dev Switched to branch 'dev' $ platform snapshots No snapshots found List the saved snapshots to retrieve the snapshot name. $ git checkout master Switched to branch 'master' Your branch is up to date with 'platform/master'. $ platform snapshots Snapshots on the project ( ), environment Master (master): +---------------------------+----------------------------+----------+----------+---------+ | Created | Snapshot name | Progress | State | Result | +---------------------------+----------------------------+----------+----------+---------+ | 2019-03-11T15:16:16-04:00 | | 100% | complete | success | +---------------------------+----------------------------+----------+----------+---------+ 4. Restore the snapshot The snapshot can be restored to the original environment: $ platform snapshot:restore Are you sure you want to restore the snapshot from 2019-03-11T15:16:16-04:00 to environment Master (master)? [Y/n] Y Restoring snapshot to Master (master) Waiting for the restore to complete... Waiting for the activity tb7sf5gd3ivfw (User restored environment master from backup ): Provisioning certificates Environment certificates - certificate 7455422: expiring on 2019-06-09 17:53:35+00:00, covering master-7rqtwti- ..platformsh.site [============================] 35 secs (complete) The snapshot was successfully restored It can also be restored to another active branch. In this case, the target is the branch feature-x: platform snapshot:restore --target=feature-x 5. Verify the restoration was successful Check that the snapshot has been restored to the environment. $ curl https://master-7rqtwti- ..platformsh.site/ {\"mysql\":{\"return\":null,\"status\":\"OK\"},\"redis\":{\"return\":null,\"status\":\"OK\"}} $ $ curl https://feature-x-c2qo5ma- ..platformsh.site/ {\"mysql\":{\"return\":null,\"status\":\"OK\"},\"redis\":{\"return\":null,\"status\":\"OK\"}} Conclusion Once a snapshot is created, undesired changes can be reverted on an active branch using the http://Platform.sh CLI.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-create-and-restore-snapshots-using-the-cli/100",
        "relurl": "/t/how-to-create-and-restore-snapshots-using-the-cli/100"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "5cd45449528e68f5bda9c3ab122acb073c01abc3",
        "title": "How to interactively debug Python applications on Platform.sh",
        "description": "Goal Interactively debug python applications running on http://Platform.sh. Assumptions An active Python application running on http://Platform.sh A configured on the project account Familiarity with the https://docs.python.org/3/library/pdb.html Optional: a http://Platform.sh Django https://github.com/platformsh/template-django2 Problems Effectively debugging web applications isn’t trivial, especially when an HTTP request goes through multiple layers until it reaches the web app, which is usually the case in cloud platforms. Debugging in a local environment is easier, but the bug might only trigger when interacting with data present on production. Note: The debugging procedure should only be performed on a non-production environment. So, if necessary clone the production environment, with the data too if need be, depending on the preconditions that trigger the bug you’re investigating. The existence of bugs is sometimes manifested by an unhandled exception being raised and bubbling up to the top of your application, or in other cases, there are no exceptions, but the actions that a request handler is supposed to perform do not occur the way you’d expect. Let’s explore the different set of debugging tools we can use and learn when to use what. Steps (dropping in a REPL shell) Sometimes identifying a bug is possible by just looking at the stack trace of the exception or getting the values of some variables at some stack in the backtrace. This is exactly what the http://werkzeug.pocoo.org/docs/0.14/debug/ WSGI wrapper from the werkzeug library does. To use it: 1. Add the new werkzeug dependency. $ pipenv install werkzeug 2. Wrap and expose the WSGI application Add the script wsgi-debug.py to the project directory. It wraps our WSGI application with DebuggedApplication and exposes it. import werkzeug.debug # For the template, it is assumed wsgi.py is in project/myapp/, # edit to match your project accordingly. import myapp.wsgi application = werkzeug.debug.DebuggedApplication( myapp.wsgi.application, evalex=True, ) 3. Setup the debug script Add the script gunicorn-debug.sh that stops the “app” service in the environment and spawns a new gunicorn instance that’s pointed to the wrapped WSGI application. #!/usr/bin/env sh et -ex # Stop the application. sv stop app # When this script exits, start it again. trap \"exit 0\" INT trap \"sv start app\" EXIT # Run a gunicorn and point it towards the wrapped WSGI application. # It's important to run in sync mode, and a single worker, because werkzeug # DebuggedApplication doesn't like forking. # Set a high enough timeout that gives us time to debug. gunicorn \\ --worker-class sync \\ --workers 1 \\ --timeout 3600 \\ --bind unix:$SOCKET \\ wsgi-debug:application And make it executable. $ chmod +x gunicorn-debug 4. Allow exceptions to propagate Tell Django to let exceptions bubble up, by setting these in your myapp/settings.py file. DEBUG = False DEBUG_PROPAGATE_EXCEPTIONS = True 5. Commit and push these changes. 6. Run the script After the deployment succeeds, ssh into the environment and run the gunicorn-debug script. $ ./gunicorn-debug Now issue the request that makes it raise the unhandled exception, and lo and behold, a beautiful debugging interface appears. The stack trace of the exception can be seen when a line of code is hovered over, and a “shell” button appears at the right that will spawn a Python shell at that line where variables (or anything else can be run, for that matter) can be printed. Note: A PIN code is required to enable the shell, which is printed in the console of the gunicorn-debug script. This is for security reasons, to prevent unauthorized access in case it is accidentally enabled on production. Steps (Debug the application code step by step with pdb.) The previous approach will help to understand why an exception is raised so that it can be fixed, but if the bug occurs at an intermediate stage before the exception is raised, this will not be enough. The werkzeug debugging interface only captures the latest state of the app and the values of each variable (i.e., it cannot go back in time). To gain extra insight into what code path is entered when the request is handled, a pdb trace point can be set at some point in the code, and then interactively execute it line by line and inspect variables as you go. See the https://docs.python.org/3/library/pdb.html for more information. 1. Add a pdb trace While still running the application with our gunicorn-debug script, add a pdb trace (import pdb; pdb.set_trace()) at some point in the code, which can even be inside a conditional. For example, to test this methodology, the following can be included in a Django application’s views.py: if request.GET.get('pdb', False) == '1': import pdb; pdb.set_trace() 2. Use the PDB shell Issue a request that makes it reach the pdb.set_trace() and then code execution will pause and the PDB shell will appear in the terminal where the gunicorn-debug script is running. From there, pdb commands can be issued, for example, to execute code line by line, to print variables, or run any arbitrary code desired. Conclusion Debugging complex bugs that are only reproduced with the data that are present on production is non trivial, but thanks to development environments data cloning capabilities, and the dynamic nature of Python, we have great tools at our disposal to properly investigate these kind of bugs.",
        "text": "Goal Interactively debug python applications running on http://Platform.sh. Assumptions An active Python application running on http://Platform.sh A configured on the project account Familiarity with the https://docs.python.org/3/library/pdb.html Optional: a http://Platform.sh Django https://github.com/platformsh/template-django2 Problems Effectively debugging web applications isn’t trivial, especially when an HTTP request goes through multiple layers until it reaches the web app, which is usually the case in cloud platforms. Debugging in a local environment is easier, but the bug might only trigger when interacting with data present on production. Note: The debugging procedure should only be performed on a non-production environment. So, if necessary clone the production environment, with the data too if need be, depending on the preconditions that trigger the bug you’re investigating. The existence of bugs is sometimes manifested by an unhandled exception being raised and bubbling up to the top of your application, or in other cases, there are no exceptions, but the actions that a request handler is supposed to perform do not occur the way you’d expect. Let’s explore the different set of debugging tools we can use and learn when to use what. Steps (dropping in a REPL shell) Sometimes identifying a bug is possible by just looking at the stack trace of the exception or getting the values of some variables at some stack in the backtrace. This is exactly what the http://werkzeug.pocoo.org/docs/0.14/debug/ WSGI wrapper from the werkzeug library does. To use it: 1. Add the new werkzeug dependency. $ pipenv install werkzeug 2. Wrap and expose the WSGI application Add the script wsgi-debug.py to the project directory. It wraps our WSGI application with DebuggedApplication and exposes it. import werkzeug.debug # For the template, it is assumed wsgi.py is in project/myapp/, # edit to match your project accordingly. import myapp.wsgi application = werkzeug.debug.DebuggedApplication( myapp.wsgi.application, evalex=True, ) 3. Setup the debug script Add the script gunicorn-debug.sh that stops the “app” service in the environment and spawns a new gunicorn instance that’s pointed to the wrapped WSGI application. #!/usr/bin/env sh et -ex # Stop the application. sv stop app # When this script exits, start it again. trap \"exit 0\" INT trap \"sv start app\" EXIT # Run a gunicorn and point it towards the wrapped WSGI application. # It's important to run in sync mode, and a single worker, because werkzeug # DebuggedApplication doesn't like forking. # Set a high enough timeout that gives us time to debug. gunicorn \\ --worker-class sync \\ --workers 1 \\ --timeout 3600 \\ --bind unix:$SOCKET \\ wsgi-debug:application And make it executable. $ chmod +x gunicorn-debug 4. Allow exceptions to propagate Tell Django to let exceptions bubble up, by setting these in your myapp/settings.py file. DEBUG = False DEBUG_PROPAGATE_EXCEPTIONS = True 5. Commit and push these changes. 6. Run the script After the deployment succeeds, ssh into the environment and run the gunicorn-debug script. $ ./gunicorn-debug Now issue the request that makes it raise the unhandled exception, and lo and behold, a beautiful debugging interface appears. The stack trace of the exception can be seen when a line of code is hovered over, and a “shell” button appears at the right that will spawn a Python shell at that line where variables (or anything else can be run, for that matter) can be printed. Note: A PIN code is required to enable the shell, which is printed in the console of the gunicorn-debug script. This is for security reasons, to prevent unauthorized access in case it is accidentally enabled on production. Steps (Debug the application code step by step with pdb.) The previous approach will help to understand why an exception is raised so that it can be fixed, but if the bug occurs at an intermediate stage before the exception is raised, this will not be enough. The werkzeug debugging interface only captures the latest state of the app and the values of each variable (i.e., it cannot go back in time). To gain extra insight into what code path is entered when the request is handled, a pdb trace point can be set at some point in the code, and then interactively execute it line by line and inspect variables as you go. See the https://docs.python.org/3/library/pdb.html for more information. 1. Add a pdb trace While still running the application with our gunicorn-debug script, add a pdb trace (import pdb; pdb.set_trace()) at some point in the code, which can even be inside a conditional. For example, to test this methodology, the following can be included in a Django application’s views.py: if request.GET.get('pdb', False) == '1': import pdb; pdb.set_trace() 2. Use the PDB shell Issue a request that makes it reach the pdb.set_trace() and then code execution will pause and the PDB shell will appear in the terminal where the gunicorn-debug script is running. From there, pdb commands can be issued, for example, to execute code line by line, to print variables, or run any arbitrary code desired. Conclusion Debugging complex bugs that are only reproduced with the data that are present on production is non trivial, but thanks to development environments data cloning capabilities, and the dynamic nature of Python, we have great tools at our disposal to properly investigate these kind of bugs.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-interactively-debug-python-applications-on-platform-sh/170",
        "relurl": "/t/how-to-interactively-debug-python-applications-on-platform-sh/170"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "5624537d7fb8d641a1054fcd30396c335bf6d0b4",
        "title": "How to derive metrics from access log via ngxtop",
        "description": "Goal To derive various metrics from the Nginx access log via https://github.com/lebinh/ngxtop/. Assumptions To complete this, you will need: The https://docs.platform.sh/gettingstarted/cli.html tool installed The https://pip.pypa.io/en/stable/installing/ tool installed Problems Sometimes, you may find the need to derive metrics from the access log files and identify bad traffic. Using raw Linux commands to perform the analysis may not be efficient enough. Steps 1. Check ngxtop installation Make sure ngxtop is installed locally via the below command: pip install --user ngxtop 2. Get web traffic overview To gather an overview of recent web traffic, use below command: platform log access -p -e master --lines 102400 | ngxtop --no-follow Summary: | count | avg_bytes_sent | 2xx | 3xx | 4xx | 5xx | |---------+------------------+-------+-------+-------+-------| | 41553 | 6342.284 | 27380 | 6474 | 100 | 7599 | Detailed: | request_path | count | avg_bytes_sent | 2xx | 3xx | 4xx | 5xx | |-----------------------------------------------+---------+------------------+-------+-------+-------+-------| | /scripts/user-widget/dist/user-widget.min.js | 1190 | 29123.529 | 962 | 228 | 0 | 0 | | /gitbook/gitbook-plugin-codetabs/codetabs.js | 1185 | 186.957 | 961 | 224 | 0 | 0 | | /gitbook/style.css | 1185 | 8088.093 | 964 | 221 | 0 | 0 | | /gitbook/gitbook-plugin-edit-link/plugin.js | 1184 | 340.759 | 961 | 223 | 0 | 0 | | /gitbook/gitbook-plugin-gtm/plugin.js | 1184 | 199.188 | 960 | 224 | 0 | 0 | | /gitbook/gitbook-plugin-atoc/atoc.js | 1183 | 323.057 | 961 | 222 | 0 | 0 | | /gitbook/gitbook-plugin-highlight/website.css | 1181 | 2065.165 | 961 | 220 | 0 | 0 | | /gitbook/gitbook-plugin-reveal/reveal.js | 1181 | 229.190 | 958 | 223 | 0 | 0 | | /gitbook/gitbook.js | 1181 | 23890.862 | 959 | 222 | 0 | 0 | | /gitbook/theme.js | 1179 | 25528.468 | 957 | 222 | 0 | 0 | 3. View top visitors It is also possible to find the top visitor IP address and HTTP User Agent: $ platform log access -p -e master --lines 102400 | ngxtop --no-follow top remote_addr http_user_agent top remote_addr | remote_addr | count | |----------------+---------| | 92.60.188.221 | 6675 | | 179.33.202.58 | 586 | | 94.143.189.241 | 547 | | 81.200.189.9 | 461 | | 35.193.89.58 | 444 | | 82.255.18.24 | 313 | | 78.246.179.170 | 305 | | 151.16.42.51 | 302 | | 89.188.6.127 | 292 | | 84.43.189.112 | 285 | top http_user_agent | http_user_agent | count | |---------------------------------------------------------------------------------------------------------------------------+---------| | Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.21 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.21 | 6623 | | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36 | 4195 | | Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36 | 3563 | | Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36 | 2242 | | Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:65.0) Gecko/20100101 Firefox/65.0 | 1799 | | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36 | 1358 | | Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:65.0) Gecko/20100101 Firefox/65.0 | 1189 | | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36 | 1136 | | Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:65.0) Gecko/20100101 Firefox/65.0 | 1012 | | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36 | 973 | 4. Filter by IP address We could further filter the visits coming from IP address 92.60.188.221: $ platform log access -p -e master --lines 102400 | ngxtop --no-follow -i 'remote_addr == \"92.60.188.221\"' top request status http_user_agent top request | request | count | |--------------------------------------------------+---------| | GET /development/ HTTP/1.1 | 6 | | POST //index.php/api/xmlrpc HTTP/1.1 | 5 | | POST //xmlrpc HTTP/1.1 | 5 | | POST //xmlrpc.php HTTP/1.1 | 5 | | POST /gitbook/gitbook-plugin-edit-link/ HTTP/1.1 | 5 | | POST /gitbook/gitbook-plugin-gtm/ HTTP/1.1 | 5 | | GET /development/logs.html HTTP/1.1 | 4 | | GET /gettingstarted/tools.html HTTP/1.1 | 4 | | GET /styles/styles.css HTTP/1.1 | 4 | | POST //soap.php HTTP/1.1 | 4 | top status | status | count | |----------+---------| | 502 | 6338 | | 200 | 223 | | 403 | 69 | | 304 | 25 | | 405 | 20 | top http_user_agent | http_user_agent | count | |-------------------------------------------------------------------------------------------------------------+---------| | Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.21 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.21 | 6623 | | Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0 | 51 | | Googlebot/2.1 (+http://www.googlebot.com/bot.html) | 1 | From the above output, 92.60.188.221 is generating a lot of bad traffic and should be . Conclusion With the help of ngxtop and Platform CLI, it is easy to perform simple analysis on the Nginx access log and identify bad traffic.",
        "text": "Goal To derive various metrics from the Nginx access log via https://github.com/lebinh/ngxtop/. Assumptions To complete this, you will need: The https://docs.platform.sh/gettingstarted/cli.html tool installed The https://pip.pypa.io/en/stable/installing/ tool installed Problems Sometimes, you may find the need to derive metrics from the access log files and identify bad traffic. Using raw Linux commands to perform the analysis may not be efficient enough. Steps 1. Check ngxtop installation Make sure ngxtop is installed locally via the below command: pip install --user ngxtop 2. Get web traffic overview To gather an overview of recent web traffic, use below command: platform log access -p -e master --lines 102400 | ngxtop --no-follow Summary: | count | avg_bytes_sent | 2xx | 3xx | 4xx | 5xx | |---------+------------------+-------+-------+-------+-------| | 41553 | 6342.284 | 27380 | 6474 | 100 | 7599 | Detailed: | request_path | count | avg_bytes_sent | 2xx | 3xx | 4xx | 5xx | |-----------------------------------------------+---------+------------------+-------+-------+-------+-------| | /scripts/user-widget/dist/user-widget.min.js | 1190 | 29123.529 | 962 | 228 | 0 | 0 | | /gitbook/gitbook-plugin-codetabs/codetabs.js | 1185 | 186.957 | 961 | 224 | 0 | 0 | | /gitbook/style.css | 1185 | 8088.093 | 964 | 221 | 0 | 0 | | /gitbook/gitbook-plugin-edit-link/plugin.js | 1184 | 340.759 | 961 | 223 | 0 | 0 | | /gitbook/gitbook-plugin-gtm/plugin.js | 1184 | 199.188 | 960 | 224 | 0 | 0 | | /gitbook/gitbook-plugin-atoc/atoc.js | 1183 | 323.057 | 961 | 222 | 0 | 0 | | /gitbook/gitbook-plugin-highlight/website.css | 1181 | 2065.165 | 961 | 220 | 0 | 0 | | /gitbook/gitbook-plugin-reveal/reveal.js | 1181 | 229.190 | 958 | 223 | 0 | 0 | | /gitbook/gitbook.js | 1181 | 23890.862 | 959 | 222 | 0 | 0 | | /gitbook/theme.js | 1179 | 25528.468 | 957 | 222 | 0 | 0 | 3. View top visitors It is also possible to find the top visitor IP address and HTTP User Agent: $ platform log access -p -e master --lines 102400 | ngxtop --no-follow top remote_addr http_user_agent top remote_addr | remote_addr | count | |----------------+---------| | 92.60.188.221 | 6675 | | 179.33.202.58 | 586 | | 94.143.189.241 | 547 | | 81.200.189.9 | 461 | | 35.193.89.58 | 444 | | 82.255.18.24 | 313 | | 78.246.179.170 | 305 | | 151.16.42.51 | 302 | | 89.188.6.127 | 292 | | 84.43.189.112 | 285 | top http_user_agent | http_user_agent | count | |---------------------------------------------------------------------------------------------------------------------------+---------| | Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.21 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.21 | 6623 | | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36 | 4195 | | Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36 | 3563 | | Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36 | 2242 | | Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:65.0) Gecko/20100101 Firefox/65.0 | 1799 | | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36 | 1358 | | Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:65.0) Gecko/20100101 Firefox/65.0 | 1189 | | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36 | 1136 | | Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:65.0) Gecko/20100101 Firefox/65.0 | 1012 | | Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36 | 973 | 4. Filter by IP address We could further filter the visits coming from IP address 92.60.188.221: $ platform log access -p -e master --lines 102400 | ngxtop --no-follow -i 'remote_addr == \"92.60.188.221\"' top request status http_user_agent top request | request | count | |--------------------------------------------------+---------| | GET /development/ HTTP/1.1 | 6 | | POST //index.php/api/xmlrpc HTTP/1.1 | 5 | | POST //xmlrpc HTTP/1.1 | 5 | | POST //xmlrpc.php HTTP/1.1 | 5 | | POST /gitbook/gitbook-plugin-edit-link/ HTTP/1.1 | 5 | | POST /gitbook/gitbook-plugin-gtm/ HTTP/1.1 | 5 | | GET /development/logs.html HTTP/1.1 | 4 | | GET /gettingstarted/tools.html HTTP/1.1 | 4 | | GET /styles/styles.css HTTP/1.1 | 4 | | POST //soap.php HTTP/1.1 | 4 | top status | status | count | |----------+---------| | 502 | 6338 | | 200 | 223 | | 403 | 69 | | 304 | 25 | | 405 | 20 | top http_user_agent | http_user_agent | count | |-------------------------------------------------------------------------------------------------------------+---------| | Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.21 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.21 | 6623 | | Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0 | 51 | | Googlebot/2.1 (+http://www.googlebot.com/bot.html) | 1 | From the above output, 92.60.188.221 is generating a lot of bad traffic and should be . Conclusion With the help of ngxtop and Platform CLI, it is easy to perform simple analysis on the Nginx access log and identify bad traffic.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-derive-metrics-from-access-log-via-ngxtop/122",
        "relurl": "/t/how-to-derive-metrics-from-access-log-via-ngxtop/122"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "594a3e03bdd259677ae1b35b7946e6d1ae3ebb4e",
        "title": "How to forward Platform.sh logs to Logz.io",
        "description": "Goal To forward http://Platform.sh logs to http://Logz.io. Assumptions An active http://platform.sh/ https://github.com/platformsh/platformsh-cli tool installed locally https://docs.platform.sh/development/ssh.html configured with the project account An active http://Logz.io account Problems Forwarding http://Platform.sh to a third-party indexer such as http://Logz.io is useful, especially when integrating log monitoring across other projects, including those that are not hosted on http://Platform.sh. http://Logz.io uses Filebeat to create configuration, auth and forwarding files when shipping logs, which must be taken into account on http://Platform.sh’s read-only file system. http://Logz.io recommends using Filebeat to forward logs to it, however there are a number of other options outlined in the “Log Forwarding” section on the http://Logz.io dashboard. The following How-to is written to ship logs from a Python application, but the steps do not require that. More information regarding accessing http://Platform.sh logs can be found in the https://docs.platform.sh/development/logs.html . Steps 1. Configure routes and services The project locally will ultimately have the following structure: ├── .platform │ ├── local │ │ ├── .gitignore │ │ ├── README.txt │ │ └── project.yaml │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml ├── Pipfile ├── Pipfile.lock ├── README.md ├── config │ └── filebeat │ └── scripts │ ├── config.sh │ └── install.sh ├── filebeat.yml └── server.py The project is a simple “Hello, world!” application using Flask. Routes must be configured in the .platform/routes.yaml: # .platform/routes.yaml https://{default}/: type: upstream upstream: \"app:http\" as well as its services in .platform/services.yaml, which is defined here with a single MySQL/MariaDB service: # .platform/services.yaml mysql: type: \"mysql:10.2\" disk: 1024 2. Set up the http://Platform.sh Application configuration Modify .platform.app.yaml to define writable mounts for https://www.elastic.co/products/beats/filebeat: # .platform.app.yaml mounts: '/.filebeat': source: local source_path: filebeat https://docs.platform.sh/configuration/app/storage.html are utilized in this configuration because after Filebeat is installed, it will attempt to write files to the project so that logs can be forwarded to http://Logz.io. Since http://Platform.sh is designed to provide a read-only file system, mounts can be used for this purpose. Modify the build hook: # .platform.app.yaml hooks: build: | if [ ! -z $LOGZ_CONFIG ]; then ./config/filebeat/scripts/install.sh fi pipenv install --system --deploy This section runs an install script (defined below) that installs Filebeat if it does not detect a project variable that we will define after the installation has completed. Modify the deploy hook: deploy: | if [ ! \"$(ls -A filebeat)\" ]; then ./config/filebeat/scripts/config.sh fi ./filebeat/filebeat run --once The deploy hook runs a configuration script on the installation if the filebeat directory is empty. The final command runs Filebeat until the logs are up to date with those previously shipped and then forwards them to http://Logz.io on every deployment. 3. Install Filebeat In config/filebeat/scripts/ modify a script install.sh: # config/filebeat/scripts/install.sh #!/usr/bin/env bash TEMP_SPLUNK_HOME=config/splunk/build # Install Splunk Universal Forwarder #!/usr/bin/env bash TEMP_BEAT_HOME=config/filebeat/build [ ! -d $TEMP_BEAT_HOME ] \u0026\u0026 mkdir -p $TEMP_BEAT_HOME cd $TEMP_BEAT_HOME # Install Filebeat curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.7.0-linux-x86_64.tar.gz tar xzvf filebeat-6.7.0-linux-x86_64.tar.gz rm filebeat-6.7.0-linux-x86_64.tar.gz # Download the certificate wget https://raw.githubusercontent.com/logzio/public-certificates/master/COMODORSADomainValidationSecureServerCA.crt mkdir -p filebeat-6.7.0-linux-x86_64/pki/tls/certs cp COMODORSADomainValidationSecureServerCA.crt filebeat-6.7.0-linux-x86_64/pki/tls/certs/ The script will define a temporary location within the config/filebeat subdirectory to install Filebeat. During the build phase mounts are not yet available, so this becomes a necessary step before the mounts are fully defined. The initial configuration requires that a certificate is downloaded along with the installation and placed in the appropriate directory. Details regarding this step are available on the Filebeat steps of the Log Forwarding section of the dashboard. Make sure to replace the installation link with the most recent version listed on the Filebeat downloads page. 4. Move installation to mount point Write a config.sh script within config/filebeat/scripts that will be used to control the initial configuration: # config/filebeat/scripts/config.sh #!/usr/bin/env bash # Move filebeat to mount with write access cd $PLATFORM_HOME cp -v -r config/filebeat/build/filebeat-6.7.0-linux-x86_64/* filebeat mkdir filebeat/registry This script is run in the deploy hook when the mounts defined in .platform.app.yaml are now available, so Filebeat can be moved from the temporary installation directory config/filebeat/build/ to app/filebeat. 5. Finish the configuration The main configuration file for Filebeat is filebeat.yml, which should be placed in the application directory outside of filebeat. On http://Logz.io within the configuration steps for Filebeat, the Wizard will construct this file based on the desired inputs that are given in its fields. An example for forwarding all file in /var/log that end with .log # filebeat.yml ############################# Filebeat ##################################### filebeat.inputs: - type: log paths: - /var/log/*.log fields: logzio_codec: plain token: type: Platform.sh fields_under_root: true encoding: utf-8 ignore_older: 3h registry_file: /filebeat/registry ############################# Output ########################################## output: logstash: hosts: [\"listener.logz.io:5015\"] ssl: certificate_authorities: [\"/app/filebeat/pki/tls/certs/COMODORSADomainValidationSecureServerCA.crt\"] filebeat.yml configures both the forwarder’s inputs and its output location using Logstash to http://Logz.io. 6. Push to http://Platform.sh Commit the changes and push to the http://Platform.sh remote: $ git push platform Filebeat will be installed during the build hook and configured to http://Logz.io during the deploy hook. Each time Filebeat is run at the end of each deployment, it will detect changes to the log files within /var/log and ship those differences to http://Logz.io. 7. Define a project variable To ensure that the installation in the build hook does not run on every build, to the project that will be visible during the build process that will keep this from happening. $ platform variable:create --level project --name LOGZ_CONFIG --value 'true' This step can be automated by https://community.platform.sh/t/how-to-use-the-platform-sh-cli-from-the-web-container/126 itself, although it will force a redeploy of the application when the variable is added. Conclusion http://Platform.sh is set up to provide a read-only file system for application containers so that consistent builds and deployments are ensured. Utilizing writable mounts however provides a simple way of setting up Filebeat to ship http://Platform.sh logs to http://Logz.io. Note: if at any time you would like to remove Splunk from your application configuration, make sure to manually remove the associated files from each mount. Simply removing the mount will not eradicate the files on it and they will exist on disk until . Remember to remove the project variable created as well if keeping the build hook described above. ",
        "text": "Goal To forward http://Platform.sh logs to http://Logz.io. Assumptions An active http://platform.sh/ https://github.com/platformsh/platformsh-cli tool installed locally https://docs.platform.sh/development/ssh.html configured with the project account An active http://Logz.io account Problems Forwarding http://Platform.sh to a third-party indexer such as http://Logz.io is useful, especially when integrating log monitoring across other projects, including those that are not hosted on http://Platform.sh. http://Logz.io uses Filebeat to create configuration, auth and forwarding files when shipping logs, which must be taken into account on http://Platform.sh’s read-only file system. http://Logz.io recommends using Filebeat to forward logs to it, however there are a number of other options outlined in the “Log Forwarding” section on the http://Logz.io dashboard. The following How-to is written to ship logs from a Python application, but the steps do not require that. More information regarding accessing http://Platform.sh logs can be found in the https://docs.platform.sh/development/logs.html . Steps 1. Configure routes and services The project locally will ultimately have the following structure: ├── .platform │ ├── local │ │ ├── .gitignore │ │ ├── README.txt │ │ └── project.yaml │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml ├── Pipfile ├── Pipfile.lock ├── README.md ├── config │ └── filebeat │ └── scripts │ ├── config.sh │ └── install.sh ├── filebeat.yml └── server.py The project is a simple “Hello, world!” application using Flask. Routes must be configured in the .platform/routes.yaml: # .platform/routes.yaml https://{default}/: type: upstream upstream: \"app:http\" as well as its services in .platform/services.yaml, which is defined here with a single MySQL/MariaDB service: # .platform/services.yaml mysql: type: \"mysql:10.2\" disk: 1024 2. Set up the http://Platform.sh Application configuration Modify .platform.app.yaml to define writable mounts for https://www.elastic.co/products/beats/filebeat: # .platform.app.yaml mounts: '/.filebeat': source: local source_path: filebeat https://docs.platform.sh/configuration/app/storage.html are utilized in this configuration because after Filebeat is installed, it will attempt to write files to the project so that logs can be forwarded to http://Logz.io. Since http://Platform.sh is designed to provide a read-only file system, mounts can be used for this purpose. Modify the build hook: # .platform.app.yaml hooks: build: | if [ ! -z $LOGZ_CONFIG ]; then ./config/filebeat/scripts/install.sh fi pipenv install --system --deploy This section runs an install script (defined below) that installs Filebeat if it does not detect a project variable that we will define after the installation has completed. Modify the deploy hook: deploy: | if [ ! \"$(ls -A filebeat)\" ]; then ./config/filebeat/scripts/config.sh fi ./filebeat/filebeat run --once The deploy hook runs a configuration script on the installation if the filebeat directory is empty. The final command runs Filebeat until the logs are up to date with those previously shipped and then forwards them to http://Logz.io on every deployment. 3. Install Filebeat In config/filebeat/scripts/ modify a script install.sh: # config/filebeat/scripts/install.sh #!/usr/bin/env bash TEMP_SPLUNK_HOME=config/splunk/build # Install Splunk Universal Forwarder #!/usr/bin/env bash TEMP_BEAT_HOME=config/filebeat/build [ ! -d $TEMP_BEAT_HOME ] \u0026\u0026 mkdir -p $TEMP_BEAT_HOME cd $TEMP_BEAT_HOME # Install Filebeat curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.7.0-linux-x86_64.tar.gz tar xzvf filebeat-6.7.0-linux-x86_64.tar.gz rm filebeat-6.7.0-linux-x86_64.tar.gz # Download the certificate wget https://raw.githubusercontent.com/logzio/public-certificates/master/COMODORSADomainValidationSecureServerCA.crt mkdir -p filebeat-6.7.0-linux-x86_64/pki/tls/certs cp COMODORSADomainValidationSecureServerCA.crt filebeat-6.7.0-linux-x86_64/pki/tls/certs/ The script will define a temporary location within the config/filebeat subdirectory to install Filebeat. During the build phase mounts are not yet available, so this becomes a necessary step before the mounts are fully defined. The initial configuration requires that a certificate is downloaded along with the installation and placed in the appropriate directory. Details regarding this step are available on the Filebeat steps of the Log Forwarding section of the dashboard. Make sure to replace the installation link with the most recent version listed on the Filebeat downloads page. 4. Move installation to mount point Write a config.sh script within config/filebeat/scripts that will be used to control the initial configuration: # config/filebeat/scripts/config.sh #!/usr/bin/env bash # Move filebeat to mount with write access cd $PLATFORM_HOME cp -v -r config/filebeat/build/filebeat-6.7.0-linux-x86_64/* filebeat mkdir filebeat/registry This script is run in the deploy hook when the mounts defined in .platform.app.yaml are now available, so Filebeat can be moved from the temporary installation directory config/filebeat/build/ to app/filebeat. 5. Finish the configuration The main configuration file for Filebeat is filebeat.yml, which should be placed in the application directory outside of filebeat. On http://Logz.io within the configuration steps for Filebeat, the Wizard will construct this file based on the desired inputs that are given in its fields. An example for forwarding all file in /var/log that end with .log # filebeat.yml ############################# Filebeat ##################################### filebeat.inputs: - type: log paths: - /var/log/*.log fields: logzio_codec: plain token: type: Platform.sh fields_under_root: true encoding: utf-8 ignore_older: 3h registry_file: /filebeat/registry ############################# Output ########################################## output: logstash: hosts: [\"listener.logz.io:5015\"] ssl: certificate_authorities: [\"/app/filebeat/pki/tls/certs/COMODORSADomainValidationSecureServerCA.crt\"] filebeat.yml configures both the forwarder’s inputs and its output location using Logstash to http://Logz.io. 6. Push to http://Platform.sh Commit the changes and push to the http://Platform.sh remote: $ git push platform Filebeat will be installed during the build hook and configured to http://Logz.io during the deploy hook. Each time Filebeat is run at the end of each deployment, it will detect changes to the log files within /var/log and ship those differences to http://Logz.io. 7. Define a project variable To ensure that the installation in the build hook does not run on every build, to the project that will be visible during the build process that will keep this from happening. $ platform variable:create --level project --name LOGZ_CONFIG --value 'true' This step can be automated by https://community.platform.sh/t/how-to-use-the-platform-sh-cli-from-the-web-container/126 itself, although it will force a redeploy of the application when the variable is added. Conclusion http://Platform.sh is set up to provide a read-only file system for application containers so that consistent builds and deployments are ensured. Utilizing writable mounts however provides a simple way of setting up Filebeat to ship http://Platform.sh logs to http://Logz.io. Note: if at any time you would like to remove Splunk from your application configuration, make sure to manually remove the associated files from each mount. Simply removing the mount will not eradicate the files on it and they will exist on disk until . Remember to remove the project variable created as well if keeping the build hook described above. ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-forward-platform-sh-logs-to-logz-io/197",
        "relurl": "/t/how-to-forward-platform-sh-logs-to-logz-io/197"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "778e3414dd1ec2d4702da4545a1df0a7a1d5ea96",
        "title": "How to forward Platform.sh logs to a Splunk Indexer",
        "description": "Goal To forward http://Platform.sh logs into a Splunk indexer. Assumptions An active http://platform.sh/ project https://github.com/platformsh/platformsh-cli tool installed locally https://docs.platform.sh/development/ssh.html configured with the project account A Splunk instance installed on the indexing server that is configured to receive logs on a specified port (default 9997) and a configured admin login. Problems Forwarding http://Platform.sh to a third-party indexer such as Splunk is useful, especially when integrating log monitoring across other projects, including those that are not hosted on http://Platform.sh. Splunk creates configuration, auth and forwarding files when shipping logs, which must be taken into account on http://Platform.sh’s read-only file system. Splunk log forwarding is, in its simplest configuration, dependent on configuring two components: a Receiver/Indexer and a Forwarder. The Forwarder will be installed on a writable mount on a http://Platform.sh project and is responsible for collecting and shipping the desired log files, whereas the Indexer receives those files and provides a dashboard for indexing, searching, and analyzing them. Splunk actions can be executed either through that dashboard or through its CLI. The following How-to is written to ship logs from a Python application, but the steps do not require that. More information regarding accessing http://Platform.sh logs can be found in the https://docs.platform.sh/development/logs.html . Steps (Splunk Indexer) This How-to is based on installing Splunk Enterprise on a local machine, but an Indexer can be created using another Splunk product that is set-up remotely. Consult the https://docs.splunk.com/Documentation to determine the correct Indexer installation. A Splunk receiver has to be enabled to receive forwarded logs on a specified port. By default, the dedicated port for Splunk indexer’s is 9997, and it can be enabled to listen on that port either through the dashboard or using the CLI. Consult the Splunk documentation for full details on how to do that: https://docs.splunk.com/Documentation/Forwarder/7.2.5/Forwarder/Enableareceiver . Steps (Splunk Universal Forwarder) 1. Configure routes and services The project locally will ultimately have the following structure: . ├── .platform │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml ├── Pipfile ├── Pipfile.lock ├── README.md ├── config │ └── splunk │ ├── scripts │ │ ├── config.sh │ │ ├── install.sh │ │ └── uninstall.sh │ └── seeds │ ├── inputs.conf │ ├── outputs.conf │ ├── splunk-launch.conf │ └── user.conf └── server.py The project is a simple “Hello, world!” application using Flask. Routes must be configured in the .platform/routes.yaml: # .platform/routes.yaml https://{default}/: type: upstream upstream: \"app:http\" as well as its services in .platform/services.yaml, which is defined here with a single MySQL/MariaDB service: # .platform/services.yaml mysql: type: \"mysql:10.2\" disk: 1024 2. Set up the http://Platform.sh Application configuration Modify .platform.app.yaml to define writable mounts for the https://docs.splunk.com/Documentation/Forwarder/7.2.5/Forwarder/Abouttheuniversalforwarder : # .platform.app.yaml mounts: 'splunk': source: local source_path: splunk '/.splunk': source: local source_path: splauths https://docs.platform.sh/configuration/app/storage.html are utilized in this configuration because after the Splunk Universal Forwarder is installed, Splunk will still attempt to write files to the project so that logs can be forwarded to the indexer. Since http://Platform.sh is designed to provide a read-only file system, mounts can be used for this purpose. Splunk writes modified log files and authorization files when a connection is made to an indexer, and those files will be written in the directories splunk and .splunk respectively. Modify the build hook: # .platform.app.yaml hooks: build: | if [ ! -z $SPLUNK_CONFIG ]; then ./config/splunk/scripts/install.sh fi pipenv install --system --deploy This section runs an install script (defined below) that installs the Forwarder if it does not detect a project variable that we will define after the installation has completed. Modify the deploy hook: deploy: | if [ ! \"$(ls -A splunk)\" ]; then ./config/splunk/scripts/config.sh fi ./splunk/splunkforwarder/bin/splunk restart The deploy hook checks to see if the splunk directory is empty, and runs a configuration script on the Forwarder if it is. The final command actually restarts the Forwarder on every deploy and is used to ship the logs to the Indexer. 3. Installing the Splunk Universal Forwarder In config/splunk/scripts/ modify a script install.sh: # config/splunk/scripts/install.sh #!/usr/bin/env bash TEMP_SPLUNK_HOME=config/splunk/build # Install Splunk Universal Forwarder [ ! -d $TEMP_SPLUNK_HOME ] \u0026\u0026 mkdir -p $TEMP_SPLUNK_HOME cd $TEMP_SPLUNK_HOME wget -O splunkforwarder-7.2.5.1-962d9a8e1586-Linux-x86_64.tgz 'https://www.splunk.com/bin/splunk/DownloadActivityServlet?architecture=x86_64\u0026platform=linux\u0026version=7.2.5.1\u0026product=universalforwarder\u0026filename=splunkforwarder-7.2.5.1-962d9a8e1586-Linux-x86_64.tgz\u0026wget=true' tar xvzf splunkforwarder-7.2.5.1-962d9a8e1586-Linux-x86_64.tgz rm splunkforwarder-7.2.5.1-962d9a8e1586-Linux-x86_64.tgz The script will define a temporary location within the config/splunk subdirectory to install the Splunk forwarder. During the build phase mounts are not yet available, so this becomes a necessary step before the mounts are fully defined. Make sure to replace the installation link with the most recent version listed on https://www.splunk.com/en_us/download/universal-forwarder.html . 4. Configuring the Splunk Universal Forwarder Write a config.sh script within config/splunk/scripts that will be used to control the initial configuration: # config/splunk/scripts/config.sh #!/usr/bin/env bash cd $PLATFORM_APP_DIR TEMP_SPLUNK_HOME=config/splunk/build/* SPLUNK_HOME=$PLATFORM_APP_DIR/splunk/splunkforwarder # Copy temp build to writable storage cp -v -r $TEMP_SPLUNK_HOME splunk # Migrate used-seed.conf to the forwarder cp -v config/splunk/seeds/user.conf $SPLUNK_HOME/etc/system/local/user-seed.conf # Start Splunk for the first time, accepting license ./splunk/splunkforwarder/bin/splunk start --accept-license # Update outputs.conf with receiver address seed cp -v $PLATFORM_APP_DIR/config/splunk/seeds/outputs.conf $PLATFORM_APP_DIR/splunk/splunkforwarder/etc/system/local/outputs.conf # Update inputs.conf with monitor inputs seed cp -v $PLATFORM_APP_DIR/config/splunk/seeds/inputs.conf $PLATFORM_APP_DIR/splunk/splunkforwarder/etc/system/local/inputs.conf This script is run in the deploy hook when the mounts defined in .platform.app.yaml are now available, so the forwarder can be moved from the temporary installation directory config/splunk/build/ to app/splunk. After that, the installation is set-up with a collection of configuration seed files located in config/splunk/seeds that are also moved to the mounted installation. The first user.conf is a user seed that sets up an admin user for the forwarder. There is already one set up upon install, but unless another admin password is set up, the CLI will not be accessible remotely. # config/splunk/seeds/user.conf [user_info] USERNAME = admin PASSWORD = testpass Once the user seed is moved to the final mounted installation location, the admin credentials can be passed to the Splunk CLI to accept its license and start it for the first time. Next, an outputs.conf seeds the outputs configuration for the forwarder, which identifies the IP address and listening port for the receiver. # config/splunk/seeds/outputs.conf [tcpout] defaultGroup=default [tcpout:default] server= :9997 [tcpout-server:// :9997] Then an inputs.conf seeds the inputs for the forwarder. In this case, the entire var/logs http://Platform.sh log directory is added to the forwarder’s list of inputs. # config/splunk/seeds/inputs.conf [monitor://var/log/] disabled = false Like the admin user seed, outputs.conf and inputs.conf are moved to the final installation directory splunk. 5. Push to http://Platform.sh Commit the changes and push to the http://Platform.sh remote: $ git push platform The Forwarder will be installed during the build hook and configured to the indexer during the deploy hook. At the end of the deploy hook the Splunk forwarder will be restarted during each deployment. Each time it is restarted, it will detect changes to the log files within /var/log and ship those differences to the indexer. 6. Final Configuration The installation is finished, and now that the application has successfully deployed and the indexer is listening on the configured port, logs will be sent there on every deployment. To ensure that the forwarder installation in the build hook does not run on every build, to the project that will be visible during the build process that will keep this from happening. $ platform variable:create --level project --name SPLUNK_CONFIG --value 'true' This step can be automated by https://community.platform.sh/t/how-to-use-the-platform-sh-cli-from-the-web-container/126 itself, although it will force a redeploy of the application when the variable is added. SSH into the environment to modify the admin credentials or add new users using the https://docs.splunk.com/Documentation/Splunk/latest/Admin/CLIadmincommands . Conclusion http://Platform.sh is set up to provide a read-only file system for application containers so that consistent builds and deployments are ensured. Utilizing writable mounts however provides a simple way of setting up a Splunk Universal Forwarder to ship http://Platform.sh logs with an Indexer. Note: if at any time you would like to remove Splunk from your application configuration, make sure to manually remove the associated files from each mount. Simply removing the mount will not eradicate the files on it and they will exist on disk until . Remember to remove the project variable created as well if keeping the build hook described above. ",
        "text": "Goal To forward http://Platform.sh logs into a Splunk indexer. Assumptions An active http://platform.sh/ project https://github.com/platformsh/platformsh-cli tool installed locally https://docs.platform.sh/development/ssh.html configured with the project account A Splunk instance installed on the indexing server that is configured to receive logs on a specified port (default 9997) and a configured admin login. Problems Forwarding http://Platform.sh to a third-party indexer such as Splunk is useful, especially when integrating log monitoring across other projects, including those that are not hosted on http://Platform.sh. Splunk creates configuration, auth and forwarding files when shipping logs, which must be taken into account on http://Platform.sh’s read-only file system. Splunk log forwarding is, in its simplest configuration, dependent on configuring two components: a Receiver/Indexer and a Forwarder. The Forwarder will be installed on a writable mount on a http://Platform.sh project and is responsible for collecting and shipping the desired log files, whereas the Indexer receives those files and provides a dashboard for indexing, searching, and analyzing them. Splunk actions can be executed either through that dashboard or through its CLI. The following How-to is written to ship logs from a Python application, but the steps do not require that. More information regarding accessing http://Platform.sh logs can be found in the https://docs.platform.sh/development/logs.html . Steps (Splunk Indexer) This How-to is based on installing Splunk Enterprise on a local machine, but an Indexer can be created using another Splunk product that is set-up remotely. Consult the https://docs.splunk.com/Documentation to determine the correct Indexer installation. A Splunk receiver has to be enabled to receive forwarded logs on a specified port. By default, the dedicated port for Splunk indexer’s is 9997, and it can be enabled to listen on that port either through the dashboard or using the CLI. Consult the Splunk documentation for full details on how to do that: https://docs.splunk.com/Documentation/Forwarder/7.2.5/Forwarder/Enableareceiver . Steps (Splunk Universal Forwarder) 1. Configure routes and services The project locally will ultimately have the following structure: . ├── .platform │ ├── routes.yaml │ └── services.yaml ├── .platform.app.yaml ├── Pipfile ├── Pipfile.lock ├── README.md ├── config │ └── splunk │ ├── scripts │ │ ├── config.sh │ │ ├── install.sh │ │ └── uninstall.sh │ └── seeds │ ├── inputs.conf │ ├── outputs.conf │ ├── splunk-launch.conf │ └── user.conf └── server.py The project is a simple “Hello, world!” application using Flask. Routes must be configured in the .platform/routes.yaml: # .platform/routes.yaml https://{default}/: type: upstream upstream: \"app:http\" as well as its services in .platform/services.yaml, which is defined here with a single MySQL/MariaDB service: # .platform/services.yaml mysql: type: \"mysql:10.2\" disk: 1024 2. Set up the http://Platform.sh Application configuration Modify .platform.app.yaml to define writable mounts for the https://docs.splunk.com/Documentation/Forwarder/7.2.5/Forwarder/Abouttheuniversalforwarder : # .platform.app.yaml mounts: 'splunk': source: local source_path: splunk '/.splunk': source: local source_path: splauths https://docs.platform.sh/configuration/app/storage.html are utilized in this configuration because after the Splunk Universal Forwarder is installed, Splunk will still attempt to write files to the project so that logs can be forwarded to the indexer. Since http://Platform.sh is designed to provide a read-only file system, mounts can be used for this purpose. Splunk writes modified log files and authorization files when a connection is made to an indexer, and those files will be written in the directories splunk and .splunk respectively. Modify the build hook: # .platform.app.yaml hooks: build: | if [ ! -z $SPLUNK_CONFIG ]; then ./config/splunk/scripts/install.sh fi pipenv install --system --deploy This section runs an install script (defined below) that installs the Forwarder if it does not detect a project variable that we will define after the installation has completed. Modify the deploy hook: deploy: | if [ ! \"$(ls -A splunk)\" ]; then ./config/splunk/scripts/config.sh fi ./splunk/splunkforwarder/bin/splunk restart The deploy hook checks to see if the splunk directory is empty, and runs a configuration script on the Forwarder if it is. The final command actually restarts the Forwarder on every deploy and is used to ship the logs to the Indexer. 3. Installing the Splunk Universal Forwarder In config/splunk/scripts/ modify a script install.sh: # config/splunk/scripts/install.sh #!/usr/bin/env bash TEMP_SPLUNK_HOME=config/splunk/build # Install Splunk Universal Forwarder [ ! -d $TEMP_SPLUNK_HOME ] \u0026\u0026 mkdir -p $TEMP_SPLUNK_HOME cd $TEMP_SPLUNK_HOME wget -O splunkforwarder-7.2.5.1-962d9a8e1586-Linux-x86_64.tgz 'https://www.splunk.com/bin/splunk/DownloadActivityServlet?architecture=x86_64\u0026platform=linux\u0026version=7.2.5.1\u0026product=universalforwarder\u0026filename=splunkforwarder-7.2.5.1-962d9a8e1586-Linux-x86_64.tgz\u0026wget=true' tar xvzf splunkforwarder-7.2.5.1-962d9a8e1586-Linux-x86_64.tgz rm splunkforwarder-7.2.5.1-962d9a8e1586-Linux-x86_64.tgz The script will define a temporary location within the config/splunk subdirectory to install the Splunk forwarder. During the build phase mounts are not yet available, so this becomes a necessary step before the mounts are fully defined. Make sure to replace the installation link with the most recent version listed on https://www.splunk.com/en_us/download/universal-forwarder.html . 4. Configuring the Splunk Universal Forwarder Write a config.sh script within config/splunk/scripts that will be used to control the initial configuration: # config/splunk/scripts/config.sh #!/usr/bin/env bash cd $PLATFORM_APP_DIR TEMP_SPLUNK_HOME=config/splunk/build/* SPLUNK_HOME=$PLATFORM_APP_DIR/splunk/splunkforwarder # Copy temp build to writable storage cp -v -r $TEMP_SPLUNK_HOME splunk # Migrate used-seed.conf to the forwarder cp -v config/splunk/seeds/user.conf $SPLUNK_HOME/etc/system/local/user-seed.conf # Start Splunk for the first time, accepting license ./splunk/splunkforwarder/bin/splunk start --accept-license # Update outputs.conf with receiver address seed cp -v $PLATFORM_APP_DIR/config/splunk/seeds/outputs.conf $PLATFORM_APP_DIR/splunk/splunkforwarder/etc/system/local/outputs.conf # Update inputs.conf with monitor inputs seed cp -v $PLATFORM_APP_DIR/config/splunk/seeds/inputs.conf $PLATFORM_APP_DIR/splunk/splunkforwarder/etc/system/local/inputs.conf This script is run in the deploy hook when the mounts defined in .platform.app.yaml are now available, so the forwarder can be moved from the temporary installation directory config/splunk/build/ to app/splunk. After that, the installation is set-up with a collection of configuration seed files located in config/splunk/seeds that are also moved to the mounted installation. The first user.conf is a user seed that sets up an admin user for the forwarder. There is already one set up upon install, but unless another admin password is set up, the CLI will not be accessible remotely. # config/splunk/seeds/user.conf [user_info] USERNAME = admin PASSWORD = testpass Once the user seed is moved to the final mounted installation location, the admin credentials can be passed to the Splunk CLI to accept its license and start it for the first time. Next, an outputs.conf seeds the outputs configuration for the forwarder, which identifies the IP address and listening port for the receiver. # config/splunk/seeds/outputs.conf [tcpout] defaultGroup=default [tcpout:default] server= :9997 [tcpout-server:// :9997] Then an inputs.conf seeds the inputs for the forwarder. In this case, the entire var/logs http://Platform.sh log directory is added to the forwarder’s list of inputs. # config/splunk/seeds/inputs.conf [monitor://var/log/] disabled = false Like the admin user seed, outputs.conf and inputs.conf are moved to the final installation directory splunk. 5. Push to http://Platform.sh Commit the changes and push to the http://Platform.sh remote: $ git push platform The Forwarder will be installed during the build hook and configured to the indexer during the deploy hook. At the end of the deploy hook the Splunk forwarder will be restarted during each deployment. Each time it is restarted, it will detect changes to the log files within /var/log and ship those differences to the indexer. 6. Final Configuration The installation is finished, and now that the application has successfully deployed and the indexer is listening on the configured port, logs will be sent there on every deployment. To ensure that the forwarder installation in the build hook does not run on every build, to the project that will be visible during the build process that will keep this from happening. $ platform variable:create --level project --name SPLUNK_CONFIG --value 'true' This step can be automated by https://community.platform.sh/t/how-to-use-the-platform-sh-cli-from-the-web-container/126 itself, although it will force a redeploy of the application when the variable is added. SSH into the environment to modify the admin credentials or add new users using the https://docs.splunk.com/Documentation/Splunk/latest/Admin/CLIadmincommands . Conclusion http://Platform.sh is set up to provide a read-only file system for application containers so that consistent builds and deployments are ensured. Utilizing writable mounts however provides a simple way of setting up a Splunk Universal Forwarder to ship http://Platform.sh logs with an Indexer. Note: if at any time you would like to remove Splunk from your application configuration, make sure to manually remove the associated files from each mount. Simply removing the mount will not eradicate the files on it and they will exist on disk until . Remember to remove the project variable created as well if keeping the build hook described above. ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-forward-platform-sh-logs-to-a-splunk-indexer/196",
        "relurl": "/t/how-to-forward-platform-sh-logs-to-a-splunk-indexer/196"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "42abc4ced5b254bc3aa42cdffc3e6596076a5fff",
        "title": "How to run R Shiny on Platform.sh",
        "description": "Goal To run R Shiny on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account An empty http://Platform.sh project Problems http://Platform.sh offers many different language runtimes by default, but some applications require different environments, such as data science applications which use R Shiny. By installing the Miniconda Python package manager, it is possible to run R applications, such as R Shiny, on http://Platform.sh. Steps 1. Set git remote to http://Platform.sh project $ git init $ platform project:set-remote 2. Create an installation script file The http://Platform.sh build hook runs Dash, not Bash, so the installation script is executed separately in install_r.sh: #!/bin/bash # Download the latest Miniconda3 release and name the file `conda.sh` curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o conda.sh # Run the downloaded `conda.sh` script using bash # — Use the `-b` flag to run the installer in batch mode # — Use the `-p` flag to specify where the package manager will actually be installed bash conda.sh -b -p $PLATFORM_APP_DIR/conda # Source the following `conda.sh` file to put the `conda` command in our path for the duration of this script . /app/conda/etc/profile.d/conda.sh # Add the above command to `.bash_profile` so that it is available during SSH sessions echo \". /app/conda/etc/profile.d/conda.sh\" ~/.bash_profile echo \"conda activate\" ~/.bash_profile # Enter the base conda environment and make sure it is up to date conda activate base conda update -n base -c defaults conda # Install R # `r-essentials` is a bundle of 80 common R packages, including `r-shiny`. # Remove `r-essentials` from the line below to only have a barebones install. # The `-n` flag gives the environment its name. conda create -n r-env r-essentials r-base # Activate the freshly created environment conda activate r-env # Install additional packages if desired # conda install r-rbokeh IMPORTANT: When using conda to install R packages, you will need to add r- before the regular CRAN or MRAN name. For instance, if you want to install rbokeh, you will need to use conda install r-rbokeh. 3. Add a start script for running scripts with R start_command.sh: #!/bin/bash # Source the `conda.sh` file to make this particular shell session able to run the `conda` command . /app/conda/etc/profile.d/conda.sh # Activate the conda environment that was created in the build hook conda activate r-env # Run the Shiny web app Rscript start_shinyapp.R 4. Add a Shiny app in the shinyapp directory In the file shinyapp/app.R: library(shiny) # Define UI for application that draws a histogram ui ",
        "text": "Goal To run R Shiny on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account An empty http://Platform.sh project Problems http://Platform.sh offers many different language runtimes by default, but some applications require different environments, such as data science applications which use R Shiny. By installing the Miniconda Python package manager, it is possible to run R applications, such as R Shiny, on http://Platform.sh. Steps 1. Set git remote to http://Platform.sh project $ git init $ platform project:set-remote 2. Create an installation script file The http://Platform.sh build hook runs Dash, not Bash, so the installation script is executed separately in install_r.sh: #!/bin/bash # Download the latest Miniconda3 release and name the file `conda.sh` curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o conda.sh # Run the downloaded `conda.sh` script using bash # — Use the `-b` flag to run the installer in batch mode # — Use the `-p` flag to specify where the package manager will actually be installed bash conda.sh -b -p $PLATFORM_APP_DIR/conda # Source the following `conda.sh` file to put the `conda` command in our path for the duration of this script . /app/conda/etc/profile.d/conda.sh # Add the above command to `.bash_profile` so that it is available during SSH sessions echo \". /app/conda/etc/profile.d/conda.sh\" ~/.bash_profile echo \"conda activate\" ~/.bash_profile # Enter the base conda environment and make sure it is up to date conda activate base conda update -n base -c defaults conda # Install R # `r-essentials` is a bundle of 80 common R packages, including `r-shiny`. # Remove `r-essentials` from the line below to only have a barebones install. # The `-n` flag gives the environment its name. conda create -n r-env r-essentials r-base # Activate the freshly created environment conda activate r-env # Install additional packages if desired # conda install r-rbokeh IMPORTANT: When using conda to install R packages, you will need to add r- before the regular CRAN or MRAN name. For instance, if you want to install rbokeh, you will need to use conda install r-rbokeh. 3. Add a start script for running scripts with R start_command.sh: #!/bin/bash # Source the `conda.sh` file to make this particular shell session able to run the `conda` command . /app/conda/etc/profile.d/conda.sh # Activate the conda environment that was created in the build hook conda activate r-env # Run the Shiny web app Rscript start_shinyapp.R 4. Add a Shiny app in the shinyapp directory In the file shinyapp/app.R: library(shiny) # Define UI for application that draws a histogram ui ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-run-r-shiny-on-platform-sh/231",
        "relurl": "/t/how-to-run-r-shiny-on-platform-sh/231"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "8a79fae9027c0fce5c6d3fa505ba95e7ea79cca4",
        "title": "How to run an Anaconda/Miniconda Python stack on Platform.sh",
        "description": "Goal To run Anaconda (the full data science Python stack) or Miniconda (just the Python package manager) on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account An empty http://Platform.sh project Problems http://Platform.sh offers several different versions of Python by default, but some applications require different environments, such as data science applications running on the Anaconda stack or packages installed with Miniconda. Running the Anaconda/Miniconda installer in the build hook of a http://Platform.sh application allows for the activation of conda virtual environments for script execution. Steps 1. Set git remote to http://Platform.sh project $ git init $ platform project:set-remote 2. Create an installation script file The http://Platform.sh build hook runs Dash, not Bash, so the installation script is executed separately in install_conda.sh: #!/bin/bash # Download an Anaconda3 release and name the file `conda.sh` curl https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh -o conda.sh # If you wish to install Miniconda3 instead, comment out the line above and uncomment the line below: # curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o conda.sh # Run the downloaded `conda.sh` script using bash # — Use the `-b` flag to run the installer in batch mode # — Use the `-p` flag to specify where the package manager will actually be installed bash conda.sh -b -p $PLATFORM_APP_DIR/conda # Source the following `conda.sh` file to put the `conda` command in our path for the duration of this script . /app/conda/etc/profile.d/conda.sh # Add the above command to `.bash_profile` so that it is available during SSH sessions echo \". /app/conda/etc/profile.d/conda.sh\" ~/.bash_profile echo \"conda activate\" ~/.bash_profile # Enter the base conda environment and prepare to configure it conda activate base # Update conda itself conda update -n base -c defaults conda # OPTIONAL: Print out debugging information in the build hook conda info 3. Add a start script for running scripts with Anaconda start_command.sh: #!/bin/bash # Source the `conda.sh` file to make this particular shell session able to run the `conda` command . /app/conda/etc/profile.d/conda.sh # Activate the conda environment that was created in the build hook conda activate base # Put any commands needed to run a web app here Make sure to put any commands needed to run a web app at the end of this file. 4. Add .platform.app.yaml Create the .platform.app.yaml https://docs.platform.sh/configuration/app-containers.html : # The name of this app. Must be unique within a project. name: app # The runtime the application uses. type: \"python:3.7\" # The hooks executed at various points in the lifecycle of the application. hooks: build: | set -e # The build hook uses `dash` rather than `bash`. # Thus, the installation occurs in separate script that runs with `bash`. bash install_conda.sh deploy: | set -e # Make the start script executable chmod +x start_command.sh # The size of the persistent disk of the application (in MB). disk: 1024 # The configuration of app when it is exposed to the web. web: commands: start: bash ./start_command.sh upstream: socket_family: tcp protocol: http locations: \"/\": passthru: true 5. Define routes ./.platform/routes.yaml https://{default}/: type: upstream upstream: app:http 6. Add empty services ./.platform/services.yaml # empty 7. Add, commit, and push these files to your http://Platform.sh project git add . git commit -m \"Adding configuration to install conda environment\" git push platform master Conclusion By executing the Anaconda or Miniconda installation Bash scripts in the build hook, a project is able to install an Conda-based execution environment on http://Platform.sh.",
        "text": "Goal To run Anaconda (the full data science Python stack) or Miniconda (just the Python package manager) on http://Platform.sh. Assumptions You will need: An SSH key configured on your http://Platform.sh account An empty http://Platform.sh project Problems http://Platform.sh offers several different versions of Python by default, but some applications require different environments, such as data science applications running on the Anaconda stack or packages installed with Miniconda. Running the Anaconda/Miniconda installer in the build hook of a http://Platform.sh application allows for the activation of conda virtual environments for script execution. Steps 1. Set git remote to http://Platform.sh project $ git init $ platform project:set-remote 2. Create an installation script file The http://Platform.sh build hook runs Dash, not Bash, so the installation script is executed separately in install_conda.sh: #!/bin/bash # Download an Anaconda3 release and name the file `conda.sh` curl https://repo.anaconda.com/archive/Anaconda3-2019.03-Linux-x86_64.sh -o conda.sh # If you wish to install Miniconda3 instead, comment out the line above and uncomment the line below: # curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -o conda.sh # Run the downloaded `conda.sh` script using bash # — Use the `-b` flag to run the installer in batch mode # — Use the `-p` flag to specify where the package manager will actually be installed bash conda.sh -b -p $PLATFORM_APP_DIR/conda # Source the following `conda.sh` file to put the `conda` command in our path for the duration of this script . /app/conda/etc/profile.d/conda.sh # Add the above command to `.bash_profile` so that it is available during SSH sessions echo \". /app/conda/etc/profile.d/conda.sh\" ~/.bash_profile echo \"conda activate\" ~/.bash_profile # Enter the base conda environment and prepare to configure it conda activate base # Update conda itself conda update -n base -c defaults conda # OPTIONAL: Print out debugging information in the build hook conda info 3. Add a start script for running scripts with Anaconda start_command.sh: #!/bin/bash # Source the `conda.sh` file to make this particular shell session able to run the `conda` command . /app/conda/etc/profile.d/conda.sh # Activate the conda environment that was created in the build hook conda activate base # Put any commands needed to run a web app here Make sure to put any commands needed to run a web app at the end of this file. 4. Add .platform.app.yaml Create the .platform.app.yaml https://docs.platform.sh/configuration/app-containers.html : # The name of this app. Must be unique within a project. name: app # The runtime the application uses. type: \"python:3.7\" # The hooks executed at various points in the lifecycle of the application. hooks: build: | set -e # The build hook uses `dash` rather than `bash`. # Thus, the installation occurs in separate script that runs with `bash`. bash install_conda.sh deploy: | set -e # Make the start script executable chmod +x start_command.sh # The size of the persistent disk of the application (in MB). disk: 1024 # The configuration of app when it is exposed to the web. web: commands: start: bash ./start_command.sh upstream: socket_family: tcp protocol: http locations: \"/\": passthru: true 5. Define routes ./.platform/routes.yaml https://{default}/: type: upstream upstream: app:http 6. Add empty services ./.platform/services.yaml # empty 7. Add, commit, and push these files to your http://Platform.sh project git add . git commit -m \"Adding configuration to install conda environment\" git push platform master Conclusion By executing the Anaconda or Miniconda installation Bash scripts in the build hook, a project is able to install an Conda-based execution environment on http://Platform.sh.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-run-an-anaconda-miniconda-python-stack-on-platform-sh/230",
        "relurl": "/t/how-to-run-an-anaconda-miniconda-python-stack-on-platform-sh/230"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "feffb81cfee90608577cd3f2a7fa88d6e8399e67",
        "title": "How to add a webhook to one or more projects",
        "description": "Goal Add a webhook to a single project, or to all of your http://Platform.sh projects. Assumptions One or more active projects on http://Platform.sh The https://docs.platform.sh/gettingstarted/cli.html installed locally Problems You can use webhooks to integrate http://Platform.sh into your automation chain to trigger actions somewhere else. They can be configured to fire on all events, or specific events that happen on specific environments. More information about https://docs.platform.sh/administration/integrations/webhooks.html is available in the public documentation. Steps (Adding a webhook to a single project) 1. Get the project ID If the local repository already has the remote project set, navigate to that directory. Otherwise, retrieve the project ID using platform project:list 2. Set up the webhook Add the webhook for the desired URL that can receive posted JSON: $ platform integration:add --type=webhook --project= --url=A-URL-THAT-CAN-RECEIVE-THE-POSTED-JSON Then specify the desired webhook properties: Events to report (--events) A list of events to report, e.g. environment.push Default: * Enter comma-separated values (or leave this blank) environment.push States to report (--states) A list of states to report, e.g. pending, in_progress, complete Default: complete Enter comma-separated values (or leave this blank) complete Included environments (--environments) The environment IDs to include Default: * Enter comma-separated values (or leave this blank) * Excluded environments (--excluded-environments) The environment IDs to exclude Enter comma-separated values (or leave this blank) Created integration wuw76ebyhb5ni (type: webhook) +----------------------+-------------------------------------------------------+ | Property | Value | +----------------------+-------------------------------------------------------+ | id | wuw76ebyhb5ni | | type | webhook | | events | - environment.push | | environments | - '*' | | excluded_environment | { } | | s | | | states | - complete | | url | | +----------------------+-------------------------------------------------------+ You can also, evidently pass all the arguments on the command line, so this step itself can be automated. Steps (Adding a webhook to all projects) 1. Add the webhooks in a single command $ platform multi -p$(platform projects --my --columns id --pipe | paste -sd \",\" -) \"integration:add --type webhook --url https://example.com/receiver --events * --states complete --environments * --excluded-environments ''\" The CLI command can be broken down into the following parts: platform multi runs CLI command on multiple projects platform projects --my --columns id --pipe outputs the list of projects with only their IDs separated by newline paste -sd \",\" - joins that array of project IDs with a comma integration:add --type webhook --url https://example.com/receiver --events * --states complete --environments * --excluded-environments '' adds a webhook for all events and all environments 2. Bonus - add the integration to all projects that do not have one configured. And what if you only want to add the integration if it does not already exist? platform multi -p$(platform multi -p$(platform projects --my --columns id --pipe | paste -sd \",\" -) \"integration:list --format=csv --no-header\" 2\u00261 | grep -B1 \"No integrations found\" |grep -o \"\\(([[:alnum:]]*)\\)$\" | cut -c2-14| paste -sd \",\" -) \"integration:add --type webhook --url https://example.com --events * --states complete --environments * --excluded-environments ''\" But then again, bash one liners… you’d be better off at this point using the API directly rather then relying on my bash-foo. Conclusions Using the http://Platform.sh CLI, webhook integrations can easily be added to single projects or even to every project. Bash is cool. ",
        "text": "Goal Add a webhook to a single project, or to all of your http://Platform.sh projects. Assumptions One or more active projects on http://Platform.sh The https://docs.platform.sh/gettingstarted/cli.html installed locally Problems You can use webhooks to integrate http://Platform.sh into your automation chain to trigger actions somewhere else. They can be configured to fire on all events, or specific events that happen on specific environments. More information about https://docs.platform.sh/administration/integrations/webhooks.html is available in the public documentation. Steps (Adding a webhook to a single project) 1. Get the project ID If the local repository already has the remote project set, navigate to that directory. Otherwise, retrieve the project ID using platform project:list 2. Set up the webhook Add the webhook for the desired URL that can receive posted JSON: $ platform integration:add --type=webhook --project= --url=A-URL-THAT-CAN-RECEIVE-THE-POSTED-JSON Then specify the desired webhook properties: Events to report (--events) A list of events to report, e.g. environment.push Default: * Enter comma-separated values (or leave this blank) environment.push States to report (--states) A list of states to report, e.g. pending, in_progress, complete Default: complete Enter comma-separated values (or leave this blank) complete Included environments (--environments) The environment IDs to include Default: * Enter comma-separated values (or leave this blank) * Excluded environments (--excluded-environments) The environment IDs to exclude Enter comma-separated values (or leave this blank) Created integration wuw76ebyhb5ni (type: webhook) +----------------------+-------------------------------------------------------+ | Property | Value | +----------------------+-------------------------------------------------------+ | id | wuw76ebyhb5ni | | type | webhook | | events | - environment.push | | environments | - '*' | | excluded_environment | { } | | s | | | states | - complete | | url | | +----------------------+-------------------------------------------------------+ You can also, evidently pass all the arguments on the command line, so this step itself can be automated. Steps (Adding a webhook to all projects) 1. Add the webhooks in a single command $ platform multi -p$(platform projects --my --columns id --pipe | paste -sd \",\" -) \"integration:add --type webhook --url https://example.com/receiver --events * --states complete --environments * --excluded-environments ''\" The CLI command can be broken down into the following parts: platform multi runs CLI command on multiple projects platform projects --my --columns id --pipe outputs the list of projects with only their IDs separated by newline paste -sd \",\" - joins that array of project IDs with a comma integration:add --type webhook --url https://example.com/receiver --events * --states complete --environments * --excluded-environments '' adds a webhook for all events and all environments 2. Bonus - add the integration to all projects that do not have one configured. And what if you only want to add the integration if it does not already exist? platform multi -p$(platform multi -p$(platform projects --my --columns id --pipe | paste -sd \",\" -) \"integration:list --format=csv --no-header\" 2\u00261 | grep -B1 \"No integrations found\" |grep -o \"\\(([[:alnum:]]*)\\)$\" | cut -c2-14| paste -sd \",\" -) \"integration:add --type webhook --url https://example.com --events * --states complete --environments * --excluded-environments ''\" But then again, bash one liners… you’d be better off at this point using the API directly rather then relying on my bash-foo. Conclusions Using the http://Platform.sh CLI, webhook integrations can easily be added to single projects or even to every project. Bash is cool. ",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-add-a-webhook-to-one-or-more-projects/139",
        "relurl": "/t/how-to-add-a-webhook-to-one-or-more-projects/139"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "209745a6abc4402c4bbfb1084a222bb6a1f36b29",
        "title": "How to use Redis for caching a Drupal 8 site",
        "description": "Goal Drupal 8 functions best with a non-database cache, and particularly well with Redis. This guide shows how to set up a Drupal 8 site to use Redis caching. Assumptions You already have a working Drupal 8 site on http://Platform.sh, built with Composer. The project’s plan size has room for an additional Redis service. A Standard plan should have room for a Drupal 8 site with Redis and one search server, although if the site has a lot of authenticated traffic it may be a good idea to increase it to a Medium plan to allow for more resources for each container. Add Redis first, then benchmark the site to determine if that’s necessary. Problems Drupal 8 contains a bug that causes it to fail if the cache backend changes between installation (which always uses the database) and the first page load (which uses the configured cache). Therefore Redis caching cannot be configured out of the box. It can and should be enabled immediately after installation, however. Steps 1. Add a Redis service Add the following to the .platform/services.yaml file to create a Redis service in the application: rediscache: type: redis:5.0 That will create a service named rediscache, of type redis, specifically version 5.0. 2. Expose the Redis service to your application In the .platform.app.yaml file under the relationships section, add the following: relationships: redis: \"rediscache:redis\" 3. Add the Redis PHP extension In the .platform.app.yaml file, add the following right after the type block: # Additional extensions runtime: extensions: - redis 4. Add the Drupal module Add the Drupal https://www.drupal.org/project/redis module. If using Composer (recommended), a single command will add it: composer require drupal/redis Then commit the resulting changes to the composer.json and composer.lock files. The Redis module may be enabled after the setup below is completed, but should not be enabled until it is. 5. Configure Drupal Place the following at the end of settings.platformsh.php. Note the inline comments, which allow for further customization. Also review the README.txt file that comes with the redis module, as it has a great deal more information on possible configuration options. The example below is intended as a “most common case”, so may need adjustment to be optimal for a particular site. note If you do not already have the http://Platform.sh Config Reader library installed and referenced at the top of the file, you will need to install it with composer require platformsh/config-reader and then add the following code before the block below: $platformsh = new \\Platformsh\\ConfigReader\\Config(); if (!$platformsh-inRuntime()) { return; } // Set redis configuration. if ($platformsh-hasRelationship('redis') \u0026\u0026 !drupal_installation_attempted() \u0026\u0026 extension_loaded('redis')) { $redis = $platformsh-credentials('redis'); // Set Redis as the default backend for any cache bin not otherwise specified. $settings['cache']['default'] = 'cache.backend.redis'; $settings['redis.connection']['host'] = $redis['host']; $settings['redis.connection']['port'] = $redis['port']; // Apply changes to the container configuration to better leverage Redis. // This includes using Redis for the lock and flood control systems, as well // as the cache tag checksum. Alternatively, copy the contents of that file // to your project-specific services.yml file, modify as appropriate, and // remove this line. $settings['container_yamls'][] = 'modules/contrib/redis/example.services.yml'; // Allow the services to work before the Redis module itself is enabled. $settings['container_yamls'][] = 'modules/contrib/redis/redis.services.yml'; // Manually add the classloader path, this is required for the container cache bin definition below // and allows to use it without the redis module being enabled. $class_loader-addPsr4('Drupal\\\\redis\\\\', 'modules/contrib/redis/src'); // Use redis for container cache. // The container cache is used to load the container definition itself, and // thus any configuration stored in the container itself is not available // yet. These lines force the container cache to use Redis rather than the // default SQL cache. $settings['bootstrap_container_definition'] = [ 'parameters' = [], 'services' = [ 'redis.factory' = [ 'class' = 'Drupal\\redis\\ClientFactory', ], 'cache.backend.redis' = [ 'class' = 'Drupal\\redis\\Cache\\CacheBackendFactory', 'arguments' = ['@redis.factory', '@cache_tags_provider.container', '@serialization.phpserialize'], ], 'cache.container' = [ 'class' = '\\Drupal\\redis\\Cache\\PhpRedis', 'factory' = ['@cache.backend.redis', 'get'], 'arguments' = ['container'], ], 'cache_tags_provider.container' = [ 'class' = 'Drupal\\redis\\Cache\\RedisCacheTagsChecksum', 'arguments' = ['@redis.factory'], ], 'serialization.phpserialize' = [ 'class' = 'Drupal\\Component\\Serialization\\PhpSerialize', ], ], ]; } The example.services.yml file noted above will also use Redis for the lock and flood control systems. 6. Deploy and Verify Redis Commit the changes to settings.platformsh.php, composer.*, .platform.app.yaml, and .platform/services.yaml. Then git push. Once the deploy is finished, run: platform ssh \"redis-cli -h redis.internal info\" That will show the Redis status information. Toward the top of the output is the memory allocation. If that amount changes as the site is used it means Drupal is correctly connecting to Redis and writing cache values to it. 7. Clear SQL cache tables Once Redis is confirmed running and in use, clear out the remaining, vestigial values in the SQL database cache as they are no longer valid. To do that, connect to the database: platform sql And run TRUNCATE cache_* for every table that begins with cache_, except for cache_form. Despite its name cache_form is not part of the cache system proper and thus should not be moved out of SQL. To see all the cache tables, run: show tables like 'cache_%'; That will list tables like cache_bootstrap, cache_config, cache_container, etc. Then run: TRUNCATE cache_bootstrap; TRUNCATE cache_config; TRUNCATE cache_container; And so on. Conclusion Using Redis for Drupal caching can improve performance, and more importantly free up valuable database disk space. That helps avoid out-of-disk lockups on the database that can impact site availability.",
        "text": "Goal Drupal 8 functions best with a non-database cache, and particularly well with Redis. This guide shows how to set up a Drupal 8 site to use Redis caching. Assumptions You already have a working Drupal 8 site on http://Platform.sh, built with Composer. The project’s plan size has room for an additional Redis service. A Standard plan should have room for a Drupal 8 site with Redis and one search server, although if the site has a lot of authenticated traffic it may be a good idea to increase it to a Medium plan to allow for more resources for each container. Add Redis first, then benchmark the site to determine if that’s necessary. Problems Drupal 8 contains a bug that causes it to fail if the cache backend changes between installation (which always uses the database) and the first page load (which uses the configured cache). Therefore Redis caching cannot be configured out of the box. It can and should be enabled immediately after installation, however. Steps 1. Add a Redis service Add the following to the .platform/services.yaml file to create a Redis service in the application: rediscache: type: redis:5.0 That will create a service named rediscache, of type redis, specifically version 5.0. 2. Expose the Redis service to your application In the .platform.app.yaml file under the relationships section, add the following: relationships: redis: \"rediscache:redis\" 3. Add the Redis PHP extension In the .platform.app.yaml file, add the following right after the type block: # Additional extensions runtime: extensions: - redis 4. Add the Drupal module Add the Drupal https://www.drupal.org/project/redis module. If using Composer (recommended), a single command will add it: composer require drupal/redis Then commit the resulting changes to the composer.json and composer.lock files. The Redis module may be enabled after the setup below is completed, but should not be enabled until it is. 5. Configure Drupal Place the following at the end of settings.platformsh.php. Note the inline comments, which allow for further customization. Also review the README.txt file that comes with the redis module, as it has a great deal more information on possible configuration options. The example below is intended as a “most common case”, so may need adjustment to be optimal for a particular site. note If you do not already have the http://Platform.sh Config Reader library installed and referenced at the top of the file, you will need to install it with composer require platformsh/config-reader and then add the following code before the block below: $platformsh = new \\Platformsh\\ConfigReader\\Config(); if (!$platformsh-inRuntime()) { return; } // Set redis configuration. if ($platformsh-hasRelationship('redis') \u0026\u0026 !drupal_installation_attempted() \u0026\u0026 extension_loaded('redis')) { $redis = $platformsh-credentials('redis'); // Set Redis as the default backend for any cache bin not otherwise specified. $settings['cache']['default'] = 'cache.backend.redis'; $settings['redis.connection']['host'] = $redis['host']; $settings['redis.connection']['port'] = $redis['port']; // Apply changes to the container configuration to better leverage Redis. // This includes using Redis for the lock and flood control systems, as well // as the cache tag checksum. Alternatively, copy the contents of that file // to your project-specific services.yml file, modify as appropriate, and // remove this line. $settings['container_yamls'][] = 'modules/contrib/redis/example.services.yml'; // Allow the services to work before the Redis module itself is enabled. $settings['container_yamls'][] = 'modules/contrib/redis/redis.services.yml'; // Manually add the classloader path, this is required for the container cache bin definition below // and allows to use it without the redis module being enabled. $class_loader-addPsr4('Drupal\\\\redis\\\\', 'modules/contrib/redis/src'); // Use redis for container cache. // The container cache is used to load the container definition itself, and // thus any configuration stored in the container itself is not available // yet. These lines force the container cache to use Redis rather than the // default SQL cache. $settings['bootstrap_container_definition'] = [ 'parameters' = [], 'services' = [ 'redis.factory' = [ 'class' = 'Drupal\\redis\\ClientFactory', ], 'cache.backend.redis' = [ 'class' = 'Drupal\\redis\\Cache\\CacheBackendFactory', 'arguments' = ['@redis.factory', '@cache_tags_provider.container', '@serialization.phpserialize'], ], 'cache.container' = [ 'class' = '\\Drupal\\redis\\Cache\\PhpRedis', 'factory' = ['@cache.backend.redis', 'get'], 'arguments' = ['container'], ], 'cache_tags_provider.container' = [ 'class' = 'Drupal\\redis\\Cache\\RedisCacheTagsChecksum', 'arguments' = ['@redis.factory'], ], 'serialization.phpserialize' = [ 'class' = 'Drupal\\Component\\Serialization\\PhpSerialize', ], ], ]; } The example.services.yml file noted above will also use Redis for the lock and flood control systems. 6. Deploy and Verify Redis Commit the changes to settings.platformsh.php, composer.*, .platform.app.yaml, and .platform/services.yaml. Then git push. Once the deploy is finished, run: platform ssh \"redis-cli -h redis.internal info\" That will show the Redis status information. Toward the top of the output is the memory allocation. If that amount changes as the site is used it means Drupal is correctly connecting to Redis and writing cache values to it. 7. Clear SQL cache tables Once Redis is confirmed running and in use, clear out the remaining, vestigial values in the SQL database cache as they are no longer valid. To do that, connect to the database: platform sql And run TRUNCATE cache_* for every table that begins with cache_, except for cache_form. Despite its name cache_form is not part of the cache system proper and thus should not be moved out of SQL. To see all the cache tables, run: show tables like 'cache_%'; That will list tables like cache_bootstrap, cache_config, cache_container, etc. Then run: TRUNCATE cache_bootstrap; TRUNCATE cache_config; TRUNCATE cache_container; And so on. Conclusion Using Redis for Drupal caching can improve performance, and more importantly free up valuable database disk space. That helps avoid out-of-disk lockups on the database that can impact site availability.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-use-redis-for-caching-a-drupal-8-site/190",
        "relurl": "/t/how-to-use-redis-for-caching-a-drupal-8-site/190"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "bdcb1b935788f4691effe5aab2bbd62bd3ec054c",
        "title": "How to connect to services on Platform.sh from your local system",
        "description": "Goal Access Platform.sh-hosted services from a local computer, over a secure SSH tunnel. Assumptions The http://Platform.sh CLI is installed locally. Whatever additional software is needed (local web server, local mysql client, etc.) is already installed locally. Problems By default the http://Platform.sh CLI will use the current environment. However, when using a 3rd party integration from GitHub or GitLab the desired environment will likely be a PR environment, not the environment that the user is working on directly. That will require extra commands as shown below. Steps 1. Open an SSH tunnel Run the following command locally: platform tunnel:open That will open an SSH tunnel to the current environment and map all services defined on the application to local ports. Accessing those ports locally with the appropriate tool (mysql command line, Redis CLI client, etc.) will connect to the service on the environment. 2. Export relationships Run the following command locally: export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" That will create a PLATFORM_RELATIONSHIPS environment variable locally that provides user/password/host/port credentials to connect to the current environment. A local application that checks that environment variable in order to connect to services on http://Platform.sh will work just the same as if it were on http://Platform.sh. (It will not, however, make any other environment variables available.) It is also possible to combine both commands into a single line, like so: platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" 3. Disconnect from the tunnel When done, clean up the open connections by running: platform tunnel:close Alternative: Connect to a Pull Request environment Both commands above will, by default, use the current branch name/environment. However, the environment created by a Pull Request from GitHub, Bitbucket, or GitLab will be different and thus the above commands will not connect to them. To access a Pull Request environment modify both commands to specify the environment name. For example, a GitHub Pull Request #42 will have an environment name on http://Platform.sh of pr-42, so run: platform tunnel:open -e pr-42 \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode -e pr-42)\" The -e pr-42 (or equivalently --environment pr-42) tells the CLI to connect to the pr-42 environment rather than the environment that corresponds to the branch currently checked out. Conclusion With access to the remote services, customers can take whatever action they need. That includes running code locally against a remote service, accessing the service using a management tool appropriate for that service, or any other task. Just remember the caveat about Pull Request environments.",
        "text": "Goal Access Platform.sh-hosted services from a local computer, over a secure SSH tunnel. Assumptions The http://Platform.sh CLI is installed locally. Whatever additional software is needed (local web server, local mysql client, etc.) is already installed locally. Problems By default the http://Platform.sh CLI will use the current environment. However, when using a 3rd party integration from GitHub or GitLab the desired environment will likely be a PR environment, not the environment that the user is working on directly. That will require extra commands as shown below. Steps 1. Open an SSH tunnel Run the following command locally: platform tunnel:open That will open an SSH tunnel to the current environment and map all services defined on the application to local ports. Accessing those ports locally with the appropriate tool (mysql command line, Redis CLI client, etc.) will connect to the service on the environment. 2. Export relationships Run the following command locally: export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" That will create a PLATFORM_RELATIONSHIPS environment variable locally that provides user/password/host/port credentials to connect to the current environment. A local application that checks that environment variable in order to connect to services on http://Platform.sh will work just the same as if it were on http://Platform.sh. (It will not, however, make any other environment variables available.) It is also possible to combine both commands into a single line, like so: platform tunnel:open \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode)\" 3. Disconnect from the tunnel When done, clean up the open connections by running: platform tunnel:close Alternative: Connect to a Pull Request environment Both commands above will, by default, use the current branch name/environment. However, the environment created by a Pull Request from GitHub, Bitbucket, or GitLab will be different and thus the above commands will not connect to them. To access a Pull Request environment modify both commands to specify the environment name. For example, a GitHub Pull Request #42 will have an environment name on http://Platform.sh of pr-42, so run: platform tunnel:open -e pr-42 \u0026\u0026 export PLATFORM_RELATIONSHIPS=\"$(platform tunnel:info --encode -e pr-42)\" The -e pr-42 (or equivalently --environment pr-42) tells the CLI to connect to the pr-42 environment rather than the environment that corresponds to the branch currently checked out. Conclusion With access to the remote services, customers can take whatever action they need. That includes running code locally against a remote service, accessing the service using a management tool appropriate for that service, or any other task. Just remember the caveat about Pull Request environments.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-connect-to-services-on-platform-sh-from-your-local-system/189",
        "relurl": "/t/how-to-connect-to-services-on-platform-sh-from-your-local-system/189"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "73b639a7e8c09ce98e784efb356e034a85fb009b",
        "title": "How to upgrade Wordpress core and dependencies with Composer",
        "description": "Goal Update WordPress to the latest version using Composer. Assumptions A WordPress installation on http://Platform.sh that uses Composer, for example one that uses the https://github.com/platformsh/template-wordpress . The https://github.com/platformsh/platformsh-cli installed locally An https://docs.platform.sh/development/ssh.html configured on the project account https://getcomposer.org/ installed locally Problems Keeping WordPress up to date is essential for security as well as for bug fixes and new features. Updating WordPress with one command lowers the burden of maintenance. Since WordPress doesn’t provide native Composer support, the template provided by http://Platform.sh depends on a https://packagist.org/packages/johnpbloch/wordpress . Steps 1. Make a branch for updating WordPress: $ platform checkout wordpress-update 2. Update composer.json Check that composer.json is set to track the latest WordPress version. It should have something like ^5.1 listed for wordpress-core. \"require\": { \"php\": \"=5.3.2\", \"johnpbloch/wordpress-core-installer\": \"^1.0\", \"johnpbloch/wordpress-core\": \"^5.1\" }, 3. Run composer update. composer update --with-dependencies Loading composer repositories with package information Updating dependencies (including require-dev) Package operations: 2 installs, 0 updates, 0 removals - Installing johnpbloch/wordpress-core-installer (1.0.2): Loading from cache - Installing johnpbloch/wordpress-core (5.1.1): Downloading (100%) Writing lock file Generating autoload files 4. Check changes Run git status to see which files have changed: git status On branch wordpress-update Your branch is up to date with 'platform/wordpress-update'. Changes not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git checkout -- ...\" to discard changes in working directory) modified: composer.json modified: composer.lock no changes added to commit (use \"git add\" and/or \"git commit -a\") 5. Push changes Now the project can be built properly by http://Platform.sh to provide the latest WordPress version. Add, commit, and push. git add composer.json git add composer.lock git commit -m \"Updating WordPress\" git push platform wordpress-update 6. Update the Wordpress database Update the WordPress database using the platform tool: platform ssh 'wp core update-db' Alternately, log into the admin area of the site an you will be prompted by WordPress to update the database. Note that you will have to re-run the database update when you merge your code changes into other branches, e.g. into master when you deploy these changes to your live site. Conclusion Relying on a package manager such as Composer greatly simplifies the maintenance and security of WordPress sites.",
        "text": "Goal Update WordPress to the latest version using Composer. Assumptions A WordPress installation on http://Platform.sh that uses Composer, for example one that uses the https://github.com/platformsh/template-wordpress . The https://github.com/platformsh/platformsh-cli installed locally An https://docs.platform.sh/development/ssh.html configured on the project account https://getcomposer.org/ installed locally Problems Keeping WordPress up to date is essential for security as well as for bug fixes and new features. Updating WordPress with one command lowers the burden of maintenance. Since WordPress doesn’t provide native Composer support, the template provided by http://Platform.sh depends on a https://packagist.org/packages/johnpbloch/wordpress . Steps 1. Make a branch for updating WordPress: $ platform checkout wordpress-update 2. Update composer.json Check that composer.json is set to track the latest WordPress version. It should have something like ^5.1 listed for wordpress-core. \"require\": { \"php\": \"=5.3.2\", \"johnpbloch/wordpress-core-installer\": \"^1.0\", \"johnpbloch/wordpress-core\": \"^5.1\" }, 3. Run composer update. composer update --with-dependencies Loading composer repositories with package information Updating dependencies (including require-dev) Package operations: 2 installs, 0 updates, 0 removals - Installing johnpbloch/wordpress-core-installer (1.0.2): Loading from cache - Installing johnpbloch/wordpress-core (5.1.1): Downloading (100%) Writing lock file Generating autoload files 4. Check changes Run git status to see which files have changed: git status On branch wordpress-update Your branch is up to date with 'platform/wordpress-update'. Changes not staged for commit: (use \"git add ...\" to update what will be committed) (use \"git checkout -- ...\" to discard changes in working directory) modified: composer.json modified: composer.lock no changes added to commit (use \"git add\" and/or \"git commit -a\") 5. Push changes Now the project can be built properly by http://Platform.sh to provide the latest WordPress version. Add, commit, and push. git add composer.json git add composer.lock git commit -m \"Updating WordPress\" git push platform wordpress-update 6. Update the Wordpress database Update the WordPress database using the platform tool: platform ssh 'wp core update-db' Alternately, log into the admin area of the site an you will be prompted by WordPress to update the database. Note that you will have to re-run the database update when you merge your code changes into other branches, e.g. into master when you deploy these changes to your live site. Conclusion Relying on a package manager such as Composer greatly simplifies the maintenance and security of WordPress sites.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-upgrade-wordpress-core-and-dependencies-with-composer/173",
        "relurl": "/t/how-to-upgrade-wordpress-core-and-dependencies-with-composer/173"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "ffea9b5d9efac32b3aba638c0b455790ee83540a",
        "title": "How to migrate database changes between environments",
        "description": "Goal Import changes to a database in a child environment to its parent using the http://Platform.sh CLI. Assumptions This guide assumes: an active application on configured to a database A local repository with the http://platform.sh/ project as git remote The https://docs.platform.sh/gettingstarted/cli.html installed locally An https://docs.platform.sh/development/ssh.html configured on the project account admin role granted for the project Problems Changes to a database in master will be visible to its child environments when they are , however changes made to a development environment - such as adding an admin user (alan) to a PostgreSQL database - do not run in the opposite direction and will not be available in master. In a Django application, admin user alan is visible from the dev branch. $ platform db:sql psql (11.1 (Debian 11.1-3.pgdg90+1), server 9.6.11) Type \"help\" for help. main= SELECT * FROM auth_user; id | password | last_login | is_superuser | username | first_name | last_name | email | is_staff | is_active | date_joined ----+---------------------------+-------------------------------+--------------+----------+------------+-----------+-------+----------+-----------+------------------------------- 2 | | 2019-03-12 15:52:38.379839+00 | t | chad | | | | t | t | 2019-03-07 16:02:26.583956+00 6 | | 2019-03-12 15:48:02+00 | t | alan | | | | t | t | 2019-03-12 15:47:19+00 (2 rows) alan is not visible in master, however. $ platform db:sql psql (11.1 (Debian 11.1-3.pgdg90+1), server 9.6.11) Type \"help\" for help. main= SELECT * FROM auth_user; id | password | last_login | is_superuser | username | first_name | last_name | email | is_staff | is_active | date_joined ----+---------------------------+-------------------------------+--------------+----------+------------+-----------+-------+----------+-----------+------------------------------- 2 | | 2019-03-11 14:40:56.401026+00 | t | chad | | | | t | t | 2019-03-07 16:02:26.583956+00 (1 row) Note: Importing a database into an active environment is a destructive operation. This guide assumes that the only change made between master and dev is the addition of a new admin user in dev. Synchronize dev if there is data in master you do not want to be overwritten. $ git checkout dev $ platform environment:synchronize http://Platform.sh also strongly recommends that you take a snapshot of the target environment before executing. $ git checkout master $ platform snapshot:create Migrating changes between environments can be done in two ways: Restoring a snapshot to the target environment Manually dumping and importing the database to the target environment Steps (Restoring a snapshot to the target environment) 1. Create a snapshot Create a snapshot of the** dev **environment that contains the database changes $ git checkout dev Switched to branch 'dev' $ platform snapshot:create Creating a snapshot of dev Waiting for the snapshot to complete... Waiting for the activity m3qerwnkyblje (User created a backup of dev): Backing up dev Backup name is [============================] 14 secs (complete) A snapshot of environment dev has been created Snapshot name: 2. Restore the snapshot from dev to master While still checked out as dev and using the from above: $ platform snapshot:restore --target=master Additional information about creating and restoring snapshots can be found in the https://docs.platform.sh/administration/snapshot-and-restore.html and in https://community.platform.sh/t/how-to-create-and-restore-snapshots-using-the-cli/100 . Steps (Manually dump to the target environment) 1. Dump the database For this example, the relationship database was defined in .platform.app.yaml with: relationships: database: \"postgresqldb:postgresql\" From dev dump the PostrgreSQL database according to its relationship name (database). The --relationship tag is useful if there are multiple database relationships present, but if only one is used platform db:dump will work without it. See the for more information. $ git checkout dev $ platform db:dump --relationship database Creating SQL dump file: / --dev-54ta5gq--postgresqldb--main--dump.sql 2. Pipe the SQL dump to the target environment ```bash $ git checkout master $ platform sql --relationship database -e master --dev-54ta5gq--postgresqldb--main--dump.sql ``` Verify Verify that the new admin user alan is now visible from master. $ platform db:sql psql (11.1 (Debian 11.1-3.pgdg90+1), server 9.6.11) Type \"help\" for help. main= SELECT * FROM auth_user; id | password | last_login | is_superuser | username | first_name | last_name | email | is_staff | is_active | date_joined ----+---------------------------+-------------------------------+--------------+----------+------------+-----------+-------+----------+-----------+------------------------------- 2 | | 2019-03-12 15:52:38.379839+00 | t | chad | | | | t | t | 2019-03-07 16:02:26.583956+00 6 | | 2019-03-12 15:48:02+00 | t | alan | | | | t | t | 2019-03-12 15:47:19+00 (2 rows) Conclusion Child environments inherit service visibility from their parents, but to ensure that development changes to not affect a production environment, updates and synchronizations do not occur in the opposite direction. If a change is made in a development environment that is desired on master, the Platform CLI can be used to migrate those changes by either creating and restoring a snapshot to the target environment or by manually dumping a database piping the dump file into the target environment.",
        "text": "Goal Import changes to a database in a child environment to its parent using the http://Platform.sh CLI. Assumptions This guide assumes: an active application on configured to a database A local repository with the http://platform.sh/ project as git remote The https://docs.platform.sh/gettingstarted/cli.html installed locally An https://docs.platform.sh/development/ssh.html configured on the project account admin role granted for the project Problems Changes to a database in master will be visible to its child environments when they are , however changes made to a development environment - such as adding an admin user (alan) to a PostgreSQL database - do not run in the opposite direction and will not be available in master. In a Django application, admin user alan is visible from the dev branch. $ platform db:sql psql (11.1 (Debian 11.1-3.pgdg90+1), server 9.6.11) Type \"help\" for help. main= SELECT * FROM auth_user; id | password | last_login | is_superuser | username | first_name | last_name | email | is_staff | is_active | date_joined ----+---------------------------+-------------------------------+--------------+----------+------------+-----------+-------+----------+-----------+------------------------------- 2 | | 2019-03-12 15:52:38.379839+00 | t | chad | | | | t | t | 2019-03-07 16:02:26.583956+00 6 | | 2019-03-12 15:48:02+00 | t | alan | | | | t | t | 2019-03-12 15:47:19+00 (2 rows) alan is not visible in master, however. $ platform db:sql psql (11.1 (Debian 11.1-3.pgdg90+1), server 9.6.11) Type \"help\" for help. main= SELECT * FROM auth_user; id | password | last_login | is_superuser | username | first_name | last_name | email | is_staff | is_active | date_joined ----+---------------------------+-------------------------------+--------------+----------+------------+-----------+-------+----------+-----------+------------------------------- 2 | | 2019-03-11 14:40:56.401026+00 | t | chad | | | | t | t | 2019-03-07 16:02:26.583956+00 (1 row) Note: Importing a database into an active environment is a destructive operation. This guide assumes that the only change made between master and dev is the addition of a new admin user in dev. Synchronize dev if there is data in master you do not want to be overwritten. $ git checkout dev $ platform environment:synchronize http://Platform.sh also strongly recommends that you take a snapshot of the target environment before executing. $ git checkout master $ platform snapshot:create Migrating changes between environments can be done in two ways: Restoring a snapshot to the target environment Manually dumping and importing the database to the target environment Steps (Restoring a snapshot to the target environment) 1. Create a snapshot Create a snapshot of the** dev **environment that contains the database changes $ git checkout dev Switched to branch 'dev' $ platform snapshot:create Creating a snapshot of dev Waiting for the snapshot to complete... Waiting for the activity m3qerwnkyblje (User created a backup of dev): Backing up dev Backup name is [============================] 14 secs (complete) A snapshot of environment dev has been created Snapshot name: 2. Restore the snapshot from dev to master While still checked out as dev and using the from above: $ platform snapshot:restore --target=master Additional information about creating and restoring snapshots can be found in the https://docs.platform.sh/administration/snapshot-and-restore.html and in https://community.platform.sh/t/how-to-create-and-restore-snapshots-using-the-cli/100 . Steps (Manually dump to the target environment) 1. Dump the database For this example, the relationship database was defined in .platform.app.yaml with: relationships: database: \"postgresqldb:postgresql\" From dev dump the PostrgreSQL database according to its relationship name (database). The --relationship tag is useful if there are multiple database relationships present, but if only one is used platform db:dump will work without it. See the for more information. $ git checkout dev $ platform db:dump --relationship database Creating SQL dump file: / --dev-54ta5gq--postgresqldb--main--dump.sql 2. Pipe the SQL dump to the target environment ```bash $ git checkout master $ platform sql --relationship database -e master --dev-54ta5gq--postgresqldb--main--dump.sql ``` Verify Verify that the new admin user alan is now visible from master. $ platform db:sql psql (11.1 (Debian 11.1-3.pgdg90+1), server 9.6.11) Type \"help\" for help. main= SELECT * FROM auth_user; id | password | last_login | is_superuser | username | first_name | last_name | email | is_staff | is_active | date_joined ----+---------------------------+-------------------------------+--------------+----------+------------+-----------+-------+----------+-----------+------------------------------- 2 | | 2019-03-12 15:52:38.379839+00 | t | chad | | | | t | t | 2019-03-07 16:02:26.583956+00 6 | | 2019-03-12 15:48:02+00 | t | alan | | | | t | t | 2019-03-12 15:47:19+00 (2 rows) Conclusion Child environments inherit service visibility from their parents, but to ensure that development changes to not affect a production environment, updates and synchronizations do not occur in the opposite direction. If a change is made in a development environment that is desired on master, the Platform CLI can be used to migrate those changes by either creating and restoring a snapshot to the target environment or by manually dumping a database piping the dump file into the target environment.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-migrate-database-changes-between-environments/124",
        "relurl": "/t/how-to-migrate-database-changes-between-environments/124"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "812e035a76790794540e6b99bfbc47786a72f317",
        "title": "How to perform a database dump via Platform.sh CLI",
        "description": "Goal Perform a database dump using the platform https://github.com/platformsh/platformsh-cli . One or more active http://platform.sh/ projects The https://github.com/platformsh/platformsh-cli installed locally An https://docs.platform.sh/development/ssh.html configured on the project account Known project IDs. (Use platform project:list) Problems Sometimes it is necessary to do a database dump of a project. This can be especially useful when doing some testing, planning a migration or for recovery purposes. Platform CLI can be used to backup the database with a single command. Steps In general, the following command will be used to perform a database dump using the Platform CLI: Usage: platform db:dump [-f|--file FILE] [-t|--timestamp] [--stdout] [-p|--project PROJECT] [--host HOST] [-e|--environment ENVIRONMENT] [-A|--app APP] 1. Single Database For a single configured database, use the command below: $ platform db:dump Creating SQL dump file:/Users/my_user/platformsh-enterprise/hiera/mysqldb--main--dump.sql 2. Multiple Databases Running the previous command with multiple databases, choosing a relationship will be prompted by the CLI: $ platform db:dump Enter a number to choose a relationship: [0] postgresql (main@postgresql.internal) [1] database (user@database.internal) Dumping the MySQL database (database, in this case) can be chosen by selecting 1. database can be chosen directly ahead of time with the CLI with the --relationship flag: $ platform db:dump --relationship database Further options can be specified, as per the example below: $ platform db:dump --relationship database -p project_id -e environment_name -A app To find the platform relationship details, SSS into the environment and run: $ echo $PLATFORM_RELATIONSHIPS | base64 --decode | json_pp 3. Compression By default the dump file will be uncompressed. To compress it, use the --gzip ( -z ) option: platform db:dump --gzip 4. Troubleshooting Error output: Permission denied (publickey). This error can sometimes show up, after running the db dump command. If this happens, try the following: the user is trying to access. The SSH key may be improperly configured. Check with platform ssh-key:list and platform ssh-key:add if necessary. Remove the user from the project, then add them back and redeploy the environment. Conclusion Database dumps can be easily done by using http://platform.sh/ CLI.",
        "text": "Goal Perform a database dump using the platform https://github.com/platformsh/platformsh-cli . One or more active http://platform.sh/ projects The https://github.com/platformsh/platformsh-cli installed locally An https://docs.platform.sh/development/ssh.html configured on the project account Known project IDs. (Use platform project:list) Problems Sometimes it is necessary to do a database dump of a project. This can be especially useful when doing some testing, planning a migration or for recovery purposes. Platform CLI can be used to backup the database with a single command. Steps In general, the following command will be used to perform a database dump using the Platform CLI: Usage: platform db:dump [-f|--file FILE] [-t|--timestamp] [--stdout] [-p|--project PROJECT] [--host HOST] [-e|--environment ENVIRONMENT] [-A|--app APP] 1. Single Database For a single configured database, use the command below: $ platform db:dump Creating SQL dump file:/Users/my_user/platformsh-enterprise/hiera/mysqldb--main--dump.sql 2. Multiple Databases Running the previous command with multiple databases, choosing a relationship will be prompted by the CLI: $ platform db:dump Enter a number to choose a relationship: [0] postgresql (main@postgresql.internal) [1] database (user@database.internal) Dumping the MySQL database (database, in this case) can be chosen by selecting 1. database can be chosen directly ahead of time with the CLI with the --relationship flag: $ platform db:dump --relationship database Further options can be specified, as per the example below: $ platform db:dump --relationship database -p project_id -e environment_name -A app To find the platform relationship details, SSS into the environment and run: $ echo $PLATFORM_RELATIONSHIPS | base64 --decode | json_pp 3. Compression By default the dump file will be uncompressed. To compress it, use the --gzip ( -z ) option: platform db:dump --gzip 4. Troubleshooting Error output: Permission denied (publickey). This error can sometimes show up, after running the db dump command. If this happens, try the following: the user is trying to access. The SSH key may be improperly configured. Check with platform ssh-key:list and platform ssh-key:add if necessary. Remove the user from the project, then add them back and redeploy the environment. Conclusion Database dumps can be easily done by using http://platform.sh/ CLI.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-perform-a-database-dump-via-platform-sh-cli/143",
        "relurl": "/t/how-to-perform-a-database-dump-via-platform-sh-cli/143"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "41ae8b2d857766a5e6cbaba24005719563ee4c21",
        "title": "How to make internal requests between two applications of a project",
        "description": "Goal This guide shows how make an internal request to a different application in the same project. The goal is to display a message sent by a backend API on our frontend. Assumptions To complete this, you will need: An empty http://Platform.sh project The https://docs.platform.sh/gettingstarted/cli.html tool installed Your ssh key loaded in your https://linux.die.net/man/1/ssh-agent and configured in the https://accounts.platform.sh/user Steps In order to implement this, setup two different applications in your project: frontend and backend. Please refer to the . PHP will be used for both applications. The steps below will create the following project structure: multiapp/ .platform/ routes.yaml services.yaml backend/ .platform.app.yaml index.php frontend/ .platform.app.yaml index.php 1. Setup the base project Create an empty directory, initialize git and add the http://Platform.sh remote: mkdir multiapp cd !$ git init . platform project:set https://.platform.sh/projects/ The git remotes should now be: git remote -v platform @git..platform.sh: .git (fetch) platform @git..platform.sh: .git (push) Create the two required http://Platform.sh configuration files: touch ./.platform/routes.yaml touch ./.platform/services.yaml 2. Create the backend application Create a subfolder (mkdir backend) and add the following files: ./backend/index.php 'I come from the backend' ]); ./backend/.platform.app.yaml name: backend type: \"php:7.3\" disk: 256 web: locations: '/': root: '' passthru: '/index.php' Edit ./.platform/routes.yaml and add the configuration for the backend application: \"https://backend.{default}/\": type: upstream upstream: \"backend:http\" Any request going to https://backend.{default}/ will be dispatched to the backend application. Commit the changes and deploy on the http://Platform.sh environment: git add . git commit -m \"Add backend app\" git push platform master The backend application can be accessed at: https://backend.master-7rqtwti- ..platformsh.site/ Verify that the application returns the message: curl https://backend.master-7rqtwti- ..platformsh.site/ | json_pp { \"message\" : \"I come from the backend\" } 3. Create the frontend application Create the frontend folder: mkdir frontend Add the configuration file ./frontend/.platform.app.yaml: name: frontend type: \"php:7.3\" disk: 256 web: locations: '/': root: '' passthru: '/index.php' ./frontend/index.php \r\n\r\nI am the frontend application\r\n\r\n Add the following configuration in ./routes.yaml: \"https://{default}/\": type: upstream upstream: \"frontend:http\" The configuration will make any incoming request to the default domain sent to our frontend application. Commit and deploy the applications: git add . git commit -m \"Add frontend application\" git push platform master The frontend is now working: curl https://master-7rqtwti- ..platformsh.site/ \r\n\r\nI am the frontend application\r\n\r\n 4. Making an internal request The public route to the backend could be used but using an internal request is way more efficient and secure. The first step is to add the relationship. Like any other data service, this will be added in ./frontend/.platform.app.yaml relationships: backend: 'backend:http' The key of the relationship will be the one used in the $PLATFORM_RELATIONSHIPS array. The first parameter is the name of our second application and http is used as the protocol there. Read more on our relationships system in . Commit and deploy the configuration change: git add . git commit -m \"Add relationship\" git push platform master Test the relationship by connecting to the frontend application: platform ssh Enter a number to choose an app: [0] frontend [1] backend \r\n\r\n 0 The relationships are exposed through the $PLATFORM_RELATIONSHIPS environment variable: web@ :~$ base64 -d :~$ curl http://backend.internal {\"message\":\"I come from the backend\"} 5. Load the message from the frontend Replace ./frontend/index.php with the following content: \r\n\r\n'.$message.'\r\n\r\n'; Our https://github.com/platformsh/platformsh-client-php implements a lot of other features automatically in a project to avoid fetching configuration settings manually. And redeploy your project: git add . git commit -m \"Add backend call on frontend\" git push platform master The message from the backend is now displayed on our frontend application: curl https://master-7rqtwti- . .platformsh.site/ \r\n\r\nI come from the backend\r\n\r\n Conclusion Making internal requests on different applications inside the same project is possible by leveraging the relationships mechanism implemented by http://Platform.sh.",
        "text": "Goal This guide shows how make an internal request to a different application in the same project. The goal is to display a message sent by a backend API on our frontend. Assumptions To complete this, you will need: An empty http://Platform.sh project The https://docs.platform.sh/gettingstarted/cli.html tool installed Your ssh key loaded in your https://linux.die.net/man/1/ssh-agent and configured in the https://accounts.platform.sh/user Steps In order to implement this, setup two different applications in your project: frontend and backend. Please refer to the . PHP will be used for both applications. The steps below will create the following project structure: multiapp/ .platform/ routes.yaml services.yaml backend/ .platform.app.yaml index.php frontend/ .platform.app.yaml index.php 1. Setup the base project Create an empty directory, initialize git and add the http://Platform.sh remote: mkdir multiapp cd !$ git init . platform project:set https://.platform.sh/projects/ The git remotes should now be: git remote -v platform @git..platform.sh: .git (fetch) platform @git..platform.sh: .git (push) Create the two required http://Platform.sh configuration files: touch ./.platform/routes.yaml touch ./.platform/services.yaml 2. Create the backend application Create a subfolder (mkdir backend) and add the following files: ./backend/index.php 'I come from the backend' ]); ./backend/.platform.app.yaml name: backend type: \"php:7.3\" disk: 256 web: locations: '/': root: '' passthru: '/index.php' Edit ./.platform/routes.yaml and add the configuration for the backend application: \"https://backend.{default}/\": type: upstream upstream: \"backend:http\" Any request going to https://backend.{default}/ will be dispatched to the backend application. Commit the changes and deploy on the http://Platform.sh environment: git add . git commit -m \"Add backend app\" git push platform master The backend application can be accessed at: https://backend.master-7rqtwti- ..platformsh.site/ Verify that the application returns the message: curl https://backend.master-7rqtwti- ..platformsh.site/ | json_pp { \"message\" : \"I come from the backend\" } 3. Create the frontend application Create the frontend folder: mkdir frontend Add the configuration file ./frontend/.platform.app.yaml: name: frontend type: \"php:7.3\" disk: 256 web: locations: '/': root: '' passthru: '/index.php' ./frontend/index.php \r\n\r\nI am the frontend application\r\n\r\n Add the following configuration in ./routes.yaml: \"https://{default}/\": type: upstream upstream: \"frontend:http\" The configuration will make any incoming request to the default domain sent to our frontend application. Commit and deploy the applications: git add . git commit -m \"Add frontend application\" git push platform master The frontend is now working: curl https://master-7rqtwti- ..platformsh.site/ \r\n\r\nI am the frontend application\r\n\r\n 4. Making an internal request The public route to the backend could be used but using an internal request is way more efficient and secure. The first step is to add the relationship. Like any other data service, this will be added in ./frontend/.platform.app.yaml relationships: backend: 'backend:http' The key of the relationship will be the one used in the $PLATFORM_RELATIONSHIPS array. The first parameter is the name of our second application and http is used as the protocol there. Read more on our relationships system in . Commit and deploy the configuration change: git add . git commit -m \"Add relationship\" git push platform master Test the relationship by connecting to the frontend application: platform ssh Enter a number to choose an app: [0] frontend [1] backend \r\n\r\n 0 The relationships are exposed through the $PLATFORM_RELATIONSHIPS environment variable: web@ :~$ base64 -d :~$ curl http://backend.internal {\"message\":\"I come from the backend\"} 5. Load the message from the frontend Replace ./frontend/index.php with the following content: \r\n\r\n'.$message.'\r\n\r\n'; Our https://github.com/platformsh/platformsh-client-php implements a lot of other features automatically in a project to avoid fetching configuration settings manually. And redeploy your project: git add . git commit -m \"Add backend call on frontend\" git push platform master The message from the backend is now displayed on our frontend application: curl https://master-7rqtwti- . .platformsh.site/ \r\n\r\nI come from the backend\r\n\r\n Conclusion Making internal requests on different applications inside the same project is possible by leveraging the relationships mechanism implemented by http://Platform.sh.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-make-internal-requests-between-two-applications-of-a-project/77",
        "relurl": "/t/how-to-make-internal-requests-between-two-applications-of-a-project/77"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "fc16388107f31957e32ec97afeb7b7990ff0f5f8",
        "title": "How to add or remove users from projects using the command line",
        "description": "Goal Add or remove a user to one or more http://Platform.sh projects using the platform https://github.com/platformsh/platformsh-cli . Assumptions You have one or more http://Platform.sh projects and you have installed the platform https://github.com/platformsh/platformsh-cli . You will need to know the project IDs. You will need the email address for each user you wish to add or remove. Problems Sometimes it is necessary to automate user access to projects, especially when managing fleets of sites. This is a case for using the CLI. Steps 1. Single projects Add user user1@example.com with the project-wide role of viewer. Confirm the change with Y. platform user:add -p -r viewer user1@example.com Adding the user user1@example.com to ( ): Project role: viewer Adding users can result in additional charges. Are you sure you want to add this user? [Y/n] Y Add user user1@example.com to the staging branch as a contributor: platform user:add -p -r staging:contributor user1@example.com Adding the user user1@example.com to ( ): Project role: viewer Role on staging: contributor Adding users can result in additional charges. Are you sure you want to add this user? [Y/n] Y 2. Multiple projects Remove a user ( mailto:user1@example.com) from multiple projects: platform multi --projects , \\ 'user:delete user1@example.com' This is the output: Running command 'user:delete user1@example.com' on 2 projects. * Project: ( ) Are you sure you want to delete the user user1@example.com? [Y/n] y User user1@example.com deleted * Project: ( ) Are you sure you want to delete the user user1@example.com? [Y/n] y User user1@example.com deleted Conclusion User management, including adding, deleting, and role assignment, can be done from the command line using the http://Platform.sh CLI. Applying the platform multi command allows you to make changes on multiple projects at once.",
        "text": "Goal Add or remove a user to one or more http://Platform.sh projects using the platform https://github.com/platformsh/platformsh-cli . Assumptions You have one or more http://Platform.sh projects and you have installed the platform https://github.com/platformsh/platformsh-cli . You will need to know the project IDs. You will need the email address for each user you wish to add or remove. Problems Sometimes it is necessary to automate user access to projects, especially when managing fleets of sites. This is a case for using the CLI. Steps 1. Single projects Add user user1@example.com with the project-wide role of viewer. Confirm the change with Y. platform user:add -p -r viewer user1@example.com Adding the user user1@example.com to ( ): Project role: viewer Adding users can result in additional charges. Are you sure you want to add this user? [Y/n] Y Add user user1@example.com to the staging branch as a contributor: platform user:add -p -r staging:contributor user1@example.com Adding the user user1@example.com to ( ): Project role: viewer Role on staging: contributor Adding users can result in additional charges. Are you sure you want to add this user? [Y/n] Y 2. Multiple projects Remove a user ( mailto:user1@example.com) from multiple projects: platform multi --projects , \\ 'user:delete user1@example.com' This is the output: Running command 'user:delete user1@example.com' on 2 projects. * Project: ( ) Are you sure you want to delete the user user1@example.com? [Y/n] y User user1@example.com deleted * Project: ( ) Are you sure you want to delete the user user1@example.com? [Y/n] y User user1@example.com deleted Conclusion User management, including adding, deleting, and role assignment, can be done from the command line using the http://Platform.sh CLI. Applying the platform multi command allows you to make changes on multiple projects at once.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-add-or-remove-users-from-projects-using-the-command-line/91",
        "relurl": "/t/how-to-add-or-remove-users-from-projects-using-the-command-line/91"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e9b1d243a15e36ca213c3db2cf271e0c4119537e",
        "title": "How to configure low-disk health notifications on Platform.sh",
        "description": "Goal Set up health notifications for low-disks on project containers. These will notify when free disk space drops below 20% and below 10%, and will issue an all-clear notification when the free disk space returns above 20%. Assumptions You have a project on You are an admin on the project You have the https://docs.platform.sh/gettingstarted/cli.html tool installed Problems Container storage can silently fill due to any number of reasons. This will often cause an application to experience erratic errors or cease to function entirely. Steps 1. Email Notifications Add the integration for email using the Platform CLI: platform integration:add --type health.email --from-address you@example.com --recipients them@example.com --recipients others@example.com Note that the --from-address can be any email address you like (including one of the recipients), and you can define one or more recipients by optionally adding more recipient flags. 2. Slack Notifications Create a custom https://api.slack.com/bot-users for your Slack group, configure it for the appropriate channels, and note the API Token. Add the integration for slack using the Platform CLI: platform integration:add --type health.slack --token YOUR_API_TOKEN --channel '#channelname' The bot will then post the notification to the channel specified by the --channel flag. 3. PagerDuty Notifications Create a PagerDuty https://support.pagerduty.com/docs/services-and-integrations using the Events API v2. Copy the Integration Key to use as the --routing-key flag in step (2). Add the integration for PagerDuty using the Platform CLI: platform integration:add --type health.pagerduty --routing-key YOUR_ROUTING_KEY 4. What to do when you get a low-disk notification There are several different options for resolving low-disk situations. For a database, depending on the application set up, you may be able to truncate a table in MySQL that is performing a logging or caching function. For an application container, you can use du -sh to work your way through your filesystem to find if there are any particular files that are taking a lot of space that can be removed. Alternatively, if the space used is necessary and will continue to grow, you can change the size of your disk to resolve the low-disk situation. For the application disk, you can change this with the disk: key in your .platform.app.yaml file. For any other services, their disk sizes can be adjusted from the relevant disk: key in your .platform/services.yaml file. Conclusion Using low-disk health notification with your environment can help you catch outages before they happen, and can be a red flag if they do lead to an outage.",
        "text": "Goal Set up health notifications for low-disks on project containers. These will notify when free disk space drops below 20% and below 10%, and will issue an all-clear notification when the free disk space returns above 20%. Assumptions You have a project on You are an admin on the project You have the https://docs.platform.sh/gettingstarted/cli.html tool installed Problems Container storage can silently fill due to any number of reasons. This will often cause an application to experience erratic errors or cease to function entirely. Steps 1. Email Notifications Add the integration for email using the Platform CLI: platform integration:add --type health.email --from-address you@example.com --recipients them@example.com --recipients others@example.com Note that the --from-address can be any email address you like (including one of the recipients), and you can define one or more recipients by optionally adding more recipient flags. 2. Slack Notifications Create a custom https://api.slack.com/bot-users for your Slack group, configure it for the appropriate channels, and note the API Token. Add the integration for slack using the Platform CLI: platform integration:add --type health.slack --token YOUR_API_TOKEN --channel '#channelname' The bot will then post the notification to the channel specified by the --channel flag. 3. PagerDuty Notifications Create a PagerDuty https://support.pagerduty.com/docs/services-and-integrations using the Events API v2. Copy the Integration Key to use as the --routing-key flag in step (2). Add the integration for PagerDuty using the Platform CLI: platform integration:add --type health.pagerduty --routing-key YOUR_ROUTING_KEY 4. What to do when you get a low-disk notification There are several different options for resolving low-disk situations. For a database, depending on the application set up, you may be able to truncate a table in MySQL that is performing a logging or caching function. For an application container, you can use du -sh to work your way through your filesystem to find if there are any particular files that are taking a lot of space that can be removed. Alternatively, if the space used is necessary and will continue to grow, you can change the size of your disk to resolve the low-disk situation. For the application disk, you can change this with the disk: key in your .platform.app.yaml file. For any other services, their disk sizes can be adjusted from the relevant disk: key in your .platform/services.yaml file. Conclusion Using low-disk health notification with your environment can help you catch outages before they happen, and can be a red flag if they do lead to an outage.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-configure-low-disk-health-notifications-on-platform-sh/121",
        "relurl": "/t/how-to-configure-low-disk-health-notifications-on-platform-sh/121"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "d0496e7c498d3c3a55511ca6fdf7022cfc5fed48",
        "title": "How to interactively debug Node.js applications on Platform.sh",
        "description": "Goal Interactively debug nodejs applications running on http://Platform.sh, Assumptions A Node.js application that is running on http://Platform.sh An issue that requires interactive debugging, possibly because it can’t be reproduced locally The http://platform.sh CLI Problems Effectively debugging web applications isn’t trivial, especially when an HTTP request goes through multiple layers until it reaches the web app, which is usually the case in cloud platforms. Debugging in a local environment is easier, but the bug might only trigger when interacting with the data that are present on production. Steps The debugging procedure should only be performed on a non-production environment. So, if this is a production issue, first clone the production environment. http://Platform.sh creates byte-for-byte clones of an environment including the data so there basically is a reproducibility guarantee. 1. Clone production Using the CLI: platform branch debug-weird-production-issue master This will create a new environment debug-weird-production-issue, based on the master branch which is the production environment. It will also checkout the branch locally. 2. Restart the application daemon in debug mode SSH into this environment with the command: platform ssh By default it will connect the current environment branch, in this case debug-weird-production-issue Stop the running process with the command: sv stop app Run the application in debug mode: node inspect server.js Note: server.js is the entry point to the application as specified in the platform.app.yaml in the web.start key, so modify this command for the entry point used. It will output something similar to: Debugger listening on ws://127.0.0.1:9229/663e3d85-31b0-4f2c-aeb4-94e2fae886cf Copy the path at the end of the outputted URL (i.e., 663e3d85-31b0-4f2c-aeb4-94e2fae886cf) 3. Forward the debugger port locally In a second terminal window, create an SSH tunnel that forwards the 9229 port: ssh -N -L9229:127.0.0.1:9229 $(platform ssh --pipe) 4. Profit! Chrome Dev tools can be used Open the Chrome web browser and visit the url: chrome-devtools://devtools/bundled/js_app.html?experiments=true\u0026v8only=true\u0026ws=127.0.0.1:9229/{path} Replace {path} with the path that was copied earlier. For example: chrome-devtools://devtools/bundled/js_app.html?experiments=true\u0026v8only=true\u0026ws=127.0.0.1:9229/663e3d85-31b0-4f2c-aeb4-94e2fae886cf Conclusion Debugging complex bugs that are only reproduced with production data present is non-trivial. Using a clone of the production data, plus the dynamic nature of Node.js and the fabulous Chrome Dev tools, any bug can be properly investigated.",
        "text": "Goal Interactively debug nodejs applications running on http://Platform.sh, Assumptions A Node.js application that is running on http://Platform.sh An issue that requires interactive debugging, possibly because it can’t be reproduced locally The http://platform.sh CLI Problems Effectively debugging web applications isn’t trivial, especially when an HTTP request goes through multiple layers until it reaches the web app, which is usually the case in cloud platforms. Debugging in a local environment is easier, but the bug might only trigger when interacting with the data that are present on production. Steps The debugging procedure should only be performed on a non-production environment. So, if this is a production issue, first clone the production environment. http://Platform.sh creates byte-for-byte clones of an environment including the data so there basically is a reproducibility guarantee. 1. Clone production Using the CLI: platform branch debug-weird-production-issue master This will create a new environment debug-weird-production-issue, based on the master branch which is the production environment. It will also checkout the branch locally. 2. Restart the application daemon in debug mode SSH into this environment with the command: platform ssh By default it will connect the current environment branch, in this case debug-weird-production-issue Stop the running process with the command: sv stop app Run the application in debug mode: node inspect server.js Note: server.js is the entry point to the application as specified in the platform.app.yaml in the web.start key, so modify this command for the entry point used. It will output something similar to: Debugger listening on ws://127.0.0.1:9229/663e3d85-31b0-4f2c-aeb4-94e2fae886cf Copy the path at the end of the outputted URL (i.e., 663e3d85-31b0-4f2c-aeb4-94e2fae886cf) 3. Forward the debugger port locally In a second terminal window, create an SSH tunnel that forwards the 9229 port: ssh -N -L9229:127.0.0.1:9229 $(platform ssh --pipe) 4. Profit! Chrome Dev tools can be used Open the Chrome web browser and visit the url: chrome-devtools://devtools/bundled/js_app.html?experiments=true\u0026v8only=true\u0026ws=127.0.0.1:9229/{path} Replace {path} with the path that was copied earlier. For example: chrome-devtools://devtools/bundled/js_app.html?experiments=true\u0026v8only=true\u0026ws=127.0.0.1:9229/663e3d85-31b0-4f2c-aeb4-94e2fae886cf Conclusion Debugging complex bugs that are only reproduced with production data present is non-trivial. Using a clone of the production data, plus the dynamic nature of Node.js and the fabulous Chrome Dev tools, any bug can be properly investigated.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-interactively-debug-node-js-applications-on-platform-sh/172",
        "relurl": "/t/how-to-interactively-debug-node-js-applications-on-platform-sh/172"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "c8ceabe4e192df4fbf57f192d02833813e3da88b",
        "title": "How to see your Nginx configuration file",
        "description": "Goal The of .platform.app.yaml is the developer’s chance to configure the Nginx server for the application. Sometimes it is helpful to see exactly what is in the Nginx configuration file as a result of the values in web. This guide shows how to read the Nginx config file using the http://Platform.sh CLI tool. Assumptions This guide assumes: an active application on http://Platform.sh A local repository with the http://platform.sh/ project as git remote The https://docs.platform.sh/gettingstarted/cli.html installed locally Problems Reading the Nginx config file is a matter of being able to make an SSH connection to your environment and then knowing where the file is. Steps The nginx.conf file can be read with this command: platform ssh 'less /etc/nginx/nginx.conf' Conclusion Nginx configuration is abstracted into the web directive of .platform.app.yaml, but you can still see how that gets expressed by looking at the resultant file directly.",
        "text": "Goal The of .platform.app.yaml is the developer’s chance to configure the Nginx server for the application. Sometimes it is helpful to see exactly what is in the Nginx configuration file as a result of the values in web. This guide shows how to read the Nginx config file using the http://Platform.sh CLI tool. Assumptions This guide assumes: an active application on http://Platform.sh A local repository with the http://platform.sh/ project as git remote The https://docs.platform.sh/gettingstarted/cli.html installed locally Problems Reading the Nginx config file is a matter of being able to make an SSH connection to your environment and then knowing where the file is. Steps The nginx.conf file can be read with this command: platform ssh 'less /etc/nginx/nginx.conf' Conclusion Nginx configuration is abstracted into the web directive of .platform.app.yaml, but you can still see how that gets expressed by looking at the resultant file directly.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-see-your-nginx-configuration-file/168",
        "relurl": "/t/how-to-see-your-nginx-configuration-file/168"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "900cbf71b33dfebbb8ef5e857eaee4c6322b518e",
        "title": "How to utilize the CLI to modify already existing Git integrations",
        "description": "Goal To learn how to use the CLI for getting more information on a Git integration, also how to modify certain configuration options. Assumptions Access to a project hosted on http://platform.sh/ Your project account has administrator rights Knowledge on using the Problems None in particular. This How to is about raising awareness to the usefulness of the CLI tool. Steps 1. Check what “integration” commands are available platform list | grep integration integration integration:add Add an integration to the project integration:delete Delete an integration from a project integration:get View details of an integration integration:list (integrations) View a list of project integration(s) integration:update Update an integration integration:validate Validate an existing integration 2. List the integrations added to your project platform integration:list +------------------+--------+-----------------------------------------------------------------------------------------------------+ | ID | Type | Summary | +------------------+--------+-----------------------------------------------------------------------------------------------------+ | | github | Repository: ... | | | | Hook URL: https://.platform.sh/api/projects/ /integrations/ /hook | +------------------+--------+-----------------------------------------------------------------------------------------------------+ View integration details with: platform integration:get [id] Add a new integration with: platform integration:add Delete an integration with: platform integration:delete [id] 3. Get the integration details platform integration:get +---------------------------------+-------------------------------------------------------------------------------------------+ | Property | Value | +---------------------------------+-------------------------------------------------------------------------------------------+ | id | | | type | github | | token | ****** | | repository | ... | | fetch_branches | true | | prune_branches | true | | build_pull_requests | true | | build_pull_requests_post_merge | false | | pull_requests_clone_parent_data | true | | hook_url | https://.platform.sh/api/projects/ /integrations/ /hook | +---------------------------------+-------------------------------------------------------------------------------------------+ 4. Check the options and select which ones to modify platform integration:update --help Command: integration:update Description: Update an integration Usage: platform integration:update [--type TYPE] [--token TOKEN] [--key KEY] [--secret SECRET] [--base-url BASE-URL] [--gitlab-project GITLAB-PROJECT] [--repository REPOSITORY] [--build-merge-requests BUILD-MERGE-REQUESTS] [--build-pu Arguments: id The ID of the integration to update Options: --type=TYPE The integration type ('bitbucket', 'github', 'gitlab', 'hipchat', 'webhook', 'health.email', 'health.pagerduty', 'health.slack') --token=TOKEN An OAuth token for the integration --key=KEY A Bitbucket OAuth consumer key --secret=SECRET A Bitbucket OAuth consumer secret --base-url=BASE-URL The base URL of the GitLab installation --gitlab-project=GITLAB-PROJECT The GitLab project (e.g. 'namespace/repo') --repository=REPOSITORY The repository to track (e.g. 'user/repo') --build-merge-requests=BUILD-MERGE-REQUESTS GitLab: build merge requests as environments [default: true] --build-pull-requests=BUILD-PULL-REQUESTS Build every pull request as an environment [default: true] --build-pull-requests-post-merge=BUILD-PULL-REQUESTS-POST-MERGE Build pull requests based on their post-merge state [default: false] --merge-requests-clone-parent-data=MERGE-REQUESTS-CLONE-PARENT-DATA GitLab: clone data for merge requests [default: true] --pull-requests-clone-parent-data=PULL-REQUESTS-CLONE-PARENT-DATA Clone the parent environment's data for pull requests [default: true] --resync-pull-requests=RESYNC-PULL-REQUESTS Re-sync pull request environment data on every build [default: false] --fetch-branches=FETCH-BRANCHES Fetch all branches from the remote (as inactive environments) [default: true] --prune-branches=PRUNE-BRANCHES Delete branches that do not exist on the remote [default: true] --room=ROOM HipChat room ID --url=URL Generic webhook: a URL to receive JSON data --events=EVENTS A list of events to report, e.g. environment.push [default: [\"*\"]] (multiple values allowed) --states=STATES A list of states to report, e.g. pending, in_progress, complete [default: [\"complete\"]] (multiple values allowed) --environments=ENVIRONMENTS The environment IDs to include [default: [\"*\"]] (multiple values allowed) --excluded-environments=EXCLUDED-ENVIRONMENTS The environment IDs to exclude (multiple values allowed) --from-address=FROM-ADDRESS The From address for alert emails [default: \"noreply@platform.sh\"] --recipients=RECIPIENTS The recipient email address(es) (multiple values allowed) --channel=CHANNEL The Slack channel --routing-key=ROUTING-KEY The PagerDuty routing key -p, --project=PROJECT The project ID or URL --host=HOST The project's API hostname -W, --no-wait Do not wait for the operation to complete --wait Wait for the operation to complete (default) -h, --help Display this help message -q, --quiet Do not output any message -V, --version Display this application version -y, --yes Answer \"yes\" to all prompts; disable interaction -n, --no Answer \"no\" to all prompts -v|vv|vvv, --verbose Increase the verbosity of messages Examples: Switch on the \"fetch branches\" option for a specific integration: platform integration:update ZXhhbXBsZSB --fetch-branches 1 Conclusion The CLI is a very handy tool for listing information about integrations, also for adding new ones (or modifying those already existing).",
        "text": "Goal To learn how to use the CLI for getting more information on a Git integration, also how to modify certain configuration options. Assumptions Access to a project hosted on http://platform.sh/ Your project account has administrator rights Knowledge on using the Problems None in particular. This How to is about raising awareness to the usefulness of the CLI tool. Steps 1. Check what “integration” commands are available platform list | grep integration integration integration:add Add an integration to the project integration:delete Delete an integration from a project integration:get View details of an integration integration:list (integrations) View a list of project integration(s) integration:update Update an integration integration:validate Validate an existing integration 2. List the integrations added to your project platform integration:list +------------------+--------+-----------------------------------------------------------------------------------------------------+ | ID | Type | Summary | +------------------+--------+-----------------------------------------------------------------------------------------------------+ | | github | Repository: ... | | | | Hook URL: https://.platform.sh/api/projects/ /integrations/ /hook | +------------------+--------+-----------------------------------------------------------------------------------------------------+ View integration details with: platform integration:get [id] Add a new integration with: platform integration:add Delete an integration with: platform integration:delete [id] 3. Get the integration details platform integration:get +---------------------------------+-------------------------------------------------------------------------------------------+ | Property | Value | +---------------------------------+-------------------------------------------------------------------------------------------+ | id | | | type | github | | token | ****** | | repository | ... | | fetch_branches | true | | prune_branches | true | | build_pull_requests | true | | build_pull_requests_post_merge | false | | pull_requests_clone_parent_data | true | | hook_url | https://.platform.sh/api/projects/ /integrations/ /hook | +---------------------------------+-------------------------------------------------------------------------------------------+ 4. Check the options and select which ones to modify platform integration:update --help Command: integration:update Description: Update an integration Usage: platform integration:update [--type TYPE] [--token TOKEN] [--key KEY] [--secret SECRET] [--base-url BASE-URL] [--gitlab-project GITLAB-PROJECT] [--repository REPOSITORY] [--build-merge-requests BUILD-MERGE-REQUESTS] [--build-pu Arguments: id The ID of the integration to update Options: --type=TYPE The integration type ('bitbucket', 'github', 'gitlab', 'hipchat', 'webhook', 'health.email', 'health.pagerduty', 'health.slack') --token=TOKEN An OAuth token for the integration --key=KEY A Bitbucket OAuth consumer key --secret=SECRET A Bitbucket OAuth consumer secret --base-url=BASE-URL The base URL of the GitLab installation --gitlab-project=GITLAB-PROJECT The GitLab project (e.g. 'namespace/repo') --repository=REPOSITORY The repository to track (e.g. 'user/repo') --build-merge-requests=BUILD-MERGE-REQUESTS GitLab: build merge requests as environments [default: true] --build-pull-requests=BUILD-PULL-REQUESTS Build every pull request as an environment [default: true] --build-pull-requests-post-merge=BUILD-PULL-REQUESTS-POST-MERGE Build pull requests based on their post-merge state [default: false] --merge-requests-clone-parent-data=MERGE-REQUESTS-CLONE-PARENT-DATA GitLab: clone data for merge requests [default: true] --pull-requests-clone-parent-data=PULL-REQUESTS-CLONE-PARENT-DATA Clone the parent environment's data for pull requests [default: true] --resync-pull-requests=RESYNC-PULL-REQUESTS Re-sync pull request environment data on every build [default: false] --fetch-branches=FETCH-BRANCHES Fetch all branches from the remote (as inactive environments) [default: true] --prune-branches=PRUNE-BRANCHES Delete branches that do not exist on the remote [default: true] --room=ROOM HipChat room ID --url=URL Generic webhook: a URL to receive JSON data --events=EVENTS A list of events to report, e.g. environment.push [default: [\"*\"]] (multiple values allowed) --states=STATES A list of states to report, e.g. pending, in_progress, complete [default: [\"complete\"]] (multiple values allowed) --environments=ENVIRONMENTS The environment IDs to include [default: [\"*\"]] (multiple values allowed) --excluded-environments=EXCLUDED-ENVIRONMENTS The environment IDs to exclude (multiple values allowed) --from-address=FROM-ADDRESS The From address for alert emails [default: \"noreply@platform.sh\"] --recipients=RECIPIENTS The recipient email address(es) (multiple values allowed) --channel=CHANNEL The Slack channel --routing-key=ROUTING-KEY The PagerDuty routing key -p, --project=PROJECT The project ID or URL --host=HOST The project's API hostname -W, --no-wait Do not wait for the operation to complete --wait Wait for the operation to complete (default) -h, --help Display this help message -q, --quiet Do not output any message -V, --version Display this application version -y, --yes Answer \"yes\" to all prompts; disable interaction -n, --no Answer \"no\" to all prompts -v|vv|vvv, --verbose Increase the verbosity of messages Examples: Switch on the \"fetch branches\" option for a specific integration: platform integration:update ZXhhbXBsZSB --fetch-branches 1 Conclusion The CLI is a very handy tool for listing information about integrations, also for adding new ones (or modifying those already existing).",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-utilize-the-cli-to-modify-already-existing-git-integrations/144",
        "relurl": "/t/how-to-utilize-the-cli-to-modify-already-existing-git-integrations/144"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "bb81b905b1bbdd5aec2b1e607c01ecbbd93de02b",
        "title": "How to run Minio on Platform.sh",
        "description": "Goal https://minio.io/ offers cloud file storage with an S3 compatible API. This guide shows how to deploy it on http://platform.sh/. Assumptions To complete this, you will need: An empty http://platform.sh/ project The https://docs.platform.sh/gettingstarted/cli.html tool installed Your ssh key loaded in your https://linux.die.net/man/1/ssh-agent and configured in the https://accounts.platform.sh/user Steps 1. Create the application directory Create an empty directory and cd into it. mkdir minio cd minio Initialize git and set the platform Git remote: git init platform project:set-remote 2. Configure the application Minio will be configured to store its configuration and files in . Edit .platform/app.yaml to have the following contents: name: app # Any type will work here type: \"nodejs:10\" hooks: build: | set -e wget https://dl.minio.io/server/minio/release/linux-amd64/minio chmod +x minio mounts: 'minio_data': source: local source_path: minio_data_dir 'minio_config': source: local source_path: minio_config_dir web: upstream: socket_family: tcp protocol: http commands: start: | ./minio server /app/minio_data --address localhost:$PORT --config-dir /app/minio_config --certs-dir /app/minio_config/certs variables: env: MINIO_ACCESS_KEY: access MINIO_SECRET_KEY: changeme disk: 2048 Change the MINIO_ACCESS_KEY and MINIO_SECRET_KEY values as required. Modify the disk value based on storage requirements, it should be at least 2048. Define a route in .platform/routes.yaml : \"https://{default}/\": type: upstream upstream: \"app:http\" Add an empty .platform/services.yaml file: touch .platform/services.yaml 3. Add, commit and push: git add . git commit -m \"Minio configured\" git push platform master 4. Test by visiting the URL of your project: platform url The Minio web UI page will be displayed and can be logged in with the MINIO_ACCESS_KEY and MINIO_SECRET_KEY values specified in .platform.app.yaml previously. https://community.platform.sh/uploads/default/e917eef889ae00fcf9be73d28609aad9c14edacc Conclusion The Minio server and web UI are running and ready to handle file requests.",
        "text": "Goal https://minio.io/ offers cloud file storage with an S3 compatible API. This guide shows how to deploy it on http://platform.sh/. Assumptions To complete this, you will need: An empty http://platform.sh/ project The https://docs.platform.sh/gettingstarted/cli.html tool installed Your ssh key loaded in your https://linux.die.net/man/1/ssh-agent and configured in the https://accounts.platform.sh/user Steps 1. Create the application directory Create an empty directory and cd into it. mkdir minio cd minio Initialize git and set the platform Git remote: git init platform project:set-remote 2. Configure the application Minio will be configured to store its configuration and files in . Edit .platform/app.yaml to have the following contents: name: app # Any type will work here type: \"nodejs:10\" hooks: build: | set -e wget https://dl.minio.io/server/minio/release/linux-amd64/minio chmod +x minio mounts: 'minio_data': source: local source_path: minio_data_dir 'minio_config': source: local source_path: minio_config_dir web: upstream: socket_family: tcp protocol: http commands: start: | ./minio server /app/minio_data --address localhost:$PORT --config-dir /app/minio_config --certs-dir /app/minio_config/certs variables: env: MINIO_ACCESS_KEY: access MINIO_SECRET_KEY: changeme disk: 2048 Change the MINIO_ACCESS_KEY and MINIO_SECRET_KEY values as required. Modify the disk value based on storage requirements, it should be at least 2048. Define a route in .platform/routes.yaml : \"https://{default}/\": type: upstream upstream: \"app:http\" Add an empty .platform/services.yaml file: touch .platform/services.yaml 3. Add, commit and push: git add . git commit -m \"Minio configured\" git push platform master 4. Test by visiting the URL of your project: platform url The Minio web UI page will be displayed and can be logged in with the MINIO_ACCESS_KEY and MINIO_SECRET_KEY values specified in .platform.app.yaml previously. https://community.platform.sh/uploads/default/e917eef889ae00fcf9be73d28609aad9c14edacc Conclusion The Minio server and web UI are running and ready to handle file requests.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-run-minio-on-platform-sh/128",
        "relurl": "/t/how-to-run-minio-on-platform-sh/128"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "6cea8d5953bb07afa63e44b5ed9fe79c9d7610a2",
        "title": "How to rely on a PKI (Public Key Infrastructure) on Platform.sh",
        "description": "Goal Using https://en.wikipedia.org/wiki/Public_key_infrastructure on your http://Platform.sh project for authentication. Background PKI is used to: centralize the authentication process automatically provide new accounts for authentication avoid a dependency on a centralized authentication server which might not be available at all times automatically expire accounts for authentication after a given period (unless they refresh) PKI starts with the root certificate authority who keeps a private key and a certificate. The certificate is public, and can be used to verify users’ certificates. When a new user (automated or not) needs a new certificate, they: generate a private key generate a Certificate Signing Request (CSR) that is sent to the root certificate authority The root certificate authority receives the CSR, validates that the user is valid, and returns a signed certificate to the user. The user can then use this signed certificate to authenticate. When a server wants to authenticate a certificate, it uses the root certificate authority’s certificate to verify the signature in the user’s certificate. This verification can be done on http://Platform.sh, which is what this How-To will show. Assumptions You will need: A local git repository that has the http://Platform.sh project as a git remote A working application setup on the http://Platform.sh project openssl Steps 1. Create a PKI Create the certificate authority private key and certificate Create the private key that the certificate authority must keep private. $ openssl genrsa -out /tmp/rootCA.key 4096 Generating RSA private key, 4096 bit long modulus .....++ .................++ $ The key will look similar to this: $ head -n2 /tmp/rootCA.key -----BEGIN RSA PRIVATE KEY----- MIIJJwIBAAKCAgEAyxCraEAvZUnGZ1OVAynDVCjp0qGZvI0nHPwHBxYH5eJ96pB6 $ Based on this, create the public certificate: $ openssl req -x509 -new -nodes -key /tmp/rootCA.key -sha256 -days 365 -out /tmp/rootCA.crt Pass the previously-created key. openssl will ask a few questions, they’re not important for the purpose of this How-To. The public certificate will look similar to this: $ head -n2 /tmp/rootCA.crt -----BEGIN CERTIFICATE----- MIIFXTCCA0WgAwIBAgIJAPs0rURM8vWtMA0GCSqGSIb3DQEBCwUAMEUxCzAJBgNV $ Create the first user’s private key and CSR Now that the certificate authority is setup, create a user’s private key and CSR. This creates the private key: $ openssl genrsa -out /tmp/user.key 4096 Generating RSA private key, 4096 bit long modulus ...........................++ .++ $ And this creates the CSR: $ openssl req -new -key /tmp/user.key -out /tmp/user.csr openssl will ask again some questions that are not important for the purpose of this How-To. Create the first user’s public certificate Based on the explanation above, use the root certificate authority’s private key to sign the CSR, which creates a new public certificate for the user. $ openssl x509 -req -in /tmp/user.csr -CA /tmp/rootCA.crt -CAkey /tmp/rootCA.key \\ -CAcreateserial -out /tmp/user.crt -days 30 -sha256 This creates a public certificate that the user can use, valid for 30 days. Note that only the user can use this certificate to authenticate, because it needs the private key (“user.key” above). Et voila! We now have a PKI with one user. 2. Authenticate users of the PKI on http://Platform.sh We’re now getting to the easy part of this How-To: we’re going to authenticate our user on http://Platform.sh using . For this test, we’re going to need 3 things: The root certificate authority’s public certificate (rootCA.crt above) The user’s private key (user.key above) The user’s public certificate (user.crt above) The first step is configuring your http://Platform.sh project to authenticate users by providing the root certificate authority’s public certificate, as such in the .platform/routes.yaml: https://{default}/: type: upstream upstream: app:http tls: client_authentication: \"require\" client_certificate_authorities: - !include type: string path: rootCA.crt Put the rootCA.crt file in the .platform/ folder. Commit and push these changes: $ git add .platform/routes.yaml .platform/rootCA.crt $ git commit -m \"Enable TLS client certificates authentication\" $ git push platform master And that’s it! Test the changes: $ curl https://my-project.com curl: (35) error:14094412:SSL routines:ssl3_read_bytes:sslv3 alert bad certificate $ curl --key user.key --cert user.crt https://my-project.com It's working! Conclusion Setting up a http://Platform.sh project to authenticate users using PKI requires only a few lines of code added to the .platform/routes.yaml file.",
        "text": "Goal Using https://en.wikipedia.org/wiki/Public_key_infrastructure on your http://Platform.sh project for authentication. Background PKI is used to: centralize the authentication process automatically provide new accounts for authentication avoid a dependency on a centralized authentication server which might not be available at all times automatically expire accounts for authentication after a given period (unless they refresh) PKI starts with the root certificate authority who keeps a private key and a certificate. The certificate is public, and can be used to verify users’ certificates. When a new user (automated or not) needs a new certificate, they: generate a private key generate a Certificate Signing Request (CSR) that is sent to the root certificate authority The root certificate authority receives the CSR, validates that the user is valid, and returns a signed certificate to the user. The user can then use this signed certificate to authenticate. When a server wants to authenticate a certificate, it uses the root certificate authority’s certificate to verify the signature in the user’s certificate. This verification can be done on http://Platform.sh, which is what this How-To will show. Assumptions You will need: A local git repository that has the http://Platform.sh project as a git remote A working application setup on the http://Platform.sh project openssl Steps 1. Create a PKI Create the certificate authority private key and certificate Create the private key that the certificate authority must keep private. $ openssl genrsa -out /tmp/rootCA.key 4096 Generating RSA private key, 4096 bit long modulus .....++ .................++ $ The key will look similar to this: $ head -n2 /tmp/rootCA.key -----BEGIN RSA PRIVATE KEY----- MIIJJwIBAAKCAgEAyxCraEAvZUnGZ1OVAynDVCjp0qGZvI0nHPwHBxYH5eJ96pB6 $ Based on this, create the public certificate: $ openssl req -x509 -new -nodes -key /tmp/rootCA.key -sha256 -days 365 -out /tmp/rootCA.crt Pass the previously-created key. openssl will ask a few questions, they’re not important for the purpose of this How-To. The public certificate will look similar to this: $ head -n2 /tmp/rootCA.crt -----BEGIN CERTIFICATE----- MIIFXTCCA0WgAwIBAgIJAPs0rURM8vWtMA0GCSqGSIb3DQEBCwUAMEUxCzAJBgNV $ Create the first user’s private key and CSR Now that the certificate authority is setup, create a user’s private key and CSR. This creates the private key: $ openssl genrsa -out /tmp/user.key 4096 Generating RSA private key, 4096 bit long modulus ...........................++ .++ $ And this creates the CSR: $ openssl req -new -key /tmp/user.key -out /tmp/user.csr openssl will ask again some questions that are not important for the purpose of this How-To. Create the first user’s public certificate Based on the explanation above, use the root certificate authority’s private key to sign the CSR, which creates a new public certificate for the user. $ openssl x509 -req -in /tmp/user.csr -CA /tmp/rootCA.crt -CAkey /tmp/rootCA.key \\ -CAcreateserial -out /tmp/user.crt -days 30 -sha256 This creates a public certificate that the user can use, valid for 30 days. Note that only the user can use this certificate to authenticate, because it needs the private key (“user.key” above). Et voila! We now have a PKI with one user. 2. Authenticate users of the PKI on http://Platform.sh We’re now getting to the easy part of this How-To: we’re going to authenticate our user on http://Platform.sh using . For this test, we’re going to need 3 things: The root certificate authority’s public certificate (rootCA.crt above) The user’s private key (user.key above) The user’s public certificate (user.crt above) The first step is configuring your http://Platform.sh project to authenticate users by providing the root certificate authority’s public certificate, as such in the .platform/routes.yaml: https://{default}/: type: upstream upstream: app:http tls: client_authentication: \"require\" client_certificate_authorities: - !include type: string path: rootCA.crt Put the rootCA.crt file in the .platform/ folder. Commit and push these changes: $ git add .platform/routes.yaml .platform/rootCA.crt $ git commit -m \"Enable TLS client certificates authentication\" $ git push platform master And that’s it! Test the changes: $ curl https://my-project.com curl: (35) error:14094412:SSL routines:ssl3_read_bytes:sslv3 alert bad certificate $ curl --key user.key --cert user.crt https://my-project.com It's working! Conclusion Setting up a http://Platform.sh project to authenticate users using PKI requires only a few lines of code added to the .platform/routes.yaml file.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-rely-on-a-pki-public-key-infrastructure-on-platform-sh/80",
        "relurl": "/t/how-to-rely-on-a-pki-public-key-infrastructure-on-platform-sh/80"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "8cbb0fa5fc43892359e5bfea9eae975b7f58277f",
        "title": "How to run a Rust web application on Platform.sh",
        "description": "Goal This guide shows how to deploy a simple Rust web app on http://Platform.sh. Assumptions To complete this, you will need: An empty http://Platform.sh project The https://docs.platform.sh/gettingstarted/cli.html tool installed Your ssh key loaded in your https://linux.die.net/man/1/ssh-agent and configured in the https://accounts.platform.sh/user The Rust environment installed Problems http://Platform.sh does not provide a dedicated Rust runtime container. It will thus be necessary to install Rust as the project is built. Steps 1. Create a Rust application Create a new Rust project: cargo new platform_howto \u0026\u0026 cd platform_howto Set the platform remote: platform project:set-remote Add the following under [dependencies] in Cargo.toml: warp = \"0.1\" clap = \"2.32.0\" Replace src/main.rs with: #[macro_use] extern crate clap; use clap::{Arg, App}; use warp::Filter; fn main() { let args = App::new(\"HOWTO\") .version(\"1.0\") .author(\"Platform.sh \") .about(\"How to run a Rust app on Platform.sh\") .arg(Arg::with_name(\"port\") .short(\"p\") .long(\"port\") .value_name(\"PORT\") .help(\"Sets a custom port\") .takes_value(true) .required(true)) .get_matches(); // Get port from command-line arguments let port = value_t!(args, \"port\", u16).unwrap_or_else(|e| e.exit()); // Match any request and return hello world! let routes = warp::any().map(|| \"Hello, World!\"); warp::serve(routes) .run(([127, 0, 0, 1], port)); } 2. Configure the application to run on http://Platform.sh: Add the following to .platform.app.yaml: name: app # Any type will work here type: \"php:7.3\" hooks: build: | set -e curl https://sh.rustup.rs rustup.sh sh rustup.sh -y --default-toolchain stable rm rustup.sh . \"$HOME/.cargo/env\" cargo build --release web: upstream: socket_family: tcp protocol: http commands: start: | target/release/platform_howto --port $PORT disk: 1024 Define a route in .platform/routes.yaml: \"https://{default}/\": type: upstream upstream: \"app:http\" Add an empty .platform/services.yaml file: touch .platform/services.yaml 3. Add, commit and push: git add . git commit -m \"Rust on Platform.sh\" git push platform master 4. Test by visiting the URL of your project: platform url Which should open a browser tab showing Hello, World! Conclusion Even without a dedicated runtime, running a Rust application on http://Platform.sh can be done by leveraging the build hook and the PORT variable.",
        "text": "Goal This guide shows how to deploy a simple Rust web app on http://Platform.sh. Assumptions To complete this, you will need: An empty http://Platform.sh project The https://docs.platform.sh/gettingstarted/cli.html tool installed Your ssh key loaded in your https://linux.die.net/man/1/ssh-agent and configured in the https://accounts.platform.sh/user The Rust environment installed Problems http://Platform.sh does not provide a dedicated Rust runtime container. It will thus be necessary to install Rust as the project is built. Steps 1. Create a Rust application Create a new Rust project: cargo new platform_howto \u0026\u0026 cd platform_howto Set the platform remote: platform project:set-remote Add the following under [dependencies] in Cargo.toml: warp = \"0.1\" clap = \"2.32.0\" Replace src/main.rs with: #[macro_use] extern crate clap; use clap::{Arg, App}; use warp::Filter; fn main() { let args = App::new(\"HOWTO\") .version(\"1.0\") .author(\"Platform.sh \") .about(\"How to run a Rust app on Platform.sh\") .arg(Arg::with_name(\"port\") .short(\"p\") .long(\"port\") .value_name(\"PORT\") .help(\"Sets a custom port\") .takes_value(true) .required(true)) .get_matches(); // Get port from command-line arguments let port = value_t!(args, \"port\", u16).unwrap_or_else(|e| e.exit()); // Match any request and return hello world! let routes = warp::any().map(|| \"Hello, World!\"); warp::serve(routes) .run(([127, 0, 0, 1], port)); } 2. Configure the application to run on http://Platform.sh: Add the following to .platform.app.yaml: name: app # Any type will work here type: \"php:7.3\" hooks: build: | set -e curl https://sh.rustup.rs rustup.sh sh rustup.sh -y --default-toolchain stable rm rustup.sh . \"$HOME/.cargo/env\" cargo build --release web: upstream: socket_family: tcp protocol: http commands: start: | target/release/platform_howto --port $PORT disk: 1024 Define a route in .platform/routes.yaml: \"https://{default}/\": type: upstream upstream: \"app:http\" Add an empty .platform/services.yaml file: touch .platform/services.yaml 3. Add, commit and push: git add . git commit -m \"Rust on Platform.sh\" git push platform master 4. Test by visiting the URL of your project: platform url Which should open a browser tab showing Hello, World! Conclusion Even without a dedicated runtime, running a Rust application on http://Platform.sh can be done by leveraging the build hook and the PORT variable.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-run-a-rust-web-application-on-platform-sh/119",
        "relurl": "/t/how-to-run-a-rust-web-application-on-platform-sh/119"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "a42d531316a10112c87ac90bce3b63c84b5afca6",
        "title": "How to configure HTTP Strict Transport Security (HSTS) on your project",
        "description": "Goal Enable HSTS on your http://Platform.sh project. Read more about HSTS on https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security. Assumptions You will need: A local git repository that has the http://platform.sh/ project as a git remote A working application setup on the http://Platform.sh project This how-to works whether you use the auto-generated domains on platformsh.site or your own custom domain. Steps 1. Check https is enabled The ./.platform/routes.yaml should contain only the https directive: https://{default}/: type: upstream upstream: app:http 2. Add the strict_transport_security configuration item to your route file: ./.platform/routes.yaml https://{default}/: type: upstream upstream: app:http tls: strict_transport_security: enabled: true include_subdomains: true preload: true Please refer to the for details about the different parameters. 3. Commit and deploy the change git add .platform/routes.yaml git commit -m \"Enable HSTS\" git push platform master 4. Check HSTS is enabled Check the response headers being sent by the server. This can checked with the browser console or curl: $ curl -I https://master-7rqtwti- ..platformsh.site HTTP/2 200 ... strict-transport-security: max-age=31536000; includeSubDomains; preload ... Any request to the http endpoint will be upgraded to https: $ curl -I http://master-7rqtwti- ..platformsh.site HTTP/1.1 301 Moved Permanently ... Location: https://master-7rqtwti- ..platformsh.site/ Strict-Transport-Security: max-age=0 ... Conclusion As http://Platform.sh configuration already includes the strict_transport_security parameter, enabling HSTS was a simple configuration change without the need to customize the response directly in your application.",
        "text": "Goal Enable HSTS on your http://Platform.sh project. Read more about HSTS on https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security. Assumptions You will need: A local git repository that has the http://platform.sh/ project as a git remote A working application setup on the http://Platform.sh project This how-to works whether you use the auto-generated domains on platformsh.site or your own custom domain. Steps 1. Check https is enabled The ./.platform/routes.yaml should contain only the https directive: https://{default}/: type: upstream upstream: app:http 2. Add the strict_transport_security configuration item to your route file: ./.platform/routes.yaml https://{default}/: type: upstream upstream: app:http tls: strict_transport_security: enabled: true include_subdomains: true preload: true Please refer to the for details about the different parameters. 3. Commit and deploy the change git add .platform/routes.yaml git commit -m \"Enable HSTS\" git push platform master 4. Check HSTS is enabled Check the response headers being sent by the server. This can checked with the browser console or curl: $ curl -I https://master-7rqtwti- ..platformsh.site HTTP/2 200 ... strict-transport-security: max-age=31536000; includeSubDomains; preload ... Any request to the http endpoint will be upgraded to https: $ curl -I http://master-7rqtwti- ..platformsh.site HTTP/1.1 301 Moved Permanently ... Location: https://master-7rqtwti- ..platformsh.site/ Strict-Transport-Security: max-age=0 ... Conclusion As http://Platform.sh configuration already includes the strict_transport_security parameter, enabling HSTS was a simple configuration change without the need to customize the response directly in your application.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-configure-http-strict-transport-security-hsts-on-your-project/70",
        "relurl": "/t/how-to-configure-http-strict-transport-security-hsts-on-your-project/70"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "fc3faea1b6b875e3332761ea52b3ff3fbbf53117",
        "title": "How to upload and download files to your application (SFTP/RSYNC)",
        "description": "Goal This guide shows how to upload and download files to your application using sftp and rsync. Assumptions To complete this, you will need: A working application setup on a http://Platform.sh project The https://docs.platform.sh/gettingstarted/cli.html tool installed Your ssh key loaded in your https://linux.die.net/man/1/ssh-agent and configured in the https://accounts.platform.sh/user rsync installed on your system or scp installed on your system or an sftp GUI client like https://filezilla-project.org/ Problems http://Platform.sh provides but it can be handy to use native tools instead. Steps Add a mount point to your application By default, all files deployed to http://Platform.sh through git are read-only. Only the folders defined as mounts are writeable. Open ./.platform.app.yaml and add a mount: mounts: 'web/uploads': source: local source_path: uploads Please refer to the if needed. Push your configuration change: git add .platform.app.yaml git commit -m \"Add mount\" git push platform master Check the mount point is writeable platform ssh web@ :~$ touch web/uploads/test.txt The command above shouldn’t trigger any error. Get the environment ssh endpoint Both rsync and sftp protocols use ssh as the transfer layer. Each http://Platform.sh environment inside a project has its own ssh endpoint. If you have the http://Platform.sh CLI installed, you can run platform ssh --pipe to get the endpoint. If not, log into your project dashboard and click on the SSH button for the environment you want to get the endpoint: Our endpoint is -master-7rqtwti--app@ssh..platform.sh Note: All the next commands will be using platform ssh --pipe as the endpoint. Feel free to replace with the full connection string if needed. Create test files Let’s create some files to play with: mkdir -p web/uploads touch web/uploads/test{0001..0010}.txt rsync Upload a file or a directory To upload a directory, make sure you don’t specify the source folder (uploads in this case) in the destination. Take a look at man rsync to view all available options. rsync -avz web/uploads \"$(platform ssh --pipe)\":web/ The log should list all files that were uploaded: sending incremental file list uploads/ uploads/test0001.txt ... uploads/test0010.txt sent 603 bytes received 210 bytes 325.20 bytes/sec total size is 0 speedup is 0.00 We could have synced only the files with the following command: rsync -avz web/uploads/* rsync -avz web/uploads \"$(platform ssh --pipe)\":web/uploads/ As rsync is transferring only the differences between the source and destination, relaunching the same command will end up in an empty transfer. Download a file or a directory Remove your local files: rm web/uploads/* We can now download them from http://Platform.sh: rsync -avz \"$(platform ssh --pipe)\":web/uploads web/ You can see in the output that all files are transferred back to our host. scp Remove the files on http://Platform.sh if you have followed the rsync steps: platform ssh \"rm web/uploads/*\" Upload a file or a directory To upload a file, specify the full path: scp web/uploads/test0001.txt \"$(platform ssh --pipe)\":web/uploads To upload a directory, add the -r argument: scp -r web/uploads \"$(platform ssh --pipe)\":web/ The output should list the 10 files being transferred. Note that scp is not using incremental transfer. All 10 files are being transferred even if they already exist at the destination. Download a file or a directory Remove a local file first: rm web/uploads/test0001.txt To download a file: scp \"$(platform ssh --pipe)\":web/uploads/test0001.txt web/uploads/ To download a directory: scp -r \"$(platform ssh --pipe)\":web/uploads web/ sftp Remove the files on http://Platform.sh if you have followed the previous steps: platform ssh \"rm web/uploads/*\" Configure your client Add a new bookmark to your client with the following configuration (based on our endpoint): hostname: ssh.eu-3.platform.sh port: 22 protocol: SFTP user: -master-7rqtwti--app key/identity file: `/path/to/your/key Filezilla needs to have a .pem key file to list it. Connect to the site. You should be presented with the content of the root folder of your app. Upload a file or a directory Drag and drop files and folders from your computer to the mount: Download a file or a directory Drag and drop files and folders from the mount to your computer. Easy enough. Conclusion Transferring and managing files on your http://Platform.sh environments can be done using native tools that use ssh, scp, rsync, or sftp.",
        "text": "Goal This guide shows how to upload and download files to your application using sftp and rsync. Assumptions To complete this, you will need: A working application setup on a http://Platform.sh project The https://docs.platform.sh/gettingstarted/cli.html tool installed Your ssh key loaded in your https://linux.die.net/man/1/ssh-agent and configured in the https://accounts.platform.sh/user rsync installed on your system or scp installed on your system or an sftp GUI client like https://filezilla-project.org/ Problems http://Platform.sh provides but it can be handy to use native tools instead. Steps Add a mount point to your application By default, all files deployed to http://Platform.sh through git are read-only. Only the folders defined as mounts are writeable. Open ./.platform.app.yaml and add a mount: mounts: 'web/uploads': source: local source_path: uploads Please refer to the if needed. Push your configuration change: git add .platform.app.yaml git commit -m \"Add mount\" git push platform master Check the mount point is writeable platform ssh web@ :~$ touch web/uploads/test.txt The command above shouldn’t trigger any error. Get the environment ssh endpoint Both rsync and sftp protocols use ssh as the transfer layer. Each http://Platform.sh environment inside a project has its own ssh endpoint. If you have the http://Platform.sh CLI installed, you can run platform ssh --pipe to get the endpoint. If not, log into your project dashboard and click on the SSH button for the environment you want to get the endpoint: Our endpoint is -master-7rqtwti--app@ssh..platform.sh Note: All the next commands will be using platform ssh --pipe as the endpoint. Feel free to replace with the full connection string if needed. Create test files Let’s create some files to play with: mkdir -p web/uploads touch web/uploads/test{0001..0010}.txt rsync Upload a file or a directory To upload a directory, make sure you don’t specify the source folder (uploads in this case) in the destination. Take a look at man rsync to view all available options. rsync -avz web/uploads \"$(platform ssh --pipe)\":web/ The log should list all files that were uploaded: sending incremental file list uploads/ uploads/test0001.txt ... uploads/test0010.txt sent 603 bytes received 210 bytes 325.20 bytes/sec total size is 0 speedup is 0.00 We could have synced only the files with the following command: rsync -avz web/uploads/* rsync -avz web/uploads \"$(platform ssh --pipe)\":web/uploads/ As rsync is transferring only the differences between the source and destination, relaunching the same command will end up in an empty transfer. Download a file or a directory Remove your local files: rm web/uploads/* We can now download them from http://Platform.sh: rsync -avz \"$(platform ssh --pipe)\":web/uploads web/ You can see in the output that all files are transferred back to our host. scp Remove the files on http://Platform.sh if you have followed the rsync steps: platform ssh \"rm web/uploads/*\" Upload a file or a directory To upload a file, specify the full path: scp web/uploads/test0001.txt \"$(platform ssh --pipe)\":web/uploads To upload a directory, add the -r argument: scp -r web/uploads \"$(platform ssh --pipe)\":web/ The output should list the 10 files being transferred. Note that scp is not using incremental transfer. All 10 files are being transferred even if they already exist at the destination. Download a file or a directory Remove a local file first: rm web/uploads/test0001.txt To download a file: scp \"$(platform ssh --pipe)\":web/uploads/test0001.txt web/uploads/ To download a directory: scp -r \"$(platform ssh --pipe)\":web/uploads web/ sftp Remove the files on http://Platform.sh if you have followed the previous steps: platform ssh \"rm web/uploads/*\" Configure your client Add a new bookmark to your client with the following configuration (based on our endpoint): hostname: ssh.eu-3.platform.sh port: 22 protocol: SFTP user: -master-7rqtwti--app key/identity file: `/path/to/your/key Filezilla needs to have a .pem key file to list it. Connect to the site. You should be presented with the content of the root folder of your app. Upload a file or a directory Drag and drop files and folders from your computer to the mount: Download a file or a directory Drag and drop files and folders from the mount to your computer. Easy enough. Conclusion Transferring and managing files on your http://Platform.sh environments can be done using native tools that use ssh, scp, rsync, or sftp.",
        "section": "How-to Guides",
        "subsections": "How-to Guides",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-upload-and-download-files-to-your-application-sftp-rsync/72",
        "relurl": "/t/how-to-upload-and-download-files-to-your-application-sftp-rsync/72"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "9781993d4b038bb106ce45c6dc4384b2d24376c3",
        "title": "Is it possible to kill a build or deploy?",
        "description": "Sometimes I push a commit and the build/deploy either takes a very long time, or it becomes permanently stuck. Am I able to do something to kill the process?",
        "text": "Sometimes I push a commit and the build/deploy either takes a very long time, or it becomes permanently stuck. Am I able to do something to kill the process?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-it-possible-to-kill-a-build-or-deploy/156",
        "relurl": "/t/is-it-possible-to-kill-a-build-or-deploy/156"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "0b1a00d9d63732a668c531cb1adb20cf109b810e",
        "title": "Using a custom certificate and LetsEncrypt certificates on the same project?",
        "description": "Is it possible to use a custom SSL certificate just for a specific domain, and use the autogenerated one for all other domains on the same project?",
        "text": "Is it possible to use a custom SSL certificate just for a specific domain, and use the autogenerated one for all other domains on the same project?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/using-a-custom-certificate-and-letsencrypt-certificates-on-the-same-project/361",
        "relurl": "/t/using-a-custom-certificate-and-letsencrypt-certificates-on-the-same-project/361"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "f01b22d27b2aacc6390703982cb9f05032d1c025",
        "title": "What does \"WARNING: [pool web] server reached max_children setting (2), consider raising it\" mean?",
        "description": "I keep seeing this in my site logs. Does this mean my site is down? How do I raise the max_children setting?",
        "text": "I keep seeing this in my site logs. Does this mean my site is down? How do I raise the max_children setting?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/what-does-warning-pool-web-server-reached-max-children-setting-2-consider-raising-it-mean/240",
        "relurl": "/t/what-does-warning-pool-web-server-reached-max-children-setting-2-consider-raising-it-mean/240"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "25319943a084d07d2f9f057e343d1d98677b06e7",
        "title": "Can I use Platform.sh for a different language?",
        "description": "We can use PHP, NodeJS, Go, and more. But my favorite language is not there. How do I add a new language (e.g. Erlang, Elixir, …) ?",
        "text": "We can use PHP, NodeJS, Go, and more. But my favorite language is not there. How do I add a new language (e.g. Erlang, Elixir, …) ?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-i-use-platform-sh-for-a-different-language/237",
        "relurl": "/t/can-i-use-platform-sh-for-a-different-language/237"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "78fb7e1961e72278eea0aab0f79464f825b8359c",
        "title": "How can I forward my logs to Splunk or Logz.io?",
        "description": "I’d like to integrate my log monitoring across my organization, including applications not hosted on http://Platform.sh.",
        "text": "I’d like to integrate my log monitoring across my organization, including applications not hosted on http://Platform.sh.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-forward-my-logs-to-splunk-or-logz-io/166",
        "relurl": "/t/how-can-i-forward-my-logs-to-splunk-or-logz-io/166"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "c45b8bac2e17d4aa2e45a5aa5ad10c8d77969e38",
        "title": "My plan has 10GB of storage. How can I see precisely how much I am using?",
        "description": "I know that some is used by my services, and some is used by the disk directive in .platform.app.yaml. Is there a way to see how much is being used from the 10GB total that my project has?",
        "text": "I know that some is used by my services, and some is used by the disk directive in .platform.app.yaml. Is there a way to see how much is being used from the 10GB total that my project has?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/my-plan-has-10gb-of-storage-how-can-i-see-precisely-how-much-i-am-using/158",
        "relurl": "/t/my-plan-has-10gb-of-storage-how-can-i-see-precisely-how-much-i-am-using/158"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "43d18bd84360b0a684103f2476382ab3c31bf3bd",
        "title": "Using platform.sh environments for continuous integration / functional tests",
        "description": "What would the pros / cons of using platform to run integration / functional tests, instead of a typical Continuous Integration service? And is using platform to run functional tests possible / a bad idea? My thinking is this: Unit tests, static analysis, code linting etc. are easy to run in isolation, so that should remain in a typical CI environment Functional tests require a functioning environment, which http://platform.sh can automatically provide Sometimes you want to run your functional tests against production data, which is also something http://platform.sh can automatically provide An example implementation could be: On build, detect if this is an environment you want to run tests in (eg. non-production, pull request environment etc.) Install any tools you need to run functional tests (eg. a headless browser and testing framework) Run the tests – fail the build if the tests fail Send notification of the passed/failed build (I think the standard github pull request integration would report this back, but I’m not sure – an email or slack notification could be sent) ",
        "text": "What would the pros / cons of using platform to run integration / functional tests, instead of a typical Continuous Integration service? And is using platform to run functional tests possible / a bad idea? My thinking is this: Unit tests, static analysis, code linting etc. are easy to run in isolation, so that should remain in a typical CI environment Functional tests require a functioning environment, which http://platform.sh can automatically provide Sometimes you want to run your functional tests against production data, which is also something http://platform.sh can automatically provide An example implementation could be: On build, detect if this is an environment you want to run tests in (eg. non-production, pull request environment etc.) Install any tools you need to run functional tests (eg. a headless browser and testing framework) Run the tests – fail the build if the tests fail Send notification of the passed/failed build (I think the standard github pull request integration would report this back, but I’m not sure – an email or slack notification could be sent) ",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/using-platform-sh-environments-for-continuous-integration-functional-tests/386",
        "relurl": "/t/using-platform-sh-environments-for-continuous-integration-functional-tests/386"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "d67481940670bd5c8011651a62b3da836f4565dc",
        "title": "Does Platform.sh support subdomains and Let's Encrypt for them?",
        "description": "I’m looking to add 500+ subdomains to my application on Platform Professional. How do I set them up in my routes.yaml file? Is there a limit to the number of subdomains I can add there? How do I set up certificates for them?",
        "text": "I’m looking to add 500+ subdomains to my application on Platform Professional. How do I set them up in my routes.yaml file? Is there a limit to the number of subdomains I can add there? How do I set up certificates for them?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/does-platform-sh-support-subdomains-and-lets-encrypt-for-them/200",
        "relurl": "/t/does-platform-sh-support-subdomains-and-lets-encrypt-for-them/200"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "1c9c3fab92af90cf13415f50e3a91d5f2b6f4a75",
        "title": "How do I access my .git directory from within my app?",
        "description": "I want to be able to access the commit hash and timestamp of the current state of my project, but I don’t see a .git repository when I SSH in to my application. How can I access the information stored in .git?",
        "text": "I want to be able to access the commit hash and timestamp of the current state of my project, but I don’t see a .git repository when I SSH in to my application. How can I access the information stored in .git?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-access-my-git-directory-from-within-my-app/349",
        "relurl": "/t/how-do-i-access-my-git-directory-from-within-my-app/349"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "c4cf6453d9b8e12a9e4b82c9aa16ab4af5f0b7d0",
        "title": "How to use FTP to upload files?",
        "description": "I want to upload a large amount of files to my production environment, how could I use FTP to upload them?",
        "text": "I want to upload a large amount of files to my production environment, how could I use FTP to upload them?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-use-ftp-to-upload-files/253",
        "relurl": "/t/how-to-use-ftp-to-upload-files/253"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "82879a07c0643378facde81719cbb0e23175a1b1",
        "title": "Transferring backups from P.sh to S3",
        "description": "I think this would make a good how to guide. Asking here first. I understand I can install s3cmd or aws-cli. Is there any examples kicking around?",
        "text": "I think this would make a good how to guide. Asking here first. I understand I can install s3cmd or aws-cli. Is there any examples kicking around?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/transferring-backups-from-p-sh-to-s3/215",
        "relurl": "/t/transferring-backups-from-p-sh-to-s3/215"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "72226d555365347488ad8026b1d97f7e53757e4c",
        "title": "Log location? dblog or syslog?",
        "description": "I’m not sure where to find Drupal logs with syslog on, and I couldn’t find any info as to whether platform recommends/cares Re: syslog vs dblog.",
        "text": "I’m not sure where to find Drupal logs with syslog on, and I couldn’t find any info as to whether platform recommends/cares Re: syslog vs dblog.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/log-location-dblog-or-syslog/210",
        "relurl": "/t/log-location-dblog-or-syslog/210"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "0cb313ca8f4e6e71030c2a7ba72af23cb25185d5",
        "title": "My project has exceeded its cpu limit. What do I do?",
        "description": "I received the below error when trying to deploy: E: Error: Resources exceeding plan limit; cpu: 1.00 0.96; try removing a service, or increase your plan size What does that error mean? It appears to indicate that my project has exceeded its cpu plan limit. How would I increase this and how do I know what my cpu limit is? Thank you!",
        "text": "I received the below error when trying to deploy: E: Error: Resources exceeding plan limit; cpu: 1.00 0.96; try removing a service, or increase your plan size What does that error mean? It appears to indicate that my project has exceeded its cpu plan limit. How would I increase this and how do I know what my cpu limit is? Thank you!",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/my-project-has-exceeded-its-cpu-limit-what-do-i-do/288",
        "relurl": "/t/my-project-has-exceeded-its-cpu-limit-what-do-i-do/288"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "aa206bfd673bc198f9cc9fda8af6ae7777788ff2",
        "title": "How can I move a Wordpress site to Platform.sh?",
        "description": "I have an existing installation of Wordpress with my old hoster, how can I move everything to http://Platform.sh?",
        "text": "I have an existing installation of Wordpress with my old hoster, how can I move everything to http://Platform.sh?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-move-a-wordpress-site-to-platform-sh/271",
        "relurl": "/t/how-can-i-move-a-wordpress-site-to-platform-sh/271"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "0383d4c645cf4b981c810c7eea5ff5a4ca57c4c2",
        "title": "How can I access to an environment variable in Javascript?",
        "description": "I would like to insert my Hotjar Identifier only on certain environments. How can I do that?",
        "text": "I would like to insert my Hotjar Identifier only on certain environments. How can I do that?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-access-to-an-environment-variable-in-javascript/251",
        "relurl": "/t/how-can-i-access-to-an-environment-variable-in-javascript/251"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "405123789db83e4cfd9bb75a83db066eb3ad347d",
        "title": "My project looks to be out of storage. How do I add more?",
        "description": "I received the following error when git pushing to my environment: E: Error: Resources exceeding plan limit; disk: 31192.00 30720.00; try removing a service, or add more storage to your plan It suggests that I add more storage to my plan. How do I do that? Also, what does it mean by removing a service?",
        "text": "I received the following error when git pushing to my environment: E: Error: Resources exceeding plan limit; disk: 31192.00 30720.00; try removing a service, or add more storage to your plan It suggests that I add more storage to my plan. How do I do that? Also, what does it mean by removing a service?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/my-project-looks-to-be-out-of-storage-how-do-i-add-more/261",
        "relurl": "/t/my-project-looks-to-be-out-of-storage-how-do-i-add-more/261"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "5719e1b1e849c206cea3d80ad951638545e83c5f",
        "title": "Is it possible to run a docker container on Platform.sh?",
        "description": "I have my own Docker container prepared. Is there any way to pull it from a registry and run it on http://Platform.sh?",
        "text": "I have my own Docker container prepared. Is there any way to pull it from a registry and run it on http://Platform.sh?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-it-possible-to-run-a-docker-container-on-platform-sh/330",
        "relurl": "/t/is-it-possible-to-run-a-docker-container-on-platform-sh/330"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "a118416f22355bd25338f7dd9653bc7c475df797",
        "title": "How do I fix `fatal: the remote end hung up unexpectedly` while pushing from a Bitbucket pipeline to Platform.sh?",
        "description": "While pushing to Platform git repository from our Bitbucket runner, the pipeline fails with the following error: fatal: the remote end hung up unexpectedly . How can I fix that?",
        "text": "While pushing to Platform git repository from our Bitbucket runner, the pipeline fails with the following error: fatal: the remote end hung up unexpectedly . How can I fix that?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-fix-fatal-the-remote-end-hung-up-unexpectedly-while-pushing-from-a-bitbucket-pipeline-to-platform-sh/492",
        "relurl": "/t/how-do-i-fix-fatal-the-remote-end-hung-up-unexpectedly-while-pushing-from-a-bitbucket-pipeline-to-platform-sh/492"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "7cc5dffcb3cdd9d558319befa8862b7fa5ba8528",
        "title": "How can I refresh my projects list in Platform CLI?",
        "description": "I have created several new projects and my Platform CLI is not refreshing the list of available projects.",
        "text": "I have created several new projects and my Platform CLI is not refreshing the list of available projects.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-refresh-my-projects-list-in-platform-cli/269",
        "relurl": "/t/how-can-i-refresh-my-projects-list-in-platform-cli/269"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "d8bd1641cda2d83292a69ff528fd23243708a6ea",
        "title": "Can I get a dedicated FTP/SFTP account for access to a specific directory?",
        "description": "Can I create an SFTP account for one of my team that only allows access to one folder, not to the whole site?",
        "text": "Can I create an SFTP account for one of my team that only allows access to one folder, not to the whole site?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-i-get-a-dedicated-ftp-sftp-account-for-access-to-a-specific-directory/452",
        "relurl": "/t/can-i-get-a-dedicated-ftp-sftp-account-for-access-to-a-specific-directory/452"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "da301bd57372ab0c5a746c043709b87c9ce5c124",
        "title": "How to build and deploy Angular Universal app?",
        "description": "Hello , We are using latest release of Angular Universal 9 with ivy (next-gen compatibility compiler) but we have an unexpected problem when we tried to build and deploy the app. We launch Ivy compilation in postinstall in sync mode script in order to avoid out of memory error, but the build doesn’t works… How to deploy our application quickly without memory error ? We used “L” plan with 6Gb of Ram, it’s 3x more than our older PaaS system. I provide most information about it, below : Build Hook yarn install yarn run build:ssr:dev The command build:ssr:dev launch webpack compilation : ng run website:build:dev \u0026\u0026 ng run website:server:dev The website will be compiled to the dist directory but this is currently not working : Build output error log $ ng run website:build:dev \u0026\u0026 ng run website:server:dev Generating ES5 bundles for differential loading... W: An unhandled exception occurred: Call retries were exceeded W: See \"/tmp/ng-1pShkt/angular-errors.log\" for further details. W: error Command failed with exit code 127. info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command. The file /tmp/ng-1pShkt/angular-errors.log is not available, so I can’t debug log. How to see Angular build error log ? How to build Angular application ? (Your documentation is only for AngularJS and it’s an old version, completely differente of Angular “next gen”). Thanks for your help ",
        "text": "Hello , We are using latest release of Angular Universal 9 with ivy (next-gen compatibility compiler) but we have an unexpected problem when we tried to build and deploy the app. We launch Ivy compilation in postinstall in sync mode script in order to avoid out of memory error, but the build doesn’t works… How to deploy our application quickly without memory error ? We used “L” plan with 6Gb of Ram, it’s 3x more than our older PaaS system. I provide most information about it, below : Build Hook yarn install yarn run build:ssr:dev The command build:ssr:dev launch webpack compilation : ng run website:build:dev \u0026\u0026 ng run website:server:dev The website will be compiled to the dist directory but this is currently not working : Build output error log $ ng run website:build:dev \u0026\u0026 ng run website:server:dev Generating ES5 bundles for differential loading... W: An unhandled exception occurred: Call retries were exceeded W: See \"/tmp/ng-1pShkt/angular-errors.log\" for further details. W: error Command failed with exit code 127. info Visit https://yarnpkg.com/en/docs/cli/run for documentation about this command. The file /tmp/ng-1pShkt/angular-errors.log is not available, so I can’t debug log. How to see Angular build error log ? How to build Angular application ? (Your documentation is only for AngularJS and it’s an old version, completely differente of Angular “next gen”). Thanks for your help ",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-build-and-deploy-angular-universal-app/526",
        "relurl": "/t/how-to-build-and-deploy-angular-universal-app/526"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "a9bb537a932ccac59b2fae03ef549d71b9336db2",
        "title": "Where do I submit bug reports?",
        "description": "If I come across a problem working with http://Platform.sh that seems like a bug, how can I bring it to your attention?",
        "text": "If I come across a problem working with http://Platform.sh that seems like a bug, how can I bring it to your attention?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/where-do-i-submit-bug-reports/286",
        "relurl": "/t/where-do-i-submit-bug-reports/286"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "df848e54a7de22f1eb4599c1655dacd042f85b68",
        "title": "What is the different between Platform.sh and Terraform?",
        "description": "What is the different between http://Platform.sh and Terraform?",
        "text": "What is the different between http://Platform.sh and Terraform?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/what-is-the-different-between-platform-sh-and-terraform/342",
        "relurl": "/t/what-is-the-different-between-platform-sh-and-terraform/342"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "65c2083de024d35b48628e16a2d6dba639e7b68f",
        "title": "What is the difference between a rebuild and a redeploy?",
        "description": "And, does the platform redeploy command also trigger a rebuild?",
        "text": "And, does the platform redeploy command also trigger a rebuild?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/what-is-the-difference-between-a-rebuild-and-a-redeploy/329",
        "relurl": "/t/what-is-the-difference-between-a-rebuild-and-a-redeploy/329"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "88877e4adc233bfb082f21c73f305ee349f13335",
        "title": "I need to connect to a VPN endpoint. What are my options?",
        "description": "I need two types of connections to a server only available through a VPN: One is a real-time http call The other is a cron every night that uploads a file to a FTP server. Is there a way to handle that on http://Platform.sh ?",
        "text": "I need two types of connections to a server only available through a VPN: One is a real-time http call The other is a cron every night that uploads a file to a FTP server. Is there a way to handle that on http://Platform.sh ?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/i-need-to-connect-to-a-vpn-endpoint-what-are-my-options/236",
        "relurl": "/t/i-need-to-connect-to-a-vpn-endpoint-what-are-my-options/236"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "3d7f1cd4cb06f504ee2be52b758dc5878d5ca5ee",
        "title": "How can I connect an external tool to my database?",
        "description": "How would I connect an external tool such as dbeaver / datagrip / … to my database ?",
        "text": "How would I connect an external tool such as dbeaver / datagrip / … to my database ?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-connect-an-external-tool-to-my-database/222",
        "relurl": "/t/how-can-i-connect-an-external-tool-to-my-database/222"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "2f48bf1762fa4bfe846cda2fae003701189a0a89",
        "title": "How can I make custom 404 pages?",
        "description": "I’d like to serve my own HTML for 404 (and other HTTP codes).",
        "text": "I’d like to serve my own HTML for 404 (and other HTTP codes).",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-make-custom-404-pages/198",
        "relurl": "/t/how-can-i-make-custom-404-pages/198"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "7e16ae0fd41457675399c236d947c8068bfb3f10",
        "title": "What is the right way to run multiple processes from my start command?",
        "description": "I’ve read that I shouldn’t background a start process, as this will cause the supervisor process to start a second copy, but is it possible to run multiple processes from the start command? What is the right way to do this?",
        "text": "I’ve read that I shouldn’t background a start process, as this will cause the supervisor process to start a second copy, but is it possible to run multiple processes from the start command? What is the right way to do this?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/what-is-the-right-way-to-run-multiple-processes-from-my-start-command/224",
        "relurl": "/t/what-is-the-right-way-to-run-multiple-processes-from-my-start-command/224"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "38098a5d0e136946b96fbb9986147e3c2ff06cef",
        "title": "Can I point my apex (naked) domain to Platform.sh if I'm using Amazon Route 53 to manage DNS?",
        "description": "Route 53 isn’t listed here… ",
        "text": "Route 53 isn’t listed here… ",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-i-point-my-apex-naked-domain-to-platform-sh-if-im-using-amazon-route-53-to-manage-dns/310",
        "relurl": "/t/can-i-point-my-apex-naked-domain-to-platform-sh-if-im-using-amazon-route-53-to-manage-dns/310"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "ef615bcf478549da087594d0990db5535ef14f1d",
        "title": "(How) Can I change the \"Code base template\" after purchasing a plan?",
        "description": "When starting a new project, I’m not sure which template I should be using, yet I have to commit to choosing one before purchasing my plan. Can I try out different templates later under the same plan? How would I do that?",
        "text": "When starting a new project, I’m not sure which template I should be using, yet I have to commit to choosing one before purchasing my plan. Can I try out different templates later under the same plan? How would I do that?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-change-the-code-base-template-after-purchasing-a-plan/241",
        "relurl": "/t/how-can-i-change-the-code-base-template-after-purchasing-a-plan/241"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "82c7c3758aa276d11a649ec3f755e52d5c337fea",
        "title": "Why is the .platform.app.yaml file separate from the .platform folder?",
        "description": "Why do we have: .platform.* files and also a .platform folder? I mean, why not everything in the same folder?",
        "text": "Why do we have: .platform.* files and also a .platform folder? I mean, why not everything in the same folder?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/why-is-the-platform-app-yaml-file-separate-from-the-platform-folder/281",
        "relurl": "/t/why-is-the-platform-app-yaml-file-separate-from-the-platform-folder/281"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "661d042742c2093406e89ba3537ab618697ae240",
        "title": "Is there a way to do the equivalent of \"screen\" on platform.sh?",
        "description": "I need to import a lot of data in my database, it’s a very long process. What can I use since screen or tmux seem to be absent?",
        "text": "I need to import a lot of data in my database, it’s a very long process. What can I use since screen or tmux seem to be absent?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-there-a-way-to-do-the-equivalent-of-screen-on-platform-sh/252",
        "relurl": "/t/is-there-a-way-to-do-the-equivalent-of-screen-on-platform-sh/252"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "3b03b2e3d54d1c63e3ccf7f0338d46a5dad869e4",
        "title": "How can I remove the \"sent via platform.sh\" in my emails",
        "description": "When sending email from platformsh the recipient gets a ‘via platformsh’ appended. I understand platformsh uses sendgrid. How should i configure my domain so I don’t get the ‘via’ platformsh message?",
        "text": "When sending email from platformsh the recipient gets a ‘via platformsh’ appended. I understand platformsh uses sendgrid. How should i configure my domain so I don’t get the ‘via’ platformsh message?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-remove-the-sent-via-platform-sh-in-my-emails/263",
        "relurl": "/t/how-can-i-remove-the-sent-via-platform-sh-in-my-emails/263"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "b8d2d0995cc2e89f16860501f1bbee7dea12b087",
        "title": "How can I receive a notification if one of my services (eg. MariaDB) dies?",
        "description": "I’d like to get an alert if a service dies or is unreachable.",
        "text": "I’d like to get an alert if a service dies or is unreachable.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-receive-a-notification-if-one-of-my-services-eg-mariadb-dies/164",
        "relurl": "/t/how-can-i-receive-a-notification-if-one-of-my-services-eg-mariadb-dies/164"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e69213f35a82db31e0c798f8dbbf567ef01acc4e",
        "title": "How can I expose non-80/443 TCP port to the Internet?",
        "description": "My mobile application needs to talk to a service running on Platform via non-80/443 TCP port. How could I expose that port?",
        "text": "My mobile application needs to talk to a service running on Platform via non-80/443 TCP port. How could I expose that port?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-expose-non-80-443-tcp-port-to-the-internet/256",
        "relurl": "/t/how-can-i-expose-non-80-443-tcp-port-to-the-internet/256"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "74b7348745133495cb8d456b058a272167c0e156",
        "title": "How do I fix `fatal: the remote end hung up unexpectedly` while pushing from a GitLab pipeline to Platform.sh?",
        "description": "While pushing to Platform git repository from our GitLab runner, the pipeline fails with the following error: fatal: the remote end hung up unexpectedly. How can I fix that?",
        "text": "While pushing to Platform git repository from our GitLab runner, the pipeline fails with the following error: fatal: the remote end hung up unexpectedly. How can I fix that?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-fix-fatal-the-remote-end-hung-up-unexpectedly-while-pushing-from-a-gitlab-pipeline-to-platform-sh/491",
        "relurl": "/t/how-do-i-fix-fatal-the-remote-end-hung-up-unexpectedly-while-pushing-from-a-gitlab-pipeline-to-platform-sh/491"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "96b4db735287085d0211042db7a16bc72fd7fca5",
        "title": "How are mounts in .platform.app.yaml sized?",
        "description": "Is everything OK as long as the total size of the mounts is less than the value in the disk directive? ",
        "text": "Is everything OK as long as the total size of the mounts is less than the value in the disk directive? ",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-are-mounts-in-platform-app-yaml-sized/157",
        "relurl": "/t/how-are-mounts-in-platform-app-yaml-sized/157"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "9dc93dc80b9c6dab43079e136e018355a37faff5",
        "title": "Why should I use Platform.sh over AWS?",
        "description": "What are the benefits of using http://Platform.sh instead?",
        "text": "What are the benefits of using http://Platform.sh instead?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/why-should-i-use-platform-sh-over-aws/314",
        "relurl": "/t/why-should-i-use-platform-sh-over-aws/314"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "b9a1f0d901c6b6e5bc240f275728588d869d2407",
        "title": "Where could I submit feature request?",
        "description": "I have some features in mind that would like to see on http://Platform.sh, where could I submit the feature request?",
        "text": "I have some features in mind that would like to see on http://Platform.sh, where could I submit the feature request?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/where-could-i-submit-feature-request/257",
        "relurl": "/t/where-could-i-submit-feature-request/257"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "1dd456534b116946a2f4e5c08ec4bc634e91c33f",
        "title": "What is Git? How to use it?",
        "description": "What is Git and how could I use it to upload my website?",
        "text": "What is Git and how could I use it to upload my website?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/what-is-git-how-to-use-it/254",
        "relurl": "/t/what-is-git-how-to-use-it/254"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "060b37643a19335e1f31453d82b701e3ff02d654",
        "title": "Is there a public product roadmap?",
        "description": "Do you have a public product roadmap?",
        "text": "Do you have a public product roadmap?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-there-a-public-product-roadmap/258",
        "relurl": "/t/is-there-a-public-product-roadmap/258"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "4a1a5a0e3a0079b0eb55763c9e0dc392c293ba02",
        "title": "How much storage can I add to my project?",
        "description": "How much do I get by default, and how much can I add?",
        "text": "How much do I get by default, and how much can I add?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-much-storage-can-i-add-to-my-project/228",
        "relurl": "/t/how-much-storage-can-i-add-to-my-project/228"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "d328ee08f5f65726b008ef0a06045d3fead0f110",
        "title": "How can I see the status of a bug?",
        "description": "I opened a ticket a while ago for a problem and received the reply that my problem is a known bug and will be fixed in the future. How can I track the status of my bug ? will I be informed when it’s fixed somehow ?",
        "text": "I opened a ticket a while ago for a problem and received the reply that my problem is a known bug and will be fixed in the future. How can I track the status of my bug ? will I be informed when it’s fixed somehow ?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-see-the-status-of-a-bug/260",
        "relurl": "/t/how-can-i-see-the-status-of-a-bug/260"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "16ad6d527398bd989cc45b9a12d59c8e927e2674",
        "title": "Is there a way to fail a build when one of the build hooks throws an error?",
        "description": "In my case I want to use npm clean-install instead of npm install to ensure that my package-lock and my package.json have not diverged.",
        "text": "In my case I want to use npm clean-install instead of npm install to ensure that my package-lock and my package.json have not diverged.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-there-a-way-to-fail-a-build-when-one-of-the-build-hooks-throws-an-error/366",
        "relurl": "/t/is-there-a-way-to-fail-a-build-when-one-of-the-build-hooks-throws-an-error/366"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "cb6205bb5e44b2d51b9fef2baccf94bc825db67c",
        "title": "Is my built code counted against my global plan storage?",
        "description": "If my plan has 5GB storage, and my built code takes up 500K, does that get taken off of my available storage (5GB)?",
        "text": "If my plan has 5GB storage, and my built code takes up 500K, does that get taken off of my available storage (5GB)?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-my-built-code-counted-against-my-global-plan-storage/163",
        "relurl": "/t/is-my-built-code-counted-against-my-global-plan-storage/163"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "bbf5e031a9b9509037da12cc27e589fa3d34445f",
        "title": "What is the routes.yaml max file size?",
        "description": "Is it a number of routes? Characters? File size?",
        "text": "Is it a number of routes? Characters? File size?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/what-is-the-routes-yaml-max-file-size/185",
        "relurl": "/t/what-is-the-routes-yaml-max-file-size/185"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "d6fe6c4e416d64bc77662aa45cb656c386ad57df",
        "title": "Can I SSH into my website?",
        "description": "Can I SSH into my website? How?",
        "text": "Can I SSH into my website? How?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-i-ssh-into-my-website/225",
        "relurl": "/t/can-i-ssh-into-my-website/225"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "35e665e5efb1489c0d07cb1637df11b054233e1f",
        "title": "Can I just have my whole project read / write and serve my PHP website from here?",
        "description": "Hello, I feel that having everything read-only by default prevents me from working fast enough - can I just make a mount to / and rsync then serve my PHP website from here, so I can open an editor and make my changes directly there? Thanks!",
        "text": "Hello, I feel that having everything read-only by default prevents me from working fast enough - can I just make a mount to / and rsync then serve my PHP website from here, so I can open an editor and make my changes directly there? Thanks!",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-i-just-have-my-whole-project-read-write-and-serve-my-php-website-from-here/245",
        "relurl": "/t/can-i-just-have-my-whole-project-read-write-and-serve-my-php-website-from-here/245"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "57fe61a78b87fb2b672f70f12efde2fbee8318fd",
        "title": "More production environments on the project",
        "description": "Is it possible to have different production environments on the same project? Or only master branch can go live?",
        "text": "Is it possible to have different production environments on the same project? Or only master branch can go live?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/more-production-environments-on-the-project/250",
        "relurl": "/t/more-production-environments-on-the-project/250"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "c1f6700ce2c7ca461e446401f23e7d27e1bd6e44",
        "title": "What does it mean when my environment has a \"dirty\" status?",
        "description": "As opposed to “active”?",
        "text": "As opposed to “active”?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/what-does-it-mean-when-my-environment-has-a-dirty-status/175",
        "relurl": "/t/what-does-it-mean-when-my-environment-has-a-dirty-status/175"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "3dc43992994769ad3d5838dd0387d1ed167509dd",
        "title": "Do you support other version control system?",
        "description": "Do you support SVN, CVS, Perforce, etc to upload my website?",
        "text": "Do you support SVN, CVS, Perforce, etc to upload my website?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/do-you-support-other-version-control-system/255",
        "relurl": "/t/do-you-support-other-version-control-system/255"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "b72e5949b7a15fee2210099c07d1778079beab3a",
        "title": "How do the queues in my project work? Is it only possible to have one thing building/deploying at a time?",
        "description": "For example, if a developer pushes something on the stage branch, and it takes 10 minutes to build, can I deploy a hotfix on master in that time?",
        "text": "For example, if a developer pushes something on the stage branch, and it takes 10 minutes to build, can I deploy a hotfix on master in that time?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-the-queues-in-my-project-work-is-it-only-possible-to-have-one-thing-building-deploying-at-a-time/165",
        "relurl": "/t/how-do-the-queues-in-my-project-work-is-it-only-possible-to-have-one-thing-building-deploying-at-a-time/165"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "6521f992150f9375174e64232d2a45ab8ad8fbde",
        "title": "Can I have root permissions?",
        "description": "Can I have root permissions on my http://Platform.sh projects ?",
        "text": "Can I have root permissions on my http://Platform.sh projects ?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-i-have-root-permissions/248",
        "relurl": "/t/can-i-have-root-permissions/248"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "5afb0e8de5fdd067d15381b4c48aa657be47f824",
        "title": "Can I get FTP access to change my html pages?",
        "description": "My developers are asking for an FTP access to update the html pages of my website. Can I get one?",
        "text": "My developers are asking for an FTP access to update the html pages of my website. Can I get one?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-i-get-ftp-access-to-change-my-html-pages/246",
        "relurl": "/t/can-i-get-ftp-access-to-change-my-html-pages/246"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "42cd2b08b4b1d2328c2c55ec3456f99778479a2e",
        "title": "How can I use nutch to index my site?",
        "description": "I’d like to use nutch to index my site and insert the index into Solr. How can I do this on Platform?",
        "text": "I’d like to use nutch to index my site and insert the index into Solr. How can I do this on Platform?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-use-nutch-to-index-my-site/244",
        "relurl": "/t/how-can-i-use-nutch-to-index-my-site/244"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "828a553b5da1e4d147a9ec5817c2369bd0681d7b",
        "title": "What is a best way to sync files from `develop` to `master`?",
        "description": "I need to move my files from the develop environment into master.",
        "text": "I need to move my files from the develop environment into master.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/what-is-a-best-way-to-sync-files-from-develop-to-master/354",
        "relurl": "/t/what-is-a-best-way-to-sync-files-from-develop-to-master/354"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "4e1814ccb1fac31223f8cd3a8dcadf326ab80d45",
        "title": "For Ruby on Rails, what's the best way to delete the database records?",
        "description": "What’s the best way to wipe my DB? I have a live ruby on rails site. I would like to purge or delete the data from the master branch Posgres database. I’d like to keep the tables intact, and only delete the rows from each. This is so that I can start my site development anew. My thinking is to change to the master branch in command line then following the reference below I would proceed with the command $ rake db:reset db:migrate Which will reset the database and reload the current schema. https://medium.com/forest-admin/rails-migrations-tricks-guide-code-cheatsheet-included-dca935354f22 Is this best way as its my first in production site no account or customers to worry about?",
        "text": "What’s the best way to wipe my DB? I have a live ruby on rails site. I would like to purge or delete the data from the master branch Posgres database. I’d like to keep the tables intact, and only delete the rows from each. This is so that I can start my site development anew. My thinking is to change to the master branch in command line then following the reference below I would proceed with the command $ rake db:reset db:migrate Which will reset the database and reload the current schema. https://medium.com/forest-admin/rails-migrations-tricks-guide-code-cheatsheet-included-dca935354f22 Is this best way as its my first in production site no account or customers to worry about?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/for-ruby-on-rails-whats-the-best-way-to-delete-the-database-records/455",
        "relurl": "/t/for-ruby-on-rails-whats-the-best-way-to-delete-the-database-records/455"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "c07b222cdb375fb8ec4bd381c55e9f3e75a2ac3f",
        "title": "How can I identify MySQL slow queries on the PaaS",
        "description": "I have identified PHP scripts that are abnormally long to execute in some cases. (Thanks to the php.access.log) I’m trying to find which MySQL request is taking so much time (up to 200 sec) and under which conditions. Is there a way to enable the mysql_slow_query log?",
        "text": "I have identified PHP scripts that are abnormally long to execute in some cases. (Thanks to the php.access.log) I’m trying to find which MySQL request is taking so much time (up to 200 sec) and under which conditions. Is there a way to enable the mysql_slow_query log?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-identify-mysql-slow-queries-on-the-paas/321",
        "relurl": "/t/how-can-i-identify-mysql-slow-queries-on-the-paas/321"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "78ebe12c97edfae6479a687ccf610f2b92e6756e",
        "title": "Odoo 13 + python",
        "description": "Has anybody got the latest version of Odoo running on platform successfully ? Cheers Dan",
        "text": "Has anybody got the latest version of Odoo running on platform successfully ? Cheers Dan",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/odoo-13-python/447",
        "relurl": "/t/odoo-13-python/447"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "d769c0cdcff5416728aeb1b809a14a08b8a8c538",
        "title": "What's the correct route conf for a websocket?",
        "description": "Hi, I’m trying to run a django/channels/daphne websocket. I believe I’ve got the backend set up correctly - at least, it works locally. I’m trying to connect from the front end via JS: var voteSocket = new WebSocket('wss://' + window.location.host + '/sync/votes/'); …which gives me a 404, presumably because the URI isn’t declared in routes.yaml If I add that route though, I get a deploy error: routes.key: 'wss://{default}/sync/votes/' scheme is not one of http, https And if I change the frontend JS to use a https URL, the browser complains that: The URL’s scheme must be either ‘ws’ or ‘wss’. ‘https’ is not allowed. So… how do I resolve that conflict? How do I declare a route that can be used as a websocket?",
        "text": "Hi, I’m trying to run a django/channels/daphne websocket. I believe I’ve got the backend set up correctly - at least, it works locally. I’m trying to connect from the front end via JS: var voteSocket = new WebSocket('wss://' + window.location.host + '/sync/votes/'); …which gives me a 404, presumably because the URI isn’t declared in routes.yaml If I add that route though, I get a deploy error: routes.key: 'wss://{default}/sync/votes/' scheme is not one of http, https And if I change the frontend JS to use a https URL, the browser complains that: The URL’s scheme must be either ‘ws’ or ‘wss’. ‘https’ is not allowed. So… how do I resolve that conflict? How do I declare a route that can be used as a websocket?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/whats-the-correct-route-conf-for-a-websocket/472",
        "relurl": "/t/whats-the-correct-route-conf-for-a-websocket/472"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "d49128c3f89e349f660fef8bdd071fe6681420b2",
        "title": "Do redirects in `routes.yaml` file also show up in `PLATFORM_ROUTES`?",
        "description": "Is this even possible if I have many (thousands) of redirects since they would be stored in an environment variable?",
        "text": "Is this even possible if I have many (thousands) of redirects since they would be stored in an environment variable?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/do-redirects-in-routes-yaml-file-also-show-up-in-platform-routes/308",
        "relurl": "/t/do-redirects-in-routes-yaml-file-also-show-up-in-platform-routes/308"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "2e8c7026eb62b735fef709855b09ef282ed7e9d7",
        "title": "What databases can I use in my project?",
        "description": "What database servers can I use in my project ? do you offer NoSQL DBs too ?",
        "text": "What database servers can I use in my project ? do you offer NoSQL DBs too ?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/what-databases-can-i-use-in-my-project/219",
        "relurl": "/t/what-databases-can-i-use-in-my-project/219"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "4f2bf0e1c2737e2f6065a4305b0e1fa47c3b65de",
        "title": "How much disk space is available during the build hook for application code?",
        "description": "I’m getting out of disk errors when running npm install.",
        "text": "I’m getting out of disk errors when running npm install.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-much-disk-space-is-available-during-the-build-hook-for-application-code/181",
        "relurl": "/t/how-much-disk-space-is-available-during-the-build-hook-for-application-code/181"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e2e75f7287eb926221c24d865614f6ec832e5e9e",
        "title": "How do I change my project name?",
        "description": "I have created a project, but I made a typo. Can I change the project name? Or do I have to delete and recreate the project completely?",
        "text": "I have created a project, but I made a typo. Can I change the project name? Or do I have to delete and recreate the project completely?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-change-my-project-name/242",
        "relurl": "/t/how-do-i-change-my-project-name/242"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "1b2e3d97805226c7aefadb8065b8f8484cf12dc2",
        "title": "How can I force a PDF download with Platform.sh?",
        "description": "I’d like to force a PDF download from my static site, rather than have that PDF open in a browser. With Apache, I’d do it like so: ForceType application/octet-stream Header set Content-Disposition attachment How do I do that with http://Platform.sh?",
        "text": "I’d like to force a PDF download from my static site, rather than have that PDF open in a browser. With Apache, I’d do it like so: ForceType application/octet-stream Header set Content-Disposition attachment How do I do that with http://Platform.sh?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-force-a-pdf-download-with-platform-sh/334",
        "relurl": "/t/how-can-i-force-a-pdf-download-with-platform-sh/334"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "9152de4909dc6ba5a2a0da17cbead906628daf3f",
        "title": "Why is my site slow?",
        "description": "It was fast before and works fine in my development environment.",
        "text": "It was fast before and works fine in my development environment.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/why-is-my-site-slow/238",
        "relurl": "/t/why-is-my-site-slow/238"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "5b8074847b47559a6748d83a431a267e6ef12422",
        "title": "How to git push without rebuilding?",
        "description": "How can I push a commit without rebuilding the app? It takes long time. No config change just css.",
        "text": "How can I push a commit without rebuilding the app? It takes long time. No config change just css.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-git-push-without-rebuilding/516",
        "relurl": "/t/how-to-git-push-without-rebuilding/516"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "7b2d198f5f3a83d1affbceec3d93a12ee9cb3ba6",
        "title": "Is it possible to increase Redis `maxmemory` configuration option?",
        "description": "I tried bumping up cluster size to L and adding a configuration: { maxmemory: 134217728 } to redis service but I did not manage to change the default 28MB limit.",
        "text": "I tried bumping up cluster size to L and adding a configuration: { maxmemory: 134217728 } to redis service but I did not manage to change the default 28MB limit.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-it-possible-to-increase-redis-maxmemory-configuration-option/350",
        "relurl": "/t/is-it-possible-to-increase-redis-maxmemory-configuration-option/350"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "a01179f56da655d929782eda5ebe04a28460981a",
        "title": "When does a variable requires a JSON formatting?",
        "description": "When providing environment variables, you have the option to provide JSON formatted values. What would be the deciding factor to create a JSON formatted variable? In my case, I’m looking to provide an empty string (\"\") as --value for my variable.",
        "text": "When providing environment variables, you have the option to provide JSON formatted values. What would be the deciding factor to create a JSON formatted variable? In my case, I’m looking to provide an empty string (\"\") as --value for my variable.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/when-does-a-variable-requires-a-json-formatting/193",
        "relurl": "/t/when-does-a-variable-requires-a-json-formatting/193"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "c0f6ed10075641cce9c786373e650156efeb3f00",
        "title": "How can I connect to my database from my local workstation?",
        "description": "I need to import data into my database from a local file. How can I do that from my local workstation?",
        "text": "I need to import data into my database from a local file. How can I do that from my local workstation?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-connect-to-my-database-from-my-local-workstation/207",
        "relurl": "/t/how-can-i-connect-to-my-database-from-my-local-workstation/207"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "ceefe1223c35ff42d119fef874ec37d701dd0dfd",
        "title": "If I add a service, can I still restore a snapshot from before that service existed?",
        "description": "Adding a new service (eg. a new MongoDB) changes the persistent data model of my project. Does that render it incompatible with backups/snapshots made before?",
        "text": "Adding a new service (eg. a new MongoDB) changes the persistent data model of my project. Does that render it incompatible with backups/snapshots made before?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/if-i-add-a-service-can-i-still-restore-a-snapshot-from-before-that-service-existed/298",
        "relurl": "/t/if-i-add-a-service-can-i-still-restore-a-snapshot-from-before-that-service-existed/298"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "4318eaf2ef85e51fbdc2ae5d9bf9bbb10fe58858",
        "title": "How to get ssh connection details from the commandline?",
        "description": "I know I can find ssh connection info , but I’d prefer not to switch out of my terminal to get this each time, or I want to automate a connection from a process of my own. I don’t recognise the IDs in that connection string, do they change?",
        "text": "I know I can find ssh connection info , but I’d prefer not to switch out of my terminal to get this each time, or I want to automate a connection from a process of my own. I don’t recognise the IDs in that connection string, do they change?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-get-ssh-connection-details-from-the-commandline/284",
        "relurl": "/t/how-to-get-ssh-connection-details-from-the-commandline/284"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "10c8c7536211b18d648871f17942514d3d4939ea",
        "title": "(Solved) Pyodbc install fails",
        "description": "Currently working on setting up a django app that needs a mssql database connection. The mssql library (posted below) operates off of Pyodbc. Pyodbc has a requirement of unixodbc. Unfortunately, Im having issues with getting this requirement installed. https://github.com/ESSolutions/django-mssql-backend https://github.com/ESSolutions/django-mssql-backend https://github.com/mkleehammer/pyodbc https://github.com/mkleehammer/pyodbc http://www.unixodbc.org/ Currently trying to following to get the dependency installed. export LD_LIBRARY_PATH=$LIBDIR:/app/unixODBC-2.3.7 wget http://www.unixodbc.org/unixODBC-2.3.7.tar.gz gunzip unixODBC*.tar.gz tar xvf unixODBC*.tar cd unixODBC* mkdir /app/unixODBC-2.3.7/etc ./configure --prefix=/app/unixODBC-2.3.7 --sysconfdir=/app/unixODBC-2.3.7 make make install The last error Im getting is below. make[2]: Nothing to be done for 'install-exec-am'. /bin/mkdir -p '/app/unixODBC-2.3.7/include' /usr/bin/install -c -m 644 odbcinst.h odbcinstext.h sql.h sqlext.h sqltypes.h sqlucode.h sqlspi.h autotest.h uodbc_stats.h uodbc_extras.h '/app/unixODBC-2.3.7/include' W: /usr/bin/install: 'odbcinst.h' and '/app/unixODBC-2.3.7/include/odbcinst.h' are the same file W: /usr/bin/install: 'odbcinstext.h' and '/app/unixODBC-2.3.7/include/odbcinstext.h' are the same file W: /usr/bin/install: 'sql.h' and '/app/unixODBC-2.3.7/include/sql.h' are the same file W: /usr/bin/install: 'sqlext.h' and '/app/unixODBC-2.3.7/include/sqlext.h' are the same file W: /usr/bin/install: 'sqltypes.h' and '/app/unixODBC-2.3.7/include/sqltypes.h' are the same file W: /usr/bin/install: 'sqlucode.h' and '/app/unixODBC-2.3.7/include/sqlucode.h' are the same file W: /usr/bin/install: 'sqlspi.h' and '/app/unixODBC-2.3.7/include/sqlspi.h' are the same file W: /usr/bin/install: 'autotest.h' and '/app/unixODBC-2.3.7/include/autotest.h' are the same file W: /usr/bin/install: 'uodbc_stats.h' and '/app/unixODBC-2.3.7/include/uodbc_stats.h' are the same file W: /usr/bin/install: 'uodbc_extras.h' and '/app/unixODBC-2.3.7/include/uodbc_extras.h' are the same file W: make[2]: *** [install-includeHEADERS] Error 1 Makefile:409: recipe for target 'install-includeHEADERS' failed make[2]: Leaving directory '/app/unixODBC-2.3.7/include' W: make[1]: *** [install-am] Error 2 Makefile:525: recipe for target 'install-am' failed make[1]: Leaving directory '/app/unixODBC-2.3.7/include' W: make: *** [install-recursive] Error 1 Makefile:548: recipe for target 'install-recursive' failed Any help with getting this dependency installed would be much appreciated.",
        "text": "Currently working on setting up a django app that needs a mssql database connection. The mssql library (posted below) operates off of Pyodbc. Pyodbc has a requirement of unixodbc. Unfortunately, Im having issues with getting this requirement installed. https://github.com/ESSolutions/django-mssql-backend https://github.com/ESSolutions/django-mssql-backend https://github.com/mkleehammer/pyodbc https://github.com/mkleehammer/pyodbc http://www.unixodbc.org/ Currently trying to following to get the dependency installed. export LD_LIBRARY_PATH=$LIBDIR:/app/unixODBC-2.3.7 wget http://www.unixodbc.org/unixODBC-2.3.7.tar.gz gunzip unixODBC*.tar.gz tar xvf unixODBC*.tar cd unixODBC* mkdir /app/unixODBC-2.3.7/etc ./configure --prefix=/app/unixODBC-2.3.7 --sysconfdir=/app/unixODBC-2.3.7 make make install The last error Im getting is below. make[2]: Nothing to be done for 'install-exec-am'. /bin/mkdir -p '/app/unixODBC-2.3.7/include' /usr/bin/install -c -m 644 odbcinst.h odbcinstext.h sql.h sqlext.h sqltypes.h sqlucode.h sqlspi.h autotest.h uodbc_stats.h uodbc_extras.h '/app/unixODBC-2.3.7/include' W: /usr/bin/install: 'odbcinst.h' and '/app/unixODBC-2.3.7/include/odbcinst.h' are the same file W: /usr/bin/install: 'odbcinstext.h' and '/app/unixODBC-2.3.7/include/odbcinstext.h' are the same file W: /usr/bin/install: 'sql.h' and '/app/unixODBC-2.3.7/include/sql.h' are the same file W: /usr/bin/install: 'sqlext.h' and '/app/unixODBC-2.3.7/include/sqlext.h' are the same file W: /usr/bin/install: 'sqltypes.h' and '/app/unixODBC-2.3.7/include/sqltypes.h' are the same file W: /usr/bin/install: 'sqlucode.h' and '/app/unixODBC-2.3.7/include/sqlucode.h' are the same file W: /usr/bin/install: 'sqlspi.h' and '/app/unixODBC-2.3.7/include/sqlspi.h' are the same file W: /usr/bin/install: 'autotest.h' and '/app/unixODBC-2.3.7/include/autotest.h' are the same file W: /usr/bin/install: 'uodbc_stats.h' and '/app/unixODBC-2.3.7/include/uodbc_stats.h' are the same file W: /usr/bin/install: 'uodbc_extras.h' and '/app/unixODBC-2.3.7/include/uodbc_extras.h' are the same file W: make[2]: *** [install-includeHEADERS] Error 1 Makefile:409: recipe for target 'install-includeHEADERS' failed make[2]: Leaving directory '/app/unixODBC-2.3.7/include' W: make[1]: *** [install-am] Error 2 Makefile:525: recipe for target 'install-am' failed make[1]: Leaving directory '/app/unixODBC-2.3.7/include' W: make: *** [install-recursive] Error 1 Makefile:548: recipe for target 'install-recursive' failed Any help with getting this dependency installed would be much appreciated.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/solved-pyodbc-install-fails/450",
        "relurl": "/t/solved-pyodbc-install-fails/450"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "50e55567fb501d05f60d895e44e520c2d2d20ebf",
        "title": "Can I serve custom error pages (4xx, 5xx)?",
        "description": "I’d like to serve a custom error page when the application returns a 5xx error rather than the standard http://Platform.sh error page. Can I do this?",
        "text": "I’d like to serve a custom error page when the application returns a 5xx error rather than the standard http://Platform.sh error page. Can I do this?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-i-serve-custom-error-pages-4xx-5xx/287",
        "relurl": "/t/can-i-serve-custom-error-pages-4xx-5xx/287"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "2feeec7c5d948f99388deddf1861814a002afb3a",
        "title": "How do I share files between multiple applications?",
        "description": "I have a multi-app project, and a mounted directory of files that I would like all applications to have access to. How do I accomplish this? Project structure: files/ application1/ .platform.app.yaml .. application2/ .platform.app.yaml .. .. ",
        "text": "I have a multi-app project, and a mounted directory of files that I would like all applications to have access to. How do I accomplish this? Project structure: files/ application1/ .platform.app.yaml .. application2/ .platform.app.yaml .. .. ",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-share-files-between-multiple-applications/304",
        "relurl": "/t/how-do-i-share-files-between-multiple-applications/304"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e044f84e01ee4fb54e93cb4184b05291219e5114",
        "title": "MongoDB configuration at Platform.sh",
        "description": "How the replica set works at http://Platform.sh? How to do a query on a MongoDB database using a UI client such as MongoDB Compass?",
        "text": "How the replica set works at http://Platform.sh? How to do a query on a MongoDB database using a UI client such as MongoDB Compass?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/mongodb-configuration-at-platform-sh/341",
        "relurl": "/t/mongodb-configuration-at-platform-sh/341"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "62a6a8b59dcaeb265016c9e4dca124c20e1748b9",
        "title": "Use the same root domain across multiple projects",
        "description": "We have two very similar projects and they’ll be served onto the same root domain, project1.mysite.com and project2.mysite.com. Since we’re using the public IPs of our cluster to connect DNS to http://P.sh, I do know you can only add a root domain once to the entire cluster (eu-2 in my case). In normal circumstances, you’ll add mysite.com as (default) domain and using routes.yaml to handle your routing. But what if we provide a full domain, rather then a root domain to our domains? platform domain:add project.mysite.com rather then platform domain:add mysite.com. Is this possible of does it have any drawbacks in using this approach?",
        "text": "We have two very similar projects and they’ll be served onto the same root domain, project1.mysite.com and project2.mysite.com. Since we’re using the public IPs of our cluster to connect DNS to http://P.sh, I do know you can only add a root domain once to the entire cluster (eu-2 in my case). In normal circumstances, you’ll add mysite.com as (default) domain and using routes.yaml to handle your routing. But what if we provide a full domain, rather then a root domain to our domains? platform domain:add project.mysite.com rather then platform domain:add mysite.com. Is this possible of does it have any drawbacks in using this approach?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/use-the-same-root-domain-across-multiple-projects/213",
        "relurl": "/t/use-the-same-root-domain-across-multiple-projects/213"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "6c3ed69901e599c563adbf6221aaed221efe7174",
        "title": "How do I monitor my application outgoing emails?",
        "description": "Hi Platform, My client application monitor physical hardware device and monitor users by mail. Therefore, emails are critical to the application. We need to ensure that email services are constantly working as expected and that emails are delivered correctly to users. How can I achieve that?",
        "text": "Hi Platform, My client application monitor physical hardware device and monitor users by mail. Therefore, emails are critical to the application. We need to ensure that email services are constantly working as expected and that emails are delivered correctly to users. How can I achieve that?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-monitor-my-application-outgoing-emails/243",
        "relurl": "/t/how-do-i-monitor-my-application-outgoing-emails/243"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e3bf6923188b2cde797850ec0eb982ee14f5f585",
        "title": "How can I access the live site using the original Platform-provided URLs?",
        "description": "If I “go live”, I lose the ability to go directly to the *.platformsh.site url. Can I go back to using that at any point?",
        "text": "If I “go live”, I lose the ability to go directly to the *.platformsh.site url. Can I go back to using that at any point?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-access-the-live-site-using-the-original-platform-provided-urls/279",
        "relurl": "/t/how-can-i-access-the-live-site-using-the-original-platform-provided-urls/279"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "7f5f3f80b98a86b4b897b8b6e49535db7b02b29c",
        "title": "How can I get information on the various Platform.sh regions?",
        "description": "For example, which ones are available, where are they, what IaaS infrastructure do they run on?",
        "text": "For example, which ones are available, where are they, what IaaS infrastructure do they run on?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-get-information-on-the-various-platform-sh-regions/331",
        "relurl": "/t/how-can-i-get-information-on-the-various-platform-sh-regions/331"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "3006582e1c36cbf81bf5a8f702e7254d760ad99c",
        "title": "The environment \"master\" is not currently active",
        "description": "I created a new project and tried to use the Platform CLI to SSH into my project’s environment via platform ssh and received the following warning message. Could not find applications: the environment \"master\" is not currently active. ",
        "text": "I created a new project and tried to use the Platform CLI to SSH into my project’s environment via platform ssh and received the following warning message. Could not find applications: the environment \"master\" is not currently active. ",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/the-environment-master-is-not-currently-active/268",
        "relurl": "/t/the-environment-master-is-not-currently-active/268"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "dc14f96a968e98d6df58d1d44cecad5abf257a04",
        "title": "Can these old TLS ciphers be deactivated?",
        "description": "When testing the encryption ciphers served by http://Platform.sh with my security scanner, some of them appear to be older/deprecated (like TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA and TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256). Can these be disabled ?",
        "text": "When testing the encryption ciphers served by http://Platform.sh with my security scanner, some of them appear to be older/deprecated (like TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA and TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256). Can these be disabled ?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-these-old-tls-ciphers-be-deactivated/463",
        "relurl": "/t/can-these-old-tls-ciphers-be-deactivated/463"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "74e79b60d6984e1ae5b1c930f013971aa1b4b489",
        "title": "Link to communicate multiple applications as web services in a form",
        "description": "Given four applications into a single project that will work as microservices: \"https://{default}/conferences\": id: conference type: upstream upstream: \"conference:http\" \"https://{default}/sessions\": id: session type: upstream upstream: \"session:http\" \"https://{default}/speakers\": id: speaker type: upstream upstream: \"speaker:http\" How can I make HTTP request to each other? E.g: Into conference application, how can I do a request into a speaker? Such as HTTP GET http://somehost/speakers",
        "text": "Given four applications into a single project that will work as microservices: \"https://{default}/conferences\": id: conference type: upstream upstream: \"conference:http\" \"https://{default}/sessions\": id: session type: upstream upstream: \"session:http\" \"https://{default}/speakers\": id: speaker type: upstream upstream: \"speaker:http\" How can I make HTTP request to each other? E.g: Into conference application, how can I do a request into a speaker? Such as HTTP GET http://somehost/speakers",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/link-to-communicate-multiple-applications-as-web-services-in-a-form/338",
        "relurl": "/t/link-to-communicate-multiple-applications-as-web-services-in-a-form/338"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "708e3a2034a4bab3a683acf60cdb829fab363fe3",
        "title": "Given a project that there are five microservices, is there any way to make four private and just one public?",
        "description": "Given four applications into a single project that will work as microservices: \"https://{default}/conferences\": id: conference type: upstream upstream: \"conference:http\" \"https://{default}/sessions\": id: session type: upstream upstream: \"session:http\" \"https://{default}/speakers\": id: speaker type: upstream upstream: \"speaker:http\" \"https://{default}/\": type: upstream upstream: \"app:http\" How can we make some resources as private? I mean, how can I make that all services internally communicate via HTTP, but just the app accesses publicly?",
        "text": "Given four applications into a single project that will work as microservices: \"https://{default}/conferences\": id: conference type: upstream upstream: \"conference:http\" \"https://{default}/sessions\": id: session type: upstream upstream: \"session:http\" \"https://{default}/speakers\": id: speaker type: upstream upstream: \"speaker:http\" \"https://{default}/\": type: upstream upstream: \"app:http\" How can we make some resources as private? I mean, how can I make that all services internally communicate via HTTP, but just the app accesses publicly?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/given-a-project-that-there-are-five-microservices-is-there-any-way-to-make-four-private-and-just-one-public/340",
        "relurl": "/t/given-a-project-that-there-are-five-microservices-is-there-any-way-to-make-four-private-and-just-one-public/340"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "d756f01f593957364ffb8278565639c01a08eb21",
        "title": "Reverse Proxy rules in router",
        "description": "Is it possible to make Reverse Proxy rules in the routes. I would like to Reverse Proxy to a legacy application for some parts which cannot be migrates easily to platform sh As far as I saw I can only add “redirect” rules. In the Netlify documentation I found that there you can configure something that they call “rewrite” rules: https://docs.netlify.com/routing/redirects/rewrites-proxies/ This is exactly the thing that I imagine having in platform sh For other purposes it would be great to have Reverse Proxy rules which can proxy to a WebSocket endpoint: https://www.nginx.com/blog/websocket-nginx/",
        "text": "Is it possible to make Reverse Proxy rules in the routes. I would like to Reverse Proxy to a legacy application for some parts which cannot be migrates easily to platform sh As far as I saw I can only add “redirect” rules. In the Netlify documentation I found that there you can configure something that they call “rewrite” rules: https://docs.netlify.com/routing/redirects/rewrites-proxies/ This is exactly the thing that I imagine having in platform sh For other purposes it would be great to have Reverse Proxy rules which can proxy to a WebSocket endpoint: https://www.nginx.com/blog/websocket-nginx/",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/reverse-proxy-rules-in-router/479",
        "relurl": "/t/reverse-proxy-rules-in-router/479"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "63be71397f0c0151ef64ee8332095165da6c3898",
        "title": "Is it possible to run X-Debug on Platform.sh?",
        "description": "I’d like to do step through debugging on my PHP projects.",
        "text": "I’d like to do step through debugging on my PHP projects.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-it-possible-to-run-x-debug-on-platform-sh/357",
        "relurl": "/t/is-it-possible-to-run-x-debug-on-platform-sh/357"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "f008b28e2686fb3de7e0d16422bceeb09767cb51",
        "title": "Will removing the account that created an integration break the integration?",
        "description": "in regards to integrations, if the project admin that creates the original integration is later removed from the project, will the integration break?",
        "text": "in regards to integrations, if the project admin that creates the original integration is later removed from the project, will the integration break?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/will-removing-the-account-that-created-an-integration-break-the-integration/204",
        "relurl": "/t/will-removing-the-account-that-created-an-integration-break-the-integration/204"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "a056c2d1ded02214e01c5666db347b1935447496",
        "title": "Is it possible to upsize my project plan using the CLI?",
        "description": "I’d like to automate my plan size changes so that I can trigger upsizes and downsizes. Can this be done with the CLI?",
        "text": "I’d like to automate my plan size changes so that I can trigger upsizes and downsizes. Can this be done with the CLI?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-it-possible-to-upsize-my-project-plan-using-the-cli/376",
        "relurl": "/t/is-it-possible-to-upsize-my-project-plan-using-the-cli/376"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "8cf18890ed19584272d6613350eb9a13a2f1c7eb",
        "title": "Is it possible to open direct access to a db service?",
        "description": "I need to allow auth0 to make queries to it in an incremental user migration. I could firewall it to an IP range for security, but I’m not sure how to do this on http://Platform.sh?",
        "text": "I need to allow auth0 to make queries to it in an incremental user migration. I could firewall it to an IP range for security, but I’m not sure how to do this on http://Platform.sh?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-it-possible-to-open-direct-access-to-a-db-service/362",
        "relurl": "/t/is-it-possible-to-open-direct-access-to-a-db-service/362"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "d78376eaeb5cb5fe82ddc78d0404e5d2858fc726",
        "title": "The connection link to several projects",
        "description": "How can I make HTTP request to each other? E.g: From my project application A, how can I do a request into a Project application B? Such as HTTP GET on http://somehost/projectB",
        "text": "How can I make HTTP request to each other? E.g: From my project application A, how can I do a request into a Project application B? Such as HTTP GET on http://somehost/projectB",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/the-connection-link-to-several-projects/339",
        "relurl": "/t/the-connection-link-to-several-projects/339"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "853b9c7b727b5d0a86c7d29559ed51ffe17454a9",
        "title": "Can I set a project variable to run different build scripts among several environments?",
        "description": "I would like a bash environment variable which would be different on environments at build time in order to build my app with it and configure different api endpoints differently.",
        "text": "I would like a bash environment variable which would be different on environments at build time in order to build my app with it and configure different api endpoints differently.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-i-set-a-project-variable-to-run-different-build-scripts-among-several-environments/249",
        "relurl": "/t/can-i-set-a-project-variable-to-run-different-build-scripts-among-several-environments/249"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "9a3063d439b4cf420cc9561d981d23ffcd9eb6b2",
        "title": "Is there a way to mount a network storage on the service instance, but not the app?",
        "description": "For example, to gain access to a backup of the Solr service?",
        "text": "For example, to gain access to a backup of the Solr service?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-there-a-way-to-mount-a-network-storage-on-the-service-instance-but-not-the-app/333",
        "relurl": "/t/is-there-a-way-to-mount-a-network-storage-on-the-service-instance-but-not-the-app/333"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e682e154c7a2352d0848e19be3d1538ea59e1d97",
        "title": "Is there a build dependencies cache on Platform.sh?",
        "description": "Are the contents of pip install, npm install, and yarn install cached from build to build?",
        "text": "Are the contents of pip install, npm install, and yarn install cached from build to build?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-there-a-build-dependencies-cache-on-platform-sh/464",
        "relurl": "/t/is-there-a-build-dependencies-cache-on-platform-sh/464"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "4be75a404886c29257645f420b417a5f8551d0c4",
        "title": "Why are my Let's Encrypt certificates failing to provision on my development environments?",
        "description": "I’ve noticed that occasionally Let’s Encrypt certificates will fail to provision on some of my development environments. The most recent example I have of this is when I’ve been working on the adding-emphemeral-redis branch of my project. They fail to provision on that branch, but not on any of my other environments.",
        "text": "I’ve noticed that occasionally Let’s Encrypt certificates will fail to provision on some of my development environments. The most recent example I have of this is when I’ve been working on the adding-emphemeral-redis branch of my project. They fail to provision on that branch, but not on any of my other environments.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/why-are-my-lets-encrypt-certificates-failing-to-provision-on-my-development-environments/459",
        "relurl": "/t/why-are-my-lets-encrypt-certificates-failing-to-provision-on-my-development-environments/459"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "9e393e9bb652758d6294bc73784c9d16218f1747",
        "title": "Is there a way to change the parent environment?",
        "description": "And, if I do so, will I lose data?",
        "text": "And, if I do so, will I lose data?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-there-a-way-to-change-the-parent-environment/295",
        "relurl": "/t/is-there-a-way-to-change-the-parent-environment/295"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "966eb00a25187a24fec6ba6dad38a3db04ba1bba",
        "title": "Is it possible to set a password on a MariaDB/MySQL service?",
        "description": "I have an application that throws an error if I don’t give it a password for a MySQL endpoint, but the default configuration for MySQL on http://Platform.sh has an empty string in the . How can I manually add a password to this service?",
        "text": "I have an application that throws an error if I don’t give it a password for a MySQL endpoint, but the default configuration for MySQL on http://Platform.sh has an empty string in the . How can I manually add a password to this service?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-it-possible-to-set-a-password-on-a-mariadb-mysql-service/432",
        "relurl": "/t/is-it-possible-to-set-a-password-on-a-mariadb-mysql-service/432"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "c18ad8b96eec6f93a45d936a0dacefe09a7bf80a",
        "title": "How can I reset or empty a service?",
        "description": "I have totally messed up my database’s data and schema. Can I just drop it and start fresh?",
        "text": "I have totally messed up my database’s data and schema. Can I just drop it and start fresh?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-can-i-reset-or-empty-a-service/381",
        "relurl": "/t/how-can-i-reset-or-empty-a-service/381"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "e5f021bc7a591bf78d60873f00f6eb5d871d6e35",
        "title": "Is it possible to post a test results somewhere?",
        "description": "Is it possible to place some artifacts from the testing somewhere publicly available after build is done? For example scenario: build is completed visual testing job triggered with Diffy ( https://diffy.website) test is completed and would like to post them somewhere I am thinking about a place where test artifacts can be stored. In our case this could be simple description and a link to actual report. Thanks Yuriy",
        "text": "Is it possible to place some artifacts from the testing somewhere publicly available after build is done? For example scenario: build is completed visual testing job triggered with Diffy ( https://diffy.website) test is completed and would like to post them somewhere I am thinking about a place where test artifacts can be stored. In our case this could be simple description and a link to actual report. Thanks Yuriy",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/is-it-possible-to-post-a-test-results-somewhere/379",
        "relurl": "/t/is-it-possible-to-post-a-test-results-somewhere/379"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "ae8a386c0b0c7ac2376882b444e987a410194d2c",
        "title": "If I have a custom certificate installed on my project and it expires, will there be an automatic switch to a LetsEncrypt cert?",
        "description": "I have a custom . If it expires what happens?",
        "text": "I have a custom . If it expires what happens?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/if-i-have-a-custom-certificate-installed-on-my-project-and-it-expires-will-there-be-an-automatic-switch-to-a-letsencrypt-cert/440",
        "relurl": "/t/if-i-have-a-custom-certificate-installed-on-my-project-and-it-expires-will-there-be-an-automatic-switch-to-a-letsencrypt-cert/440"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "93a5ba1096cdac4fc6212c29be897e958771b1a1",
        "title": "How do I serve some requests from a different PHP front controller?",
        "description": "Most of the requests for my app are served from public/index.php, but I want to serve requests to /api from public/sites/api/api.php. Is this possible on http://platform.sh?",
        "text": "Most of the requests for my app are served from public/index.php, but I want to serve requests to /api from public/sites/api/api.php. Is this possible on http://platform.sh?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-serve-some-requests-from-a-different-php-front-controller/487",
        "relurl": "/t/how-do-i-serve-some-requests-from-a-different-php-front-controller/487"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "0056ab3940df4e9d16a1ef60599ac50d81655a53",
        "title": "For persistent Redis, what persistence methods does Platform.sh employs? RDB, AOF, or both?",
        "description": "https://redis.io/topics/persistence",
        "text": "https://redis.io/topics/persistence",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/for-persistent-redis-what-persistence-methods-does-platform-sh-employs-rdb-aof-or-both/517",
        "relurl": "/t/for-persistent-redis-what-persistence-methods-does-platform-sh-employs-rdb-aof-or-both/517"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "530aec95c5c76f234f3a267d29ff78e433ecf3db",
        "title": "Where is my database storage going to?",
        "description": "Question I’ve followed the following article to check my database storage usage: https://community.platform.sh/t/how-to-determine-database-usage-of-an-environment/180 /c/how-to http://platform.sh/ https://github.com/platformsh/platformsh-cli http://platform.sh/ https://github.com/platformsh/platformsh-cli https://docs.platform.sh/development/ssh.html http://Platform.sh But the size it reports seems wrong to me. I’ve imported a 3GB sized database backup. But when I check platform db:size it is reporting 9GB. Checking database service db... +----------------+-----------------+--------+ | Allocated disk | Estimated usage | % used | +----------------+-----------------+--------+ | 14.6 GiB | 9.4 GiB | ~ 64% | +----------------+-----------------+--------+ Why the big difference? Answer There is some overhead in operating databases so that accounts for some disk space. (e.g. on mysql information_schema/mysql, etc…) If you just measured the file that you import, be aware that the space used in the file is always smaller than when it is unpacked into the database. Databases are faster and more flexible than a flat file. But they create additional data to be able to do this. The platform db:size makes a good estimate on how much is used, but it can deviate by a few percentages. InnoDB does not reclaim removed space. So everything you once inserted and removed, is still there. This is especially true for cache tables (which get a lot of insert/deletes). db:size actually has a trick to help you identify wasted space and clean those up. platform db:size --cleanup -p YOUR_PROJECT_ID -e master Checking database service db... +----------------+-----------------+--------+ | Allocated disk | Estimated usage | % used | +----------------+-----------------+--------+ | 14.6 GiB | 9.4 GiB | ~ 64% | +----------------+-----------------+--------+ Warning This is an estimate of the database's disk usage. It does not represent its real size on disk. You can save space by running the following commands during a maintenance window: ALTER TABLE `main`.`cache_page` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`xmlsitemap` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`cache_block` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`queue` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`cache_advagg_aggregates` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`sessions` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`batch` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`field_revision_field_address_line_3` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`field_data_field_paragraph_background` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`field_data_field_bs_comms` ENGINE=\"InnoDB\"; Warning: Running these may lock up your database for several minutes. Only run these when you know what you're doing. That should clean up some storage, and you can even run those queries in a cron once a week to keep it nice and clean (Sunday early morning for example). Recommendations Move cache tables to a dedicated instance that can take the load off your database. Make sure you have less than 80% disk used on databases. Databases don’t work well with little disk space. ",
        "text": "Question I’ve followed the following article to check my database storage usage: https://community.platform.sh/t/how-to-determine-database-usage-of-an-environment/180 /c/how-to http://platform.sh/ https://github.com/platformsh/platformsh-cli http://platform.sh/ https://github.com/platformsh/platformsh-cli https://docs.platform.sh/development/ssh.html http://Platform.sh But the size it reports seems wrong to me. I’ve imported a 3GB sized database backup. But when I check platform db:size it is reporting 9GB. Checking database service db... +----------------+-----------------+--------+ | Allocated disk | Estimated usage | % used | +----------------+-----------------+--------+ | 14.6 GiB | 9.4 GiB | ~ 64% | +----------------+-----------------+--------+ Why the big difference? Answer There is some overhead in operating databases so that accounts for some disk space. (e.g. on mysql information_schema/mysql, etc…) If you just measured the file that you import, be aware that the space used in the file is always smaller than when it is unpacked into the database. Databases are faster and more flexible than a flat file. But they create additional data to be able to do this. The platform db:size makes a good estimate on how much is used, but it can deviate by a few percentages. InnoDB does not reclaim removed space. So everything you once inserted and removed, is still there. This is especially true for cache tables (which get a lot of insert/deletes). db:size actually has a trick to help you identify wasted space and clean those up. platform db:size --cleanup -p YOUR_PROJECT_ID -e master Checking database service db... +----------------+-----------------+--------+ | Allocated disk | Estimated usage | % used | +----------------+-----------------+--------+ | 14.6 GiB | 9.4 GiB | ~ 64% | +----------------+-----------------+--------+ Warning This is an estimate of the database's disk usage. It does not represent its real size on disk. You can save space by running the following commands during a maintenance window: ALTER TABLE `main`.`cache_page` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`xmlsitemap` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`cache_block` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`queue` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`cache_advagg_aggregates` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`sessions` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`batch` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`field_revision_field_address_line_3` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`field_data_field_paragraph_background` ENGINE=\"InnoDB\"; ALTER TABLE `main`.`field_data_field_bs_comms` ENGINE=\"InnoDB\"; Warning: Running these may lock up your database for several minutes. Only run these when you know what you're doing. That should clean up some storage, and you can even run those queries in a cron once a week to keep it nice and clean (Sunday early morning for example). Recommendations Move cache tables to a dedicated instance that can take the load off your database. Make sure you have less than 80% disk used on databases. Databases don’t work well with little disk space. ",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/where-is-my-database-storage-going-to/467",
        "relurl": "/t/where-is-my-database-storage-going-to/467"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "674deb06dcbe9d66ce0807d74d8f229521656a4f",
        "title": "Expose Elasticsearch over HTTP",
        "description": "Is there a way to expose the Elasticsearch service for READ over HTTP through a route?",
        "text": "Is there a way to expose the Elasticsearch service for READ over HTTP through a route?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/expose-elasticsearch-over-http/510",
        "relurl": "/t/expose-elasticsearch-over-http/510"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "44618c600be830a9a3ad5e89ab322ff53842155a",
        "title": "Can I set a friendly/custom/vanity domain for my development branches on the grid?",
        "description": "I don’t like the look of the develop-fdn5g37-lsk5nfuysnbehv6.us-1.platformsh.site sort of link that I get with my non-master branches, and I’d like something shorter and cuter, or easier to remember for my presentations. Can I set it to be dev.myproject.org instead?",
        "text": "I don’t like the look of the develop-fdn5g37-lsk5nfuysnbehv6.us-1.platformsh.site sort of link that I get with my non-master branches, and I’d like something shorter and cuter, or easier to remember for my presentations. Can I set it to be dev.myproject.org instead?",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/can-i-set-a-friendly-custom-vanity-domain-for-my-development-branches-on-the-grid/559",
        "relurl": "/t/can-i-set-a-friendly-custom-vanity-domain-for-my-development-branches-on-the-grid/559"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "a3d4932b5ff493a1d60bc8c479d419b48e977734",
        "title": "Connecting to MongoDB with GUI tool like Robo 3T or Mongo Compass",
        "description": "Hey, Im wondering how I can connect via a GUI tool like Robo 3T or Mongo Compass to have a look in the database. Im able to tunnel into the database and use the mongo cli like so: % platform tunnel:open % mongo mongodb://main:main@localhost:30000/main ^ this works % mongodump --port 30000 -u main -p main --authenticationDatabase main --db main 2020-04-26T13:02:47.152+1200 writing main.startrek to 2020-04-26T13:02:47.302+1200 done dumping main.startrek (108 documents) ^ this also works but if I try to connect via a GUI tool it doesn’t, I’m unsure what I’m doing wrong because the tunnel is working because I can dump the database through it, as well as query it through mongo cli, just not through a GUI tool. https://community.platform.sh/uploads/default/01ec804d8f51ca22920fed5797a6c8116006a6de ",
        "text": "Hey, Im wondering how I can connect via a GUI tool like Robo 3T or Mongo Compass to have a look in the database. Im able to tunnel into the database and use the mongo cli like so: % platform tunnel:open % mongo mongodb://main:main@localhost:30000/main ^ this works % mongodump --port 30000 -u main -p main --authenticationDatabase main --db main 2020-04-26T13:02:47.152+1200 writing main.startrek to 2020-04-26T13:02:47.302+1200 done dumping main.startrek (108 documents) ^ this also works but if I try to connect via a GUI tool it doesn’t, I’m unsure what I’m doing wrong because the tunnel is working because I can dump the database through it, as well as query it through mongo cli, just not through a GUI tool. https://community.platform.sh/uploads/default/01ec804d8f51ca22920fed5797a6c8116006a6de ",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/connecting-to-mongodb-with-gui-tool-like-robo-3t-or-mongo-compass/544",
        "relurl": "/t/connecting-to-mongodb-with-gui-tool-like-robo-3t-or-mongo-compass/544"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "2d63b8924785554f1a6fb137f0ebb640799288f4",
        "title": "How to share code between multi-apps from a monorepo",
        "description": "I have several services in a monorepo that use a shared library. I want to be able to install the shared library when running tests and deploying. Here’s the layout .platform .platform/routes.yaml quote-api composer.json events composer.json .platform/routes.yaml \"https://quote-api.{default}/\": type: upstream upstream: \"quote-api:http\" quote-api/composer.json { \"require\": { \"paqman/events\": \"*\" }, \"repositories\": [ { \"type\": \"path\", \"url\": \"../events\" } ] } But when http://platform.sh is building each service, it says “Moving the application to the output directory” which I assume moves it to some sort of isolated area so now I get the error message: Source path \"../events\" is not found for package paqman/events Note: I do not want to use a package manager as I want to get the exact shared code with the commit (releasing a new package for a branch etc. would be difficult)",
        "text": "I have several services in a monorepo that use a shared library. I want to be able to install the shared library when running tests and deploying. Here’s the layout .platform .platform/routes.yaml quote-api composer.json events composer.json .platform/routes.yaml \"https://quote-api.{default}/\": type: upstream upstream: \"quote-api:http\" quote-api/composer.json { \"require\": { \"paqman/events\": \"*\" }, \"repositories\": [ { \"type\": \"path\", \"url\": \"../events\" } ] } But when http://platform.sh is building each service, it says “Moving the application to the output directory” which I assume moves it to some sort of isolated area so now I get the error message: Source path \"../events\" is not found for package paqman/events Note: I do not want to use a package manager as I want to get the exact shared code with the commit (releasing a new package for a branch etc. would be difficult)",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-share-code-between-multi-apps-from-a-monorepo/550",
        "relurl": "/t/how-to-share-code-between-multi-apps-from-a-monorepo/550"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "2d14e8df39b813f184e046f46c8a76fb836fb57f",
        "title": "How do I use MongoDB with my Java app?",
        "description": "Java is one of the most popular programming languages in the MongoDB Community. For new users, it’s essential to provide an overview of how to work with the MongoDB Java driver at http://Platform.sh. Java has several frameworks and combinations to work with MongoDB, but it’s important to provide an overview of how to work with the MongoDB Java driver and how to use MongoDB as a Java developer. Therefore, https://docs.platform.sh/languages/java.html integration between Java and MongoDB. This integration became easier thanks to the https://github.com/platformsh/config-reader-java . MongoDB database = config.getCredential(\"mongodb\", MongoDB::new); MongoClient mongoClient = database.get(); final MongoDatabase mongoDatabase = mongoClient.getDatabase(database.getDatabase()); MongoCollection collection = mongoDatabase.getCollection(\"scientist\"); Document doc = new Document(\"name\", \"Ada Lovelace\").append(\"city\", \"London\"); collection.insertOne(doc); Document myDoc = collection.find(eq(\"_id\", doc.get(\"_id\"))).first(); However, we have the option to overwrite the configurations only when it is running at http://Platform.sh such as https://community.platform.sh/t/how-to-overwrite-spring-data-mongodb-variable-to-access-platform-sh-services/528 and https://community.platform.sh/t/how-to-overwrite-configuration-to-jakarta-microprofile-to-access-platform-sh-services/520 . Articles https://platform.sh/blog/2019/spring-mvc-and-mongodb-a-match-made-in-platform.sh-heaven/ https://platform.sh/blog/2019/what-is-new-with-jakarta-nosql/ https://platform.sh/blog/2020/what-is-new-with-jakarta-nosql-part-ii/ https://dzone.com/articles/deploy-quarkus-faster-in-the-cloud-with-platformsh-3 More Java combinations We have several articles and source code on this repository: https://github.com/platformsh/java-quick-start https://github.com/platformsh/java-quick-start This repository has several Java combinations it includes Spring, Jakarta EE, MicroProfile, MongoDB, JPA, and so on.",
        "text": "Java is one of the most popular programming languages in the MongoDB Community. For new users, it’s essential to provide an overview of how to work with the MongoDB Java driver at http://Platform.sh. Java has several frameworks and combinations to work with MongoDB, but it’s important to provide an overview of how to work with the MongoDB Java driver and how to use MongoDB as a Java developer. Therefore, https://docs.platform.sh/languages/java.html integration between Java and MongoDB. This integration became easier thanks to the https://github.com/platformsh/config-reader-java . MongoDB database = config.getCredential(\"mongodb\", MongoDB::new); MongoClient mongoClient = database.get(); final MongoDatabase mongoDatabase = mongoClient.getDatabase(database.getDatabase()); MongoCollection collection = mongoDatabase.getCollection(\"scientist\"); Document doc = new Document(\"name\", \"Ada Lovelace\").append(\"city\", \"London\"); collection.insertOne(doc); Document myDoc = collection.find(eq(\"_id\", doc.get(\"_id\"))).first(); However, we have the option to overwrite the configurations only when it is running at http://Platform.sh such as https://community.platform.sh/t/how-to-overwrite-spring-data-mongodb-variable-to-access-platform-sh-services/528 and https://community.platform.sh/t/how-to-overwrite-configuration-to-jakarta-microprofile-to-access-platform-sh-services/520 . Articles https://platform.sh/blog/2019/spring-mvc-and-mongodb-a-match-made-in-platform.sh-heaven/ https://platform.sh/blog/2019/what-is-new-with-jakarta-nosql/ https://platform.sh/blog/2020/what-is-new-with-jakarta-nosql-part-ii/ https://dzone.com/articles/deploy-quarkus-faster-in-the-cloud-with-platformsh-3 More Java combinations We have several articles and source code on this repository: https://github.com/platformsh/java-quick-start https://github.com/platformsh/java-quick-start This repository has several Java combinations it includes Spring, Jakarta EE, MicroProfile, MongoDB, JPA, and so on.",
        "section": "Questions \u0026 Answers",
        "subsections": "Questions \u0026 Answers",
        "image": "",
        "url": "https://community.platform.sh//t/how-do-i-use-mongodb-with-my-java-app/582",
        "relurl": "/t/how-do-i-use-mongodb-with-my-java-app/582"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "1840f7a2235edbb4b002cf72a382452660db85e9",
        "title": "Spring and Platform.sh Getting started",
        "description": "Deploy Friday: E09 Spring Framework The Spring Framework is an application framework and inversion of control container for the Java platform. Try Spring: https://github.com/platformsh-templates/spring-mvc-maven-mongodb Articles Article Link https://dzone.com/articles/introduction-to-spring-data-mongodb-reactive-and-h https://github.com/platformsh/java-quick-start/tree/master/spring/spring-mongodb-reactive https://dzone.com/articles/introduction-of-spring-webflux-and-how-to-apply-cl https://github.com/platformsh/java-quick-start/tree/master/spring/spring-webflux https://platform.sh/blog/2019/spring-data-redis-in-the-cloud/ https://github.com/platformsh/java-quick-start/tree/master/spring/spring-boot-maven-redis https://platform.sh/blog/2019/simplify-your-script-build-with-gradle/ https://github.com/platformsh-templates/spring-boot-gradle-mysql https://platform.sh/blog/2019/elasticsearch-vs-solr-have-both-with-spring-data-and-platform.sh/ https://github.com/platformsh/java-quick-start/tree/master/spring/spring-mvc-maven-elasticsearch and https://github.com/platformsh/java-quick-start/tree/master/spring/spring-mvc-maven-solr https://platform.sh/blog/2019/spring-mvc-and-mongodb-a-match-made-in-platform.sh-heaven/ https://github.com/platformsh/java-quick-start/tree/master/spring/spring-mvc-maven-mongodb https://platform.sh/blog/2019/java-hello-world-at-platform.sh/ ",
        "text": "Deploy Friday: E09 Spring Framework The Spring Framework is an application framework and inversion of control container for the Java platform. Try Spring: https://github.com/platformsh-templates/spring-mvc-maven-mongodb Articles Article Link https://dzone.com/articles/introduction-to-spring-data-mongodb-reactive-and-h https://github.com/platformsh/java-quick-start/tree/master/spring/spring-mongodb-reactive https://dzone.com/articles/introduction-of-spring-webflux-and-how-to-apply-cl https://github.com/platformsh/java-quick-start/tree/master/spring/spring-webflux https://platform.sh/blog/2019/spring-data-redis-in-the-cloud/ https://github.com/platformsh/java-quick-start/tree/master/spring/spring-boot-maven-redis https://platform.sh/blog/2019/simplify-your-script-build-with-gradle/ https://github.com/platformsh-templates/spring-boot-gradle-mysql https://platform.sh/blog/2019/elasticsearch-vs-solr-have-both-with-spring-data-and-platform.sh/ https://github.com/platformsh/java-quick-start/tree/master/spring/spring-mvc-maven-elasticsearch and https://github.com/platformsh/java-quick-start/tree/master/spring/spring-mvc-maven-solr https://platform.sh/blog/2019/spring-mvc-and-mongodb-a-match-made-in-platform.sh-heaven/ https://github.com/platformsh/java-quick-start/tree/master/spring/spring-mvc-maven-mongodb https://platform.sh/blog/2019/java-hello-world-at-platform.sh/ ",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/spring-and-platform-sh-getting-started/583",
        "relurl": "/t/spring-and-platform-sh-getting-started/583"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "2441b1d1ee6e21d690f144f9dc9e040e15177750",
        "title": "Quarkus and Platform.sh Getting started",
        "description": "Deploy Friday: E06 Quarkus Supersonic Subatomic Java QuarkusIO, the Supersonic Subatomic Java, promises to deliver small artifacts, extremely fast boot time, and lower time-to-first-request. Try Quarkus: https://github.com/platformsh-templates/quarkus Articles Article Links https://dzone.com/articles/deploy-quarkus-faster-in-the-cloud-with-platformsh-3 https://dzone.com/articles/deploy-quarkus-faster-in-the-cloud-with-platformsh-2 https://dzone.com/articles/deploy-quarkus-faster-in-the-cloud-with-platformsh-1 https://github.com/platformsh-examples/quarkus/tree/master/elasticsearch https://dzone.com/articles/deploy-quarkus-faster-in-the-cloud-with-platformsh https://github.com/platformsh-examples/quarkus/tree/master/panache https://dzone.com/articles/quarkus-supersonic-subatomic-java-deploy-faster-in https://github.com/platformsh-examples/quarkus/tree/master/jpa https://dzone.com/articles/quarkus-supersonic-subatomic-java-goes-faster-in-t https://github.com/platformsh-templates/quarkus ",
        "text": "Deploy Friday: E06 Quarkus Supersonic Subatomic Java QuarkusIO, the Supersonic Subatomic Java, promises to deliver small artifacts, extremely fast boot time, and lower time-to-first-request. Try Quarkus: https://github.com/platformsh-templates/quarkus Articles Article Links https://dzone.com/articles/deploy-quarkus-faster-in-the-cloud-with-platformsh-3 https://dzone.com/articles/deploy-quarkus-faster-in-the-cloud-with-platformsh-2 https://dzone.com/articles/deploy-quarkus-faster-in-the-cloud-with-platformsh-1 https://github.com/platformsh-examples/quarkus/tree/master/elasticsearch https://dzone.com/articles/deploy-quarkus-faster-in-the-cloud-with-platformsh https://github.com/platformsh-examples/quarkus/tree/master/panache https://dzone.com/articles/quarkus-supersonic-subatomic-java-deploy-faster-in https://github.com/platformsh-examples/quarkus/tree/master/jpa https://dzone.com/articles/quarkus-supersonic-subatomic-java-goes-faster-in-t https://github.com/platformsh-templates/quarkus ",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/quarkus-and-platform-sh-getting-started/564",
        "relurl": "/t/quarkus-and-platform-sh-getting-started/564"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "09eae73ed02dc174db655f7e50e53d0c5e2b3940",
        "title": "Hokus is an open-source CMS for Hugo - completely free to use",
        "description": "Developers love Hugo because of its astonishing speed. Content editors love Hokus because it brings the same speed benefit while hiding the complexity of editing raw files to edit a website. When you save your content, you can see your updated website in your browser almost instantly. Hokus is a multi-platform desktop application (Windows, macOS, and Linux are supported). Because it runs on your computer, you can use it even when you are offline. Unlike Netlify, Hokus does not assume any infrastructure vendor, meaning that you are free to move your website to another host without losing the user interface you are used to. This tutorial will show how to install Hokus, run a website locally and then move it to the cloud easily with http://Platform.sh. Steps https://github.com/julianoappelklein/hokus/releases https://docs.platform.sh/gettingstarted/own-code/create-project.html https://github.com/platformsh-templates/hugo https://github.com/platformsh-templates/hugo it with https://docs.platform.sh/administration/integrations/github.html Download the project with a git clone from the Git Repository that you wish. Cick in new Site, plus on the left button to create a new project. https://community.platform.sh/uploads/default/41d507beda8230e89a9f0ecbfbab746e49341d90 Now we have a Site configured, let’s use it! (keep in mind that Hokus will assume a default configuration for your website and to add more fields to your forms, or new forms, you need to edit the file hokus.toml). Select the website you just created and select the workspace “source” Click source - click select - then in sidebar. https://community.platform.sh/uploads/default/672fe35c6168598b8817f31321be9d8b9581fa90 On the left, you’ll see both the Post and Main Config option. If your current post is not showing, beware of the folder structure. We also have the option to customize it with the hokus.toml file. On this repository we have the posts under the post structure; therefore, we went in the collections and changed the folder line: folder = \"content/post/\" Now it’s time to run your website locally. ON the right sidebar, click in the “play” icon, and select the default option. Your browser will open showing your website! Click in the Posts button. You have the option to create, edit and delete posts.Open an existing post or create a new one. https://community.platform.sh/uploads/default/6984f91e3f659b7917e81eb8120a3f98c69cebef All the posts work with Markdown, so you can add bullets or pictures with Markdown After editing the post, click on the Save button (bottom right corner). Your changes will update your website in your browser instantly! https://community.platform.sh/uploads/default/f949075cbc28d7222c0aceef32d961de56dfa94a Are you happy with what you have? Let’s move it to the cloud with http://Platform.sh. We need to set the https://docs.platform.sh/configuration/app-containers.html and the https://docs.platform.sh/configuration/routes.html you can see respectively. File: .platform.app.yaml # .platform.app.yaml # The name of this application, which must be unique within a project. name: 'app' # The type key specifies the language and version for your application. type: golang:1.14 # The hooks that will be triggered when the package is deployed. hooks: # Build hooks can modify the application files on disk but not access any services like databases. build: !include type: string path: build.sh # The size of the persistent disk of the application (in MB). disk: 5120 # The configuration of the application when it is exposed to the web. web: locations: '/': # The public directory of the application relative to its root. root: 'public' index: ['index.html'] scripts: false allow: true expires: 1d File `.platform/routes.yaml` \"https://{default}/\": type: upstream upstream: \"app:http\" The last step is to push your changes to a remote repository. git push origin master Done, Hugo, Hokus, and http://Platform.sh at your service! https://community.platform.sh/uploads/default/68458a2ccb5d235315d4399b1cf64a0c3e93355a References https://www.hokuscms.com/ https://github.com/platformsh-examples/hokus-cms ",
        "text": "Developers love Hugo because of its astonishing speed. Content editors love Hokus because it brings the same speed benefit while hiding the complexity of editing raw files to edit a website. When you save your content, you can see your updated website in your browser almost instantly. Hokus is a multi-platform desktop application (Windows, macOS, and Linux are supported). Because it runs on your computer, you can use it even when you are offline. Unlike Netlify, Hokus does not assume any infrastructure vendor, meaning that you are free to move your website to another host without losing the user interface you are used to. This tutorial will show how to install Hokus, run a website locally and then move it to the cloud easily with http://Platform.sh. Steps https://github.com/julianoappelklein/hokus/releases https://docs.platform.sh/gettingstarted/own-code/create-project.html https://github.com/platformsh-templates/hugo https://github.com/platformsh-templates/hugo it with https://docs.platform.sh/administration/integrations/github.html Download the project with a git clone from the Git Repository that you wish. Cick in new Site, plus on the left button to create a new project. https://community.platform.sh/uploads/default/41d507beda8230e89a9f0ecbfbab746e49341d90 Now we have a Site configured, let’s use it! (keep in mind that Hokus will assume a default configuration for your website and to add more fields to your forms, or new forms, you need to edit the file hokus.toml). Select the website you just created and select the workspace “source” Click source - click select - then in sidebar. https://community.platform.sh/uploads/default/672fe35c6168598b8817f31321be9d8b9581fa90 On the left, you’ll see both the Post and Main Config option. If your current post is not showing, beware of the folder structure. We also have the option to customize it with the hokus.toml file. On this repository we have the posts under the post structure; therefore, we went in the collections and changed the folder line: folder = \"content/post/\" Now it’s time to run your website locally. ON the right sidebar, click in the “play” icon, and select the default option. Your browser will open showing your website! Click in the Posts button. You have the option to create, edit and delete posts.Open an existing post or create a new one. https://community.platform.sh/uploads/default/6984f91e3f659b7917e81eb8120a3f98c69cebef All the posts work with Markdown, so you can add bullets or pictures with Markdown After editing the post, click on the Save button (bottom right corner). Your changes will update your website in your browser instantly! https://community.platform.sh/uploads/default/f949075cbc28d7222c0aceef32d961de56dfa94a Are you happy with what you have? Let’s move it to the cloud with http://Platform.sh. We need to set the https://docs.platform.sh/configuration/app-containers.html and the https://docs.platform.sh/configuration/routes.html you can see respectively. File: .platform.app.yaml # .platform.app.yaml # The name of this application, which must be unique within a project. name: 'app' # The type key specifies the language and version for your application. type: golang:1.14 # The hooks that will be triggered when the package is deployed. hooks: # Build hooks can modify the application files on disk but not access any services like databases. build: !include type: string path: build.sh # The size of the persistent disk of the application (in MB). disk: 5120 # The configuration of the application when it is exposed to the web. web: locations: '/': # The public directory of the application relative to its root. root: 'public' index: ['index.html'] scripts: false allow: true expires: 1d File `.platform/routes.yaml` \"https://{default}/\": type: upstream upstream: \"app:http\" The last step is to push your changes to a remote repository. git push origin master Done, Hugo, Hokus, and http://Platform.sh at your service! https://community.platform.sh/uploads/default/68458a2ccb5d235315d4399b1cf64a0c3e93355a References https://www.hokuscms.com/ https://github.com/platformsh-examples/hokus-cms ",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/hokus-is-an-open-source-cms-for-hugo-completely-free-to-use/551",
        "relurl": "/t/hokus-is-an-open-source-cms-for-hugo-completely-free-to-use/551"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "0c14095828f6a0fe91313d100b23ad19455292ec",
        "title": "Multiple Applications (Tomcat)",
        "description": "http://Platform.sh supports building multiple applications per project (for example, RESTful web services with a front-end, or a main website and a blog). All of the applications share a common configuration through the files present in the .platform/ folder at the root of the Git repository. Every application exists in its own subdirectory, and each of them contain its own application configuration via a .platform.app.yaml file, so that your directory structure looks something like this: $ ls -a .git/ .platform/ routes.yaml services.yaml app/ .platform.app.yaml pom.xml [...] app2/ .platform.app.yaml pom.xml [...] pom.xml When you push via Git, http://Platform.sh will build each application separately. Only the application(s) that have been modified will be rebuilt. https://github.com/platformsh-examples/tomcat-multi-app https://docs.platform.sh/configuration/app/multi-app.html ",
        "text": "http://Platform.sh supports building multiple applications per project (for example, RESTful web services with a front-end, or a main website and a blog). All of the applications share a common configuration through the files present in the .platform/ folder at the root of the Git repository. Every application exists in its own subdirectory, and each of them contain its own application configuration via a .platform.app.yaml file, so that your directory structure looks something like this: $ ls -a .git/ .platform/ routes.yaml services.yaml app/ .platform.app.yaml pom.xml [...] app2/ .platform.app.yaml pom.xml [...] pom.xml When you push via Git, http://Platform.sh will build each application separately. Only the application(s) that have been modified will be rebuilt. https://github.com/platformsh-examples/tomcat-multi-app https://docs.platform.sh/configuration/app/multi-app.html ",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/multiple-applications-tomcat/468",
        "relurl": "/t/multiple-applications-tomcat/468"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "dda9e1a0e25a119b8f5617658617061ff05295f6",
        "title": "Tips for running Java on Platform.sh",
        "description": "Goal This tutorial will explain some Java commands and some recommendations to optimize the process of running your Java applications safety on http://Platform.sh. Assumptions You either have a Java application that you want to run on http://Platform.sh, or you already have a Java application already running on http://Platform.sh A text editor of your choice. Steps Once you have your Java application running on http://Platform.sh, there are several steps you should take to optimize the application runtime. Minimum requirement (Memory) The first one is Xmx, which defines the maximum size that the JVM can use. The optimal value can be derived from the application’s generated configuration files and will scale with your container size automatically. To extract the appropriate value on the command line, use $(jq .info.limits.memory /run/config.json). The second parameter is ExitOnOutOfMemoryError. When you enable this option, the JVM exits on the first occurrence of an out-of-memory error. http://Platform.sh will restart the application automatically. These are the basic recommended parameters for running a Java application. So, the command will start with: java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError //The rest of the arguments and the jar file. Garbage collector (Optional) When migrating the application to a cloud environment, it is often essential to analyze the Garbage Collector’s log and behavior. For this, there are two options: Placing the log into the http://Platform.sh app.log file (which captures STDOUT). Creating a log file specifically for the GC. To use the STDOUT log, you can add the parameter -XX: + PrintGCDetails, E.g.: java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails //The rest of the arguments and the jar file. Java supports a number of different garbage collection strategies. Which one is optimal for your application will vary depending on your available memory, Java version, and application profile. Determining which is best for your application is out of scope, but the main options and how to enable them are: Name Command Flag Description Serial Garbage Collector -XX:+UseSerialGC This is the simplest GC implementation, as it basically works with a single thread. Parallel Garbage Collector -XX:+UseParallelGC Unlike Serial Garbage Collector, this uses multiple threads for managing heap space. But it also freezes other application threads while performing GC. CMS Garbage Collector -XX:+USeParNewGC The Concurrent Mark Sweep (CMS) implementation uses multiple garbage collector threads for garbage collection. It’s for applications that prefer shorter garbage collection pauses, and that can afford to share processor resources with the garbage collector while the application is running. G1 Garbage Collector -XX:+UseG1GC Garbage First, G1, is for applications running on multiprocessor machines with large memory space. The default stragtegy on Java 9 and later is G1. The GC strategy to use can be set in the start line with: Serial java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails -XX:+UseSerialGC //The rest of the arguments and the jar file. Parallel Garbage Collector java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails -XX:+UseParallelGC //The rest of the arguments and the jar file. CMS Garbage Collector java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails -XX:+USeParNewGC //The rest of the arguments and the jar file. G1 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails -XX:+UseG1GC //The rest of the arguments and the jar file. Java 8 Optimization Ideally, all applications should run the latest LTS release of the JVM at least. That is currently Java 11. Java 11 has a number of performance improvements, particularly on container-based environments such as http://Platform.sh. However, in many cases, this is not possible. If you are still running on Java 8 there are two additional considerations. The default garbage collector for Java 8 is Parallel GC. In most cases G1 will offer better performance. We recommend enabling it, as above. Furthermore, there is the UseStringDeduplication flag which works to eliminate duplicate Strings within the GC process. That flag can save between 13% to 30% of memory, depending on application. However, this can impact on the pause time of your app. java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails References https://community.platform.sh/t/java-memory-commands/481 https://community.platform.sh/t/how-to-migrate-my-java-application-to-platfrom-sh/529 https://community.platform.sh/t/garbage-collector-log/482 ",
        "text": "Goal This tutorial will explain some Java commands and some recommendations to optimize the process of running your Java applications safety on http://Platform.sh. Assumptions You either have a Java application that you want to run on http://Platform.sh, or you already have a Java application already running on http://Platform.sh A text editor of your choice. Steps Once you have your Java application running on http://Platform.sh, there are several steps you should take to optimize the application runtime. Minimum requirement (Memory) The first one is Xmx, which defines the maximum size that the JVM can use. The optimal value can be derived from the application’s generated configuration files and will scale with your container size automatically. To extract the appropriate value on the command line, use $(jq .info.limits.memory /run/config.json). The second parameter is ExitOnOutOfMemoryError. When you enable this option, the JVM exits on the first occurrence of an out-of-memory error. http://Platform.sh will restart the application automatically. These are the basic recommended parameters for running a Java application. So, the command will start with: java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError //The rest of the arguments and the jar file. Garbage collector (Optional) When migrating the application to a cloud environment, it is often essential to analyze the Garbage Collector’s log and behavior. For this, there are two options: Placing the log into the http://Platform.sh app.log file (which captures STDOUT). Creating a log file specifically for the GC. To use the STDOUT log, you can add the parameter -XX: + PrintGCDetails, E.g.: java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails //The rest of the arguments and the jar file. Java supports a number of different garbage collection strategies. Which one is optimal for your application will vary depending on your available memory, Java version, and application profile. Determining which is best for your application is out of scope, but the main options and how to enable them are: Name Command Flag Description Serial Garbage Collector -XX:+UseSerialGC This is the simplest GC implementation, as it basically works with a single thread. Parallel Garbage Collector -XX:+UseParallelGC Unlike Serial Garbage Collector, this uses multiple threads for managing heap space. But it also freezes other application threads while performing GC. CMS Garbage Collector -XX:+USeParNewGC The Concurrent Mark Sweep (CMS) implementation uses multiple garbage collector threads for garbage collection. It’s for applications that prefer shorter garbage collection pauses, and that can afford to share processor resources with the garbage collector while the application is running. G1 Garbage Collector -XX:+UseG1GC Garbage First, G1, is for applications running on multiprocessor machines with large memory space. The default stragtegy on Java 9 and later is G1. The GC strategy to use can be set in the start line with: Serial java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails -XX:+UseSerialGC //The rest of the arguments and the jar file. Parallel Garbage Collector java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails -XX:+UseParallelGC //The rest of the arguments and the jar file. CMS Garbage Collector java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails -XX:+USeParNewGC //The rest of the arguments and the jar file. G1 java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails -XX:+UseG1GC //The rest of the arguments and the jar file. Java 8 Optimization Ideally, all applications should run the latest LTS release of the JVM at least. That is currently Java 11. Java 11 has a number of performance improvements, particularly on container-based environments such as http://Platform.sh. However, in many cases, this is not possible. If you are still running on Java 8 there are two additional considerations. The default garbage collector for Java 8 is Parallel GC. In most cases G1 will offer better performance. We recommend enabling it, as above. Furthermore, there is the UseStringDeduplication flag which works to eliminate duplicate Strings within the GC process. That flag can save between 13% to 30% of memory, depending on application. However, this can impact on the pause time of your app. java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+UseG1GC -XX:+UseStringDeduplication -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails References https://community.platform.sh/t/java-memory-commands/481 https://community.platform.sh/t/how-to-migrate-my-java-application-to-platfrom-sh/529 https://community.platform.sh/t/garbage-collector-log/482 ",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/tips-for-running-java-on-platform-sh/531",
        "relurl": "/t/tips-for-running-java-on-platform-sh/531"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "eef2aafc2b60649d7d2d9fc2524852dce8152851",
        "title": "Java Memory commands",
        "description": "Java Heap space is used by the Java runtime to allocate memory to Objects and JRE classes. Whenever we create an object, it’s always created in the Heap space. This post, we’ll explain to how to set the memory size on http://Platform.sh. Garbage Collection runs on the heap memory to free the memory used by objects that don’t have any reference. Any object created in the heap space has global access and can be referenced from anywhere in the application. To set the memory size on JVM on http://Platform.sh we need to append the memory settings with the Java startup parameter. http://Platform.sh has a command that returns the memory that is available with -$(jq .info.limits.memory /run/config.json) that value is on megabytes. As a recommendation, you can use the fine maximum JVM memory. web: commands: start: java -jar -Xmx$(jq .info.limits.memory /run/config.json)m target/microprofile-microbundle.jar --port $PORT Below, more arguments to set the memory: -Xmn size Sets the initial and maximum size (in bytes) of the heap for the young generation (nursery). Append the letter k or K to indicate kilobytes, m or M to indicate megabytes, or g or G to indicate gigabytes. The young generation region of the heap is used for new objects. GC is performed in this region more often than in other regions. If the size for the young generation is too small, then a lot of minor garbage collections are performed. If the size is too large, then only full garbage collections are performed, which can take a long time to complete. Oracle recommends that you keep the size for the young generation greater than 25% and less than 50% of the overall heap size. #-Xms size Sets the initial size (in bytes) of the heap. This value must be a multiple of 1024 and greater than 1 MB. Append the letter k or K to indicate kilobytes, m or M to indicate megabytes, g or G to indicate gigabytes. -Xmx size Specifies the maximum size (in bytes) of the memory allocation pool in bytes. This value must be a multiple of 1024 and greater than 2 MB. Append the letter k or K to indicate kilobytes, m or M to indicate megabytes, and g or G to indicate gigabytes. The default value is chosen at runtime based on system configuration. For server deployments, -Xms and -Xmx are often set to the same value. As mentioned, previously use the $(jq .info.limits.memory /run/config.json) to create the Xmx command. E.g.: java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -Xnoclassgc Disables garbage collection (GC) of classes. This can save some GC time, which shortens interruptions during the application run. When you specify -Xnoclassgc at startup, the class objects in the application are left untouched during GC and are always considered live. -Xss size Sets the thread stack size (in bytes). Append the letter k or K to indicate KB, m or M to indicate MB, and g or G to indicate GB. ExitOnOutOfMemoryError When you enable this option, the JVM exits on the first occurrence of an out-of-memory error. It can be used if you prefer restarting an instance of the JVM rather than handling out of memory errors. Recommended Java memory command at http://Platform.sh java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError ... GC Log activated java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails ... References https://community.platform.sh/t/garbage-collector-log/482 ",
        "text": "Java Heap space is used by the Java runtime to allocate memory to Objects and JRE classes. Whenever we create an object, it’s always created in the Heap space. This post, we’ll explain to how to set the memory size on http://Platform.sh. Garbage Collection runs on the heap memory to free the memory used by objects that don’t have any reference. Any object created in the heap space has global access and can be referenced from anywhere in the application. To set the memory size on JVM on http://Platform.sh we need to append the memory settings with the Java startup parameter. http://Platform.sh has a command that returns the memory that is available with -$(jq .info.limits.memory /run/config.json) that value is on megabytes. As a recommendation, you can use the fine maximum JVM memory. web: commands: start: java -jar -Xmx$(jq .info.limits.memory /run/config.json)m target/microprofile-microbundle.jar --port $PORT Below, more arguments to set the memory: -Xmn size Sets the initial and maximum size (in bytes) of the heap for the young generation (nursery). Append the letter k or K to indicate kilobytes, m or M to indicate megabytes, or g or G to indicate gigabytes. The young generation region of the heap is used for new objects. GC is performed in this region more often than in other regions. If the size for the young generation is too small, then a lot of minor garbage collections are performed. If the size is too large, then only full garbage collections are performed, which can take a long time to complete. Oracle recommends that you keep the size for the young generation greater than 25% and less than 50% of the overall heap size. #-Xms size Sets the initial size (in bytes) of the heap. This value must be a multiple of 1024 and greater than 1 MB. Append the letter k or K to indicate kilobytes, m or M to indicate megabytes, g or G to indicate gigabytes. -Xmx size Specifies the maximum size (in bytes) of the memory allocation pool in bytes. This value must be a multiple of 1024 and greater than 2 MB. Append the letter k or K to indicate kilobytes, m or M to indicate megabytes, and g or G to indicate gigabytes. The default value is chosen at runtime based on system configuration. For server deployments, -Xms and -Xmx are often set to the same value. As mentioned, previously use the $(jq .info.limits.memory /run/config.json) to create the Xmx command. E.g.: java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -Xnoclassgc Disables garbage collection (GC) of classes. This can save some GC time, which shortens interruptions during the application run. When you specify -Xnoclassgc at startup, the class objects in the application are left untouched during GC and are always considered live. -Xss size Sets the thread stack size (in bytes). Append the letter k or K to indicate KB, m or M to indicate MB, and g or G to indicate GB. ExitOnOutOfMemoryError When you enable this option, the JVM exits on the first occurrence of an out-of-memory error. It can be used if you prefer restarting an instance of the JVM rather than handling out of memory errors. Recommended Java memory command at http://Platform.sh java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError ... GC Log activated java -jar -Xmx$(jq .info.limits.memory /run/config.json)m -XX:+ExitOnOutOfMemoryError -XX:+PrintGCDetails ... References https://community.platform.sh/t/garbage-collector-log/482 ",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/java-memory-commands/481",
        "relurl": "/t/java-memory-commands/481"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "c1b5b43e9322d5adfe76792220565f0760cafeb2",
        "title": "Garbage Collector Log",
        "description": "Java garbage collection is the process by which Java programs perform automatic memory management. This post, we’ll explain how to add the log in the http://Platform.sh application. How to Generate GC Log File? In order to understand the GC log, you first need to generate one. Passing the following system properties to your JVM would generate GC logs. To set the JVM log on http://Platform.sh we need to append it with the Java startup parameter. E.g., To define the GC log. From your app’s https://docs.platform.sh/configuration/app-containers.html . web: commands: start: java -jar -Xmx2048m -Xlog:gc*=debug:stdout:time,uptimemillis,tid target/microprofile-microbundle.jar --port $PORT When you append the GC log, the next step is to check the information, the default file to the registry is on /tmp/log/app.log. Therefore you can check the GC information from the command below once you’re inside the application container. tail -f /tmp/log/app.log To access the machine through ssh, please check the PSH documentation: https://docs.platform.sh/development/ssh.html -XX:+PrintGC The flag -XX:+PrintGC (or the alias -verbose:gc) activates the “simple” GC logging mode, which prints a line for every young generation GC and every full GC. -XX:+PrintGCDetails If we use -XX:+PrintGCDetails instead of -XX:+PrintGC, we activate the “detailed” GC logging mode which differs depending on the GC algorithm used. We start by taking a look at the output produced by a young generation GC using the Throughput Collector. Unified JVM Logging Java 9 comes with a unified logging architecture ( http://openjdk.java.net/jeps/158 ) that pipes a lot of messages that the JVM generates through the same mechanism, which can be configured with the -Xlog option. E.g.: -Xlog:gc*=debug:stdout:time,uptimemillis,tid -Xloggc By default the GC log is written to stdout. With -Xloggc: we may instead specify an output file. Note that this flag implicitly sets -XX:+PrintGC and -XX:+PrintGCTimeStamps as well. Still, I would recommend setting these flags explicitly if desired, in order to safeguard yourself against unexpected changes in new JVM versions. Migrate to Java 11 The benefits of using the latest LTS version are the increase of support to the containers, therefore, beyond the new APIs and security fixes, there is an increase of performance to run on GC. D Command-Line Options This appendix describes some command-line options that can be useful when diagnosing problems with the Java HotSpot VM. References https://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html https://stackify.com/what-is-java-garbage-collection/ https://dzone.com/articles/understanding-garbage-collection-log See also Java Memory arguments: https://community.platform.sh/t/java-memory-commands/481 ",
        "text": "Java garbage collection is the process by which Java programs perform automatic memory management. This post, we’ll explain how to add the log in the http://Platform.sh application. How to Generate GC Log File? In order to understand the GC log, you first need to generate one. Passing the following system properties to your JVM would generate GC logs. To set the JVM log on http://Platform.sh we need to append it with the Java startup parameter. E.g., To define the GC log. From your app’s https://docs.platform.sh/configuration/app-containers.html . web: commands: start: java -jar -Xmx2048m -Xlog:gc*=debug:stdout:time,uptimemillis,tid target/microprofile-microbundle.jar --port $PORT When you append the GC log, the next step is to check the information, the default file to the registry is on /tmp/log/app.log. Therefore you can check the GC information from the command below once you’re inside the application container. tail -f /tmp/log/app.log To access the machine through ssh, please check the PSH documentation: https://docs.platform.sh/development/ssh.html -XX:+PrintGC The flag -XX:+PrintGC (or the alias -verbose:gc) activates the “simple” GC logging mode, which prints a line for every young generation GC and every full GC. -XX:+PrintGCDetails If we use -XX:+PrintGCDetails instead of -XX:+PrintGC, we activate the “detailed” GC logging mode which differs depending on the GC algorithm used. We start by taking a look at the output produced by a young generation GC using the Throughput Collector. Unified JVM Logging Java 9 comes with a unified logging architecture ( http://openjdk.java.net/jeps/158 ) that pipes a lot of messages that the JVM generates through the same mechanism, which can be configured with the -Xlog option. E.g.: -Xlog:gc*=debug:stdout:time,uptimemillis,tid -Xloggc By default the GC log is written to stdout. With -Xloggc: we may instead specify an output file. Note that this flag implicitly sets -XX:+PrintGC and -XX:+PrintGCTimeStamps as well. Still, I would recommend setting these flags explicitly if desired, in order to safeguard yourself against unexpected changes in new JVM versions. Migrate to Java 11 The benefits of using the latest LTS version are the increase of support to the containers, therefore, beyond the new APIs and security fixes, there is an increase of performance to run on GC. D Command-Line Options This appendix describes some command-line options that can be useful when diagnosing problems with the Java HotSpot VM. References https://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html https://stackify.com/what-is-java-garbage-collection/ https://dzone.com/articles/understanding-garbage-collection-log See also Java Memory arguments: https://community.platform.sh/t/java-memory-commands/481 ",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/garbage-collector-log/482",
        "relurl": "/t/garbage-collector-log/482"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "2d036908792920f9482048b80392348b07fcf747",
        "title": "Java Email at Platform.sh",
        "description": "Electronic mail is a method of exchanging messages between people using electronic devices. In this post, we’ll explain how to send an email message with Java through http://Platform.sh. By default, only the master environment can send emails. For non-master environments, you can configure outgoing emails . Emails from http://Platform.sh are sent via a SendGrid-based SMTP proxy. Each http://Platform.sh project is provisioned as a SendGrid sub-account. These SendGrid sub-accounts are capped at 12k emails per month. Below the sample code that uses https://javaee.github.io/javamail/ : import sh.platform.config.Config; import javax.mail.Message; import javax.mail.MessagingException; import javax.mail.Session; import javax.mail.Transport; import javax.mail.internet.InternetAddress; import javax.mail.internet.MimeMessage; import java.util.Properties; import java.util.logging.Level; import java.util.logging.Logger; public class JavaEmailSender { private static final Logger LOGGER = Logger.getLogger(JavaEmailSender.class.getName()); public void send() { Config config = new Config(); String to = \"\";//change accordingly String from = \"\";//change accordingly String host = config.getSmtpHost(); //or IP address //Get the session object Properties properties = System.getProperties(); properties.setProperty(\"mail.smtp.host\", host); Session session = Session.getDefaultInstance(properties); //compose the message try { MimeMessage message = new MimeMessage(session); message.setFrom(new InternetAddress(from)); message.addRecipient(Message.RecipientType.TO, new InternetAddress(to)); message.setSubject(\"Ping\"); message.setText(\"Hello, this is example of sending email \"); // Send message Transport.send(message); System.out.println(\"message sent successfully....\"); } catch (MessagingException exp) { exp.printStackTrace(); LOGGER.log(Level.SEVERE, \"there is an error to send an message\", exp); } } } There is plenty of additional l documentation about using JavaMail, like this one, that shows how to send email with HTML format and an attachment: https://mkyong.com/java/java-how-to-send-email/ References https://docs.platform.sh/development/email.html https://mkyong.com/java/java-how-to-send-email/ https://javaee.github.io/javamail/ ",
        "text": "Electronic mail is a method of exchanging messages between people using electronic devices. In this post, we’ll explain how to send an email message with Java through http://Platform.sh. By default, only the master environment can send emails. For non-master environments, you can configure outgoing emails . Emails from http://Platform.sh are sent via a SendGrid-based SMTP proxy. Each http://Platform.sh project is provisioned as a SendGrid sub-account. These SendGrid sub-accounts are capped at 12k emails per month. Below the sample code that uses https://javaee.github.io/javamail/ : import sh.platform.config.Config; import javax.mail.Message; import javax.mail.MessagingException; import javax.mail.Session; import javax.mail.Transport; import javax.mail.internet.InternetAddress; import javax.mail.internet.MimeMessage; import java.util.Properties; import java.util.logging.Level; import java.util.logging.Logger; public class JavaEmailSender { private static final Logger LOGGER = Logger.getLogger(JavaEmailSender.class.getName()); public void send() { Config config = new Config(); String to = \"\";//change accordingly String from = \"\";//change accordingly String host = config.getSmtpHost(); //or IP address //Get the session object Properties properties = System.getProperties(); properties.setProperty(\"mail.smtp.host\", host); Session session = Session.getDefaultInstance(properties); //compose the message try { MimeMessage message = new MimeMessage(session); message.setFrom(new InternetAddress(from)); message.addRecipient(Message.RecipientType.TO, new InternetAddress(to)); message.setSubject(\"Ping\"); message.setText(\"Hello, this is example of sending email \"); // Send message Transport.send(message); System.out.println(\"message sent successfully....\"); } catch (MessagingException exp) { exp.printStackTrace(); LOGGER.log(Level.SEVERE, \"there is an error to send an message\", exp); } } } There is plenty of additional l documentation about using JavaMail, like this one, that shows how to send email with HTML format and an attachment: https://mkyong.com/java/java-how-to-send-email/ References https://docs.platform.sh/development/email.html https://mkyong.com/java/java-how-to-send-email/ https://javaee.github.io/javamail/ ",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/java-email-at-platform-sh/483",
        "relurl": "/t/java-email-at-platform-sh/483"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "f354d8595e6b65a7f014a653d644365b7ab5444f",
        "title": "How to kill stuck processes that block your deployments",
        "description": "One of the most common causes for environment builds that get stuck is a runaway/stuck process. To ensure data consistency, the deployment flow tries to terminate running processes gracefully. Sometimes, this does not work and the deployment ends up waiting forever due to a blocking process. Using two simple commands when connected to SSH can help you get things moving without having to wait for our support team to intervene. As an example, let’s assume a cron process is stuck. The demo application contains one blocker.sh script: #!/bin/sh sleep 3600 which is configured as an every 5 minute cron in .platform.app.yaml: blocker: spec: '*/5 * * * *' cmd: '/bin/bash /app/block.sh' Now, this process is blocking our new deployment - the log is stuck at: Redeploying environment master Preparing deployment Closing services router and app First thing to do is see if you can connect with SSH to the environment (this should work most of the time). If the SSH connection is successful, run: ps fuxa. The output will be a list of processes, similar to this one: web@app.0:~$ ps fuxa USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 153 0.9 0.0 197656 33176 ? Sl 15:05 0:00 /usr/bin/python2.7 /etc/platform/commands/notify web 159 0.0 0.0 271388 26620 ? Sl 15:05 0:00 \\_ /usr/bin/python2.7 /etc/platform/commands/notify web 162 0.0 0.0 4280 740 ? S 15:05 0:00 \\_ /bin/dash -c /bin/bash /app/blocker.sh web 165 0.0 0.0 12920 2740 ? S 15:05 0:00 \\_ /bin/bash /app/blocker.sh web 166 0.0 0.0 7580 668 ? S 15:05 0:00 \\_ sleep 3600 root 1 0.0 0.0 15816 1096 ? Ss+ 15:02 0:00 init [2] root 74 0.0 0.0 4204 1132 ? Ss 15:02 0:00 runsvdir -P /etc/service log: ................................................................................................................... root 80 0.0 0.0 4052 704 ? Ss 15:02 0:00 \\_ runsv tideways root 81 0.0 0.0 4052 696 ? Ss 15:02 0:00 \\_ runsv ssh root 90 0.0 0.0 72104 5608 ? S 15:02 0:00 | \\_ /usr/sbin/sshd -D root 167 0.0 0.0 94876 6472 ? Ss 15:06 0:00 | \\_ sshd: web [priv] web 173 0.0 0.0 94876 3668 ? S 15:06 0:00 | \\_ sshd: web@pts/0 web 174 0.0 0.0 21768 3852 pts/0 Ss 15:06 0:00 | \\_ -bash web 190 0.0 0.0 37448 3172 pts/0 R+ 15:06 0:00 | \\_ ps fuxa root 82 0.0 0.0 4052 700 ? Ss 15:02 0:00 \\_ runsv nginx root 115 0.0 0.0 36984 6684 ? S 15:02 0:00 | \\_ nginx: master process /usr/sbin/nginx -g daemon off; error_log /var/log/error.log; -c /etc/nginx/nginx.conf web 121 0.0 0.0 45460 11560 ? S 15:02 0:00 | \\_ nginx: worker process root 83 0.0 0.0 4052 752 ? Ss 15:02 0:00 \\_ runsv newrelic root 84 0.0 0.0 4052 644 ? Ss 15:02 0:00 \\_ runsv idmapd root 87 0.0 0.0 23348 2152 ? S 15:02 0:00 | \\_ /usr/sbin/rpc.idmapd -f -C -p /run/rpc_pipefs root 85 0.0 0.0 4052 700 ? Ss 15:02 0:00 \\_ runsv app web 111 0.0 0.0 359464 30288 ? Ss 15:02 0:00 \\_ php-fpm: master process (/etc/php/7.2-zts/fpm/php-fpm.conf) web 116 0.0 0.0 12932 296 ? S 15:02 0:00 \\_ /bin/bash /etc/platform/start-app web 117 0.0 0.0 7584 656 ? S 15:02 0:00 \\_ tee -a /var/log/app.log You can probably see already our stuck process is here: web 162 0.0 0.0 4280 740 ? S 15:05 0:00 \\_ /bin/dash -c /bin/bash /app/blocker.sh web 165 0.0 0.0 12920 2740 ? S 15:05 0:00 \\_ /bin/bash /app/blocker.sh web 166 0.0 0.0 7580 668 ? S 15:05 0:00 \\_ sleep 3600 The important thing in the above output is the number listed in the second column, after the web user: this is the process ID, a unique identifier for each running process. As we’re interested in stopping the process, it’d be very useful to somehow forcefully stop it. This can be done with the kill -9 command, followed by the list of process IDs you want to stop. Therefore, to stop the cron in our example, we’d need to run: kill -9 162 165 166 Be careful: there might be more processes that block the deployment ! Inspect the process list carefully (all application processes will be under the web user) and repeat the previous command for all of them. Once done, the SSH connection will be terminated and you’ll see the friendly: Message from bot@platform.sh at 15:09:36: This container is being dematerialized. See you on the other side. message. Your previously stuck deployment will now continue. Note: if the SSH connection cannot be established, you will need to open a support ticket.",
        "text": "One of the most common causes for environment builds that get stuck is a runaway/stuck process. To ensure data consistency, the deployment flow tries to terminate running processes gracefully. Sometimes, this does not work and the deployment ends up waiting forever due to a blocking process. Using two simple commands when connected to SSH can help you get things moving without having to wait for our support team to intervene. As an example, let’s assume a cron process is stuck. The demo application contains one blocker.sh script: #!/bin/sh sleep 3600 which is configured as an every 5 minute cron in .platform.app.yaml: blocker: spec: '*/5 * * * *' cmd: '/bin/bash /app/block.sh' Now, this process is blocking our new deployment - the log is stuck at: Redeploying environment master Preparing deployment Closing services router and app First thing to do is see if you can connect with SSH to the environment (this should work most of the time). If the SSH connection is successful, run: ps fuxa. The output will be a list of processes, similar to this one: web@app.0:~$ ps fuxa USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 153 0.9 0.0 197656 33176 ? Sl 15:05 0:00 /usr/bin/python2.7 /etc/platform/commands/notify web 159 0.0 0.0 271388 26620 ? Sl 15:05 0:00 \\_ /usr/bin/python2.7 /etc/platform/commands/notify web 162 0.0 0.0 4280 740 ? S 15:05 0:00 \\_ /bin/dash -c /bin/bash /app/blocker.sh web 165 0.0 0.0 12920 2740 ? S 15:05 0:00 \\_ /bin/bash /app/blocker.sh web 166 0.0 0.0 7580 668 ? S 15:05 0:00 \\_ sleep 3600 root 1 0.0 0.0 15816 1096 ? Ss+ 15:02 0:00 init [2] root 74 0.0 0.0 4204 1132 ? Ss 15:02 0:00 runsvdir -P /etc/service log: ................................................................................................................... root 80 0.0 0.0 4052 704 ? Ss 15:02 0:00 \\_ runsv tideways root 81 0.0 0.0 4052 696 ? Ss 15:02 0:00 \\_ runsv ssh root 90 0.0 0.0 72104 5608 ? S 15:02 0:00 | \\_ /usr/sbin/sshd -D root 167 0.0 0.0 94876 6472 ? Ss 15:06 0:00 | \\_ sshd: web [priv] web 173 0.0 0.0 94876 3668 ? S 15:06 0:00 | \\_ sshd: web@pts/0 web 174 0.0 0.0 21768 3852 pts/0 Ss 15:06 0:00 | \\_ -bash web 190 0.0 0.0 37448 3172 pts/0 R+ 15:06 0:00 | \\_ ps fuxa root 82 0.0 0.0 4052 700 ? Ss 15:02 0:00 \\_ runsv nginx root 115 0.0 0.0 36984 6684 ? S 15:02 0:00 | \\_ nginx: master process /usr/sbin/nginx -g daemon off; error_log /var/log/error.log; -c /etc/nginx/nginx.conf web 121 0.0 0.0 45460 11560 ? S 15:02 0:00 | \\_ nginx: worker process root 83 0.0 0.0 4052 752 ? Ss 15:02 0:00 \\_ runsv newrelic root 84 0.0 0.0 4052 644 ? Ss 15:02 0:00 \\_ runsv idmapd root 87 0.0 0.0 23348 2152 ? S 15:02 0:00 | \\_ /usr/sbin/rpc.idmapd -f -C -p /run/rpc_pipefs root 85 0.0 0.0 4052 700 ? Ss 15:02 0:00 \\_ runsv app web 111 0.0 0.0 359464 30288 ? Ss 15:02 0:00 \\_ php-fpm: master process (/etc/php/7.2-zts/fpm/php-fpm.conf) web 116 0.0 0.0 12932 296 ? S 15:02 0:00 \\_ /bin/bash /etc/platform/start-app web 117 0.0 0.0 7584 656 ? S 15:02 0:00 \\_ tee -a /var/log/app.log You can probably see already our stuck process is here: web 162 0.0 0.0 4280 740 ? S 15:05 0:00 \\_ /bin/dash -c /bin/bash /app/blocker.sh web 165 0.0 0.0 12920 2740 ? S 15:05 0:00 \\_ /bin/bash /app/blocker.sh web 166 0.0 0.0 7580 668 ? S 15:05 0:00 \\_ sleep 3600 The important thing in the above output is the number listed in the second column, after the web user: this is the process ID, a unique identifier for each running process. As we’re interested in stopping the process, it’d be very useful to somehow forcefully stop it. This can be done with the kill -9 command, followed by the list of process IDs you want to stop. Therefore, to stop the cron in our example, we’d need to run: kill -9 162 165 166 Be careful: there might be more processes that block the deployment ! Inspect the process list carefully (all application processes will be under the web user) and repeat the previous command for all of them. Once done, the SSH connection will be terminated and you’ll see the friendly: Message from bot@platform.sh at 15:09:36: This container is being dematerialized. See you on the other side. message. Your previously stuck deployment will now continue. Note: if the SSH connection cannot be established, you will need to open a support ticket.",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/how-to-kill-stuck-processes-that-block-your-deployments/473",
        "relurl": "/t/how-to-kill-stuck-processes-that-block-your-deployments/473"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "fecba4a9ca27e045ed28286c41c431a09f85f6da",
        "title": "Consequences of restoring backups after the persistent data model has been changed",
        "description": "Goal The ability to restore environment backups is a powerful tool, but only when backups are taken regularly to reflect the rapid changes that can occur over the course of the development of your application. Failing to do so has some consequences, as older backups that don’t reflect major changes can result in data loss once restored. The goal in this tutorial is to explore some of these consequences, and to illustrate how taking the time to https://community.platform.sh/t/how-to-set-up-automated-environment-tasks/127 can alleviate plenty of headaches down the line. Preparation You will need: A http://Platform.sh account A http://Platform.sh project http://Platform.sh CLI installed locally A local copy of that project Problems http://Platform.sh gives you the ability to quickly create backups of the state of your environments, and then easily restore those backups to those environments should you need to. This is not yet an automatic process, and both creation and restoration must be executed manually by the user. But, of course, people forget to take frequent backups - they’re busy developing. It’s for this reason that we often recommend users https://community.platform.sh/t/how-to-set-up-automated-environment-tasks/127 in order to automate this process. If you have not yet set up automatic backups as described in the above How-to, you may very well find yourself in the following situation. I’ve created a backup close to the start of an environment’s history. I’ve made a lot of changes to that environment (added services, added data to those services, created new mounts, etc.). Something went wrong. Well, look, I have a backup right here. But it’s old. What will happen? This tutorial is meant to explore this exact scenario, with the intended takeaways: the importance of setting up automatic backups on your projects using the How-to above as a guide (so you don’t find yourself in this situation in the first place). some greater understanding of how http://Platform.sh handles your data during backups, restores, and syncs. Steps Creating backups note This tutorial uses a modified version of our https://github.com/platformsh/language-examples project. That project is intended to show how to interact with all of our managed services with each of our runtimes. It’s a great resource, and it even powers most of the examples in our public documentation, but it’s gigantic, and much more than we need for this tutorial. We removed all of the runtimes except for Python, leaving us with only the main and python application container directories. We also removed all of the services save postgresql, but we will be adding a few of them back in the steps below. It is not necessary that you use the same repository if going through this tutorial step by step, just try to match the steps in your own project. 1. Create a new branch Assuming you have already created a project and initialized it with some code, create a new environment platform branch add-mongo master 2. Create a backup Then create a backup of that environment in its current state. platform backup:create Creating a backup of add-mongo Waiting for the activity is5ytp2lbm7p4 (Chad Carlson created a backup of add-mongo): Creating snapshot of add-mongo Created backup bivzemjpeqvohqey2t7fo7vs5m [============================] 15 secs (complete) Activity is5ytp2lbm7p4 succeeded Backup name: bivzemjpeqvohqey2t7fo7vs5m So what happened here? Each service, which includes applications, has its own persistent storage. During a backup, a copy is made for each of them. It’s the collection of these backups that then makes up backup bivzemjpeqvohqey2t7fo7vs5m. Adding a new service 1. Configure a new service First, let’s add a new service to the project, one that did not exist when the backup was created. Modify your services.yaml file to include MongoDB: dbmongo: type: mongodb:3.6 disk: 1024 size: S and your .platform.app.yaml to include the new relationship: relationships: mongodb: 'dbmongo:mongodb' Then commit and push the changes to http://Platform.sh. git add . git commit -m \"Adds mongodb.\" git push platform add-mongo http://Platform.sh will provision MongoDB to your virtual cluster, and expose the following credentials in your PLATFORM_RELATIONSHIPS environment variable: { \"username\": \"main\", \"scheme\": \"mongodb\", \"service\": \"mongodb\", \"ip\": \"169.254.117.167\", \"hostname\": \"ldh423mk2e7o6qto2syljqbg5u.mongodb.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"mongodb.internal\", \"rel\": \"mongodb\", \"path\": \"main\", \"query\": { \"is_master\": true }, \"password\": \"main\", \"type\": \"mongodb:3.6\", \"port\": 27017 } 2. Develop and verify At this point, MongoDB is available for you to develop with. Over the next few days, you may make the following changes as you are developing on the environment: you create a new collection in main called starwars. your application adds a number of documents to that collection. (In the case of our language examples project, the Python app adds a document with the contents {\"name\": \"Rey\", \"occupation\": \"Jedi\"} as a test each time the site is visited) You can verify those documents have been added locally by opening an SSH tunnel to the service (platform:tunnel single -r mongodb) and then connecting to MongoDB via that tunnel and the credentials above: $ mongo --port 30000 -u main -p main --authenticationDatabase main MongoDB shell version v4.0.3 use main switched to db main show collections starwars db.starwars.find() { \"_id\" : ObjectId(\"5e4457d05908440effc53a20\"), \"name\" : \"Rey\", \"occupation\" : \"Jedi\" } { \"_id\" : ObjectId(\"5e4457d35908440effc53a22\"), \"name\" : \"Rey\", \"occupation\" : \"Jedi\" } { \"_id\" : ObjectId(\"5e4457d75908440effc53a24\"), \"name\" : \"Rey\", \"occupation\" : \"Jedi\" } { \"_id\" : ObjectId(\"5e4457d95908440effc53a26\"), \"name\" : \"Rey\", \"occupation\" : \"Jedi\" } { \"_id\" : ObjectId(\"5e44585c5908440effc53a28\"), \"name\" : \"Rey\", \"occupation\" : \"Jedi\" } There’s all the newly added data. 3. Restore the backup Now, if we restore the backup we created at the beginning (before MongoDB was a part of the cluster) using the command platform backup:restore bivzemjpeqvohqey2t7fo7vs5m The restore will do a few things: All persistent data currently in that environment is wiped. Backup bivzemjpeqvohqey2t7fo7vs5m, which contains a backup of each service present when it was taken, is then applied to the containers present in the environment one by one. Because of that first point, and because no backup for MongoDB exists in backup bivzemjpeqvohqey2t7fo7vs5m, all data and code pertaining to MongoDB is erased before anything else happens. If you attempt to open a tunnel and locally connect to MongoDB straight away, the service won’t even be recognized as existing in the project. Run platform redeploy, re-open the tunnel to MongoDB, and then repeat the prior steps to connect to MongoDB: $ mongo --port 30000 -u main -p main --authenticationDatabase main MongoDB shell version v4.0.3 use main switched to db main show collections starwars As you can see, the added collection no longer exists on the service. This is our first example of why it’s important to create backups regularly. If one had been taken some time after MongoDB existed, we would still have been able to keep some of its data. Mounts 1. Configure a new mount Another case where this would be relevant is the addition of new mounts to the project. If you were to create a mount that was not included in backup bivzemjpeqvohqey2t7fo7vs5m, would the files in that mount be erased? You can probably guess already based on the previous example, but let’s find out. You can add a mount to an application by adding the following lines to your .platform.app.yaml file: mounts: 'add-mount': source: local source_path: add-mount Then commit and push to http://Platform.sh: git add . git commit -m \"Add a mount.\" git push platform add-mongo 2. Add data and verify Let’s just create a simple file mkdir mount-data \u0026\u0026 touch mount-data/test.txt echo \"Here's our mounted data on Platform.sh.\" mount-data/test.txt and then upload it to the newly defined mount: platform mount:upload --mount add-mount --source ./mount-data You can then verify that the file was uploaded to the project by runing platform ssh to SSH into the application container. Then run, web@app.0:~$ echo \"$( ",
        "text": "Goal The ability to restore environment backups is a powerful tool, but only when backups are taken regularly to reflect the rapid changes that can occur over the course of the development of your application. Failing to do so has some consequences, as older backups that don’t reflect major changes can result in data loss once restored. The goal in this tutorial is to explore some of these consequences, and to illustrate how taking the time to https://community.platform.sh/t/how-to-set-up-automated-environment-tasks/127 can alleviate plenty of headaches down the line. Preparation You will need: A http://Platform.sh account A http://Platform.sh project http://Platform.sh CLI installed locally A local copy of that project Problems http://Platform.sh gives you the ability to quickly create backups of the state of your environments, and then easily restore those backups to those environments should you need to. This is not yet an automatic process, and both creation and restoration must be executed manually by the user. But, of course, people forget to take frequent backups - they’re busy developing. It’s for this reason that we often recommend users https://community.platform.sh/t/how-to-set-up-automated-environment-tasks/127 in order to automate this process. If you have not yet set up automatic backups as described in the above How-to, you may very well find yourself in the following situation. I’ve created a backup close to the start of an environment’s history. I’ve made a lot of changes to that environment (added services, added data to those services, created new mounts, etc.). Something went wrong. Well, look, I have a backup right here. But it’s old. What will happen? This tutorial is meant to explore this exact scenario, with the intended takeaways: the importance of setting up automatic backups on your projects using the How-to above as a guide (so you don’t find yourself in this situation in the first place). some greater understanding of how http://Platform.sh handles your data during backups, restores, and syncs. Steps Creating backups note This tutorial uses a modified version of our https://github.com/platformsh/language-examples project. That project is intended to show how to interact with all of our managed services with each of our runtimes. It’s a great resource, and it even powers most of the examples in our public documentation, but it’s gigantic, and much more than we need for this tutorial. We removed all of the runtimes except for Python, leaving us with only the main and python application container directories. We also removed all of the services save postgresql, but we will be adding a few of them back in the steps below. It is not necessary that you use the same repository if going through this tutorial step by step, just try to match the steps in your own project. 1. Create a new branch Assuming you have already created a project and initialized it with some code, create a new environment platform branch add-mongo master 2. Create a backup Then create a backup of that environment in its current state. platform backup:create Creating a backup of add-mongo Waiting for the activity is5ytp2lbm7p4 (Chad Carlson created a backup of add-mongo): Creating snapshot of add-mongo Created backup bivzemjpeqvohqey2t7fo7vs5m [============================] 15 secs (complete) Activity is5ytp2lbm7p4 succeeded Backup name: bivzemjpeqvohqey2t7fo7vs5m So what happened here? Each service, which includes applications, has its own persistent storage. During a backup, a copy is made for each of them. It’s the collection of these backups that then makes up backup bivzemjpeqvohqey2t7fo7vs5m. Adding a new service 1. Configure a new service First, let’s add a new service to the project, one that did not exist when the backup was created. Modify your services.yaml file to include MongoDB: dbmongo: type: mongodb:3.6 disk: 1024 size: S and your .platform.app.yaml to include the new relationship: relationships: mongodb: 'dbmongo:mongodb' Then commit and push the changes to http://Platform.sh. git add . git commit -m \"Adds mongodb.\" git push platform add-mongo http://Platform.sh will provision MongoDB to your virtual cluster, and expose the following credentials in your PLATFORM_RELATIONSHIPS environment variable: { \"username\": \"main\", \"scheme\": \"mongodb\", \"service\": \"mongodb\", \"ip\": \"169.254.117.167\", \"hostname\": \"ldh423mk2e7o6qto2syljqbg5u.mongodb.service._.eu-3.platformsh.site\", \"cluster\": \"rjify4yjcwxaa-master-7rqtwti\", \"host\": \"mongodb.internal\", \"rel\": \"mongodb\", \"path\": \"main\", \"query\": { \"is_master\": true }, \"password\": \"main\", \"type\": \"mongodb:3.6\", \"port\": 27017 } 2. Develop and verify At this point, MongoDB is available for you to develop with. Over the next few days, you may make the following changes as you are developing on the environment: you create a new collection in main called starwars. your application adds a number of documents to that collection. (In the case of our language examples project, the Python app adds a document with the contents {\"name\": \"Rey\", \"occupation\": \"Jedi\"} as a test each time the site is visited) You can verify those documents have been added locally by opening an SSH tunnel to the service (platform:tunnel single -r mongodb) and then connecting to MongoDB via that tunnel and the credentials above: $ mongo --port 30000 -u main -p main --authenticationDatabase main MongoDB shell version v4.0.3 use main switched to db main show collections starwars db.starwars.find() { \"_id\" : ObjectId(\"5e4457d05908440effc53a20\"), \"name\" : \"Rey\", \"occupation\" : \"Jedi\" } { \"_id\" : ObjectId(\"5e4457d35908440effc53a22\"), \"name\" : \"Rey\", \"occupation\" : \"Jedi\" } { \"_id\" : ObjectId(\"5e4457d75908440effc53a24\"), \"name\" : \"Rey\", \"occupation\" : \"Jedi\" } { \"_id\" : ObjectId(\"5e4457d95908440effc53a26\"), \"name\" : \"Rey\", \"occupation\" : \"Jedi\" } { \"_id\" : ObjectId(\"5e44585c5908440effc53a28\"), \"name\" : \"Rey\", \"occupation\" : \"Jedi\" } There’s all the newly added data. 3. Restore the backup Now, if we restore the backup we created at the beginning (before MongoDB was a part of the cluster) using the command platform backup:restore bivzemjpeqvohqey2t7fo7vs5m The restore will do a few things: All persistent data currently in that environment is wiped. Backup bivzemjpeqvohqey2t7fo7vs5m, which contains a backup of each service present when it was taken, is then applied to the containers present in the environment one by one. Because of that first point, and because no backup for MongoDB exists in backup bivzemjpeqvohqey2t7fo7vs5m, all data and code pertaining to MongoDB is erased before anything else happens. If you attempt to open a tunnel and locally connect to MongoDB straight away, the service won’t even be recognized as existing in the project. Run platform redeploy, re-open the tunnel to MongoDB, and then repeat the prior steps to connect to MongoDB: $ mongo --port 30000 -u main -p main --authenticationDatabase main MongoDB shell version v4.0.3 use main switched to db main show collections starwars As you can see, the added collection no longer exists on the service. This is our first example of why it’s important to create backups regularly. If one had been taken some time after MongoDB existed, we would still have been able to keep some of its data. Mounts 1. Configure a new mount Another case where this would be relevant is the addition of new mounts to the project. If you were to create a mount that was not included in backup bivzemjpeqvohqey2t7fo7vs5m, would the files in that mount be erased? You can probably guess already based on the previous example, but let’s find out. You can add a mount to an application by adding the following lines to your .platform.app.yaml file: mounts: 'add-mount': source: local source_path: add-mount Then commit and push to http://Platform.sh: git add . git commit -m \"Add a mount.\" git push platform add-mongo 2. Add data and verify Let’s just create a simple file mkdir mount-data \u0026\u0026 touch mount-data/test.txt echo \"Here's our mounted data on Platform.sh.\" mount-data/test.txt and then upload it to the newly defined mount: platform mount:upload --mount add-mount --source ./mount-data You can then verify that the file was uploaded to the project by runing platform ssh to SSH into the application container. Then run, web@app.0:~$ echo \"$( ",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/consequences-of-restoring-backups-after-the-persistent-data-model-has-been-changed/461",
        "relurl": "/t/consequences-of-restoring-backups-after-the-persistent-data-model-has-been-changed/461"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "879db006eecc051f69937eecbb477e89999bfe8c",
        "title": "Preventing stuck deployments caused by long-running cron jobs",
        "description": "The default way of deploying to production on http://Platform.sh is to simply push master to the platform remote. However, if a cron job is running, , and the deployment will not finish until the cron job is finished. When this happens, the application goes down. In order to prevent this from happening, you can use a custom deployment script that first makes sure that no Django management commands are running in the production environment. In the example below, we are checking if any Django management commands are running. If so, we abort the deployment. I have tested the script using Python 3.7.5, but it should work on 3.6.0 and later. If you’re not running Python, you can use it as inspiration for porting it to your language of choice. If you do, please consider sharing it with the community, for example as a reply to this post. As a caveat, note that if a cron job starts after the build has started, it might still block the deployment. The probability of this increases if your build takes a long time. #!/usr/bin/env python import sys from os import getenv from subprocess import CompletedProcess, run def git(command: str) - None: run(f\"git {command}\", shell=True) def main() - None: git(\"checkout master\") git(\"pull origin\") print(\"Getting running processes in order to prevent stuck deployment…\") completed_process: CompletedProcess = run( f\"ssh {getenv('PLATFORM_SH_SSH_DESTINATION')} 'ps auxf'\", capture_output=True, shell=True, ) output = str(completed_process.stdout) print(\"Making sure that process list was received…\") if \"/app/.global/bin/gunicorn\" not in output: sys.exit(\"Gunicorn process not found. Are you able to connect using SSH?\") print(\"Process list received. Checking for running Django management commands…\") if \"manage.py\" in output: sys.exit(\"Running Django management command detected. Not safe to continue.\") print( \"No running commands detected. Pushing changes to the production environment…\" ) git(\"push platform\") if __name__ == \"__main__\": main() To use the script: Create a Python file, e.g. deploy.py, in the root of your project and paste the script contents into it. Set the environment variable PLATFORM_SH_SSH_DESTINATION to the user@host destination of your production environment. Check if there are any modifications you need to make. For example, you may be running other kinds of jobs than just Django management commands, so you may need to add other strings in addition to manage.py Run ./deploy.py when you’re ready to deploy. Commit the script so you don’t lose it. ",
        "text": "The default way of deploying to production on http://Platform.sh is to simply push master to the platform remote. However, if a cron job is running, , and the deployment will not finish until the cron job is finished. When this happens, the application goes down. In order to prevent this from happening, you can use a custom deployment script that first makes sure that no Django management commands are running in the production environment. In the example below, we are checking if any Django management commands are running. If so, we abort the deployment. I have tested the script using Python 3.7.5, but it should work on 3.6.0 and later. If you’re not running Python, you can use it as inspiration for porting it to your language of choice. If you do, please consider sharing it with the community, for example as a reply to this post. As a caveat, note that if a cron job starts after the build has started, it might still block the deployment. The probability of this increases if your build takes a long time. #!/usr/bin/env python import sys from os import getenv from subprocess import CompletedProcess, run def git(command: str) - None: run(f\"git {command}\", shell=True) def main() - None: git(\"checkout master\") git(\"pull origin\") print(\"Getting running processes in order to prevent stuck deployment…\") completed_process: CompletedProcess = run( f\"ssh {getenv('PLATFORM_SH_SSH_DESTINATION')} 'ps auxf'\", capture_output=True, shell=True, ) output = str(completed_process.stdout) print(\"Making sure that process list was received…\") if \"/app/.global/bin/gunicorn\" not in output: sys.exit(\"Gunicorn process not found. Are you able to connect using SSH?\") print(\"Process list received. Checking for running Django management commands…\") if \"manage.py\" in output: sys.exit(\"Running Django management command detected. Not safe to continue.\") print( \"No running commands detected. Pushing changes to the production environment…\" ) git(\"push platform\") if __name__ == \"__main__\": main() To use the script: Create a Python file, e.g. deploy.py, in the root of your project and paste the script contents into it. Set the environment variable PLATFORM_SH_SSH_DESTINATION to the user@host destination of your production environment. Check if there are any modifications you need to make. For example, you may be running other kinds of jobs than just Django management commands, so you may need to add other strings in addition to manage.py Run ./deploy.py when you’re ready to deploy. Commit the script so you don’t lose it. ",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/preventing-stuck-deployments-caused-by-long-running-cron-jobs/434",
        "relurl": "/t/preventing-stuck-deployments-caused-by-long-running-cron-jobs/434"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "cc40a421ad7b01871f84ddedf39d7787fd1eac04",
        "title": "Compiling a Drupal site with Tome",
        "description": "Goal To demonstrate how a “compile to static” tool, such as the https://www.drupal.org/project/tome , can be used on http://Platform.sh using multi-application configurations. Preparation This tutorial will start from a basic Drupal 8 Composer site. However, it will be easier to start from an empty Git repository. It can be done from a Drupal 8 template site but that requires moving a number of hidden files and is therefore more prone to error. You will need: A newly created empty site on http://Platform.sh. Git and SSH installed locally. Problems How can you serve two closely related sites off of the same project? How can static site generation work in production, rather than compiling locally or during build? Steps 1. Create an empty working directory On your local system, create an empty directory for your project and initialize it for Git. Eg: mkdir project cd project git init 2. Download Drupal 8 Clone the Drupal 8 http://Platform.sh template into your project, then remove the git repository from it. git clone https://github.com/platformsh-templates/drupal8.git drupal --depth=1 rm -rf drupal/.git That Git repository is what the Drupal 8 template in the new project wizard populates from. The above commands are essentially what the new project wizard does, although it does not use a subdirectory. 2. Move the .platform definition files to the repo root The .platform directory must be in the repository root, so move it there: mv drupal/.platform . The .platform.app.yaml file stays in the drupal directory so that the drupal directory becomes an application container itself. That will let you create another directory for the static site. 3. Add a network-storage service Open .platform/services.yaml in your favorite text editor and add the following: files: type: network-storage:1.0 disk: 512 That creates a new network-storage service named files. Files there can be shared between the two application containers. 4. Update Drupal mounts Open drupal/.platform.app.yaml in your favorite text editor and update the mounts section to be as follows: mounts: '/web/sites/default/files': source: local source_path: files '/tmp': source: local source_path: tmp '/private': source: local source_path: private '/.drush': source: local source_path: drush '/drush-backups': source: local source_path: drush-backups '/.console': source: local source_path: console '/content': source: local source_path: content '/config': source: local source_path: config '/static': source: service service: files source_path: static Most of those are simply converting the normal Drupal mounts to the new-style mount syntax. The most important are the final three paths. Tome will automatically run a config-export every time configuration is exported, so the /config directory must be writeable. It will also write out a JSON version of all content as it is created and edited to /content. The /static directory is where you’ll instruct Tome to write out the static version of the site. Of particular note, the /static mount uses the network storage service, not the local file system. That will make it available to other application containers. Note: Because these directories are writeable, they will not be tracked in Git. That includes configuration exports, which will not be deployable via Git in this setup. 5. Install Drupal Tome module Run: cd drupal composer require drupal/tome to add the Tome module. (It’s actually a suite of modules). You’ll enable it later once the code is deployed. Once that’s done return to the repository root directory: cd .. 6. Set Tome export location Modify the drupal/web/sites/default/settings.php file. Add the following line at the location of your choice: $settings['tome_static_directory'] = '../static/html'; That will set the Tome export directory to the /static/html directory, which is inside the writeable mount you created above. Because Tome completely removes the output directory before each rebuild it needs to be in a subdirectory of the mount. 7. Disable allowed-host checking In the drupal/web/sites/default/settings.platformsh.php file, look for a comment line that reads // Set trusted hosts based on Platform.sh routes. The code block below that configures the hosts that Drupal is allowed to requests for. Normally it is a security feature, however, on http://Platform.sh Professional the domain is already mangled by the router and will always be a “safe” domain. That makes this code block unnecessary. (It is included in the template only for http://Platform.sh Enterprise.) The Tome module’s export functionality, however, is incompatible with this code block as it will need to use the un-prefixed domain for the export. It is safe to simply remove/comment out this entire block. Alternatively, just comment out the last line that sets $settings['trusted_host_patterns']. 8. Add a static site application to serve files Create a new directory called static, and inside it create a .platform.app.yaml file. Give it the following contents: # Note that this is different than what the Drupal site is called. name: 'static' # The type here doesn't really matter since it's just a static site. type: 'php:7.3' # This must be set, but can be set at the minimum value. disk: 16 mounts: 'static': source: service service: files source_path: static web: locations: '/': root: 'static/html' index: ['index.html'] scripts: false allow: true That file is all that is needed to create a second application container. This container will be called static, and will mount the same network storage directory as the app container that holds the Drupal site. It will be empty aside from that mount, which is fine as the mount is where the files will be served from. Note that the web root is the static/html directory, not static itself, as Tome was configured above to write files to static/html. 9. Update routes Open .platform/routes.yaml in your editor. Update the route definitions to be as follows: \"https://drupal.{default}/\": type: upstream upstream: \"app:http\" cache: enabled: true # Base the cache on the session cookie and custom Drupal cookies. Ignore all other cookies. cookies: ['/^SS?ESS/', '/^Drupal.visitor/'] \"https://{default}/\": type: upstream upstream: \"static:http\" (You can also include a www redirect route if you want, but that is unnecessary.) This configuration creates two routes: The drupal. subdomain will be where the Drupal site lives, and it will run as any other Drupal site. The main domain (and/or www. subdomain if desired) will be served by the static application. Once the configuration is complete and you are happy with the result, you will most likely want to come back and add HTTP caching to the static route, including a default_ttl. 10. Commit and push Commit all of the files you just created to Git: git add . git commit -m \"Setting up Tome\" Now add a remote for your empty http://Platform.sh project. The Git URL can be found in your Management Console in the browser. The command will be something along the lines of: git remote add platform bzh2mp6iabike@git.eu-3.platform.sh:bzh2mp6iabike.git And then push all of the code to the platform remote: git push -u platform master If everything is setup correctly it will push and deploy a two-application cluster, containing the Drupal container, static container, and the MariaDB and Redis services used by Drupal itself. 11. Install Drupal and Tome Once deployment is finished it will show you the domain names that are served. Go to the drupal. domain in your browser. Complete the Drupal installation process as normal. Once Drupal is installed, select “Extend” in the menu and enable the “Tome” module. That will also enable several sub-modules. You don’t need to create any content at this point, but if you do it will make the demonstration of Tome’s functionality more interesting. 12. Export the site Go to /admin/config/tome/static/generate in your Drupal site. The generation form will ask you for the domain to generate for; give it the unprefixed domain (which could be a dev domain or your production domain, depending on whether or not you’ve gone live yet). Click “Submit”. Tome will generate a static version of your site as it is seen by an anonymous user. (Which means you must allow anonymous user to see the site.) The process takes anywhere from a few seconds to a few minutes depending on how much content you have. As the site was only just created it should be quite fast. Now go to the unprefixed version of your site (the domain without drupal.) in your favorite browser. You should see the anonymous version of your site, now served entirely as static files. Conclusion In this tutorial, you have seen how to: Create a multi-application project on http://Platform.sh. Use network-storage to share content between sites. Configure an application to serve static files. All of which can be applied to other configurations if needed. Note that in the case of Tome the generation process does delete files first, so it is recommended to configure caching at the router level for the static container to minimize any disruption to the site as it is being regenerated.",
        "text": "Goal To demonstrate how a “compile to static” tool, such as the https://www.drupal.org/project/tome , can be used on http://Platform.sh using multi-application configurations. Preparation This tutorial will start from a basic Drupal 8 Composer site. However, it will be easier to start from an empty Git repository. It can be done from a Drupal 8 template site but that requires moving a number of hidden files and is therefore more prone to error. You will need: A newly created empty site on http://Platform.sh. Git and SSH installed locally. Problems How can you serve two closely related sites off of the same project? How can static site generation work in production, rather than compiling locally or during build? Steps 1. Create an empty working directory On your local system, create an empty directory for your project and initialize it for Git. Eg: mkdir project cd project git init 2. Download Drupal 8 Clone the Drupal 8 http://Platform.sh template into your project, then remove the git repository from it. git clone https://github.com/platformsh-templates/drupal8.git drupal --depth=1 rm -rf drupal/.git That Git repository is what the Drupal 8 template in the new project wizard populates from. The above commands are essentially what the new project wizard does, although it does not use a subdirectory. 2. Move the .platform definition files to the repo root The .platform directory must be in the repository root, so move it there: mv drupal/.platform . The .platform.app.yaml file stays in the drupal directory so that the drupal directory becomes an application container itself. That will let you create another directory for the static site. 3. Add a network-storage service Open .platform/services.yaml in your favorite text editor and add the following: files: type: network-storage:1.0 disk: 512 That creates a new network-storage service named files. Files there can be shared between the two application containers. 4. Update Drupal mounts Open drupal/.platform.app.yaml in your favorite text editor and update the mounts section to be as follows: mounts: '/web/sites/default/files': source: local source_path: files '/tmp': source: local source_path: tmp '/private': source: local source_path: private '/.drush': source: local source_path: drush '/drush-backups': source: local source_path: drush-backups '/.console': source: local source_path: console '/content': source: local source_path: content '/config': source: local source_path: config '/static': source: service service: files source_path: static Most of those are simply converting the normal Drupal mounts to the new-style mount syntax. The most important are the final three paths. Tome will automatically run a config-export every time configuration is exported, so the /config directory must be writeable. It will also write out a JSON version of all content as it is created and edited to /content. The /static directory is where you’ll instruct Tome to write out the static version of the site. Of particular note, the /static mount uses the network storage service, not the local file system. That will make it available to other application containers. Note: Because these directories are writeable, they will not be tracked in Git. That includes configuration exports, which will not be deployable via Git in this setup. 5. Install Drupal Tome module Run: cd drupal composer require drupal/tome to add the Tome module. (It’s actually a suite of modules). You’ll enable it later once the code is deployed. Once that’s done return to the repository root directory: cd .. 6. Set Tome export location Modify the drupal/web/sites/default/settings.php file. Add the following line at the location of your choice: $settings['tome_static_directory'] = '../static/html'; That will set the Tome export directory to the /static/html directory, which is inside the writeable mount you created above. Because Tome completely removes the output directory before each rebuild it needs to be in a subdirectory of the mount. 7. Disable allowed-host checking In the drupal/web/sites/default/settings.platformsh.php file, look for a comment line that reads // Set trusted hosts based on Platform.sh routes. The code block below that configures the hosts that Drupal is allowed to requests for. Normally it is a security feature, however, on http://Platform.sh Professional the domain is already mangled by the router and will always be a “safe” domain. That makes this code block unnecessary. (It is included in the template only for http://Platform.sh Enterprise.) The Tome module’s export functionality, however, is incompatible with this code block as it will need to use the un-prefixed domain for the export. It is safe to simply remove/comment out this entire block. Alternatively, just comment out the last line that sets $settings['trusted_host_patterns']. 8. Add a static site application to serve files Create a new directory called static, and inside it create a .platform.app.yaml file. Give it the following contents: # Note that this is different than what the Drupal site is called. name: 'static' # The type here doesn't really matter since it's just a static site. type: 'php:7.3' # This must be set, but can be set at the minimum value. disk: 16 mounts: 'static': source: service service: files source_path: static web: locations: '/': root: 'static/html' index: ['index.html'] scripts: false allow: true That file is all that is needed to create a second application container. This container will be called static, and will mount the same network storage directory as the app container that holds the Drupal site. It will be empty aside from that mount, which is fine as the mount is where the files will be served from. Note that the web root is the static/html directory, not static itself, as Tome was configured above to write files to static/html. 9. Update routes Open .platform/routes.yaml in your editor. Update the route definitions to be as follows: \"https://drupal.{default}/\": type: upstream upstream: \"app:http\" cache: enabled: true # Base the cache on the session cookie and custom Drupal cookies. Ignore all other cookies. cookies: ['/^SS?ESS/', '/^Drupal.visitor/'] \"https://{default}/\": type: upstream upstream: \"static:http\" (You can also include a www redirect route if you want, but that is unnecessary.) This configuration creates two routes: The drupal. subdomain will be where the Drupal site lives, and it will run as any other Drupal site. The main domain (and/or www. subdomain if desired) will be served by the static application. Once the configuration is complete and you are happy with the result, you will most likely want to come back and add HTTP caching to the static route, including a default_ttl. 10. Commit and push Commit all of the files you just created to Git: git add . git commit -m \"Setting up Tome\" Now add a remote for your empty http://Platform.sh project. The Git URL can be found in your Management Console in the browser. The command will be something along the lines of: git remote add platform bzh2mp6iabike@git.eu-3.platform.sh:bzh2mp6iabike.git And then push all of the code to the platform remote: git push -u platform master If everything is setup correctly it will push and deploy a two-application cluster, containing the Drupal container, static container, and the MariaDB and Redis services used by Drupal itself. 11. Install Drupal and Tome Once deployment is finished it will show you the domain names that are served. Go to the drupal. domain in your browser. Complete the Drupal installation process as normal. Once Drupal is installed, select “Extend” in the menu and enable the “Tome” module. That will also enable several sub-modules. You don’t need to create any content at this point, but if you do it will make the demonstration of Tome’s functionality more interesting. 12. Export the site Go to /admin/config/tome/static/generate in your Drupal site. The generation form will ask you for the domain to generate for; give it the unprefixed domain (which could be a dev domain or your production domain, depending on whether or not you’ve gone live yet). Click “Submit”. Tome will generate a static version of your site as it is seen by an anonymous user. (Which means you must allow anonymous user to see the site.) The process takes anywhere from a few seconds to a few minutes depending on how much content you have. As the site was only just created it should be quite fast. Now go to the unprefixed version of your site (the domain without drupal.) in your favorite browser. You should see the anonymous version of your site, now served entirely as static files. Conclusion In this tutorial, you have seen how to: Create a multi-application project on http://Platform.sh. Use network-storage to share content between sites. Configure an application to serve static files. All of which can be applied to other configurations if needed. Note that in the case of Tome the generation process does delete files first, so it is recommended to configure caching at the router level for the static container to minimize any disruption to the site as it is being regenerated.",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/compiling-a-drupal-site-with-tome/363",
        "relurl": "/t/compiling-a-drupal-site-with-tome/363"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "69e493ce38a4fc00fe39aff1404bba2c741cc215",
        "title": "Installing Drupal and Mautic together in one project",
        "description": "Goal To set up a multi-app site with both Drupal 8 and Mautic. The process is also very similar for any other site that combines two templates. Preparation You will need: A new project with no code. You will need at least a Medium plan to go live, but you can start with a Dev plan for setup. You will need a Medium plan and a domain configured before the Drupal Mautic module can be configured, however. A local working Git client and Composer. Problems Drupal needs a Mautic site URL in order to connect to it. It will do so over HTTP, not through a backend connection. That means the domain name will be different on every branch. Generally you would only care about Mautic tracking on production anyway, so that is not a major issue. Steps 1. Download the Drupal 8 template The http://Platform.sh Drupal 8 template is available on https://github.com/platformsh-templates/drupal8. Clone it to your local computer, then remove the .git directory to reset the repository. # Download Drupal 8 git clone https://github.com/platformsh-templates/drupal8.git d8-mautic # Remove the git repository itself. cd d8-mautic rm -rf .git Resetting the repository is technically optional, but the existing history is useless for you and as you will be moving a lot of files around it’s easier to not deal with Git at this point. 2. Move Drupal to a subdirectory Make a new directory named drupal and move all of the existing files into it, except for the .platform subdirectory. Make sure to include the various other dot files. mkdir drupal mv * drupal mv .platform.app.yaml .env* .editorconfig .gitattributes .gitignore drupal 3. Add Mautic Download the https://github.com/platformsh-templates/mautic into the project directory and remove its .git repository as well. # Download Mautic $ git clone https://github.com/platformsh-templates/mautic.git # Remove the git repository itself. $ cd mautic $ rm -rf .git 4. Rename applications The two applications will need unique names, such as drupal and mautic. (You may use other names if you wish.) Change the name field in each application’s .platform.app.yaml file: # in drupal/.platform.app.yaml name: drupal # in mautic/.platform.app.yaml name: mautic Note: If you have an existing Drupal site you are modifying to include Mautic, do not rename that application. Doing so will result in data loss. 5. Update routes.yaml Update the .platform/routes.yaml file to supply different routes for each application. The exact configuration will vary depending on your preferred domains. For example, the following configuration will serve Drupal from www.YOURSITE.com and Mautic from mautic.YOURSITE.com: \"https://www.{default}/\": type: upstream upstream: \"drupal:http\" cache: enabled: true cookies: ['/^SS?ESS/', '/^Drupal.visitor/'] \"https://{default}/\": type: redirect to: \"https://www.{default}/\" \"https://mautic.{default}/\": type: upstream upstream: \"mautic:http\" cache: enabled: true 6. Merge the services.yaml files At this point, Drupal’s original services.yaml is in .platform/services.yaml and Mautic’s is in mautic/.platform/services.yaml. The second will not be used so its content should be merged into the main one, and then it can be removed. By default, Mautic and Drupal both use MariaDB; Drupal also uses Redis, and Mautic also uses RabbitMQ. If you wish to add additional services you may do so. The same MariaDB service can hold the database for both applications. A typical configuration for running both applications would looks like this: db: type: mariadb:10.4 disk: 2048 configuration: schemas: - drupal - mautic endpoints: drupal: default_schema: drupal privileges: drupal: admin mautic: default_schema: mautic privileges: mautic: admin cache: type: redis:5.0 queuerabbit: type: rabbitmq:3.7 disk: 256 That includes the cache service from Drupal, the queuerabbit service from Mautic, and a single MariaDB 10.4 server to serve both databases. It defines two databases, drupal and mautic, and then creates two separate endpoints of the same name that have full access to their respective databases only. Note: If you are adding Mautic to an existing Drupal site, you must name the Drupal database main and the Drupal endpoint mysql. Doing otherwise will result in data loss. Once that is done, remove the now-unused Mautic .platform directory. rm -rf mautic/.platform 7. Update each application’s relationships definition. In drupal/.platform.app.yaml, change the database relationship to point to the drupal endpoint: # in drupal/.platform.app.yaml relationships: database: 'db:drupal' ## Uncomment this line to enable Redis caching for Drupal. # redis: 'cache:redis' # in mautic/.platform.app.yaml relationships: database: \"db:mautic\" rabbitmqqueue: \"queuerabbit:rabbitmq\" 8. Reduce disk usage OR increase plan size The default templates for Drupal and Mautic, when combined, will ask for a total of about 6 GB of storage. By default plans on http://Platform.sh start with 5 GB. You may either increase the disk usage of your plan to a value higher than 6 GB, OR you can reduce the disk space requested by each application container. For the latter, change the disk key in both .platform.app.yaml files to 1024: disk: 1024 That will give each application 1 GB of disk space. Both applications will share the disk space used for the database (2 GB in the example above). 9. Add the Drupal Mautic module to the project Install the drupal/mautic module in the Drupal instance, using Composer: cd drupal composer require drupal/mautic Because of the way Composer works that will require downloading all dependencies, even though they will not be needed. That’s fine. When it is done go back to the project directory: cd .. 10. Commit and deploy Initialize a new Git repository and commit all of the files you’ve created to it. git init git add . git commit -m \"Add Drupal and Mautic.\" Next, set a Git remote for the empty http://Platform.sh project you have waiting for it, using the http://Platform.sh CLI. You can find the exact command to copy and paste in the project’s setup wizard. Then push your code to the new remote. platform project:set-remote YOUR_PROJECT_ID_HERE git push -u platform master The codebase will push to http://Platform.sh, and both applications will be built and deployed. 11. Install both applications Once the deploy is complete, run through the web installer for both applications. The environment URL for each one will be visible in the CLI output as well as in the web console. Consult the README.md file for each application for steps that should be taken post-install. They are not required for completing the integration but doing so will lead to a better experience for both applications. You may also do any additional configuration desired for both applications either now or afterward. 12. Enable and configure the Drupal module Once logged into Drupal as the site administrator, go to /admin/modules and enable the Mautic module. Once the module is enabled, go to the module’s configuration page at /admin/config/system/mautic. Check “Include Mautic Javascript Code”. For the Mautic URL, enter the domain name of your Mautic site followed by /mtc.js. If you are on a development plan, it will be something similar to: https://mautic.master-t6dnbai-aqatptktdkxmi.us-2.platformsh.site/mtc.js If you already have a domain name configured, it will be whatever the domain name is that Mautic is served from instead. Click “save configuration”. The setup is now complete. Browse to the Drupal home page and inspect the source code to locate the Mautic tracking Javascript code. Conclusion In this tutorial you have learned how to create a multi-application project on http://Platform.sh based on two separate application templates. You’ve also seen how to add a module to Drupal and configure it with Mautic. The same basic process applies to any other two-template project, although the specific names and services will of course vary depending on the applications. Example An example repository for this tutorial can be found https://github.com/platformsh-examples/d8-mautic . Note that it is not maintained and may include out of date versions by the time you read this.",
        "text": "Goal To set up a multi-app site with both Drupal 8 and Mautic. The process is also very similar for any other site that combines two templates. Preparation You will need: A new project with no code. You will need at least a Medium plan to go live, but you can start with a Dev plan for setup. You will need a Medium plan and a domain configured before the Drupal Mautic module can be configured, however. A local working Git client and Composer. Problems Drupal needs a Mautic site URL in order to connect to it. It will do so over HTTP, not through a backend connection. That means the domain name will be different on every branch. Generally you would only care about Mautic tracking on production anyway, so that is not a major issue. Steps 1. Download the Drupal 8 template The http://Platform.sh Drupal 8 template is available on https://github.com/platformsh-templates/drupal8. Clone it to your local computer, then remove the .git directory to reset the repository. # Download Drupal 8 git clone https://github.com/platformsh-templates/drupal8.git d8-mautic # Remove the git repository itself. cd d8-mautic rm -rf .git Resetting the repository is technically optional, but the existing history is useless for you and as you will be moving a lot of files around it’s easier to not deal with Git at this point. 2. Move Drupal to a subdirectory Make a new directory named drupal and move all of the existing files into it, except for the .platform subdirectory. Make sure to include the various other dot files. mkdir drupal mv * drupal mv .platform.app.yaml .env* .editorconfig .gitattributes .gitignore drupal 3. Add Mautic Download the https://github.com/platformsh-templates/mautic into the project directory and remove its .git repository as well. # Download Mautic $ git clone https://github.com/platformsh-templates/mautic.git # Remove the git repository itself. $ cd mautic $ rm -rf .git 4. Rename applications The two applications will need unique names, such as drupal and mautic. (You may use other names if you wish.) Change the name field in each application’s .platform.app.yaml file: # in drupal/.platform.app.yaml name: drupal # in mautic/.platform.app.yaml name: mautic Note: If you have an existing Drupal site you are modifying to include Mautic, do not rename that application. Doing so will result in data loss. 5. Update routes.yaml Update the .platform/routes.yaml file to supply different routes for each application. The exact configuration will vary depending on your preferred domains. For example, the following configuration will serve Drupal from www.YOURSITE.com and Mautic from mautic.YOURSITE.com: \"https://www.{default}/\": type: upstream upstream: \"drupal:http\" cache: enabled: true cookies: ['/^SS?ESS/', '/^Drupal.visitor/'] \"https://{default}/\": type: redirect to: \"https://www.{default}/\" \"https://mautic.{default}/\": type: upstream upstream: \"mautic:http\" cache: enabled: true 6. Merge the services.yaml files At this point, Drupal’s original services.yaml is in .platform/services.yaml and Mautic’s is in mautic/.platform/services.yaml. The second will not be used so its content should be merged into the main one, and then it can be removed. By default, Mautic and Drupal both use MariaDB; Drupal also uses Redis, and Mautic also uses RabbitMQ. If you wish to add additional services you may do so. The same MariaDB service can hold the database for both applications. A typical configuration for running both applications would looks like this: db: type: mariadb:10.4 disk: 2048 configuration: schemas: - drupal - mautic endpoints: drupal: default_schema: drupal privileges: drupal: admin mautic: default_schema: mautic privileges: mautic: admin cache: type: redis:5.0 queuerabbit: type: rabbitmq:3.7 disk: 256 That includes the cache service from Drupal, the queuerabbit service from Mautic, and a single MariaDB 10.4 server to serve both databases. It defines two databases, drupal and mautic, and then creates two separate endpoints of the same name that have full access to their respective databases only. Note: If you are adding Mautic to an existing Drupal site, you must name the Drupal database main and the Drupal endpoint mysql. Doing otherwise will result in data loss. Once that is done, remove the now-unused Mautic .platform directory. rm -rf mautic/.platform 7. Update each application’s relationships definition. In drupal/.platform.app.yaml, change the database relationship to point to the drupal endpoint: # in drupal/.platform.app.yaml relationships: database: 'db:drupal' ## Uncomment this line to enable Redis caching for Drupal. # redis: 'cache:redis' # in mautic/.platform.app.yaml relationships: database: \"db:mautic\" rabbitmqqueue: \"queuerabbit:rabbitmq\" 8. Reduce disk usage OR increase plan size The default templates for Drupal and Mautic, when combined, will ask for a total of about 6 GB of storage. By default plans on http://Platform.sh start with 5 GB. You may either increase the disk usage of your plan to a value higher than 6 GB, OR you can reduce the disk space requested by each application container. For the latter, change the disk key in both .platform.app.yaml files to 1024: disk: 1024 That will give each application 1 GB of disk space. Both applications will share the disk space used for the database (2 GB in the example above). 9. Add the Drupal Mautic module to the project Install the drupal/mautic module in the Drupal instance, using Composer: cd drupal composer require drupal/mautic Because of the way Composer works that will require downloading all dependencies, even though they will not be needed. That’s fine. When it is done go back to the project directory: cd .. 10. Commit and deploy Initialize a new Git repository and commit all of the files you’ve created to it. git init git add . git commit -m \"Add Drupal and Mautic.\" Next, set a Git remote for the empty http://Platform.sh project you have waiting for it, using the http://Platform.sh CLI. You can find the exact command to copy and paste in the project’s setup wizard. Then push your code to the new remote. platform project:set-remote YOUR_PROJECT_ID_HERE git push -u platform master The codebase will push to http://Platform.sh, and both applications will be built and deployed. 11. Install both applications Once the deploy is complete, run through the web installer for both applications. The environment URL for each one will be visible in the CLI output as well as in the web console. Consult the README.md file for each application for steps that should be taken post-install. They are not required for completing the integration but doing so will lead to a better experience for both applications. You may also do any additional configuration desired for both applications either now or afterward. 12. Enable and configure the Drupal module Once logged into Drupal as the site administrator, go to /admin/modules and enable the Mautic module. Once the module is enabled, go to the module’s configuration page at /admin/config/system/mautic. Check “Include Mautic Javascript Code”. For the Mautic URL, enter the domain name of your Mautic site followed by /mtc.js. If you are on a development plan, it will be something similar to: https://mautic.master-t6dnbai-aqatptktdkxmi.us-2.platformsh.site/mtc.js If you already have a domain name configured, it will be whatever the domain name is that Mautic is served from instead. Click “save configuration”. The setup is now complete. Browse to the Drupal home page and inspect the source code to locate the Mautic tracking Javascript code. Conclusion In this tutorial you have learned how to create a multi-application project on http://Platform.sh based on two separate application templates. You’ve also seen how to add a module to Drupal and configure it with Mautic. The same basic process applies to any other two-template project, although the specific names and services will of course vary depending on the applications. Example An example repository for this tutorial can be found https://github.com/platformsh-examples/d8-mautic . Note that it is not maintained and may include out of date versions by the time you read this.",
        "section": "Tutorials",
        "subsections": "Tutorials",
        "image": "",
        "url": "https://community.platform.sh//t/installing-drupal-and-mautic-together-in-one-project/412",
        "relurl": "/t/installing-drupal-and-mautic-together-in-one-project/412"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "81ba15cf84e22e2354d0ef5a8272e09e6d43c5fb",
        "title": "About the Activity scripts category",
        "description": "A place to share and get feedback on Activity scripts for http://Platform.sh. Activity scripts let you customize the http://Platform.sh workflow by responding to “activities,” events within the project life-cycle. See the https://docs.platform.sh/administration/integrations/activity-scripts.html for more details.",
        "text": "A place to share and get feedback on Activity scripts for http://Platform.sh. Activity scripts let you customize the http://Platform.sh workflow by responding to “activities,” events within the project life-cycle. See the https://docs.platform.sh/administration/integrations/activity-scripts.html for more details.",
        "section": "Activity scripts",
        "subsections": "Activity scripts",
        "image": "",
        "url": "https://community.platform.sh//t/about-the-activity-scripts-category/561",
        "relurl": "/t/about-the-activity-scripts-category/561"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "283b44e7e222d1c43318e7e265c3c7e29f2b2104",
        "title": "Script that creates a Github Status providing the URL of a new deployed environnement",
        "description": "Hello, Please find the script there to follow its update! https://github.com/Plopix/platformsh-activity-scripts/blob/master/github/env_url.js https://github.com/Plopix/platformsh-activity-scripts/blob/master/github/env_url.js \u0026\u0026 \u0026\u0026 https://github.com/Plopix/platformsh-activity-scripts/blob/master/github/env_url.js Don’t hesitate to give me your feedback. Thanks",
        "text": "Hello, Please find the script there to follow its update! https://github.com/Plopix/platformsh-activity-scripts/blob/master/github/env_url.js https://github.com/Plopix/platformsh-activity-scripts/blob/master/github/env_url.js \u0026\u0026 \u0026\u0026 https://github.com/Plopix/platformsh-activity-scripts/blob/master/github/env_url.js Don’t hesitate to give me your feedback. Thanks",
        "section": "Activity scripts",
        "subsections": "Activity scripts",
        "image": "",
        "url": "https://community.platform.sh//t/script-that-creates-a-github-status-providing-the-url-of-a-new-deployed-environnement/577",
        "relurl": "/t/script-that-creates-a-github-status-providing-the-url-of-a-new-deployed-environnement/577"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "aa6d18602b239e1a1b3833d9eaf18c0b02281eae",
        "title": "Script to check for deprecated or newer available runtimes and services versions",
        "description": "Overview Every time you redeploy an environment, the script will: check the versions of your project’s services and runtimes. compare those versions with the https://docs.platform.sh/registry/images/registry.json . send a Slack notification about the ones which you need to upgrade. https://community.platform.sh/uploads/default/a9bfce0a3251ab3f968bd419c322cdc9948e12c4 Colors Red: 1 or more service is deprecated. Orange: 1 or more service is not at the latest version. Green: All services are at the latest version. Script /** * Returns a key/value object containing all variables relevant for the activity. * * That includes project level variables, plus any variables visible for * the relevant environment for the activity, if any. * * Note that JSON-encoded values will show up as a string, and need to be * decoded with JSON.parse(). */ function variables() { var vars = {}; activity.payload.deployment.variables.forEach(function(variable) { vars[variable.name] = variable.value; }); return vars; } /** * Sends a color-coded formatted message to Slack. * * You must first configure a Platform.sh variable named \"SLACK_URL\". * That is the group and channel to which the message will be sent. * * To control what events it will run on, use the --events switch in * the Platform.sh CLI. * * @param {string} title * The title of the message block to send. * @param {string} message * The message body to send. */ function sendSlackMessage(title, message) { if ((new Date).getDay() === 5) { message += '\\r\\nCongratulations for deploying on a Friday! :calendar:'; } var body = { 'attachments': [{ 'title': title, 'text': message, 'color': color, }], }; var url = variables()['SLACK_URL']; if (!url) { throw new Error('You must define a SLACK_URL project variable.'); } var resp = fetch(url,{ method: 'POST', headers: { 'Content-Type': 'application/json', }, body: JSON.stringify(body), }); if (!resp.ok) { console.log('[LOG] Sending slack message failed: ' + resp.body.text()); } } /** * Compare the services and runtimes versions with the Platform.sh public registry. * * @param {json} services * The services/runtimes that your project is using within the activity.payload.deployment payload. * @param {json} registry * The Platform.sh registry available at: https://docs.platform.sh/registry/images/registry.json */ function compareVersions(services, registry) { var results = Object.keys(services).map(function(serviceName) { var service = services[serviceName]; var s = service.type.split(':'); var type = s[0]; var version = s[1]; var registryService = registry[type]; var name = registryService.name; if(!registryService) { return 'Unsupported '+ type; } var versions = registryService.versions; // Check for supported versions var indexOfSupported = versions.supported.indexOf(version); if(indexOfSupported !== -1) { var resp = 'Your ' + name + ' ' + version + ' is the most recent one.\\n'; if(indexOfSupported ",
        "text": "Overview Every time you redeploy an environment, the script will: check the versions of your project’s services and runtimes. compare those versions with the https://docs.platform.sh/registry/images/registry.json . send a Slack notification about the ones which you need to upgrade. https://community.platform.sh/uploads/default/a9bfce0a3251ab3f968bd419c322cdc9948e12c4 Colors Red: 1 or more service is deprecated. Orange: 1 or more service is not at the latest version. Green: All services are at the latest version. Script /** * Returns a key/value object containing all variables relevant for the activity. * * That includes project level variables, plus any variables visible for * the relevant environment for the activity, if any. * * Note that JSON-encoded values will show up as a string, and need to be * decoded with JSON.parse(). */ function variables() { var vars = {}; activity.payload.deployment.variables.forEach(function(variable) { vars[variable.name] = variable.value; }); return vars; } /** * Sends a color-coded formatted message to Slack. * * You must first configure a Platform.sh variable named \"SLACK_URL\". * That is the group and channel to which the message will be sent. * * To control what events it will run on, use the --events switch in * the Platform.sh CLI. * * @param {string} title * The title of the message block to send. * @param {string} message * The message body to send. */ function sendSlackMessage(title, message) { if ((new Date).getDay() === 5) { message += '\\r\\nCongratulations for deploying on a Friday! :calendar:'; } var body = { 'attachments': [{ 'title': title, 'text': message, 'color': color, }], }; var url = variables()['SLACK_URL']; if (!url) { throw new Error('You must define a SLACK_URL project variable.'); } var resp = fetch(url,{ method: 'POST', headers: { 'Content-Type': 'application/json', }, body: JSON.stringify(body), }); if (!resp.ok) { console.log('[LOG] Sending slack message failed: ' + resp.body.text()); } } /** * Compare the services and runtimes versions with the Platform.sh public registry. * * @param {json} services * The services/runtimes that your project is using within the activity.payload.deployment payload. * @param {json} registry * The Platform.sh registry available at: https://docs.platform.sh/registry/images/registry.json */ function compareVersions(services, registry) { var results = Object.keys(services).map(function(serviceName) { var service = services[serviceName]; var s = service.type.split(':'); var type = s[0]; var version = s[1]; var registryService = registry[type]; var name = registryService.name; if(!registryService) { return 'Unsupported '+ type; } var versions = registryService.versions; // Check for supported versions var indexOfSupported = versions.supported.indexOf(version); if(indexOfSupported !== -1) { var resp = 'Your ' + name + ' ' + version + ' is the most recent one.\\n'; if(indexOfSupported ",
        "section": "Activity scripts",
        "subsections": "Activity scripts",
        "image": "",
        "url": "https://community.platform.sh//t/script-to-check-for-deprecated-or-newer-available-runtimes-and-services-versions/575",
        "relurl": "/t/script-to-check-for-deprecated-or-newer-available-runtimes-and-services-versions/575"
    },
    {
        "site": "community",
        "source": "secondary",
        "rank": 4,
        "documentId": "b04dd04d473aba6383887776cbe077d9caaedfb9",
        "title": "Post activities to a Slack room",
        "description": "Here’s a simple script to post activities to Slack. You’ll need to create a SLACK_URL project variable, which you set to the value Slack gives you when setting up an integration on their end. You can customize the message format by changing the body variable. See https://api.slack.com/messaging/composing/layouts for the various options there. It doesn’t filter the type of activity or the environment. You can do that when configuring the script on http://Platform.sh directly. /** * Sends a color-coded formatted message to Slack. * * You must first configure a Platform.sh variable named \"SLACK_URL\". * That is the group and channel to which the message will be sent. * * To control what events it will run on, use the --events switch in * the Platform.sh CLI. * * @param {string} title * The title of the message block to send. * @param {string} message * The message body to send. */ function sendSlackMessage(title, message) { console.log((new Date).getDay()); if ((new Date).getDay() === 5) { message += \"\\r\\nOn a Friday! :calendar:\"; } var color = activity.result === 'success' ? '#66c000' : '#ff0000'; var body = { 'attachments': [{ \"title\": title, \"text\": message, \"color\": color, }], }; var url = variables()['SLACK_URL']; if (!url) { throw new Error('You must define a SLACK_URL project variable.'); } var resp = fetch(url,{ method: 'POST', headers: { 'Content-Type': 'application/json', }, body: JSON.stringify(body), }); if (!resp.ok) { console.log(\"Sending slack message failed: \" + resp.body.text()); } } function variables() { var vars = {}; activity.payload.deployment.variables.forEach(function(variable) { vars[variable.name] = variable.value; }); return vars; } sendSlackMessage(activity.text, activity.log); ",
        "text": "Here’s a simple script to post activities to Slack. You’ll need to create a SLACK_URL project variable, which you set to the value Slack gives you when setting up an integration on their end. You can customize the message format by changing the body variable. See https://api.slack.com/messaging/composing/layouts for the various options there. It doesn’t filter the type of activity or the environment. You can do that when configuring the script on http://Platform.sh directly. /** * Sends a color-coded formatted message to Slack. * * You must first configure a Platform.sh variable named \"SLACK_URL\". * That is the group and channel to which the message will be sent. * * To control what events it will run on, use the --events switch in * the Platform.sh CLI. * * @param {string} title * The title of the message block to send. * @param {string} message * The message body to send. */ function sendSlackMessage(title, message) { console.log((new Date).getDay()); if ((new Date).getDay() === 5) { message += \"\\r\\nOn a Friday! :calendar:\"; } var color = activity.result === 'success' ? '#66c000' : '#ff0000'; var body = { 'attachments': [{ \"title\": title, \"text\": message, \"color\": color, }], }; var url = variables()['SLACK_URL']; if (!url) { throw new Error('You must define a SLACK_URL project variable.'); } var resp = fetch(url,{ method: 'POST', headers: { 'Content-Type': 'application/json', }, body: JSON.stringify(body), }); if (!resp.ok) { console.log(\"Sending slack message failed: \" + resp.body.text()); } } function variables() { var vars = {}; activity.payload.deployment.variables.forEach(function(variable) { vars[variable.name] = variable.value; }); return vars; } sendSlackMessage(activity.text, activity.log); ",
        "section": "Activity scripts",
        "subsections": "Activity scripts",
        "image": "",
        "url": "https://community.platform.sh//t/post-activities-to-a-slack-room/574",
        "relurl": "/t/post-activities-to-a-slack-room/574"
    }
]